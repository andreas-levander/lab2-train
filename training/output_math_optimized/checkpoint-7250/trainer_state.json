{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9220985691573926,
  "eval_steps": 500,
  "global_step": 7250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0001271860095389507,
      "grad_norm": 25.235754013061523,
      "learning_rate": 4e-05,
      "loss": 7.0802,
      "step": 1
    },
    {
      "epoch": 0.0002543720190779014,
      "grad_norm": 28.79157829284668,
      "learning_rate": 8e-05,
      "loss": 8.5716,
      "step": 2
    },
    {
      "epoch": 0.00038155802861685216,
      "grad_norm": 20.99134063720703,
      "learning_rate": 0.00012,
      "loss": 6.7888,
      "step": 3
    },
    {
      "epoch": 0.0005087440381558028,
      "grad_norm": 24.660505294799805,
      "learning_rate": 0.00016,
      "loss": 6.9037,
      "step": 4
    },
    {
      "epoch": 0.0006359300476947536,
      "grad_norm": 14.697561264038086,
      "learning_rate": 0.0002,
      "loss": 4.6433,
      "step": 5
    },
    {
      "epoch": 0.0007631160572337043,
      "grad_norm": 7.546289920806885,
      "learning_rate": 0.00019997454499172713,
      "loss": 2.6816,
      "step": 6
    },
    {
      "epoch": 0.000890302066772655,
      "grad_norm": 3.9802160263061523,
      "learning_rate": 0.00019994908998345425,
      "loss": 2.0222,
      "step": 7
    },
    {
      "epoch": 0.0010174880763116057,
      "grad_norm": 3.8139662742614746,
      "learning_rate": 0.0001999236349751814,
      "loss": 1.994,
      "step": 8
    },
    {
      "epoch": 0.0011446740858505565,
      "grad_norm": 3.04292893409729,
      "learning_rate": 0.00019989817996690848,
      "loss": 1.5539,
      "step": 9
    },
    {
      "epoch": 0.0012718600953895071,
      "grad_norm": 3.067430019378662,
      "learning_rate": 0.00019987272495863563,
      "loss": 1.7329,
      "step": 10
    },
    {
      "epoch": 0.0013990461049284578,
      "grad_norm": 2.4566595554351807,
      "learning_rate": 0.00019984726995036275,
      "loss": 1.6818,
      "step": 11
    },
    {
      "epoch": 0.0015262321144674086,
      "grad_norm": 2.9944956302642822,
      "learning_rate": 0.00019982181494208987,
      "loss": 1.3103,
      "step": 12
    },
    {
      "epoch": 0.0016534181240063592,
      "grad_norm": 2.443159341812134,
      "learning_rate": 0.000199796359933817,
      "loss": 1.3692,
      "step": 13
    },
    {
      "epoch": 0.00178060413354531,
      "grad_norm": 2.9638233184814453,
      "learning_rate": 0.0001997709049255441,
      "loss": 1.3894,
      "step": 14
    },
    {
      "epoch": 0.0019077901430842607,
      "grad_norm": 3.1217575073242188,
      "learning_rate": 0.00019974544991727125,
      "loss": 1.4988,
      "step": 15
    },
    {
      "epoch": 0.0020349761526232114,
      "grad_norm": 3.8669605255126953,
      "learning_rate": 0.00019971999490899834,
      "loss": 1.0422,
      "step": 16
    },
    {
      "epoch": 0.002162162162162162,
      "grad_norm": 2.859541177749634,
      "learning_rate": 0.0001996945399007255,
      "loss": 1.1098,
      "step": 17
    },
    {
      "epoch": 0.002289348171701113,
      "grad_norm": 3.021761894226074,
      "learning_rate": 0.0001996690848924526,
      "loss": 1.4484,
      "step": 18
    },
    {
      "epoch": 0.0024165341812400635,
      "grad_norm": 2.2747395038604736,
      "learning_rate": 0.00019964362988417973,
      "loss": 1.2449,
      "step": 19
    },
    {
      "epoch": 0.0025437201907790143,
      "grad_norm": 2.334585428237915,
      "learning_rate": 0.00019961817487590684,
      "loss": 0.9831,
      "step": 20
    },
    {
      "epoch": 0.002670906200317965,
      "grad_norm": 2.0449867248535156,
      "learning_rate": 0.00019959271986763396,
      "loss": 1.351,
      "step": 21
    },
    {
      "epoch": 0.0027980922098569156,
      "grad_norm": 3.1305184364318848,
      "learning_rate": 0.00019956726485936108,
      "loss": 1.681,
      "step": 22
    },
    {
      "epoch": 0.0029252782193958664,
      "grad_norm": 2.5517611503601074,
      "learning_rate": 0.0001995418098510882,
      "loss": 1.2525,
      "step": 23
    },
    {
      "epoch": 0.0030524642289348172,
      "grad_norm": 3.4207401275634766,
      "learning_rate": 0.00019951635484281532,
      "loss": 1.7575,
      "step": 24
    },
    {
      "epoch": 0.003179650238473768,
      "grad_norm": 2.374241352081299,
      "learning_rate": 0.00019949089983454247,
      "loss": 1.1743,
      "step": 25
    },
    {
      "epoch": 0.0033068362480127185,
      "grad_norm": 2.383052110671997,
      "learning_rate": 0.00019946544482626958,
      "loss": 1.1609,
      "step": 26
    },
    {
      "epoch": 0.0034340222575516693,
      "grad_norm": 3.087226152420044,
      "learning_rate": 0.0001994399898179967,
      "loss": 1.1893,
      "step": 27
    },
    {
      "epoch": 0.00356120826709062,
      "grad_norm": 3.1754212379455566,
      "learning_rate": 0.00019941453480972382,
      "loss": 1.4076,
      "step": 28
    },
    {
      "epoch": 0.0036883942766295706,
      "grad_norm": 2.194232940673828,
      "learning_rate": 0.00019938907980145094,
      "loss": 1.0239,
      "step": 29
    },
    {
      "epoch": 0.0038155802861685214,
      "grad_norm": 2.9823575019836426,
      "learning_rate": 0.00019936362479317806,
      "loss": 0.9854,
      "step": 30
    },
    {
      "epoch": 0.003942766295707472,
      "grad_norm": 2.1095125675201416,
      "learning_rate": 0.00019933816978490518,
      "loss": 1.0063,
      "step": 31
    },
    {
      "epoch": 0.004069952305246423,
      "grad_norm": 1.9620110988616943,
      "learning_rate": 0.00019931271477663232,
      "loss": 0.998,
      "step": 32
    },
    {
      "epoch": 0.0041971383147853735,
      "grad_norm": 2.6606063842773438,
      "learning_rate": 0.00019928725976835942,
      "loss": 1.0492,
      "step": 33
    },
    {
      "epoch": 0.004324324324324324,
      "grad_norm": 2.5173909664154053,
      "learning_rate": 0.00019926180476008656,
      "loss": 1.0676,
      "step": 34
    },
    {
      "epoch": 0.004451510333863275,
      "grad_norm": 3.12283992767334,
      "learning_rate": 0.00019923634975181368,
      "loss": 0.9998,
      "step": 35
    },
    {
      "epoch": 0.004578696343402226,
      "grad_norm": 3.274404525756836,
      "learning_rate": 0.0001992108947435408,
      "loss": 0.9366,
      "step": 36
    },
    {
      "epoch": 0.004705882352941176,
      "grad_norm": 2.5381524562835693,
      "learning_rate": 0.00019918543973526794,
      "loss": 1.1184,
      "step": 37
    },
    {
      "epoch": 0.004833068362480127,
      "grad_norm": 3.310621500015259,
      "learning_rate": 0.00019915998472699504,
      "loss": 1.3034,
      "step": 38
    },
    {
      "epoch": 0.004960254372019078,
      "grad_norm": 2.1390247344970703,
      "learning_rate": 0.00019913452971872218,
      "loss": 1.0441,
      "step": 39
    },
    {
      "epoch": 0.005087440381558029,
      "grad_norm": 2.4762814044952393,
      "learning_rate": 0.00019910907471044927,
      "loss": 1.2428,
      "step": 40
    },
    {
      "epoch": 0.0052146263910969794,
      "grad_norm": 2.2829694747924805,
      "learning_rate": 0.00019908361970217642,
      "loss": 1.15,
      "step": 41
    },
    {
      "epoch": 0.00534181240063593,
      "grad_norm": 2.243908405303955,
      "learning_rate": 0.00019905816469390354,
      "loss": 1.2312,
      "step": 42
    },
    {
      "epoch": 0.005468998410174881,
      "grad_norm": 1.5852503776550293,
      "learning_rate": 0.00019903270968563066,
      "loss": 1.1216,
      "step": 43
    },
    {
      "epoch": 0.005596184419713831,
      "grad_norm": 1.8144475221633911,
      "learning_rate": 0.0001990072546773578,
      "loss": 1.0764,
      "step": 44
    },
    {
      "epoch": 0.005723370429252782,
      "grad_norm": 2.030712366104126,
      "learning_rate": 0.0001989817996690849,
      "loss": 1.1703,
      "step": 45
    },
    {
      "epoch": 0.005850556438791733,
      "grad_norm": 1.7478528022766113,
      "learning_rate": 0.00019895634466081204,
      "loss": 0.797,
      "step": 46
    },
    {
      "epoch": 0.005977742448330684,
      "grad_norm": 1.8574844598770142,
      "learning_rate": 0.00019893088965253913,
      "loss": 1.2188,
      "step": 47
    },
    {
      "epoch": 0.0061049284578696345,
      "grad_norm": 1.7076733112335205,
      "learning_rate": 0.00019890543464426628,
      "loss": 1.1633,
      "step": 48
    },
    {
      "epoch": 0.006232114467408585,
      "grad_norm": 1.7235664129257202,
      "learning_rate": 0.0001988799796359934,
      "loss": 1.1283,
      "step": 49
    },
    {
      "epoch": 0.006359300476947536,
      "grad_norm": 2.223548173904419,
      "learning_rate": 0.00019885452462772052,
      "loss": 1.4148,
      "step": 50
    },
    {
      "epoch": 0.006486486486486486,
      "grad_norm": 2.1170237064361572,
      "learning_rate": 0.00019882906961944763,
      "loss": 1.0244,
      "step": 51
    },
    {
      "epoch": 0.006613672496025437,
      "grad_norm": 2.3120856285095215,
      "learning_rate": 0.00019880361461117475,
      "loss": 1.005,
      "step": 52
    },
    {
      "epoch": 0.006740858505564388,
      "grad_norm": 2.514014720916748,
      "learning_rate": 0.00019877815960290187,
      "loss": 1.0218,
      "step": 53
    },
    {
      "epoch": 0.006868044515103339,
      "grad_norm": 2.3965229988098145,
      "learning_rate": 0.000198752704594629,
      "loss": 1.1115,
      "step": 54
    },
    {
      "epoch": 0.0069952305246422895,
      "grad_norm": 2.648559093475342,
      "learning_rate": 0.00019872724958635614,
      "loss": 1.3386,
      "step": 55
    },
    {
      "epoch": 0.00712241653418124,
      "grad_norm": 2.342863082885742,
      "learning_rate": 0.00019870179457808326,
      "loss": 1.2255,
      "step": 56
    },
    {
      "epoch": 0.00724960254372019,
      "grad_norm": 2.1712043285369873,
      "learning_rate": 0.00019867633956981037,
      "loss": 1.4452,
      "step": 57
    },
    {
      "epoch": 0.007376788553259141,
      "grad_norm": 1.9689851999282837,
      "learning_rate": 0.0001986508845615375,
      "loss": 0.9605,
      "step": 58
    },
    {
      "epoch": 0.007503974562798092,
      "grad_norm": 3.316723585128784,
      "learning_rate": 0.0001986254295532646,
      "loss": 1.173,
      "step": 59
    },
    {
      "epoch": 0.007631160572337043,
      "grad_norm": 2.070754051208496,
      "learning_rate": 0.00019859997454499173,
      "loss": 1.0775,
      "step": 60
    },
    {
      "epoch": 0.007758346581875994,
      "grad_norm": 1.984117865562439,
      "learning_rate": 0.00019857451953671885,
      "loss": 0.9035,
      "step": 61
    },
    {
      "epoch": 0.007885532591414944,
      "grad_norm": 1.6782841682434082,
      "learning_rate": 0.00019854906452844597,
      "loss": 0.865,
      "step": 62
    },
    {
      "epoch": 0.008012718600953895,
      "grad_norm": 1.7270151376724243,
      "learning_rate": 0.0001985236095201731,
      "loss": 0.891,
      "step": 63
    },
    {
      "epoch": 0.008139904610492845,
      "grad_norm": 2.6706109046936035,
      "learning_rate": 0.00019849815451190023,
      "loss": 1.2712,
      "step": 64
    },
    {
      "epoch": 0.008267090620031796,
      "grad_norm": 1.9226136207580566,
      "learning_rate": 0.00019847269950362735,
      "loss": 0.8699,
      "step": 65
    },
    {
      "epoch": 0.008394276629570747,
      "grad_norm": 2.182175636291504,
      "learning_rate": 0.00019844724449535447,
      "loss": 1.0324,
      "step": 66
    },
    {
      "epoch": 0.008521462639109698,
      "grad_norm": 2.09533429145813,
      "learning_rate": 0.0001984217894870816,
      "loss": 1.0046,
      "step": 67
    },
    {
      "epoch": 0.008648648648648649,
      "grad_norm": 2.436490058898926,
      "learning_rate": 0.00019839633447880873,
      "loss": 1.2963,
      "step": 68
    },
    {
      "epoch": 0.0087758346581876,
      "grad_norm": 2.3671579360961914,
      "learning_rate": 0.00019837087947053583,
      "loss": 1.1116,
      "step": 69
    },
    {
      "epoch": 0.00890302066772655,
      "grad_norm": 2.0921521186828613,
      "learning_rate": 0.00019834542446226297,
      "loss": 0.7508,
      "step": 70
    },
    {
      "epoch": 0.009030206677265501,
      "grad_norm": 2.0404553413391113,
      "learning_rate": 0.00019831996945399006,
      "loss": 0.9673,
      "step": 71
    },
    {
      "epoch": 0.009157392686804452,
      "grad_norm": 2.462395191192627,
      "learning_rate": 0.0001982945144457172,
      "loss": 1.1717,
      "step": 72
    },
    {
      "epoch": 0.009284578696343403,
      "grad_norm": 2.0672833919525146,
      "learning_rate": 0.00019826905943744433,
      "loss": 0.9791,
      "step": 73
    },
    {
      "epoch": 0.009411764705882352,
      "grad_norm": 2.1465139389038086,
      "learning_rate": 0.00019824360442917145,
      "loss": 0.8326,
      "step": 74
    },
    {
      "epoch": 0.009538950715421303,
      "grad_norm": 2.383054733276367,
      "learning_rate": 0.0001982181494208986,
      "loss": 1.2331,
      "step": 75
    },
    {
      "epoch": 0.009666136724960254,
      "grad_norm": 1.1486345529556274,
      "learning_rate": 0.00019819269441262568,
      "loss": 0.668,
      "step": 76
    },
    {
      "epoch": 0.009793322734499205,
      "grad_norm": 2.4666972160339355,
      "learning_rate": 0.00019816723940435283,
      "loss": 1.2655,
      "step": 77
    },
    {
      "epoch": 0.009920508744038155,
      "grad_norm": 1.8159973621368408,
      "learning_rate": 0.00019814178439607992,
      "loss": 1.0401,
      "step": 78
    },
    {
      "epoch": 0.010047694753577106,
      "grad_norm": 1.8714649677276611,
      "learning_rate": 0.00019811632938780707,
      "loss": 1.2172,
      "step": 79
    },
    {
      "epoch": 0.010174880763116057,
      "grad_norm": 1.697888731956482,
      "learning_rate": 0.00019809087437953419,
      "loss": 1.2072,
      "step": 80
    },
    {
      "epoch": 0.010302066772655008,
      "grad_norm": 1.8971234560012817,
      "learning_rate": 0.0001980654193712613,
      "loss": 1.1903,
      "step": 81
    },
    {
      "epoch": 0.010429252782193959,
      "grad_norm": 1.5984665155410767,
      "learning_rate": 0.00019803996436298842,
      "loss": 1.0432,
      "step": 82
    },
    {
      "epoch": 0.01055643879173291,
      "grad_norm": 1.871292233467102,
      "learning_rate": 0.00019801450935471554,
      "loss": 1.1997,
      "step": 83
    },
    {
      "epoch": 0.01068362480127186,
      "grad_norm": 1.8634264469146729,
      "learning_rate": 0.0001979890543464427,
      "loss": 1.2353,
      "step": 84
    },
    {
      "epoch": 0.010810810810810811,
      "grad_norm": 1.576752781867981,
      "learning_rate": 0.00019796359933816978,
      "loss": 0.8546,
      "step": 85
    },
    {
      "epoch": 0.010937996820349762,
      "grad_norm": 1.8224403858184814,
      "learning_rate": 0.00019793814432989693,
      "loss": 1.0361,
      "step": 86
    },
    {
      "epoch": 0.011065182829888713,
      "grad_norm": 1.6713811159133911,
      "learning_rate": 0.00019791268932162404,
      "loss": 0.9457,
      "step": 87
    },
    {
      "epoch": 0.011192368839427662,
      "grad_norm": 1.9545059204101562,
      "learning_rate": 0.00019788723431335116,
      "loss": 1.2544,
      "step": 88
    },
    {
      "epoch": 0.011319554848966613,
      "grad_norm": 2.0525095462799072,
      "learning_rate": 0.00019786177930507828,
      "loss": 1.1013,
      "step": 89
    },
    {
      "epoch": 0.011446740858505564,
      "grad_norm": 2.2569048404693604,
      "learning_rate": 0.0001978363242968054,
      "loss": 1.3271,
      "step": 90
    },
    {
      "epoch": 0.011573926868044515,
      "grad_norm": 1.6967509984970093,
      "learning_rate": 0.00019781086928853252,
      "loss": 1.1328,
      "step": 91
    },
    {
      "epoch": 0.011701112877583466,
      "grad_norm": 2.0318360328674316,
      "learning_rate": 0.00019778541428025964,
      "loss": 1.2638,
      "step": 92
    },
    {
      "epoch": 0.011828298887122416,
      "grad_norm": 1.6559040546417236,
      "learning_rate": 0.00019775995927198678,
      "loss": 1.1077,
      "step": 93
    },
    {
      "epoch": 0.011955484896661367,
      "grad_norm": 2.503852128982544,
      "learning_rate": 0.0001977345042637139,
      "loss": 1.0932,
      "step": 94
    },
    {
      "epoch": 0.012082670906200318,
      "grad_norm": 1.6437054872512817,
      "learning_rate": 0.00019770904925544102,
      "loss": 0.7558,
      "step": 95
    },
    {
      "epoch": 0.012209856915739269,
      "grad_norm": 3.0534191131591797,
      "learning_rate": 0.00019768359424716814,
      "loss": 1.1675,
      "step": 96
    },
    {
      "epoch": 0.01233704292527822,
      "grad_norm": 3.1349856853485107,
      "learning_rate": 0.00019765813923889526,
      "loss": 1.1917,
      "step": 97
    },
    {
      "epoch": 0.01246422893481717,
      "grad_norm": 1.7043015956878662,
      "learning_rate": 0.00019763268423062238,
      "loss": 0.9756,
      "step": 98
    },
    {
      "epoch": 0.012591414944356122,
      "grad_norm": 1.994836449623108,
      "learning_rate": 0.00019760722922234952,
      "loss": 1.1376,
      "step": 99
    },
    {
      "epoch": 0.012718600953895072,
      "grad_norm": 2.018413543701172,
      "learning_rate": 0.00019758177421407662,
      "loss": 1.1584,
      "step": 100
    },
    {
      "epoch": 0.012845786963434021,
      "grad_norm": 1.8145164251327515,
      "learning_rate": 0.00019755631920580376,
      "loss": 1.1107,
      "step": 101
    },
    {
      "epoch": 0.012972972972972972,
      "grad_norm": 1.8123723268508911,
      "learning_rate": 0.00019753086419753085,
      "loss": 0.9102,
      "step": 102
    },
    {
      "epoch": 0.013100158982511923,
      "grad_norm": 1.821948766708374,
      "learning_rate": 0.000197505409189258,
      "loss": 1.2646,
      "step": 103
    },
    {
      "epoch": 0.013227344992050874,
      "grad_norm": 1.5347867012023926,
      "learning_rate": 0.00019747995418098512,
      "loss": 0.78,
      "step": 104
    },
    {
      "epoch": 0.013354531001589825,
      "grad_norm": 1.6919152736663818,
      "learning_rate": 0.00019745449917271224,
      "loss": 0.909,
      "step": 105
    },
    {
      "epoch": 0.013481717011128776,
      "grad_norm": 1.2162379026412964,
      "learning_rate": 0.00019742904416443938,
      "loss": 1.0237,
      "step": 106
    },
    {
      "epoch": 0.013608903020667727,
      "grad_norm": 1.4143611192703247,
      "learning_rate": 0.00019740358915616647,
      "loss": 0.8677,
      "step": 107
    },
    {
      "epoch": 0.013736089030206677,
      "grad_norm": 1.6075633764266968,
      "learning_rate": 0.00019737813414789362,
      "loss": 1.2259,
      "step": 108
    },
    {
      "epoch": 0.013863275039745628,
      "grad_norm": 2.2036256790161133,
      "learning_rate": 0.0001973526791396207,
      "loss": 1.0964,
      "step": 109
    },
    {
      "epoch": 0.013990461049284579,
      "grad_norm": 2.024566411972046,
      "learning_rate": 0.00019732722413134786,
      "loss": 1.0026,
      "step": 110
    },
    {
      "epoch": 0.01411764705882353,
      "grad_norm": 2.8184683322906494,
      "learning_rate": 0.00019730176912307498,
      "loss": 1.0078,
      "step": 111
    },
    {
      "epoch": 0.01424483306836248,
      "grad_norm": 2.406609296798706,
      "learning_rate": 0.0001972763141148021,
      "loss": 1.0855,
      "step": 112
    },
    {
      "epoch": 0.014372019077901432,
      "grad_norm": 2.0357847213745117,
      "learning_rate": 0.00019725085910652924,
      "loss": 1.3743,
      "step": 113
    },
    {
      "epoch": 0.01449920508744038,
      "grad_norm": 2.3164408206939697,
      "learning_rate": 0.00019722540409825633,
      "loss": 0.9952,
      "step": 114
    },
    {
      "epoch": 0.014626391096979332,
      "grad_norm": 1.6401392221450806,
      "learning_rate": 0.00019719994908998348,
      "loss": 0.9736,
      "step": 115
    },
    {
      "epoch": 0.014753577106518282,
      "grad_norm": 1.5828107595443726,
      "learning_rate": 0.00019717449408171057,
      "loss": 1.056,
      "step": 116
    },
    {
      "epoch": 0.014880763116057233,
      "grad_norm": 2.4050116539001465,
      "learning_rate": 0.00019714903907343771,
      "loss": 1.0319,
      "step": 117
    },
    {
      "epoch": 0.015007949125596184,
      "grad_norm": 2.124842643737793,
      "learning_rate": 0.00019712358406516483,
      "loss": 0.9226,
      "step": 118
    },
    {
      "epoch": 0.015135135135135135,
      "grad_norm": 1.7928118705749512,
      "learning_rate": 0.00019709812905689195,
      "loss": 0.8893,
      "step": 119
    },
    {
      "epoch": 0.015262321144674086,
      "grad_norm": 1.7435917854309082,
      "learning_rate": 0.00019707267404861907,
      "loss": 1.0539,
      "step": 120
    },
    {
      "epoch": 0.015389507154213037,
      "grad_norm": 1.9259151220321655,
      "learning_rate": 0.0001970472190403462,
      "loss": 1.1473,
      "step": 121
    },
    {
      "epoch": 0.015516693163751987,
      "grad_norm": 1.8309040069580078,
      "learning_rate": 0.00019702176403207334,
      "loss": 0.8371,
      "step": 122
    },
    {
      "epoch": 0.015643879173290937,
      "grad_norm": 1.873215913772583,
      "learning_rate": 0.00019699630902380043,
      "loss": 0.9186,
      "step": 123
    },
    {
      "epoch": 0.015771065182829887,
      "grad_norm": 2.0310041904449463,
      "learning_rate": 0.00019697085401552757,
      "loss": 0.9868,
      "step": 124
    },
    {
      "epoch": 0.01589825119236884,
      "grad_norm": 1.8805608749389648,
      "learning_rate": 0.0001969453990072547,
      "loss": 0.8578,
      "step": 125
    },
    {
      "epoch": 0.01602543720190779,
      "grad_norm": 2.7396833896636963,
      "learning_rate": 0.0001969199439989818,
      "loss": 1.1046,
      "step": 126
    },
    {
      "epoch": 0.01615262321144674,
      "grad_norm": 2.33272385597229,
      "learning_rate": 0.00019689448899070893,
      "loss": 0.9848,
      "step": 127
    },
    {
      "epoch": 0.01627980922098569,
      "grad_norm": 1.9833509922027588,
      "learning_rate": 0.00019686903398243605,
      "loss": 1.1582,
      "step": 128
    },
    {
      "epoch": 0.01640699523052464,
      "grad_norm": 1.8234416246414185,
      "learning_rate": 0.00019684357897416317,
      "loss": 1.0284,
      "step": 129
    },
    {
      "epoch": 0.016534181240063592,
      "grad_norm": 3.96686053276062,
      "learning_rate": 0.0001968181239658903,
      "loss": 1.3945,
      "step": 130
    },
    {
      "epoch": 0.016661367249602543,
      "grad_norm": 2.5125246047973633,
      "learning_rate": 0.0001967926689576174,
      "loss": 1.2776,
      "step": 131
    },
    {
      "epoch": 0.016788553259141494,
      "grad_norm": 1.606093406677246,
      "learning_rate": 0.00019676721394934455,
      "loss": 0.9015,
      "step": 132
    },
    {
      "epoch": 0.016915739268680445,
      "grad_norm": 1.5892367362976074,
      "learning_rate": 0.00019674175894107167,
      "loss": 0.7835,
      "step": 133
    },
    {
      "epoch": 0.017042925278219396,
      "grad_norm": 1.7953044176101685,
      "learning_rate": 0.0001967163039327988,
      "loss": 1.2915,
      "step": 134
    },
    {
      "epoch": 0.017170111287758347,
      "grad_norm": 2.1180992126464844,
      "learning_rate": 0.0001966908489245259,
      "loss": 1.3756,
      "step": 135
    },
    {
      "epoch": 0.017297297297297298,
      "grad_norm": 1.6130623817443848,
      "learning_rate": 0.00019666539391625303,
      "loss": 0.9167,
      "step": 136
    },
    {
      "epoch": 0.01742448330683625,
      "grad_norm": 1.5534881353378296,
      "learning_rate": 0.00019663993890798017,
      "loss": 0.726,
      "step": 137
    },
    {
      "epoch": 0.0175516693163752,
      "grad_norm": 2.1536757946014404,
      "learning_rate": 0.00019661448389970726,
      "loss": 1.0934,
      "step": 138
    },
    {
      "epoch": 0.01767885532591415,
      "grad_norm": 2.7353382110595703,
      "learning_rate": 0.0001965890288914344,
      "loss": 1.041,
      "step": 139
    },
    {
      "epoch": 0.0178060413354531,
      "grad_norm": 1.2596911191940308,
      "learning_rate": 0.0001965635738831615,
      "loss": 0.8382,
      "step": 140
    },
    {
      "epoch": 0.017933227344992052,
      "grad_norm": 2.41678786277771,
      "learning_rate": 0.00019653811887488865,
      "loss": 1.1354,
      "step": 141
    },
    {
      "epoch": 0.018060413354531003,
      "grad_norm": 1.7425823211669922,
      "learning_rate": 0.00019651266386661576,
      "loss": 0.7669,
      "step": 142
    },
    {
      "epoch": 0.018187599364069953,
      "grad_norm": 1.8027974367141724,
      "learning_rate": 0.00019648720885834288,
      "loss": 1.0201,
      "step": 143
    },
    {
      "epoch": 0.018314785373608904,
      "grad_norm": 2.4364988803863525,
      "learning_rate": 0.00019646175385007003,
      "loss": 1.1079,
      "step": 144
    },
    {
      "epoch": 0.018441971383147855,
      "grad_norm": 2.338409900665283,
      "learning_rate": 0.00019643629884179712,
      "loss": 0.9641,
      "step": 145
    },
    {
      "epoch": 0.018569157392686806,
      "grad_norm": 2.822993040084839,
      "learning_rate": 0.00019641084383352427,
      "loss": 0.8944,
      "step": 146
    },
    {
      "epoch": 0.018696343402225757,
      "grad_norm": 2.0072007179260254,
      "learning_rate": 0.00019638538882525136,
      "loss": 0.9069,
      "step": 147
    },
    {
      "epoch": 0.018823529411764704,
      "grad_norm": 1.965710163116455,
      "learning_rate": 0.0001963599338169785,
      "loss": 0.882,
      "step": 148
    },
    {
      "epoch": 0.018950715421303655,
      "grad_norm": 1.6533384323120117,
      "learning_rate": 0.00019633447880870562,
      "loss": 0.9669,
      "step": 149
    },
    {
      "epoch": 0.019077901430842606,
      "grad_norm": 2.190452814102173,
      "learning_rate": 0.00019630902380043274,
      "loss": 1.1155,
      "step": 150
    },
    {
      "epoch": 0.019205087440381557,
      "grad_norm": 1.7368247509002686,
      "learning_rate": 0.0001962835687921599,
      "loss": 0.8138,
      "step": 151
    },
    {
      "epoch": 0.019332273449920508,
      "grad_norm": 2.165379762649536,
      "learning_rate": 0.00019625811378388698,
      "loss": 0.9214,
      "step": 152
    },
    {
      "epoch": 0.01945945945945946,
      "grad_norm": 2.6424901485443115,
      "learning_rate": 0.00019623265877561413,
      "loss": 1.3618,
      "step": 153
    },
    {
      "epoch": 0.01958664546899841,
      "grad_norm": 2.053947687149048,
      "learning_rate": 0.00019620720376734122,
      "loss": 0.9898,
      "step": 154
    },
    {
      "epoch": 0.01971383147853736,
      "grad_norm": 2.135862112045288,
      "learning_rate": 0.00019618174875906836,
      "loss": 1.1553,
      "step": 155
    },
    {
      "epoch": 0.01984101748807631,
      "grad_norm": 1.6319019794464111,
      "learning_rate": 0.00019615629375079548,
      "loss": 0.7953,
      "step": 156
    },
    {
      "epoch": 0.019968203497615262,
      "grad_norm": 1.7599250078201294,
      "learning_rate": 0.0001961308387425226,
      "loss": 1.2136,
      "step": 157
    },
    {
      "epoch": 0.020095389507154213,
      "grad_norm": 2.263983726501465,
      "learning_rate": 0.00019610538373424972,
      "loss": 0.9194,
      "step": 158
    },
    {
      "epoch": 0.020222575516693164,
      "grad_norm": 1.5196160078048706,
      "learning_rate": 0.00019607992872597684,
      "loss": 1.0861,
      "step": 159
    },
    {
      "epoch": 0.020349761526232114,
      "grad_norm": 1.8958760499954224,
      "learning_rate": 0.00019605447371770396,
      "loss": 1.1455,
      "step": 160
    },
    {
      "epoch": 0.020476947535771065,
      "grad_norm": 1.422534704208374,
      "learning_rate": 0.0001960290187094311,
      "loss": 1.0969,
      "step": 161
    },
    {
      "epoch": 0.020604133545310016,
      "grad_norm": 1.7516024112701416,
      "learning_rate": 0.00019600356370115822,
      "loss": 1.0818,
      "step": 162
    },
    {
      "epoch": 0.020731319554848967,
      "grad_norm": 2.0420165061950684,
      "learning_rate": 0.00019597810869288534,
      "loss": 0.8924,
      "step": 163
    },
    {
      "epoch": 0.020858505564387918,
      "grad_norm": 1.7692776918411255,
      "learning_rate": 0.00019595265368461246,
      "loss": 0.979,
      "step": 164
    },
    {
      "epoch": 0.02098569157392687,
      "grad_norm": 1.483707308769226,
      "learning_rate": 0.00019592719867633958,
      "loss": 1.0474,
      "step": 165
    },
    {
      "epoch": 0.02111287758346582,
      "grad_norm": 2.1403307914733887,
      "learning_rate": 0.0001959017436680667,
      "loss": 1.135,
      "step": 166
    },
    {
      "epoch": 0.02124006359300477,
      "grad_norm": 2.2935056686401367,
      "learning_rate": 0.00019587628865979381,
      "loss": 0.7538,
      "step": 167
    },
    {
      "epoch": 0.02136724960254372,
      "grad_norm": 1.655283808708191,
      "learning_rate": 0.00019585083365152096,
      "loss": 1.1417,
      "step": 168
    },
    {
      "epoch": 0.021494435612082672,
      "grad_norm": 1.7189825773239136,
      "learning_rate": 0.00019582537864324805,
      "loss": 0.9115,
      "step": 169
    },
    {
      "epoch": 0.021621621621621623,
      "grad_norm": 1.4350183010101318,
      "learning_rate": 0.0001957999236349752,
      "loss": 0.9128,
      "step": 170
    },
    {
      "epoch": 0.021748807631160574,
      "grad_norm": 2.0751476287841797,
      "learning_rate": 0.00019577446862670232,
      "loss": 0.8886,
      "step": 171
    },
    {
      "epoch": 0.021875993640699525,
      "grad_norm": 1.9009156227111816,
      "learning_rate": 0.00019574901361842944,
      "loss": 0.886,
      "step": 172
    },
    {
      "epoch": 0.022003179650238475,
      "grad_norm": 2.0067713260650635,
      "learning_rate": 0.00019572355861015655,
      "loss": 0.822,
      "step": 173
    },
    {
      "epoch": 0.022130365659777426,
      "grad_norm": 2.2011756896972656,
      "learning_rate": 0.00019569810360188367,
      "loss": 0.7784,
      "step": 174
    },
    {
      "epoch": 0.022257551669316374,
      "grad_norm": 2.2676470279693604,
      "learning_rate": 0.00019567264859361082,
      "loss": 1.2137,
      "step": 175
    },
    {
      "epoch": 0.022384737678855324,
      "grad_norm": 1.9533414840698242,
      "learning_rate": 0.0001956471935853379,
      "loss": 0.8741,
      "step": 176
    },
    {
      "epoch": 0.022511923688394275,
      "grad_norm": 2.17110013961792,
      "learning_rate": 0.00019562173857706506,
      "loss": 0.9663,
      "step": 177
    },
    {
      "epoch": 0.022639109697933226,
      "grad_norm": 2.934060573577881,
      "learning_rate": 0.00019559628356879215,
      "loss": 0.9205,
      "step": 178
    },
    {
      "epoch": 0.022766295707472177,
      "grad_norm": 2.401308298110962,
      "learning_rate": 0.0001955708285605193,
      "loss": 0.9985,
      "step": 179
    },
    {
      "epoch": 0.022893481717011128,
      "grad_norm": 1.4973108768463135,
      "learning_rate": 0.0001955453735522464,
      "loss": 1.0054,
      "step": 180
    },
    {
      "epoch": 0.02302066772655008,
      "grad_norm": 2.0482020378112793,
      "learning_rate": 0.00019551991854397353,
      "loss": 0.9676,
      "step": 181
    },
    {
      "epoch": 0.02314785373608903,
      "grad_norm": 2.2062323093414307,
      "learning_rate": 0.00019549446353570068,
      "loss": 0.8911,
      "step": 182
    },
    {
      "epoch": 0.02327503974562798,
      "grad_norm": 1.9401551485061646,
      "learning_rate": 0.00019546900852742777,
      "loss": 1.1186,
      "step": 183
    },
    {
      "epoch": 0.02340222575516693,
      "grad_norm": 2.4909069538116455,
      "learning_rate": 0.00019544355351915491,
      "loss": 1.0098,
      "step": 184
    },
    {
      "epoch": 0.023529411764705882,
      "grad_norm": 3.4491422176361084,
      "learning_rate": 0.000195418098510882,
      "loss": 1.0309,
      "step": 185
    },
    {
      "epoch": 0.023656597774244833,
      "grad_norm": 1.7018026113510132,
      "learning_rate": 0.00019539264350260915,
      "loss": 0.7919,
      "step": 186
    },
    {
      "epoch": 0.023783783783783784,
      "grad_norm": 1.8276910781860352,
      "learning_rate": 0.00019536718849433627,
      "loss": 1.1191,
      "step": 187
    },
    {
      "epoch": 0.023910969793322735,
      "grad_norm": 2.266075372695923,
      "learning_rate": 0.0001953417334860634,
      "loss": 0.8133,
      "step": 188
    },
    {
      "epoch": 0.024038155802861685,
      "grad_norm": 1.8074783086776733,
      "learning_rate": 0.0001953162784777905,
      "loss": 0.9271,
      "step": 189
    },
    {
      "epoch": 0.024165341812400636,
      "grad_norm": 1.6712461709976196,
      "learning_rate": 0.00019529082346951763,
      "loss": 0.948,
      "step": 190
    },
    {
      "epoch": 0.024292527821939587,
      "grad_norm": 2.235441207885742,
      "learning_rate": 0.00019526536846124477,
      "loss": 1.1229,
      "step": 191
    },
    {
      "epoch": 0.024419713831478538,
      "grad_norm": 2.230375051498413,
      "learning_rate": 0.0001952399134529719,
      "loss": 1.4909,
      "step": 192
    },
    {
      "epoch": 0.02454689984101749,
      "grad_norm": 1.7114619016647339,
      "learning_rate": 0.000195214458444699,
      "loss": 1.0863,
      "step": 193
    },
    {
      "epoch": 0.02467408585055644,
      "grad_norm": 1.5038706064224243,
      "learning_rate": 0.00019518900343642613,
      "loss": 1.3298,
      "step": 194
    },
    {
      "epoch": 0.02480127186009539,
      "grad_norm": 2.054405450820923,
      "learning_rate": 0.00019516354842815325,
      "loss": 0.9188,
      "step": 195
    },
    {
      "epoch": 0.02492845786963434,
      "grad_norm": 1.7744452953338623,
      "learning_rate": 0.00019513809341988037,
      "loss": 0.9435,
      "step": 196
    },
    {
      "epoch": 0.025055643879173292,
      "grad_norm": 1.674375057220459,
      "learning_rate": 0.00019511263841160749,
      "loss": 0.8602,
      "step": 197
    },
    {
      "epoch": 0.025182829888712243,
      "grad_norm": 1.8088890314102173,
      "learning_rate": 0.0001950871834033346,
      "loss": 1.1424,
      "step": 198
    },
    {
      "epoch": 0.025310015898251194,
      "grad_norm": 2.5443453788757324,
      "learning_rate": 0.00019506172839506175,
      "loss": 0.8611,
      "step": 199
    },
    {
      "epoch": 0.025437201907790145,
      "grad_norm": 2.170790672302246,
      "learning_rate": 0.00019503627338678887,
      "loss": 0.9483,
      "step": 200
    },
    {
      "epoch": 0.025564387917329092,
      "grad_norm": 1.3958923816680908,
      "learning_rate": 0.000195010818378516,
      "loss": 0.764,
      "step": 201
    },
    {
      "epoch": 0.025691573926868043,
      "grad_norm": 2.150207996368408,
      "learning_rate": 0.0001949853633702431,
      "loss": 1.3997,
      "step": 202
    },
    {
      "epoch": 0.025818759936406994,
      "grad_norm": 1.5839755535125732,
      "learning_rate": 0.00019495990836197022,
      "loss": 0.8946,
      "step": 203
    },
    {
      "epoch": 0.025945945945945945,
      "grad_norm": 2.492039203643799,
      "learning_rate": 0.00019493445335369734,
      "loss": 1.1088,
      "step": 204
    },
    {
      "epoch": 0.026073131955484895,
      "grad_norm": 1.7582709789276123,
      "learning_rate": 0.00019490899834542446,
      "loss": 0.9394,
      "step": 205
    },
    {
      "epoch": 0.026200317965023846,
      "grad_norm": 1.5147655010223389,
      "learning_rate": 0.0001948835433371516,
      "loss": 0.8337,
      "step": 206
    },
    {
      "epoch": 0.026327503974562797,
      "grad_norm": 1.6539710760116577,
      "learning_rate": 0.0001948580883288787,
      "loss": 0.9313,
      "step": 207
    },
    {
      "epoch": 0.026454689984101748,
      "grad_norm": 1.7670769691467285,
      "learning_rate": 0.00019483263332060585,
      "loss": 0.9881,
      "step": 208
    },
    {
      "epoch": 0.0265818759936407,
      "grad_norm": 1.7741461992263794,
      "learning_rate": 0.00019480717831233296,
      "loss": 0.872,
      "step": 209
    },
    {
      "epoch": 0.02670906200317965,
      "grad_norm": 2.2969303131103516,
      "learning_rate": 0.00019478172330406008,
      "loss": 0.7791,
      "step": 210
    },
    {
      "epoch": 0.0268362480127186,
      "grad_norm": 1.963552474975586,
      "learning_rate": 0.0001947562682957872,
      "loss": 0.8926,
      "step": 211
    },
    {
      "epoch": 0.02696343402225755,
      "grad_norm": 1.8589422702789307,
      "learning_rate": 0.00019473081328751432,
      "loss": 0.9472,
      "step": 212
    },
    {
      "epoch": 0.027090620031796502,
      "grad_norm": 2.0481297969818115,
      "learning_rate": 0.00019470535827924147,
      "loss": 0.7236,
      "step": 213
    },
    {
      "epoch": 0.027217806041335453,
      "grad_norm": 1.9630241394042969,
      "learning_rate": 0.00019467990327096856,
      "loss": 1.1318,
      "step": 214
    },
    {
      "epoch": 0.027344992050874404,
      "grad_norm": 3.011293411254883,
      "learning_rate": 0.0001946544482626957,
      "loss": 0.9562,
      "step": 215
    },
    {
      "epoch": 0.027472178060413355,
      "grad_norm": 2.0758399963378906,
      "learning_rate": 0.0001946289932544228,
      "loss": 0.9893,
      "step": 216
    },
    {
      "epoch": 0.027599364069952306,
      "grad_norm": 1.8874276876449585,
      "learning_rate": 0.00019460353824614994,
      "loss": 0.8381,
      "step": 217
    },
    {
      "epoch": 0.027726550079491256,
      "grad_norm": 2.4322774410247803,
      "learning_rate": 0.00019457808323787706,
      "loss": 0.9446,
      "step": 218
    },
    {
      "epoch": 0.027853736089030207,
      "grad_norm": 2.2085561752319336,
      "learning_rate": 0.00019455262822960418,
      "loss": 0.927,
      "step": 219
    },
    {
      "epoch": 0.027980922098569158,
      "grad_norm": 1.9175777435302734,
      "learning_rate": 0.00019452717322133132,
      "loss": 1.0702,
      "step": 220
    },
    {
      "epoch": 0.02810810810810811,
      "grad_norm": 2.8778295516967773,
      "learning_rate": 0.00019450171821305842,
      "loss": 1.0852,
      "step": 221
    },
    {
      "epoch": 0.02823529411764706,
      "grad_norm": 1.5687286853790283,
      "learning_rate": 0.00019447626320478556,
      "loss": 0.7528,
      "step": 222
    },
    {
      "epoch": 0.02836248012718601,
      "grad_norm": 1.8754706382751465,
      "learning_rate": 0.00019445080819651268,
      "loss": 0.968,
      "step": 223
    },
    {
      "epoch": 0.02848966613672496,
      "grad_norm": 1.4987449645996094,
      "learning_rate": 0.0001944253531882398,
      "loss": 0.9474,
      "step": 224
    },
    {
      "epoch": 0.028616852146263912,
      "grad_norm": 1.810566782951355,
      "learning_rate": 0.00019439989817996692,
      "loss": 0.9498,
      "step": 225
    },
    {
      "epoch": 0.028744038155802863,
      "grad_norm": 1.6795035600662231,
      "learning_rate": 0.00019437444317169404,
      "loss": 1.1693,
      "step": 226
    },
    {
      "epoch": 0.028871224165341814,
      "grad_norm": 2.640944719314575,
      "learning_rate": 0.00019434898816342116,
      "loss": 1.0434,
      "step": 227
    },
    {
      "epoch": 0.02899841017488076,
      "grad_norm": 2.2045445442199707,
      "learning_rate": 0.00019432353315514827,
      "loss": 0.9669,
      "step": 228
    },
    {
      "epoch": 0.029125596184419712,
      "grad_norm": 2.8130364418029785,
      "learning_rate": 0.00019429807814687542,
      "loss": 1.4211,
      "step": 229
    },
    {
      "epoch": 0.029252782193958663,
      "grad_norm": 1.8612041473388672,
      "learning_rate": 0.00019427262313860254,
      "loss": 0.7134,
      "step": 230
    },
    {
      "epoch": 0.029379968203497614,
      "grad_norm": 1.9502755403518677,
      "learning_rate": 0.00019424716813032966,
      "loss": 1.0401,
      "step": 231
    },
    {
      "epoch": 0.029507154213036565,
      "grad_norm": 1.6742221117019653,
      "learning_rate": 0.00019422171312205678,
      "loss": 0.9058,
      "step": 232
    },
    {
      "epoch": 0.029634340222575516,
      "grad_norm": 1.9609196186065674,
      "learning_rate": 0.0001941962581137839,
      "loss": 0.9559,
      "step": 233
    },
    {
      "epoch": 0.029761526232114466,
      "grad_norm": 2.1123251914978027,
      "learning_rate": 0.00019417080310551101,
      "loss": 0.9539,
      "step": 234
    },
    {
      "epoch": 0.029888712241653417,
      "grad_norm": 1.9344271421432495,
      "learning_rate": 0.00019414534809723813,
      "loss": 1.0538,
      "step": 235
    },
    {
      "epoch": 0.030015898251192368,
      "grad_norm": 2.4391517639160156,
      "learning_rate": 0.00019411989308896525,
      "loss": 1.1737,
      "step": 236
    },
    {
      "epoch": 0.03014308426073132,
      "grad_norm": 1.8161226511001587,
      "learning_rate": 0.0001940944380806924,
      "loss": 0.97,
      "step": 237
    },
    {
      "epoch": 0.03027027027027027,
      "grad_norm": 2.809237003326416,
      "learning_rate": 0.0001940689830724195,
      "loss": 1.0097,
      "step": 238
    },
    {
      "epoch": 0.03039745627980922,
      "grad_norm": 1.3929200172424316,
      "learning_rate": 0.00019404352806414663,
      "loss": 1.0421,
      "step": 239
    },
    {
      "epoch": 0.03052464228934817,
      "grad_norm": 1.9181894063949585,
      "learning_rate": 0.00019401807305587375,
      "loss": 0.7963,
      "step": 240
    },
    {
      "epoch": 0.030651828298887122,
      "grad_norm": 1.9425971508026123,
      "learning_rate": 0.00019399261804760087,
      "loss": 1.1243,
      "step": 241
    },
    {
      "epoch": 0.030779014308426073,
      "grad_norm": 1.9748353958129883,
      "learning_rate": 0.000193967163039328,
      "loss": 0.659,
      "step": 242
    },
    {
      "epoch": 0.030906200317965024,
      "grad_norm": 2.2088496685028076,
      "learning_rate": 0.0001939417080310551,
      "loss": 0.8832,
      "step": 243
    },
    {
      "epoch": 0.031033386327503975,
      "grad_norm": 1.8115757703781128,
      "learning_rate": 0.00019391625302278226,
      "loss": 0.9371,
      "step": 244
    },
    {
      "epoch": 0.031160572337042926,
      "grad_norm": 2.407935857772827,
      "learning_rate": 0.00019389079801450935,
      "loss": 1.1899,
      "step": 245
    },
    {
      "epoch": 0.03128775834658187,
      "grad_norm": 2.1173627376556396,
      "learning_rate": 0.0001938653430062365,
      "loss": 1.0082,
      "step": 246
    },
    {
      "epoch": 0.03141494435612083,
      "grad_norm": 1.704736590385437,
      "learning_rate": 0.0001938398879979636,
      "loss": 1.2842,
      "step": 247
    },
    {
      "epoch": 0.031542130365659775,
      "grad_norm": 1.419809341430664,
      "learning_rate": 0.00019381443298969073,
      "loss": 0.9754,
      "step": 248
    },
    {
      "epoch": 0.03166931637519873,
      "grad_norm": 1.2625670433044434,
      "learning_rate": 0.00019378897798141788,
      "loss": 0.8799,
      "step": 249
    },
    {
      "epoch": 0.03179650238473768,
      "grad_norm": 2.193735122680664,
      "learning_rate": 0.00019376352297314497,
      "loss": 0.9711,
      "step": 250
    },
    {
      "epoch": 0.03192368839427663,
      "grad_norm": 1.3799712657928467,
      "learning_rate": 0.00019373806796487211,
      "loss": 0.7405,
      "step": 251
    },
    {
      "epoch": 0.03205087440381558,
      "grad_norm": 1.916124701499939,
      "learning_rate": 0.0001937126129565992,
      "loss": 1.1527,
      "step": 252
    },
    {
      "epoch": 0.03217806041335453,
      "grad_norm": 2.1333694458007812,
      "learning_rate": 0.00019368715794832635,
      "loss": 1.1118,
      "step": 253
    },
    {
      "epoch": 0.03230524642289348,
      "grad_norm": 1.361778974533081,
      "learning_rate": 0.00019366170294005347,
      "loss": 0.9195,
      "step": 254
    },
    {
      "epoch": 0.032432432432432434,
      "grad_norm": 2.0721421241760254,
      "learning_rate": 0.0001936362479317806,
      "loss": 1.0954,
      "step": 255
    },
    {
      "epoch": 0.03255961844197138,
      "grad_norm": 2.132791042327881,
      "learning_rate": 0.0001936107929235077,
      "loss": 1.2564,
      "step": 256
    },
    {
      "epoch": 0.032686804451510336,
      "grad_norm": 1.333014726638794,
      "learning_rate": 0.00019358533791523483,
      "loss": 0.7231,
      "step": 257
    },
    {
      "epoch": 0.03281399046104928,
      "grad_norm": 1.7117761373519897,
      "learning_rate": 0.00019355988290696197,
      "loss": 1.2924,
      "step": 258
    },
    {
      "epoch": 0.03294117647058824,
      "grad_norm": 1.4331084489822388,
      "learning_rate": 0.00019353442789868906,
      "loss": 0.972,
      "step": 259
    },
    {
      "epoch": 0.033068362480127185,
      "grad_norm": 2.044530153274536,
      "learning_rate": 0.0001935089728904162,
      "loss": 1.0241,
      "step": 260
    },
    {
      "epoch": 0.03319554848966614,
      "grad_norm": 1.7310482263565063,
      "learning_rate": 0.00019348351788214333,
      "loss": 0.8687,
      "step": 261
    },
    {
      "epoch": 0.03332273449920509,
      "grad_norm": 1.8200480937957764,
      "learning_rate": 0.00019345806287387045,
      "loss": 1.0256,
      "step": 262
    },
    {
      "epoch": 0.03344992050874404,
      "grad_norm": 1.8301423788070679,
      "learning_rate": 0.00019343260786559757,
      "loss": 0.8673,
      "step": 263
    },
    {
      "epoch": 0.03357710651828299,
      "grad_norm": 1.554537057876587,
      "learning_rate": 0.00019340715285732468,
      "loss": 1.0675,
      "step": 264
    },
    {
      "epoch": 0.03370429252782194,
      "grad_norm": 2.1644845008850098,
      "learning_rate": 0.0001933816978490518,
      "loss": 0.8757,
      "step": 265
    },
    {
      "epoch": 0.03383147853736089,
      "grad_norm": 1.7100330591201782,
      "learning_rate": 0.00019335624284077892,
      "loss": 0.9756,
      "step": 266
    },
    {
      "epoch": 0.033958664546899844,
      "grad_norm": 1.588010549545288,
      "learning_rate": 0.00019333078783250604,
      "loss": 0.9053,
      "step": 267
    },
    {
      "epoch": 0.03408585055643879,
      "grad_norm": 2.2059972286224365,
      "learning_rate": 0.0001933053328242332,
      "loss": 0.9938,
      "step": 268
    },
    {
      "epoch": 0.03421303656597774,
      "grad_norm": 1.8193867206573486,
      "learning_rate": 0.0001932798778159603,
      "loss": 0.8951,
      "step": 269
    },
    {
      "epoch": 0.03434022257551669,
      "grad_norm": 1.3278236389160156,
      "learning_rate": 0.00019325442280768742,
      "loss": 0.7975,
      "step": 270
    },
    {
      "epoch": 0.03446740858505564,
      "grad_norm": 1.8520108461380005,
      "learning_rate": 0.00019322896779941454,
      "loss": 0.9573,
      "step": 271
    },
    {
      "epoch": 0.034594594594594595,
      "grad_norm": 1.2262637615203857,
      "learning_rate": 0.00019320351279114166,
      "loss": 0.7721,
      "step": 272
    },
    {
      "epoch": 0.03472178060413354,
      "grad_norm": 1.718123197555542,
      "learning_rate": 0.00019317805778286878,
      "loss": 0.9842,
      "step": 273
    },
    {
      "epoch": 0.0348489666136725,
      "grad_norm": 2.286438465118408,
      "learning_rate": 0.0001931526027745959,
      "loss": 1.3245,
      "step": 274
    },
    {
      "epoch": 0.034976152623211444,
      "grad_norm": 2.2452754974365234,
      "learning_rate": 0.00019312714776632305,
      "loss": 0.8389,
      "step": 275
    },
    {
      "epoch": 0.0351033386327504,
      "grad_norm": 1.6658774614334106,
      "learning_rate": 0.00019310169275805014,
      "loss": 0.6766,
      "step": 276
    },
    {
      "epoch": 0.035230524642289346,
      "grad_norm": 1.7411412000656128,
      "learning_rate": 0.00019307623774977728,
      "loss": 0.7899,
      "step": 277
    },
    {
      "epoch": 0.0353577106518283,
      "grad_norm": 1.3576003313064575,
      "learning_rate": 0.0001930507827415044,
      "loss": 0.9693,
      "step": 278
    },
    {
      "epoch": 0.03548489666136725,
      "grad_norm": 2.1912717819213867,
      "learning_rate": 0.00019302532773323152,
      "loss": 0.9758,
      "step": 279
    },
    {
      "epoch": 0.0356120826709062,
      "grad_norm": 1.7217351198196411,
      "learning_rate": 0.00019299987272495867,
      "loss": 1.0406,
      "step": 280
    },
    {
      "epoch": 0.03573926868044515,
      "grad_norm": 1.646202802658081,
      "learning_rate": 0.00019297441771668576,
      "loss": 0.9588,
      "step": 281
    },
    {
      "epoch": 0.035866454689984104,
      "grad_norm": 1.9065519571304321,
      "learning_rate": 0.0001929489627084129,
      "loss": 1.0337,
      "step": 282
    },
    {
      "epoch": 0.03599364069952305,
      "grad_norm": 1.6760920286178589,
      "learning_rate": 0.00019292350770014,
      "loss": 0.7999,
      "step": 283
    },
    {
      "epoch": 0.036120826709062005,
      "grad_norm": 1.8230949640274048,
      "learning_rate": 0.00019289805269186714,
      "loss": 0.8667,
      "step": 284
    },
    {
      "epoch": 0.03624801271860095,
      "grad_norm": 2.4598240852355957,
      "learning_rate": 0.00019287259768359426,
      "loss": 1.2035,
      "step": 285
    },
    {
      "epoch": 0.03637519872813991,
      "grad_norm": 1.83902907371521,
      "learning_rate": 0.00019284714267532138,
      "loss": 0.8442,
      "step": 286
    },
    {
      "epoch": 0.036502384737678854,
      "grad_norm": 2.1646595001220703,
      "learning_rate": 0.00019282168766704852,
      "loss": 0.9089,
      "step": 287
    },
    {
      "epoch": 0.03662957074721781,
      "grad_norm": 1.5054173469543457,
      "learning_rate": 0.00019279623265877562,
      "loss": 0.9466,
      "step": 288
    },
    {
      "epoch": 0.036756756756756756,
      "grad_norm": 2.145789861679077,
      "learning_rate": 0.00019277077765050276,
      "loss": 1.162,
      "step": 289
    },
    {
      "epoch": 0.03688394276629571,
      "grad_norm": 1.6546162366867065,
      "learning_rate": 0.00019274532264222985,
      "loss": 0.8827,
      "step": 290
    },
    {
      "epoch": 0.03701112877583466,
      "grad_norm": 1.7283401489257812,
      "learning_rate": 0.000192719867633957,
      "loss": 0.9869,
      "step": 291
    },
    {
      "epoch": 0.03713831478537361,
      "grad_norm": 1.9542316198349,
      "learning_rate": 0.00019269441262568412,
      "loss": 1.2332,
      "step": 292
    },
    {
      "epoch": 0.03726550079491256,
      "grad_norm": 1.3954885005950928,
      "learning_rate": 0.00019266895761741124,
      "loss": 0.7133,
      "step": 293
    },
    {
      "epoch": 0.037392686804451514,
      "grad_norm": 1.8785241842269897,
      "learning_rate": 0.00019264350260913836,
      "loss": 0.6981,
      "step": 294
    },
    {
      "epoch": 0.03751987281399046,
      "grad_norm": 2.447256565093994,
      "learning_rate": 0.00019261804760086547,
      "loss": 1.2473,
      "step": 295
    },
    {
      "epoch": 0.03764705882352941,
      "grad_norm": 1.343336582183838,
      "learning_rate": 0.0001925925925925926,
      "loss": 0.8351,
      "step": 296
    },
    {
      "epoch": 0.03777424483306836,
      "grad_norm": 1.63563871383667,
      "learning_rate": 0.0001925671375843197,
      "loss": 0.9338,
      "step": 297
    },
    {
      "epoch": 0.03790143084260731,
      "grad_norm": 1.975118637084961,
      "learning_rate": 0.00019254168257604686,
      "loss": 1.074,
      "step": 298
    },
    {
      "epoch": 0.038028616852146264,
      "grad_norm": 1.4448885917663574,
      "learning_rate": 0.00019251622756777398,
      "loss": 0.9485,
      "step": 299
    },
    {
      "epoch": 0.03815580286168521,
      "grad_norm": 1.612450361251831,
      "learning_rate": 0.0001924907725595011,
      "loss": 0.9153,
      "step": 300
    },
    {
      "epoch": 0.038282988871224166,
      "grad_norm": 1.6793805360794067,
      "learning_rate": 0.00019246531755122821,
      "loss": 0.9112,
      "step": 301
    },
    {
      "epoch": 0.038410174880763114,
      "grad_norm": 2.0125374794006348,
      "learning_rate": 0.00019243986254295533,
      "loss": 0.8176,
      "step": 302
    },
    {
      "epoch": 0.03853736089030207,
      "grad_norm": 1.6888295412063599,
      "learning_rate": 0.00019241440753468245,
      "loss": 0.9411,
      "step": 303
    },
    {
      "epoch": 0.038664546899841015,
      "grad_norm": 1.598353624343872,
      "learning_rate": 0.00019238895252640957,
      "loss": 1.1399,
      "step": 304
    },
    {
      "epoch": 0.03879173290937997,
      "grad_norm": 1.7642675638198853,
      "learning_rate": 0.0001923634975181367,
      "loss": 1.1046,
      "step": 305
    },
    {
      "epoch": 0.03891891891891892,
      "grad_norm": 1.4892371892929077,
      "learning_rate": 0.00019233804250986383,
      "loss": 0.7154,
      "step": 306
    },
    {
      "epoch": 0.03904610492845787,
      "grad_norm": 1.6151514053344727,
      "learning_rate": 0.00019231258750159095,
      "loss": 0.9718,
      "step": 307
    },
    {
      "epoch": 0.03917329093799682,
      "grad_norm": 1.6739139556884766,
      "learning_rate": 0.00019228713249331807,
      "loss": 0.9262,
      "step": 308
    },
    {
      "epoch": 0.03930047694753577,
      "grad_norm": 1.7445993423461914,
      "learning_rate": 0.0001922616774850452,
      "loss": 0.862,
      "step": 309
    },
    {
      "epoch": 0.03942766295707472,
      "grad_norm": 2.6640615463256836,
      "learning_rate": 0.0001922362224767723,
      "loss": 0.8556,
      "step": 310
    },
    {
      "epoch": 0.039554848966613675,
      "grad_norm": 2.306739330291748,
      "learning_rate": 0.00019221076746849946,
      "loss": 1.0554,
      "step": 311
    },
    {
      "epoch": 0.03968203497615262,
      "grad_norm": 3.189340353012085,
      "learning_rate": 0.00019218531246022655,
      "loss": 1.0555,
      "step": 312
    },
    {
      "epoch": 0.039809220985691576,
      "grad_norm": 1.9783365726470947,
      "learning_rate": 0.0001921598574519537,
      "loss": 1.0946,
      "step": 313
    },
    {
      "epoch": 0.039936406995230524,
      "grad_norm": 3.573298692703247,
      "learning_rate": 0.00019213440244368078,
      "loss": 0.972,
      "step": 314
    },
    {
      "epoch": 0.04006359300476948,
      "grad_norm": 1.9289319515228271,
      "learning_rate": 0.00019210894743540793,
      "loss": 0.9844,
      "step": 315
    },
    {
      "epoch": 0.040190779014308425,
      "grad_norm": 2.283369302749634,
      "learning_rate": 0.00019208349242713505,
      "loss": 1.2025,
      "step": 316
    },
    {
      "epoch": 0.04031796502384738,
      "grad_norm": 1.4788262844085693,
      "learning_rate": 0.00019205803741886217,
      "loss": 0.7521,
      "step": 317
    },
    {
      "epoch": 0.04044515103338633,
      "grad_norm": 1.6451468467712402,
      "learning_rate": 0.0001920325824105893,
      "loss": 0.9623,
      "step": 318
    },
    {
      "epoch": 0.04057233704292528,
      "grad_norm": 1.444057822227478,
      "learning_rate": 0.0001920071274023164,
      "loss": 0.9333,
      "step": 319
    },
    {
      "epoch": 0.04069952305246423,
      "grad_norm": 1.6539134979248047,
      "learning_rate": 0.00019198167239404355,
      "loss": 0.8352,
      "step": 320
    },
    {
      "epoch": 0.04082670906200318,
      "grad_norm": 1.681694746017456,
      "learning_rate": 0.00019195621738577064,
      "loss": 0.8448,
      "step": 321
    },
    {
      "epoch": 0.04095389507154213,
      "grad_norm": 1.7956124544143677,
      "learning_rate": 0.0001919307623774978,
      "loss": 0.974,
      "step": 322
    },
    {
      "epoch": 0.04108108108108108,
      "grad_norm": 1.6624659299850464,
      "learning_rate": 0.0001919053073692249,
      "loss": 0.9319,
      "step": 323
    },
    {
      "epoch": 0.04120826709062003,
      "grad_norm": 1.570343255996704,
      "learning_rate": 0.00019187985236095203,
      "loss": 0.8415,
      "step": 324
    },
    {
      "epoch": 0.04133545310015898,
      "grad_norm": 1.7492382526397705,
      "learning_rate": 0.00019185439735267914,
      "loss": 1.0339,
      "step": 325
    },
    {
      "epoch": 0.041462639109697934,
      "grad_norm": 1.4518474340438843,
      "learning_rate": 0.00019182894234440626,
      "loss": 1.0006,
      "step": 326
    },
    {
      "epoch": 0.04158982511923688,
      "grad_norm": 2.1322357654571533,
      "learning_rate": 0.0001918034873361334,
      "loss": 1.0736,
      "step": 327
    },
    {
      "epoch": 0.041717011128775836,
      "grad_norm": 2.1076231002807617,
      "learning_rate": 0.0001917780323278605,
      "loss": 0.8511,
      "step": 328
    },
    {
      "epoch": 0.04184419713831478,
      "grad_norm": 2.502218008041382,
      "learning_rate": 0.00019175257731958765,
      "loss": 0.9707,
      "step": 329
    },
    {
      "epoch": 0.04197138314785374,
      "grad_norm": 2.114065647125244,
      "learning_rate": 0.00019172712231131477,
      "loss": 0.9294,
      "step": 330
    },
    {
      "epoch": 0.042098569157392685,
      "grad_norm": 1.8917957544326782,
      "learning_rate": 0.00019170166730304188,
      "loss": 0.8101,
      "step": 331
    },
    {
      "epoch": 0.04222575516693164,
      "grad_norm": 1.7752281427383423,
      "learning_rate": 0.000191676212294769,
      "loss": 0.9178,
      "step": 332
    },
    {
      "epoch": 0.042352941176470586,
      "grad_norm": 2.3162245750427246,
      "learning_rate": 0.00019165075728649612,
      "loss": 0.7262,
      "step": 333
    },
    {
      "epoch": 0.04248012718600954,
      "grad_norm": 2.1325459480285645,
      "learning_rate": 0.00019162530227822324,
      "loss": 0.9826,
      "step": 334
    },
    {
      "epoch": 0.04260731319554849,
      "grad_norm": 1.8166451454162598,
      "learning_rate": 0.00019159984726995036,
      "loss": 0.9125,
      "step": 335
    },
    {
      "epoch": 0.04273449920508744,
      "grad_norm": 1.9824925661087036,
      "learning_rate": 0.0001915743922616775,
      "loss": 0.8794,
      "step": 336
    },
    {
      "epoch": 0.04286168521462639,
      "grad_norm": 2.35233211517334,
      "learning_rate": 0.00019154893725340462,
      "loss": 1.1302,
      "step": 337
    },
    {
      "epoch": 0.042988871224165344,
      "grad_norm": 1.835847020149231,
      "learning_rate": 0.00019152348224513174,
      "loss": 1.1275,
      "step": 338
    },
    {
      "epoch": 0.04311605723370429,
      "grad_norm": 1.9382667541503906,
      "learning_rate": 0.00019149802723685886,
      "loss": 0.6318,
      "step": 339
    },
    {
      "epoch": 0.043243243243243246,
      "grad_norm": 1.312990427017212,
      "learning_rate": 0.00019147257222858598,
      "loss": 0.6198,
      "step": 340
    },
    {
      "epoch": 0.04337042925278219,
      "grad_norm": 1.6959329843521118,
      "learning_rate": 0.0001914471172203131,
      "loss": 0.6846,
      "step": 341
    },
    {
      "epoch": 0.04349761526232115,
      "grad_norm": 1.9092265367507935,
      "learning_rate": 0.00019142166221204024,
      "loss": 0.8369,
      "step": 342
    },
    {
      "epoch": 0.043624801271860095,
      "grad_norm": 2.1956348419189453,
      "learning_rate": 0.00019139620720376734,
      "loss": 0.9803,
      "step": 343
    },
    {
      "epoch": 0.04375198728139905,
      "grad_norm": 1.2594941854476929,
      "learning_rate": 0.00019137075219549448,
      "loss": 0.7755,
      "step": 344
    },
    {
      "epoch": 0.043879173290937996,
      "grad_norm": 2.8091161251068115,
      "learning_rate": 0.0001913452971872216,
      "loss": 1.5836,
      "step": 345
    },
    {
      "epoch": 0.04400635930047695,
      "grad_norm": 2.0146007537841797,
      "learning_rate": 0.00019131984217894872,
      "loss": 0.8293,
      "step": 346
    },
    {
      "epoch": 0.0441335453100159,
      "grad_norm": 2.5337414741516113,
      "learning_rate": 0.00019129438717067584,
      "loss": 0.9451,
      "step": 347
    },
    {
      "epoch": 0.04426073131955485,
      "grad_norm": 2.2639384269714355,
      "learning_rate": 0.00019126893216240296,
      "loss": 1.0579,
      "step": 348
    },
    {
      "epoch": 0.0443879173290938,
      "grad_norm": 1.9296945333480835,
      "learning_rate": 0.0001912434771541301,
      "loss": 1.0175,
      "step": 349
    },
    {
      "epoch": 0.04451510333863275,
      "grad_norm": 1.5882983207702637,
      "learning_rate": 0.0001912180221458572,
      "loss": 1.006,
      "step": 350
    },
    {
      "epoch": 0.0446422893481717,
      "grad_norm": 2.6720457077026367,
      "learning_rate": 0.00019119256713758434,
      "loss": 1.0714,
      "step": 351
    },
    {
      "epoch": 0.04476947535771065,
      "grad_norm": 2.075242757797241,
      "learning_rate": 0.00019116711212931143,
      "loss": 0.9674,
      "step": 352
    },
    {
      "epoch": 0.0448966613672496,
      "grad_norm": 2.2574825286865234,
      "learning_rate": 0.00019114165712103858,
      "loss": 1.1031,
      "step": 353
    },
    {
      "epoch": 0.04502384737678855,
      "grad_norm": 1.8668931722640991,
      "learning_rate": 0.0001911162021127657,
      "loss": 1.0843,
      "step": 354
    },
    {
      "epoch": 0.045151033386327505,
      "grad_norm": 1.5907822847366333,
      "learning_rate": 0.00019109074710449282,
      "loss": 0.9009,
      "step": 355
    },
    {
      "epoch": 0.04527821939586645,
      "grad_norm": 1.8822542428970337,
      "learning_rate": 0.00019106529209621996,
      "loss": 0.8556,
      "step": 356
    },
    {
      "epoch": 0.04540540540540541,
      "grad_norm": 1.7585749626159668,
      "learning_rate": 0.00019103983708794705,
      "loss": 1.3653,
      "step": 357
    },
    {
      "epoch": 0.045532591414944354,
      "grad_norm": 1.8945053815841675,
      "learning_rate": 0.0001910143820796742,
      "loss": 1.1868,
      "step": 358
    },
    {
      "epoch": 0.04565977742448331,
      "grad_norm": 2.112715721130371,
      "learning_rate": 0.0001909889270714013,
      "loss": 0.7868,
      "step": 359
    },
    {
      "epoch": 0.045786963434022256,
      "grad_norm": 1.2989929914474487,
      "learning_rate": 0.00019096347206312844,
      "loss": 0.8508,
      "step": 360
    },
    {
      "epoch": 0.04591414944356121,
      "grad_norm": 1.6562196016311646,
      "learning_rate": 0.00019093801705485555,
      "loss": 0.8796,
      "step": 361
    },
    {
      "epoch": 0.04604133545310016,
      "grad_norm": 1.9958841800689697,
      "learning_rate": 0.00019091256204658267,
      "loss": 1.2549,
      "step": 362
    },
    {
      "epoch": 0.04616852146263911,
      "grad_norm": 1.6994818449020386,
      "learning_rate": 0.0001908871070383098,
      "loss": 0.8224,
      "step": 363
    },
    {
      "epoch": 0.04629570747217806,
      "grad_norm": 1.5807081460952759,
      "learning_rate": 0.0001908616520300369,
      "loss": 0.7733,
      "step": 364
    },
    {
      "epoch": 0.04642289348171701,
      "grad_norm": 1.9263890981674194,
      "learning_rate": 0.00019083619702176406,
      "loss": 1.0682,
      "step": 365
    },
    {
      "epoch": 0.04655007949125596,
      "grad_norm": 1.7162114381790161,
      "learning_rate": 0.00019081074201349115,
      "loss": 0.714,
      "step": 366
    },
    {
      "epoch": 0.046677265500794915,
      "grad_norm": 1.8499960899353027,
      "learning_rate": 0.0001907852870052183,
      "loss": 1.093,
      "step": 367
    },
    {
      "epoch": 0.04680445151033386,
      "grad_norm": 2.210373878479004,
      "learning_rate": 0.0001907598319969454,
      "loss": 0.8077,
      "step": 368
    },
    {
      "epoch": 0.04693163751987282,
      "grad_norm": 2.3666632175445557,
      "learning_rate": 0.00019073437698867253,
      "loss": 0.9774,
      "step": 369
    },
    {
      "epoch": 0.047058823529411764,
      "grad_norm": 1.7445154190063477,
      "learning_rate": 0.00019070892198039965,
      "loss": 0.8832,
      "step": 370
    },
    {
      "epoch": 0.04718600953895072,
      "grad_norm": 2.2088944911956787,
      "learning_rate": 0.00019068346697212677,
      "loss": 0.986,
      "step": 371
    },
    {
      "epoch": 0.047313195548489666,
      "grad_norm": 1.4240710735321045,
      "learning_rate": 0.0001906580119638539,
      "loss": 0.7756,
      "step": 372
    },
    {
      "epoch": 0.04744038155802862,
      "grad_norm": 2.0841176509857178,
      "learning_rate": 0.00019063255695558103,
      "loss": 0.8882,
      "step": 373
    },
    {
      "epoch": 0.04756756756756757,
      "grad_norm": 2.242774486541748,
      "learning_rate": 0.00019060710194730813,
      "loss": 1.0292,
      "step": 374
    },
    {
      "epoch": 0.04769475357710652,
      "grad_norm": 1.415672779083252,
      "learning_rate": 0.00019058164693903527,
      "loss": 0.8108,
      "step": 375
    },
    {
      "epoch": 0.04782193958664547,
      "grad_norm": 1.5423493385314941,
      "learning_rate": 0.0001905561919307624,
      "loss": 0.9734,
      "step": 376
    },
    {
      "epoch": 0.047949125596184416,
      "grad_norm": 2.180241346359253,
      "learning_rate": 0.0001905307369224895,
      "loss": 1.0509,
      "step": 377
    },
    {
      "epoch": 0.04807631160572337,
      "grad_norm": 2.3991618156433105,
      "learning_rate": 0.00019050528191421663,
      "loss": 0.8876,
      "step": 378
    },
    {
      "epoch": 0.04820349761526232,
      "grad_norm": 1.8328680992126465,
      "learning_rate": 0.00019047982690594375,
      "loss": 0.7049,
      "step": 379
    },
    {
      "epoch": 0.04833068362480127,
      "grad_norm": 1.8131436109542847,
      "learning_rate": 0.0001904543718976709,
      "loss": 0.8578,
      "step": 380
    },
    {
      "epoch": 0.04845786963434022,
      "grad_norm": 1.7775095701217651,
      "learning_rate": 0.00019042891688939798,
      "loss": 0.8613,
      "step": 381
    },
    {
      "epoch": 0.048585055643879174,
      "grad_norm": 1.9797052145004272,
      "learning_rate": 0.00019040346188112513,
      "loss": 0.9067,
      "step": 382
    },
    {
      "epoch": 0.04871224165341812,
      "grad_norm": 1.752293348312378,
      "learning_rate": 0.00019037800687285222,
      "loss": 0.8192,
      "step": 383
    },
    {
      "epoch": 0.048839427662957076,
      "grad_norm": 2.1304492950439453,
      "learning_rate": 0.00019035255186457937,
      "loss": 0.7296,
      "step": 384
    },
    {
      "epoch": 0.04896661367249602,
      "grad_norm": 2.689049005508423,
      "learning_rate": 0.00019032709685630649,
      "loss": 0.9236,
      "step": 385
    },
    {
      "epoch": 0.04909379968203498,
      "grad_norm": 2.1385719776153564,
      "learning_rate": 0.0001903016418480336,
      "loss": 0.8165,
      "step": 386
    },
    {
      "epoch": 0.049220985691573925,
      "grad_norm": 1.9252177476882935,
      "learning_rate": 0.00019027618683976075,
      "loss": 1.0604,
      "step": 387
    },
    {
      "epoch": 0.04934817170111288,
      "grad_norm": 1.872300148010254,
      "learning_rate": 0.00019025073183148784,
      "loss": 1.0157,
      "step": 388
    },
    {
      "epoch": 0.04947535771065183,
      "grad_norm": 1.6499167680740356,
      "learning_rate": 0.000190225276823215,
      "loss": 1.027,
      "step": 389
    },
    {
      "epoch": 0.04960254372019078,
      "grad_norm": 1.7066529989242554,
      "learning_rate": 0.00019019982181494208,
      "loss": 0.9826,
      "step": 390
    },
    {
      "epoch": 0.04972972972972973,
      "grad_norm": 1.464273452758789,
      "learning_rate": 0.00019017436680666923,
      "loss": 0.8893,
      "step": 391
    },
    {
      "epoch": 0.04985691573926868,
      "grad_norm": 1.5507148504257202,
      "learning_rate": 0.00019014891179839634,
      "loss": 0.8732,
      "step": 392
    },
    {
      "epoch": 0.04998410174880763,
      "grad_norm": 1.4993327856063843,
      "learning_rate": 0.00019012345679012346,
      "loss": 0.6868,
      "step": 393
    },
    {
      "epoch": 0.050111287758346584,
      "grad_norm": 1.6799064874649048,
      "learning_rate": 0.0001900980017818506,
      "loss": 0.6996,
      "step": 394
    },
    {
      "epoch": 0.05023847376788553,
      "grad_norm": 1.906393051147461,
      "learning_rate": 0.0001900725467735777,
      "loss": 0.9599,
      "step": 395
    },
    {
      "epoch": 0.050365659777424486,
      "grad_norm": 1.9912419319152832,
      "learning_rate": 0.00019004709176530485,
      "loss": 0.9534,
      "step": 396
    },
    {
      "epoch": 0.05049284578696343,
      "grad_norm": 1.5852093696594238,
      "learning_rate": 0.00019002163675703194,
      "loss": 0.9342,
      "step": 397
    },
    {
      "epoch": 0.05062003179650239,
      "grad_norm": 1.2470147609710693,
      "learning_rate": 0.00018999618174875908,
      "loss": 0.726,
      "step": 398
    },
    {
      "epoch": 0.050747217806041335,
      "grad_norm": 1.935491919517517,
      "learning_rate": 0.0001899707267404862,
      "loss": 0.8769,
      "step": 399
    },
    {
      "epoch": 0.05087440381558029,
      "grad_norm": 1.8295127153396606,
      "learning_rate": 0.00018994527173221332,
      "loss": 0.7995,
      "step": 400
    },
    {
      "epoch": 0.05100158982511924,
      "grad_norm": 2.5021533966064453,
      "learning_rate": 0.00018991981672394044,
      "loss": 0.8064,
      "step": 401
    },
    {
      "epoch": 0.051128775834658184,
      "grad_norm": 2.0577187538146973,
      "learning_rate": 0.00018989436171566756,
      "loss": 1.0553,
      "step": 402
    },
    {
      "epoch": 0.05125596184419714,
      "grad_norm": 2.0114667415618896,
      "learning_rate": 0.00018986890670739468,
      "loss": 0.8794,
      "step": 403
    },
    {
      "epoch": 0.051383147853736086,
      "grad_norm": 2.179656982421875,
      "learning_rate": 0.00018984345169912182,
      "loss": 0.9683,
      "step": 404
    },
    {
      "epoch": 0.05151033386327504,
      "grad_norm": 2.122152805328369,
      "learning_rate": 0.00018981799669084894,
      "loss": 0.9555,
      "step": 405
    },
    {
      "epoch": 0.05163751987281399,
      "grad_norm": 2.4037373065948486,
      "learning_rate": 0.00018979254168257606,
      "loss": 0.9165,
      "step": 406
    },
    {
      "epoch": 0.05176470588235294,
      "grad_norm": 2.3327512741088867,
      "learning_rate": 0.00018976708667430318,
      "loss": 0.923,
      "step": 407
    },
    {
      "epoch": 0.05189189189189189,
      "grad_norm": 1.8505555391311646,
      "learning_rate": 0.0001897416316660303,
      "loss": 1.1746,
      "step": 408
    },
    {
      "epoch": 0.052019077901430844,
      "grad_norm": 2.3334786891937256,
      "learning_rate": 0.00018971617665775742,
      "loss": 0.8007,
      "step": 409
    },
    {
      "epoch": 0.05214626391096979,
      "grad_norm": 2.037682056427002,
      "learning_rate": 0.00018969072164948454,
      "loss": 0.8768,
      "step": 410
    },
    {
      "epoch": 0.052273449920508745,
      "grad_norm": 1.8292160034179688,
      "learning_rate": 0.00018966526664121168,
      "loss": 1.0059,
      "step": 411
    },
    {
      "epoch": 0.05240063593004769,
      "grad_norm": 1.6571054458618164,
      "learning_rate": 0.00018963981163293877,
      "loss": 1.0509,
      "step": 412
    },
    {
      "epoch": 0.05252782193958665,
      "grad_norm": 1.7511467933654785,
      "learning_rate": 0.00018961435662466592,
      "loss": 0.8377,
      "step": 413
    },
    {
      "epoch": 0.052655007949125594,
      "grad_norm": 1.7517125606536865,
      "learning_rate": 0.00018958890161639304,
      "loss": 0.8256,
      "step": 414
    },
    {
      "epoch": 0.05278219395866455,
      "grad_norm": 2.1797609329223633,
      "learning_rate": 0.00018956344660812016,
      "loss": 1.4674,
      "step": 415
    },
    {
      "epoch": 0.052909379968203496,
      "grad_norm": 2.252676486968994,
      "learning_rate": 0.00018953799159984728,
      "loss": 0.8725,
      "step": 416
    },
    {
      "epoch": 0.05303656597774245,
      "grad_norm": 2.193898916244507,
      "learning_rate": 0.0001895125365915744,
      "loss": 0.841,
      "step": 417
    },
    {
      "epoch": 0.0531637519872814,
      "grad_norm": 1.978886604309082,
      "learning_rate": 0.00018948708158330154,
      "loss": 1.1607,
      "step": 418
    },
    {
      "epoch": 0.05329093799682035,
      "grad_norm": 1.8415151834487915,
      "learning_rate": 0.00018946162657502863,
      "loss": 0.7461,
      "step": 419
    },
    {
      "epoch": 0.0534181240063593,
      "grad_norm": 1.6601437330245972,
      "learning_rate": 0.00018943617156675578,
      "loss": 0.7956,
      "step": 420
    },
    {
      "epoch": 0.053545310015898254,
      "grad_norm": 1.9847228527069092,
      "learning_rate": 0.00018941071655848287,
      "loss": 0.9458,
      "step": 421
    },
    {
      "epoch": 0.0536724960254372,
      "grad_norm": 2.3989861011505127,
      "learning_rate": 0.00018938526155021001,
      "loss": 0.8441,
      "step": 422
    },
    {
      "epoch": 0.053799682034976155,
      "grad_norm": 1.8863509893417358,
      "learning_rate": 0.00018935980654193713,
      "loss": 1.0628,
      "step": 423
    },
    {
      "epoch": 0.0539268680445151,
      "grad_norm": 2.3589370250701904,
      "learning_rate": 0.00018933435153366425,
      "loss": 1.0045,
      "step": 424
    },
    {
      "epoch": 0.05405405405405406,
      "grad_norm": 2.0524747371673584,
      "learning_rate": 0.0001893088965253914,
      "loss": 0.9499,
      "step": 425
    },
    {
      "epoch": 0.054181240063593004,
      "grad_norm": 1.404415488243103,
      "learning_rate": 0.0001892834415171185,
      "loss": 0.7807,
      "step": 426
    },
    {
      "epoch": 0.05430842607313196,
      "grad_norm": 1.4957849979400635,
      "learning_rate": 0.00018925798650884564,
      "loss": 0.8378,
      "step": 427
    },
    {
      "epoch": 0.054435612082670906,
      "grad_norm": 1.6177133321762085,
      "learning_rate": 0.00018923253150057273,
      "loss": 1.0058,
      "step": 428
    },
    {
      "epoch": 0.054562798092209853,
      "grad_norm": 1.4753159284591675,
      "learning_rate": 0.00018920707649229987,
      "loss": 0.945,
      "step": 429
    },
    {
      "epoch": 0.05468998410174881,
      "grad_norm": 1.4980106353759766,
      "learning_rate": 0.000189181621484027,
      "loss": 0.8188,
      "step": 430
    },
    {
      "epoch": 0.054817170111287755,
      "grad_norm": 1.520909309387207,
      "learning_rate": 0.0001891561664757541,
      "loss": 0.9156,
      "step": 431
    },
    {
      "epoch": 0.05494435612082671,
      "grad_norm": 2.047058343887329,
      "learning_rate": 0.00018913071146748123,
      "loss": 1.0548,
      "step": 432
    },
    {
      "epoch": 0.05507154213036566,
      "grad_norm": 1.6576471328735352,
      "learning_rate": 0.00018910525645920835,
      "loss": 0.8094,
      "step": 433
    },
    {
      "epoch": 0.05519872813990461,
      "grad_norm": 1.948209285736084,
      "learning_rate": 0.0001890798014509355,
      "loss": 1.0279,
      "step": 434
    },
    {
      "epoch": 0.05532591414944356,
      "grad_norm": 2.552795886993408,
      "learning_rate": 0.0001890543464426626,
      "loss": 0.9749,
      "step": 435
    },
    {
      "epoch": 0.05545310015898251,
      "grad_norm": 1.668681025505066,
      "learning_rate": 0.00018902889143438973,
      "loss": 1.0279,
      "step": 436
    },
    {
      "epoch": 0.05558028616852146,
      "grad_norm": 2.0785696506500244,
      "learning_rate": 0.00018900343642611685,
      "loss": 0.9333,
      "step": 437
    },
    {
      "epoch": 0.055707472178060415,
      "grad_norm": 2.0862574577331543,
      "learning_rate": 0.00018897798141784397,
      "loss": 0.7823,
      "step": 438
    },
    {
      "epoch": 0.05583465818759936,
      "grad_norm": 2.0596165657043457,
      "learning_rate": 0.0001889525264095711,
      "loss": 0.8061,
      "step": 439
    },
    {
      "epoch": 0.055961844197138316,
      "grad_norm": 1.6640400886535645,
      "learning_rate": 0.0001889270714012982,
      "loss": 0.7747,
      "step": 440
    },
    {
      "epoch": 0.056089030206677264,
      "grad_norm": 2.3208320140838623,
      "learning_rate": 0.00018890161639302533,
      "loss": 1.0882,
      "step": 441
    },
    {
      "epoch": 0.05621621621621622,
      "grad_norm": 1.6930716037750244,
      "learning_rate": 0.00018887616138475247,
      "loss": 0.7289,
      "step": 442
    },
    {
      "epoch": 0.056343402225755165,
      "grad_norm": 1.539743185043335,
      "learning_rate": 0.0001888507063764796,
      "loss": 0.9188,
      "step": 443
    },
    {
      "epoch": 0.05647058823529412,
      "grad_norm": 1.369856357574463,
      "learning_rate": 0.0001888252513682067,
      "loss": 0.9205,
      "step": 444
    },
    {
      "epoch": 0.05659777424483307,
      "grad_norm": 1.430885910987854,
      "learning_rate": 0.00018879979635993383,
      "loss": 0.9024,
      "step": 445
    },
    {
      "epoch": 0.05672496025437202,
      "grad_norm": 1.5964986085891724,
      "learning_rate": 0.00018877434135166095,
      "loss": 1.0073,
      "step": 446
    },
    {
      "epoch": 0.05685214626391097,
      "grad_norm": 2.0646045207977295,
      "learning_rate": 0.00018874888634338806,
      "loss": 0.8378,
      "step": 447
    },
    {
      "epoch": 0.05697933227344992,
      "grad_norm": 1.557852029800415,
      "learning_rate": 0.00018872343133511518,
      "loss": 0.8081,
      "step": 448
    },
    {
      "epoch": 0.05710651828298887,
      "grad_norm": 2.0353968143463135,
      "learning_rate": 0.00018869797632684233,
      "loss": 1.0003,
      "step": 449
    },
    {
      "epoch": 0.057233704292527825,
      "grad_norm": 2.34513521194458,
      "learning_rate": 0.00018867252131856942,
      "loss": 0.7458,
      "step": 450
    },
    {
      "epoch": 0.05736089030206677,
      "grad_norm": 1.669046401977539,
      "learning_rate": 0.00018864706631029657,
      "loss": 0.956,
      "step": 451
    },
    {
      "epoch": 0.057488076311605726,
      "grad_norm": 1.6772761344909668,
      "learning_rate": 0.00018862161130202369,
      "loss": 0.9239,
      "step": 452
    },
    {
      "epoch": 0.057615262321144674,
      "grad_norm": 1.813186526298523,
      "learning_rate": 0.0001885961562937508,
      "loss": 0.9115,
      "step": 453
    },
    {
      "epoch": 0.05774244833068363,
      "grad_norm": 1.6531074047088623,
      "learning_rate": 0.00018857070128547792,
      "loss": 0.7467,
      "step": 454
    },
    {
      "epoch": 0.057869634340222575,
      "grad_norm": 2.1067349910736084,
      "learning_rate": 0.00018854524627720504,
      "loss": 0.7972,
      "step": 455
    },
    {
      "epoch": 0.05799682034976152,
      "grad_norm": 1.7056173086166382,
      "learning_rate": 0.0001885197912689322,
      "loss": 0.961,
      "step": 456
    },
    {
      "epoch": 0.05812400635930048,
      "grad_norm": 1.6013503074645996,
      "learning_rate": 0.00018849433626065928,
      "loss": 0.9973,
      "step": 457
    },
    {
      "epoch": 0.058251192368839425,
      "grad_norm": 2.310380458831787,
      "learning_rate": 0.00018846888125238642,
      "loss": 1.0362,
      "step": 458
    },
    {
      "epoch": 0.05837837837837838,
      "grad_norm": 2.1604409217834473,
      "learning_rate": 0.00018844342624411352,
      "loss": 1.1897,
      "step": 459
    },
    {
      "epoch": 0.058505564387917326,
      "grad_norm": 1.625042200088501,
      "learning_rate": 0.00018841797123584066,
      "loss": 0.8919,
      "step": 460
    },
    {
      "epoch": 0.05863275039745628,
      "grad_norm": 2.0293216705322266,
      "learning_rate": 0.00018839251622756778,
      "loss": 1.0626,
      "step": 461
    },
    {
      "epoch": 0.05875993640699523,
      "grad_norm": 2.0514960289001465,
      "learning_rate": 0.0001883670612192949,
      "loss": 0.8737,
      "step": 462
    },
    {
      "epoch": 0.05888712241653418,
      "grad_norm": 1.2049468755722046,
      "learning_rate": 0.00018834160621102205,
      "loss": 0.719,
      "step": 463
    },
    {
      "epoch": 0.05901430842607313,
      "grad_norm": 1.433740496635437,
      "learning_rate": 0.00018831615120274914,
      "loss": 0.664,
      "step": 464
    },
    {
      "epoch": 0.059141494435612084,
      "grad_norm": 2.3237688541412354,
      "learning_rate": 0.00018829069619447628,
      "loss": 0.8679,
      "step": 465
    },
    {
      "epoch": 0.05926868044515103,
      "grad_norm": 1.7303941249847412,
      "learning_rate": 0.0001882652411862034,
      "loss": 0.8902,
      "step": 466
    },
    {
      "epoch": 0.059395866454689986,
      "grad_norm": 2.1381497383117676,
      "learning_rate": 0.00018823978617793052,
      "loss": 1.0439,
      "step": 467
    },
    {
      "epoch": 0.05952305246422893,
      "grad_norm": 2.332277536392212,
      "learning_rate": 0.00018821433116965764,
      "loss": 0.9968,
      "step": 468
    },
    {
      "epoch": 0.05965023847376789,
      "grad_norm": 2.316132068634033,
      "learning_rate": 0.00018818887616138476,
      "loss": 0.8781,
      "step": 469
    },
    {
      "epoch": 0.059777424483306835,
      "grad_norm": 1.9305967092514038,
      "learning_rate": 0.00018816342115311188,
      "loss": 0.835,
      "step": 470
    },
    {
      "epoch": 0.05990461049284579,
      "grad_norm": 2.3119540214538574,
      "learning_rate": 0.000188137966144839,
      "loss": 0.9095,
      "step": 471
    },
    {
      "epoch": 0.060031796502384736,
      "grad_norm": 1.8897631168365479,
      "learning_rate": 0.00018811251113656614,
      "loss": 0.7511,
      "step": 472
    },
    {
      "epoch": 0.06015898251192369,
      "grad_norm": 1.7591286897659302,
      "learning_rate": 0.00018808705612829326,
      "loss": 0.8173,
      "step": 473
    },
    {
      "epoch": 0.06028616852146264,
      "grad_norm": 1.741403341293335,
      "learning_rate": 0.00018806160112002038,
      "loss": 0.8474,
      "step": 474
    },
    {
      "epoch": 0.06041335453100159,
      "grad_norm": 2.608015537261963,
      "learning_rate": 0.0001880361461117475,
      "loss": 0.7308,
      "step": 475
    },
    {
      "epoch": 0.06054054054054054,
      "grad_norm": 1.5233354568481445,
      "learning_rate": 0.00018801069110347462,
      "loss": 0.752,
      "step": 476
    },
    {
      "epoch": 0.060667726550079494,
      "grad_norm": 1.7868788242340088,
      "learning_rate": 0.00018798523609520174,
      "loss": 0.6916,
      "step": 477
    },
    {
      "epoch": 0.06079491255961844,
      "grad_norm": 1.7068514823913574,
      "learning_rate": 0.00018795978108692885,
      "loss": 0.814,
      "step": 478
    },
    {
      "epoch": 0.060922098569157396,
      "grad_norm": 2.2879607677459717,
      "learning_rate": 0.00018793432607865597,
      "loss": 0.8469,
      "step": 479
    },
    {
      "epoch": 0.06104928457869634,
      "grad_norm": 2.9909520149230957,
      "learning_rate": 0.00018790887107038312,
      "loss": 0.9572,
      "step": 480
    },
    {
      "epoch": 0.0611764705882353,
      "grad_norm": 1.9166553020477295,
      "learning_rate": 0.00018788341606211024,
      "loss": 0.7434,
      "step": 481
    },
    {
      "epoch": 0.061303656597774245,
      "grad_norm": 2.864243268966675,
      "learning_rate": 0.00018785796105383736,
      "loss": 1.1535,
      "step": 482
    },
    {
      "epoch": 0.06143084260731319,
      "grad_norm": 2.150804042816162,
      "learning_rate": 0.00018783250604556447,
      "loss": 1.0383,
      "step": 483
    },
    {
      "epoch": 0.061558028616852146,
      "grad_norm": 3.3195955753326416,
      "learning_rate": 0.0001878070510372916,
      "loss": 0.8818,
      "step": 484
    },
    {
      "epoch": 0.061685214626391094,
      "grad_norm": 2.5192739963531494,
      "learning_rate": 0.0001877815960290187,
      "loss": 0.8118,
      "step": 485
    },
    {
      "epoch": 0.06181240063593005,
      "grad_norm": 1.8349074125289917,
      "learning_rate": 0.00018775614102074583,
      "loss": 0.8451,
      "step": 486
    },
    {
      "epoch": 0.061939586645468996,
      "grad_norm": 2.3829500675201416,
      "learning_rate": 0.00018773068601247298,
      "loss": 0.7506,
      "step": 487
    },
    {
      "epoch": 0.06206677265500795,
      "grad_norm": 1.93537175655365,
      "learning_rate": 0.00018770523100420007,
      "loss": 1.0356,
      "step": 488
    },
    {
      "epoch": 0.0621939586645469,
      "grad_norm": 1.8995493650436401,
      "learning_rate": 0.00018767977599592721,
      "loss": 0.9021,
      "step": 489
    },
    {
      "epoch": 0.06232114467408585,
      "grad_norm": 2.4513792991638184,
      "learning_rate": 0.00018765432098765433,
      "loss": 0.9472,
      "step": 490
    },
    {
      "epoch": 0.0624483306836248,
      "grad_norm": 2.1172475814819336,
      "learning_rate": 0.00018762886597938145,
      "loss": 1.0017,
      "step": 491
    },
    {
      "epoch": 0.06257551669316375,
      "grad_norm": 2.4516632556915283,
      "learning_rate": 0.0001876034109711086,
      "loss": 0.833,
      "step": 492
    },
    {
      "epoch": 0.0627027027027027,
      "grad_norm": 2.030259132385254,
      "learning_rate": 0.0001875779559628357,
      "loss": 0.8261,
      "step": 493
    },
    {
      "epoch": 0.06282988871224165,
      "grad_norm": 1.6942309141159058,
      "learning_rate": 0.00018755250095456284,
      "loss": 1.0154,
      "step": 494
    },
    {
      "epoch": 0.06295707472178061,
      "grad_norm": 1.8566113710403442,
      "learning_rate": 0.00018752704594628993,
      "loss": 0.9305,
      "step": 495
    },
    {
      "epoch": 0.06308426073131955,
      "grad_norm": 1.623138189315796,
      "learning_rate": 0.00018750159093801707,
      "loss": 0.6575,
      "step": 496
    },
    {
      "epoch": 0.0632114467408585,
      "grad_norm": 2.327333927154541,
      "learning_rate": 0.0001874761359297442,
      "loss": 0.8728,
      "step": 497
    },
    {
      "epoch": 0.06333863275039746,
      "grad_norm": 1.6772186756134033,
      "learning_rate": 0.0001874506809214713,
      "loss": 0.9087,
      "step": 498
    },
    {
      "epoch": 0.06346581875993641,
      "grad_norm": 1.7690906524658203,
      "learning_rate": 0.00018742522591319843,
      "loss": 0.8776,
      "step": 499
    },
    {
      "epoch": 0.06359300476947535,
      "grad_norm": 2.501124620437622,
      "learning_rate": 0.00018739977090492555,
      "loss": 0.9876,
      "step": 500
    },
    {
      "epoch": 0.06372019077901431,
      "grad_norm": 1.7216092348098755,
      "learning_rate": 0.0001873743158966527,
      "loss": 1.1198,
      "step": 501
    },
    {
      "epoch": 0.06384737678855326,
      "grad_norm": 2.1522035598754883,
      "learning_rate": 0.00018734886088837978,
      "loss": 1.2254,
      "step": 502
    },
    {
      "epoch": 0.06397456279809222,
      "grad_norm": 1.5514198541641235,
      "learning_rate": 0.00018732340588010693,
      "loss": 0.828,
      "step": 503
    },
    {
      "epoch": 0.06410174880763116,
      "grad_norm": 1.2214096784591675,
      "learning_rate": 0.00018729795087183405,
      "loss": 0.6278,
      "step": 504
    },
    {
      "epoch": 0.06422893481717011,
      "grad_norm": 1.9821103811264038,
      "learning_rate": 0.00018727249586356117,
      "loss": 1.0297,
      "step": 505
    },
    {
      "epoch": 0.06435612082670907,
      "grad_norm": 1.9913395643234253,
      "learning_rate": 0.0001872470408552883,
      "loss": 0.9975,
      "step": 506
    },
    {
      "epoch": 0.06448330683624802,
      "grad_norm": 1.64852774143219,
      "learning_rate": 0.0001872215858470154,
      "loss": 0.6583,
      "step": 507
    },
    {
      "epoch": 0.06461049284578696,
      "grad_norm": 2.308598756790161,
      "learning_rate": 0.00018719613083874252,
      "loss": 1.0462,
      "step": 508
    },
    {
      "epoch": 0.06473767885532591,
      "grad_norm": 2.2703776359558105,
      "learning_rate": 0.00018717067583046964,
      "loss": 1.062,
      "step": 509
    },
    {
      "epoch": 0.06486486486486487,
      "grad_norm": 2.043283700942993,
      "learning_rate": 0.00018714522082219676,
      "loss": 1.1848,
      "step": 510
    },
    {
      "epoch": 0.06499205087440381,
      "grad_norm": 1.581076979637146,
      "learning_rate": 0.0001871197658139239,
      "loss": 0.829,
      "step": 511
    },
    {
      "epoch": 0.06511923688394276,
      "grad_norm": 1.54107666015625,
      "learning_rate": 0.00018709431080565103,
      "loss": 0.9057,
      "step": 512
    },
    {
      "epoch": 0.06524642289348172,
      "grad_norm": 2.184509515762329,
      "learning_rate": 0.00018706885579737815,
      "loss": 0.9837,
      "step": 513
    },
    {
      "epoch": 0.06537360890302067,
      "grad_norm": 1.817252516746521,
      "learning_rate": 0.00018704340078910526,
      "loss": 0.7857,
      "step": 514
    },
    {
      "epoch": 0.06550079491255961,
      "grad_norm": 1.8483153581619263,
      "learning_rate": 0.00018701794578083238,
      "loss": 1.0649,
      "step": 515
    },
    {
      "epoch": 0.06562798092209857,
      "grad_norm": 1.4501742124557495,
      "learning_rate": 0.0001869924907725595,
      "loss": 0.9488,
      "step": 516
    },
    {
      "epoch": 0.06575516693163752,
      "grad_norm": 1.5463829040527344,
      "learning_rate": 0.00018696703576428662,
      "loss": 0.7442,
      "step": 517
    },
    {
      "epoch": 0.06588235294117648,
      "grad_norm": 1.2230924367904663,
      "learning_rate": 0.00018694158075601377,
      "loss": 0.8137,
      "step": 518
    },
    {
      "epoch": 0.06600953895071542,
      "grad_norm": 1.673723578453064,
      "learning_rate": 0.00018691612574774086,
      "loss": 0.8978,
      "step": 519
    },
    {
      "epoch": 0.06613672496025437,
      "grad_norm": 2.2330322265625,
      "learning_rate": 0.000186890670739468,
      "loss": 0.8884,
      "step": 520
    },
    {
      "epoch": 0.06626391096979332,
      "grad_norm": 1.5813747644424438,
      "learning_rate": 0.00018686521573119512,
      "loss": 0.7622,
      "step": 521
    },
    {
      "epoch": 0.06639109697933228,
      "grad_norm": 1.8395178318023682,
      "learning_rate": 0.00018683976072292224,
      "loss": 0.6718,
      "step": 522
    },
    {
      "epoch": 0.06651828298887122,
      "grad_norm": 2.5382139682769775,
      "learning_rate": 0.0001868143057146494,
      "loss": 1.0507,
      "step": 523
    },
    {
      "epoch": 0.06664546899841017,
      "grad_norm": 2.3782572746276855,
      "learning_rate": 0.00018678885070637648,
      "loss": 0.933,
      "step": 524
    },
    {
      "epoch": 0.06677265500794913,
      "grad_norm": 2.435149908065796,
      "learning_rate": 0.00018676339569810362,
      "loss": 0.9451,
      "step": 525
    },
    {
      "epoch": 0.06689984101748808,
      "grad_norm": 1.6368309259414673,
      "learning_rate": 0.00018673794068983072,
      "loss": 0.9844,
      "step": 526
    },
    {
      "epoch": 0.06702702702702702,
      "grad_norm": 1.9915794134140015,
      "learning_rate": 0.00018671248568155786,
      "loss": 0.8369,
      "step": 527
    },
    {
      "epoch": 0.06715421303656598,
      "grad_norm": 2.0404903888702393,
      "learning_rate": 0.00018668703067328498,
      "loss": 0.8247,
      "step": 528
    },
    {
      "epoch": 0.06728139904610493,
      "grad_norm": 1.8578819036483765,
      "learning_rate": 0.0001866615756650121,
      "loss": 1.2415,
      "step": 529
    },
    {
      "epoch": 0.06740858505564389,
      "grad_norm": 2.0410211086273193,
      "learning_rate": 0.00018663612065673925,
      "loss": 0.8365,
      "step": 530
    },
    {
      "epoch": 0.06753577106518283,
      "grad_norm": 1.4694796800613403,
      "learning_rate": 0.00018661066564846634,
      "loss": 0.9226,
      "step": 531
    },
    {
      "epoch": 0.06766295707472178,
      "grad_norm": 1.7098369598388672,
      "learning_rate": 0.00018658521064019348,
      "loss": 0.8761,
      "step": 532
    },
    {
      "epoch": 0.06779014308426073,
      "grad_norm": 2.2011051177978516,
      "learning_rate": 0.00018655975563192057,
      "loss": 0.814,
      "step": 533
    },
    {
      "epoch": 0.06791732909379969,
      "grad_norm": 1.5582599639892578,
      "learning_rate": 0.00018653430062364772,
      "loss": 0.7732,
      "step": 534
    },
    {
      "epoch": 0.06804451510333863,
      "grad_norm": 1.8139017820358276,
      "learning_rate": 0.00018650884561537484,
      "loss": 0.6204,
      "step": 535
    },
    {
      "epoch": 0.06817170111287758,
      "grad_norm": 1.5136126279830933,
      "learning_rate": 0.00018648339060710196,
      "loss": 0.7524,
      "step": 536
    },
    {
      "epoch": 0.06829888712241654,
      "grad_norm": 1.8772250413894653,
      "learning_rate": 0.00018645793559882908,
      "loss": 1.0915,
      "step": 537
    },
    {
      "epoch": 0.06842607313195548,
      "grad_norm": 2.1407711505889893,
      "learning_rate": 0.0001864324805905562,
      "loss": 0.9054,
      "step": 538
    },
    {
      "epoch": 0.06855325914149443,
      "grad_norm": 1.7656168937683105,
      "learning_rate": 0.00018640702558228331,
      "loss": 0.8092,
      "step": 539
    },
    {
      "epoch": 0.06868044515103339,
      "grad_norm": 2.4513864517211914,
      "learning_rate": 0.00018638157057401043,
      "loss": 1.3548,
      "step": 540
    },
    {
      "epoch": 0.06880763116057234,
      "grad_norm": 1.6253708600997925,
      "learning_rate": 0.00018635611556573758,
      "loss": 1.0446,
      "step": 541
    },
    {
      "epoch": 0.06893481717011128,
      "grad_norm": 1.9253513813018799,
      "learning_rate": 0.0001863306605574647,
      "loss": 0.8499,
      "step": 542
    },
    {
      "epoch": 0.06906200317965024,
      "grad_norm": 2.001598596572876,
      "learning_rate": 0.00018630520554919182,
      "loss": 0.9185,
      "step": 543
    },
    {
      "epoch": 0.06918918918918919,
      "grad_norm": 1.5407750606536865,
      "learning_rate": 0.00018627975054091893,
      "loss": 0.995,
      "step": 544
    },
    {
      "epoch": 0.06931637519872814,
      "grad_norm": 1.6453412771224976,
      "learning_rate": 0.00018625429553264605,
      "loss": 0.7178,
      "step": 545
    },
    {
      "epoch": 0.06944356120826709,
      "grad_norm": 2.239044189453125,
      "learning_rate": 0.00018622884052437317,
      "loss": 0.7497,
      "step": 546
    },
    {
      "epoch": 0.06957074721780604,
      "grad_norm": 2.6162285804748535,
      "learning_rate": 0.0001862033855161003,
      "loss": 0.9336,
      "step": 547
    },
    {
      "epoch": 0.069697933227345,
      "grad_norm": 2.3324520587921143,
      "learning_rate": 0.0001861779305078274,
      "loss": 0.7446,
      "step": 548
    },
    {
      "epoch": 0.06982511923688395,
      "grad_norm": 1.6366578340530396,
      "learning_rate": 0.00018615247549955456,
      "loss": 0.6242,
      "step": 549
    },
    {
      "epoch": 0.06995230524642289,
      "grad_norm": 2.0813121795654297,
      "learning_rate": 0.00018612702049128167,
      "loss": 0.8734,
      "step": 550
    },
    {
      "epoch": 0.07007949125596184,
      "grad_norm": 2.6620705127716064,
      "learning_rate": 0.0001861015654830088,
      "loss": 1.3285,
      "step": 551
    },
    {
      "epoch": 0.0702066772655008,
      "grad_norm": 2.4683873653411865,
      "learning_rate": 0.0001860761104747359,
      "loss": 0.8226,
      "step": 552
    },
    {
      "epoch": 0.07033386327503975,
      "grad_norm": 2.0723564624786377,
      "learning_rate": 0.00018605065546646303,
      "loss": 0.8483,
      "step": 553
    },
    {
      "epoch": 0.07046104928457869,
      "grad_norm": 2.0719032287597656,
      "learning_rate": 0.00018602520045819018,
      "loss": 1.2004,
      "step": 554
    },
    {
      "epoch": 0.07058823529411765,
      "grad_norm": 1.8483219146728516,
      "learning_rate": 0.00018599974544991727,
      "loss": 0.851,
      "step": 555
    },
    {
      "epoch": 0.0707154213036566,
      "grad_norm": 1.8137807846069336,
      "learning_rate": 0.00018597429044164441,
      "loss": 0.7958,
      "step": 556
    },
    {
      "epoch": 0.07084260731319555,
      "grad_norm": 1.5429682731628418,
      "learning_rate": 0.0001859488354333715,
      "loss": 0.8384,
      "step": 557
    },
    {
      "epoch": 0.0709697933227345,
      "grad_norm": 2.21653151512146,
      "learning_rate": 0.00018592338042509865,
      "loss": 0.9359,
      "step": 558
    },
    {
      "epoch": 0.07109697933227345,
      "grad_norm": 2.191702365875244,
      "learning_rate": 0.00018589792541682577,
      "loss": 0.7477,
      "step": 559
    },
    {
      "epoch": 0.0712241653418124,
      "grad_norm": 1.9169849157333374,
      "learning_rate": 0.0001858724704085529,
      "loss": 0.7652,
      "step": 560
    },
    {
      "epoch": 0.07135135135135136,
      "grad_norm": 1.6203668117523193,
      "learning_rate": 0.00018584701540028003,
      "loss": 0.9034,
      "step": 561
    },
    {
      "epoch": 0.0714785373608903,
      "grad_norm": 1.8605599403381348,
      "learning_rate": 0.00018582156039200713,
      "loss": 1.0076,
      "step": 562
    },
    {
      "epoch": 0.07160572337042925,
      "grad_norm": 1.923280119895935,
      "learning_rate": 0.00018579610538373427,
      "loss": 1.1336,
      "step": 563
    },
    {
      "epoch": 0.07173290937996821,
      "grad_norm": 2.5576553344726562,
      "learning_rate": 0.00018577065037546136,
      "loss": 1.0589,
      "step": 564
    },
    {
      "epoch": 0.07186009538950715,
      "grad_norm": 1.545425534248352,
      "learning_rate": 0.0001857451953671885,
      "loss": 0.7974,
      "step": 565
    },
    {
      "epoch": 0.0719872813990461,
      "grad_norm": 1.4957393407821655,
      "learning_rate": 0.00018571974035891563,
      "loss": 0.8879,
      "step": 566
    },
    {
      "epoch": 0.07211446740858506,
      "grad_norm": 1.7759391069412231,
      "learning_rate": 0.00018569428535064275,
      "loss": 0.9276,
      "step": 567
    },
    {
      "epoch": 0.07224165341812401,
      "grad_norm": 2.196702241897583,
      "learning_rate": 0.00018566883034236987,
      "loss": 0.8606,
      "step": 568
    },
    {
      "epoch": 0.07236883942766295,
      "grad_norm": 2.209186315536499,
      "learning_rate": 0.00018564337533409698,
      "loss": 0.9778,
      "step": 569
    },
    {
      "epoch": 0.0724960254372019,
      "grad_norm": 2.092081308364868,
      "learning_rate": 0.00018561792032582413,
      "loss": 0.8851,
      "step": 570
    },
    {
      "epoch": 0.07262321144674086,
      "grad_norm": 2.0238680839538574,
      "learning_rate": 0.00018559246531755122,
      "loss": 0.9364,
      "step": 571
    },
    {
      "epoch": 0.07275039745627981,
      "grad_norm": 1.4206727743148804,
      "learning_rate": 0.00018556701030927837,
      "loss": 0.8511,
      "step": 572
    },
    {
      "epoch": 0.07287758346581875,
      "grad_norm": 1.4409382343292236,
      "learning_rate": 0.0001855415553010055,
      "loss": 0.808,
      "step": 573
    },
    {
      "epoch": 0.07300476947535771,
      "grad_norm": 2.4185330867767334,
      "learning_rate": 0.0001855161002927326,
      "loss": 0.9489,
      "step": 574
    },
    {
      "epoch": 0.07313195548489666,
      "grad_norm": 1.7264814376831055,
      "learning_rate": 0.00018549064528445972,
      "loss": 0.7552,
      "step": 575
    },
    {
      "epoch": 0.07325914149443562,
      "grad_norm": 1.5561702251434326,
      "learning_rate": 0.00018546519027618684,
      "loss": 0.7989,
      "step": 576
    },
    {
      "epoch": 0.07338632750397456,
      "grad_norm": 1.4541901350021362,
      "learning_rate": 0.00018543973526791396,
      "loss": 0.8777,
      "step": 577
    },
    {
      "epoch": 0.07351351351351351,
      "grad_norm": 2.6625139713287354,
      "learning_rate": 0.00018541428025964108,
      "loss": 0.8084,
      "step": 578
    },
    {
      "epoch": 0.07364069952305247,
      "grad_norm": 2.3901162147521973,
      "learning_rate": 0.00018538882525136823,
      "loss": 1.0072,
      "step": 579
    },
    {
      "epoch": 0.07376788553259142,
      "grad_norm": 1.733461618423462,
      "learning_rate": 0.00018536337024309534,
      "loss": 0.9021,
      "step": 580
    },
    {
      "epoch": 0.07389507154213036,
      "grad_norm": 1.8002657890319824,
      "learning_rate": 0.00018533791523482246,
      "loss": 0.7773,
      "step": 581
    },
    {
      "epoch": 0.07402225755166932,
      "grad_norm": 2.0245907306671143,
      "learning_rate": 0.00018531246022654958,
      "loss": 0.884,
      "step": 582
    },
    {
      "epoch": 0.07414944356120827,
      "grad_norm": 1.396533489227295,
      "learning_rate": 0.0001852870052182767,
      "loss": 0.6637,
      "step": 583
    },
    {
      "epoch": 0.07427662957074722,
      "grad_norm": 2.0563504695892334,
      "learning_rate": 0.00018526155021000382,
      "loss": 0.9204,
      "step": 584
    },
    {
      "epoch": 0.07440381558028616,
      "grad_norm": 2.505108594894409,
      "learning_rate": 0.00018523609520173097,
      "loss": 0.9743,
      "step": 585
    },
    {
      "epoch": 0.07453100158982512,
      "grad_norm": 1.535110354423523,
      "learning_rate": 0.00018521064019345806,
      "loss": 0.7774,
      "step": 586
    },
    {
      "epoch": 0.07465818759936407,
      "grad_norm": 1.6265268325805664,
      "learning_rate": 0.0001851851851851852,
      "loss": 0.9117,
      "step": 587
    },
    {
      "epoch": 0.07478537360890303,
      "grad_norm": 2.2108049392700195,
      "learning_rate": 0.00018515973017691232,
      "loss": 1.108,
      "step": 588
    },
    {
      "epoch": 0.07491255961844197,
      "grad_norm": 1.8069086074829102,
      "learning_rate": 0.00018513427516863944,
      "loss": 0.9812,
      "step": 589
    },
    {
      "epoch": 0.07503974562798092,
      "grad_norm": 2.1152467727661133,
      "learning_rate": 0.00018510882016036656,
      "loss": 0.9383,
      "step": 590
    },
    {
      "epoch": 0.07516693163751988,
      "grad_norm": 2.19840407371521,
      "learning_rate": 0.00018508336515209368,
      "loss": 0.9119,
      "step": 591
    },
    {
      "epoch": 0.07529411764705882,
      "grad_norm": 2.025627374649048,
      "learning_rate": 0.00018505791014382082,
      "loss": 1.2059,
      "step": 592
    },
    {
      "epoch": 0.07542130365659777,
      "grad_norm": 1.7841097116470337,
      "learning_rate": 0.00018503245513554792,
      "loss": 0.9097,
      "step": 593
    },
    {
      "epoch": 0.07554848966613673,
      "grad_norm": 2.19004487991333,
      "learning_rate": 0.00018500700012727506,
      "loss": 1.0223,
      "step": 594
    },
    {
      "epoch": 0.07567567567567568,
      "grad_norm": 1.7926933765411377,
      "learning_rate": 0.00018498154511900215,
      "loss": 0.9508,
      "step": 595
    },
    {
      "epoch": 0.07580286168521462,
      "grad_norm": 1.7780911922454834,
      "learning_rate": 0.0001849560901107293,
      "loss": 0.8901,
      "step": 596
    },
    {
      "epoch": 0.07593004769475357,
      "grad_norm": 1.6833356618881226,
      "learning_rate": 0.00018493063510245642,
      "loss": 0.5938,
      "step": 597
    },
    {
      "epoch": 0.07605723370429253,
      "grad_norm": 1.919776201248169,
      "learning_rate": 0.00018490518009418354,
      "loss": 0.8051,
      "step": 598
    },
    {
      "epoch": 0.07618441971383148,
      "grad_norm": 1.754972219467163,
      "learning_rate": 0.00018487972508591068,
      "loss": 0.9982,
      "step": 599
    },
    {
      "epoch": 0.07631160572337042,
      "grad_norm": 1.4550585746765137,
      "learning_rate": 0.00018485427007763777,
      "loss": 0.5742,
      "step": 600
    },
    {
      "epoch": 0.07643879173290938,
      "grad_norm": 2.237449884414673,
      "learning_rate": 0.00018482881506936492,
      "loss": 0.8568,
      "step": 601
    },
    {
      "epoch": 0.07656597774244833,
      "grad_norm": 1.8917667865753174,
      "learning_rate": 0.000184803360061092,
      "loss": 0.7929,
      "step": 602
    },
    {
      "epoch": 0.07669316375198729,
      "grad_norm": 2.1312062740325928,
      "learning_rate": 0.00018477790505281916,
      "loss": 0.7658,
      "step": 603
    },
    {
      "epoch": 0.07682034976152623,
      "grad_norm": 1.825700283050537,
      "learning_rate": 0.00018475245004454628,
      "loss": 0.9069,
      "step": 604
    },
    {
      "epoch": 0.07694753577106518,
      "grad_norm": 2.1939241886138916,
      "learning_rate": 0.0001847269950362734,
      "loss": 0.8944,
      "step": 605
    },
    {
      "epoch": 0.07707472178060414,
      "grad_norm": 1.6969691514968872,
      "learning_rate": 0.0001847015400280005,
      "loss": 0.7176,
      "step": 606
    },
    {
      "epoch": 0.07720190779014309,
      "grad_norm": 2.2784299850463867,
      "learning_rate": 0.00018467608501972763,
      "loss": 0.846,
      "step": 607
    },
    {
      "epoch": 0.07732909379968203,
      "grad_norm": 2.0995397567749023,
      "learning_rate": 0.00018465063001145478,
      "loss": 1.0045,
      "step": 608
    },
    {
      "epoch": 0.07745627980922098,
      "grad_norm": 1.6884924173355103,
      "learning_rate": 0.00018462517500318187,
      "loss": 0.9568,
      "step": 609
    },
    {
      "epoch": 0.07758346581875994,
      "grad_norm": 2.0271549224853516,
      "learning_rate": 0.00018459971999490902,
      "loss": 0.8932,
      "step": 610
    },
    {
      "epoch": 0.0777106518282989,
      "grad_norm": 1.9055994749069214,
      "learning_rate": 0.00018457426498663613,
      "loss": 1.0199,
      "step": 611
    },
    {
      "epoch": 0.07783783783783783,
      "grad_norm": 1.655145287513733,
      "learning_rate": 0.00018454880997836325,
      "loss": 0.8128,
      "step": 612
    },
    {
      "epoch": 0.07796502384737679,
      "grad_norm": 2.0708367824554443,
      "learning_rate": 0.00018452335497009037,
      "loss": 0.9118,
      "step": 613
    },
    {
      "epoch": 0.07809220985691574,
      "grad_norm": 2.293285846710205,
      "learning_rate": 0.0001844978999618175,
      "loss": 0.821,
      "step": 614
    },
    {
      "epoch": 0.0782193958664547,
      "grad_norm": 1.4293795824050903,
      "learning_rate": 0.0001844724449535446,
      "loss": 0.5319,
      "step": 615
    },
    {
      "epoch": 0.07834658187599364,
      "grad_norm": 1.7495605945587158,
      "learning_rate": 0.00018444698994527175,
      "loss": 0.5745,
      "step": 616
    },
    {
      "epoch": 0.07847376788553259,
      "grad_norm": 1.7062475681304932,
      "learning_rate": 0.00018442153493699887,
      "loss": 0.8189,
      "step": 617
    },
    {
      "epoch": 0.07860095389507155,
      "grad_norm": 2.400190591812134,
      "learning_rate": 0.000184396079928726,
      "loss": 1.1996,
      "step": 618
    },
    {
      "epoch": 0.07872813990461049,
      "grad_norm": 1.9940228462219238,
      "learning_rate": 0.0001843706249204531,
      "loss": 1.2799,
      "step": 619
    },
    {
      "epoch": 0.07885532591414944,
      "grad_norm": 1.5071890354156494,
      "learning_rate": 0.00018434516991218023,
      "loss": 0.6139,
      "step": 620
    },
    {
      "epoch": 0.0789825119236884,
      "grad_norm": 1.9308849573135376,
      "learning_rate": 0.00018431971490390735,
      "loss": 0.6112,
      "step": 621
    },
    {
      "epoch": 0.07910969793322735,
      "grad_norm": 1.5519901514053345,
      "learning_rate": 0.00018429425989563447,
      "loss": 0.9123,
      "step": 622
    },
    {
      "epoch": 0.07923688394276629,
      "grad_norm": 1.6804845333099365,
      "learning_rate": 0.0001842688048873616,
      "loss": 0.8599,
      "step": 623
    },
    {
      "epoch": 0.07936406995230524,
      "grad_norm": 1.7860277891159058,
      "learning_rate": 0.0001842433498790887,
      "loss": 1.0164,
      "step": 624
    },
    {
      "epoch": 0.0794912559618442,
      "grad_norm": 1.374133825302124,
      "learning_rate": 0.00018421789487081585,
      "loss": 0.765,
      "step": 625
    },
    {
      "epoch": 0.07961844197138315,
      "grad_norm": 1.939441442489624,
      "learning_rate": 0.00018419243986254294,
      "loss": 1.1629,
      "step": 626
    },
    {
      "epoch": 0.07974562798092209,
      "grad_norm": 1.3933378458023071,
      "learning_rate": 0.0001841669848542701,
      "loss": 0.7102,
      "step": 627
    },
    {
      "epoch": 0.07987281399046105,
      "grad_norm": 1.6068830490112305,
      "learning_rate": 0.0001841415298459972,
      "loss": 0.8301,
      "step": 628
    },
    {
      "epoch": 0.08,
      "grad_norm": 2.769893169403076,
      "learning_rate": 0.00018411607483772433,
      "loss": 1.1063,
      "step": 629
    },
    {
      "epoch": 0.08012718600953896,
      "grad_norm": 2.2454850673675537,
      "learning_rate": 0.00018409061982945147,
      "loss": 0.9784,
      "step": 630
    },
    {
      "epoch": 0.0802543720190779,
      "grad_norm": 1.633183240890503,
      "learning_rate": 0.00018406516482117856,
      "loss": 0.7685,
      "step": 631
    },
    {
      "epoch": 0.08038155802861685,
      "grad_norm": 1.7489738464355469,
      "learning_rate": 0.0001840397098129057,
      "loss": 1.1499,
      "step": 632
    },
    {
      "epoch": 0.0805087440381558,
      "grad_norm": 2.8041696548461914,
      "learning_rate": 0.0001840142548046328,
      "loss": 0.9846,
      "step": 633
    },
    {
      "epoch": 0.08063593004769476,
      "grad_norm": 2.4032320976257324,
      "learning_rate": 0.00018398879979635995,
      "loss": 0.9664,
      "step": 634
    },
    {
      "epoch": 0.0807631160572337,
      "grad_norm": 1.655434012413025,
      "learning_rate": 0.00018396334478808707,
      "loss": 1.1482,
      "step": 635
    },
    {
      "epoch": 0.08089030206677265,
      "grad_norm": 2.3803036212921143,
      "learning_rate": 0.00018393788977981418,
      "loss": 0.7643,
      "step": 636
    },
    {
      "epoch": 0.08101748807631161,
      "grad_norm": 1.483502745628357,
      "learning_rate": 0.00018391243477154133,
      "loss": 0.7979,
      "step": 637
    },
    {
      "epoch": 0.08114467408585056,
      "grad_norm": 1.7797880172729492,
      "learning_rate": 0.00018388697976326842,
      "loss": 1.0365,
      "step": 638
    },
    {
      "epoch": 0.0812718600953895,
      "grad_norm": 2.0064961910247803,
      "learning_rate": 0.00018386152475499557,
      "loss": 0.8723,
      "step": 639
    },
    {
      "epoch": 0.08139904610492846,
      "grad_norm": 1.9249398708343506,
      "learning_rate": 0.00018383606974672266,
      "loss": 0.6747,
      "step": 640
    },
    {
      "epoch": 0.08152623211446741,
      "grad_norm": 3.534879684448242,
      "learning_rate": 0.0001838106147384498,
      "loss": 0.8472,
      "step": 641
    },
    {
      "epoch": 0.08165341812400637,
      "grad_norm": 1.4848201274871826,
      "learning_rate": 0.00018378515973017692,
      "loss": 0.6084,
      "step": 642
    },
    {
      "epoch": 0.0817806041335453,
      "grad_norm": 1.900475025177002,
      "learning_rate": 0.00018375970472190404,
      "loss": 1.129,
      "step": 643
    },
    {
      "epoch": 0.08190779014308426,
      "grad_norm": 1.9498156309127808,
      "learning_rate": 0.00018373424971363116,
      "loss": 0.8228,
      "step": 644
    },
    {
      "epoch": 0.08203497615262322,
      "grad_norm": 1.905612587928772,
      "learning_rate": 0.00018370879470535828,
      "loss": 0.9485,
      "step": 645
    },
    {
      "epoch": 0.08216216216216216,
      "grad_norm": 1.4220695495605469,
      "learning_rate": 0.00018368333969708543,
      "loss": 0.751,
      "step": 646
    },
    {
      "epoch": 0.08228934817170111,
      "grad_norm": 1.8844460248947144,
      "learning_rate": 0.00018365788468881254,
      "loss": 0.9278,
      "step": 647
    },
    {
      "epoch": 0.08241653418124006,
      "grad_norm": 1.5069949626922607,
      "learning_rate": 0.00018363242968053966,
      "loss": 0.755,
      "step": 648
    },
    {
      "epoch": 0.08254372019077902,
      "grad_norm": 1.4467284679412842,
      "learning_rate": 0.00018360697467226678,
      "loss": 0.8955,
      "step": 649
    },
    {
      "epoch": 0.08267090620031796,
      "grad_norm": 1.9340417385101318,
      "learning_rate": 0.0001835815196639939,
      "loss": 0.9434,
      "step": 650
    },
    {
      "epoch": 0.08279809220985691,
      "grad_norm": 2.1584134101867676,
      "learning_rate": 0.00018355606465572102,
      "loss": 0.9739,
      "step": 651
    },
    {
      "epoch": 0.08292527821939587,
      "grad_norm": 2.0686240196228027,
      "learning_rate": 0.00018353060964744814,
      "loss": 0.7571,
      "step": 652
    },
    {
      "epoch": 0.08305246422893482,
      "grad_norm": 1.8461954593658447,
      "learning_rate": 0.00018350515463917526,
      "loss": 0.9246,
      "step": 653
    },
    {
      "epoch": 0.08317965023847376,
      "grad_norm": 2.560924530029297,
      "learning_rate": 0.0001834796996309024,
      "loss": 0.6627,
      "step": 654
    },
    {
      "epoch": 0.08330683624801272,
      "grad_norm": 2.178854465484619,
      "learning_rate": 0.0001834542446226295,
      "loss": 0.8236,
      "step": 655
    },
    {
      "epoch": 0.08343402225755167,
      "grad_norm": 1.925044059753418,
      "learning_rate": 0.00018342878961435664,
      "loss": 0.753,
      "step": 656
    },
    {
      "epoch": 0.08356120826709063,
      "grad_norm": 1.8474657535552979,
      "learning_rate": 0.00018340333460608376,
      "loss": 1.0764,
      "step": 657
    },
    {
      "epoch": 0.08368839427662957,
      "grad_norm": 2.37419056892395,
      "learning_rate": 0.00018337787959781088,
      "loss": 0.8514,
      "step": 658
    },
    {
      "epoch": 0.08381558028616852,
      "grad_norm": 1.5903918743133545,
      "learning_rate": 0.000183352424589538,
      "loss": 0.9712,
      "step": 659
    },
    {
      "epoch": 0.08394276629570747,
      "grad_norm": 1.4249870777130127,
      "learning_rate": 0.00018332696958126512,
      "loss": 0.4742,
      "step": 660
    },
    {
      "epoch": 0.08406995230524643,
      "grad_norm": 2.1118903160095215,
      "learning_rate": 0.00018330151457299226,
      "loss": 0.8985,
      "step": 661
    },
    {
      "epoch": 0.08419713831478537,
      "grad_norm": 1.779412031173706,
      "learning_rate": 0.00018327605956471935,
      "loss": 0.8557,
      "step": 662
    },
    {
      "epoch": 0.08432432432432432,
      "grad_norm": 1.80742609500885,
      "learning_rate": 0.0001832506045564465,
      "loss": 0.919,
      "step": 663
    },
    {
      "epoch": 0.08445151033386328,
      "grad_norm": 2.6376261711120605,
      "learning_rate": 0.0001832251495481736,
      "loss": 1.0591,
      "step": 664
    },
    {
      "epoch": 0.08457869634340223,
      "grad_norm": 2.6469032764434814,
      "learning_rate": 0.00018319969453990074,
      "loss": 0.7834,
      "step": 665
    },
    {
      "epoch": 0.08470588235294117,
      "grad_norm": 1.4386909008026123,
      "learning_rate": 0.00018317423953162785,
      "loss": 0.7889,
      "step": 666
    },
    {
      "epoch": 0.08483306836248013,
      "grad_norm": 1.4391634464263916,
      "learning_rate": 0.00018314878452335497,
      "loss": 0.5941,
      "step": 667
    },
    {
      "epoch": 0.08496025437201908,
      "grad_norm": 2.024007797241211,
      "learning_rate": 0.00018312332951508212,
      "loss": 0.8821,
      "step": 668
    },
    {
      "epoch": 0.08508744038155804,
      "grad_norm": 1.804302453994751,
      "learning_rate": 0.0001830978745068092,
      "loss": 0.941,
      "step": 669
    },
    {
      "epoch": 0.08521462639109698,
      "grad_norm": 1.917109727859497,
      "learning_rate": 0.00018307241949853636,
      "loss": 1.0363,
      "step": 670
    },
    {
      "epoch": 0.08534181240063593,
      "grad_norm": 1.5714983940124512,
      "learning_rate": 0.00018304696449026345,
      "loss": 0.7847,
      "step": 671
    },
    {
      "epoch": 0.08546899841017488,
      "grad_norm": 1.455223560333252,
      "learning_rate": 0.0001830215094819906,
      "loss": 0.7842,
      "step": 672
    },
    {
      "epoch": 0.08559618441971382,
      "grad_norm": 1.373719573020935,
      "learning_rate": 0.0001829960544737177,
      "loss": 0.7049,
      "step": 673
    },
    {
      "epoch": 0.08572337042925278,
      "grad_norm": 1.7619211673736572,
      "learning_rate": 0.00018297059946544483,
      "loss": 0.874,
      "step": 674
    },
    {
      "epoch": 0.08585055643879173,
      "grad_norm": 1.7029269933700562,
      "learning_rate": 0.00018294514445717195,
      "loss": 1.0788,
      "step": 675
    },
    {
      "epoch": 0.08597774244833069,
      "grad_norm": 2.4489517211914062,
      "learning_rate": 0.00018291968944889907,
      "loss": 1.0746,
      "step": 676
    },
    {
      "epoch": 0.08610492845786963,
      "grad_norm": 1.9609097242355347,
      "learning_rate": 0.00018289423444062621,
      "loss": 0.965,
      "step": 677
    },
    {
      "epoch": 0.08623211446740858,
      "grad_norm": 2.0867481231689453,
      "learning_rate": 0.00018286877943235333,
      "loss": 0.9213,
      "step": 678
    },
    {
      "epoch": 0.08635930047694754,
      "grad_norm": 1.3631198406219482,
      "learning_rate": 0.00018284332442408045,
      "loss": 0.5808,
      "step": 679
    },
    {
      "epoch": 0.08648648648648649,
      "grad_norm": 2.363952875137329,
      "learning_rate": 0.00018281786941580757,
      "loss": 0.8391,
      "step": 680
    },
    {
      "epoch": 0.08661367249602543,
      "grad_norm": 2.5047082901000977,
      "learning_rate": 0.0001827924144075347,
      "loss": 1.0617,
      "step": 681
    },
    {
      "epoch": 0.08674085850556439,
      "grad_norm": 1.7529109716415405,
      "learning_rate": 0.0001827669593992618,
      "loss": 0.6673,
      "step": 682
    },
    {
      "epoch": 0.08686804451510334,
      "grad_norm": 2.2576727867126465,
      "learning_rate": 0.00018274150439098893,
      "loss": 0.7822,
      "step": 683
    },
    {
      "epoch": 0.0869952305246423,
      "grad_norm": 2.054887294769287,
      "learning_rate": 0.00018271604938271605,
      "loss": 0.7121,
      "step": 684
    },
    {
      "epoch": 0.08712241653418124,
      "grad_norm": 2.3027710914611816,
      "learning_rate": 0.0001826905943744432,
      "loss": 1.0872,
      "step": 685
    },
    {
      "epoch": 0.08724960254372019,
      "grad_norm": 2.0059802532196045,
      "learning_rate": 0.0001826651393661703,
      "loss": 0.9492,
      "step": 686
    },
    {
      "epoch": 0.08737678855325914,
      "grad_norm": 1.9505681991577148,
      "learning_rate": 0.00018263968435789743,
      "loss": 0.9697,
      "step": 687
    },
    {
      "epoch": 0.0875039745627981,
      "grad_norm": 1.9981067180633545,
      "learning_rate": 0.00018261422934962455,
      "loss": 0.8998,
      "step": 688
    },
    {
      "epoch": 0.08763116057233704,
      "grad_norm": 1.965214490890503,
      "learning_rate": 0.00018258877434135167,
      "loss": 1.0037,
      "step": 689
    },
    {
      "epoch": 0.08775834658187599,
      "grad_norm": 1.823422908782959,
      "learning_rate": 0.00018256331933307879,
      "loss": 0.6014,
      "step": 690
    },
    {
      "epoch": 0.08788553259141495,
      "grad_norm": 2.1182174682617188,
      "learning_rate": 0.0001825378643248059,
      "loss": 0.7043,
      "step": 691
    },
    {
      "epoch": 0.0880127186009539,
      "grad_norm": 1.9490602016448975,
      "learning_rate": 0.00018251240931653305,
      "loss": 0.9997,
      "step": 692
    },
    {
      "epoch": 0.08813990461049284,
      "grad_norm": 1.764167308807373,
      "learning_rate": 0.00018248695430826014,
      "loss": 0.7406,
      "step": 693
    },
    {
      "epoch": 0.0882670906200318,
      "grad_norm": 1.619451642036438,
      "learning_rate": 0.0001824614992999873,
      "loss": 0.6437,
      "step": 694
    },
    {
      "epoch": 0.08839427662957075,
      "grad_norm": 1.851861596107483,
      "learning_rate": 0.0001824360442917144,
      "loss": 0.6396,
      "step": 695
    },
    {
      "epoch": 0.0885214626391097,
      "grad_norm": 2.10787296295166,
      "learning_rate": 0.00018241058928344153,
      "loss": 0.8551,
      "step": 696
    },
    {
      "epoch": 0.08864864864864865,
      "grad_norm": 1.8418266773223877,
      "learning_rate": 0.00018238513427516864,
      "loss": 0.7193,
      "step": 697
    },
    {
      "epoch": 0.0887758346581876,
      "grad_norm": 1.9435572624206543,
      "learning_rate": 0.00018235967926689576,
      "loss": 0.8148,
      "step": 698
    },
    {
      "epoch": 0.08890302066772655,
      "grad_norm": 1.6771379709243774,
      "learning_rate": 0.0001823342242586229,
      "loss": 0.881,
      "step": 699
    },
    {
      "epoch": 0.0890302066772655,
      "grad_norm": 2.062885284423828,
      "learning_rate": 0.00018230876925035,
      "loss": 1.0884,
      "step": 700
    },
    {
      "epoch": 0.08915739268680445,
      "grad_norm": 1.3772863149642944,
      "learning_rate": 0.00018228331424207715,
      "loss": 0.7056,
      "step": 701
    },
    {
      "epoch": 0.0892845786963434,
      "grad_norm": 2.3207037448883057,
      "learning_rate": 0.00018225785923380426,
      "loss": 1.1005,
      "step": 702
    },
    {
      "epoch": 0.08941176470588236,
      "grad_norm": 2.0037083625793457,
      "learning_rate": 0.00018223240422553138,
      "loss": 1.0225,
      "step": 703
    },
    {
      "epoch": 0.0895389507154213,
      "grad_norm": 1.7995933294296265,
      "learning_rate": 0.0001822069492172585,
      "loss": 0.6772,
      "step": 704
    },
    {
      "epoch": 0.08966613672496025,
      "grad_norm": 1.7610853910446167,
      "learning_rate": 0.00018218149420898562,
      "loss": 0.8512,
      "step": 705
    },
    {
      "epoch": 0.0897933227344992,
      "grad_norm": 2.063293218612671,
      "learning_rate": 0.00018215603920071277,
      "loss": 0.6326,
      "step": 706
    },
    {
      "epoch": 0.08992050874403816,
      "grad_norm": 1.7038092613220215,
      "learning_rate": 0.00018213058419243986,
      "loss": 0.9138,
      "step": 707
    },
    {
      "epoch": 0.0900476947535771,
      "grad_norm": 1.9480024576187134,
      "learning_rate": 0.000182105129184167,
      "loss": 0.8574,
      "step": 708
    },
    {
      "epoch": 0.09017488076311606,
      "grad_norm": 1.8378057479858398,
      "learning_rate": 0.00018207967417589412,
      "loss": 0.8942,
      "step": 709
    },
    {
      "epoch": 0.09030206677265501,
      "grad_norm": 2.0479989051818848,
      "learning_rate": 0.00018205421916762124,
      "loss": 0.6285,
      "step": 710
    },
    {
      "epoch": 0.09042925278219396,
      "grad_norm": 2.639441967010498,
      "learning_rate": 0.00018202876415934836,
      "loss": 0.8192,
      "step": 711
    },
    {
      "epoch": 0.0905564387917329,
      "grad_norm": 2.355694055557251,
      "learning_rate": 0.00018200330915107548,
      "loss": 0.8814,
      "step": 712
    },
    {
      "epoch": 0.09068362480127186,
      "grad_norm": 3.1156423091888428,
      "learning_rate": 0.0001819778541428026,
      "loss": 0.8264,
      "step": 713
    },
    {
      "epoch": 0.09081081081081081,
      "grad_norm": 1.760589838027954,
      "learning_rate": 0.00018195239913452972,
      "loss": 0.8488,
      "step": 714
    },
    {
      "epoch": 0.09093799682034977,
      "grad_norm": 2.2609689235687256,
      "learning_rate": 0.00018192694412625686,
      "loss": 0.614,
      "step": 715
    },
    {
      "epoch": 0.09106518282988871,
      "grad_norm": 2.4993302822113037,
      "learning_rate": 0.00018190148911798398,
      "loss": 1.1346,
      "step": 716
    },
    {
      "epoch": 0.09119236883942766,
      "grad_norm": 2.0051045417785645,
      "learning_rate": 0.0001818760341097111,
      "loss": 0.7745,
      "step": 717
    },
    {
      "epoch": 0.09131955484896662,
      "grad_norm": 1.8662109375,
      "learning_rate": 0.00018185057910143822,
      "loss": 0.7325,
      "step": 718
    },
    {
      "epoch": 0.09144674085850557,
      "grad_norm": 2.5576775074005127,
      "learning_rate": 0.00018182512409316534,
      "loss": 0.8478,
      "step": 719
    },
    {
      "epoch": 0.09157392686804451,
      "grad_norm": 2.2727999687194824,
      "learning_rate": 0.00018179966908489246,
      "loss": 0.9116,
      "step": 720
    },
    {
      "epoch": 0.09170111287758347,
      "grad_norm": 2.8521311283111572,
      "learning_rate": 0.00018177421407661957,
      "loss": 0.968,
      "step": 721
    },
    {
      "epoch": 0.09182829888712242,
      "grad_norm": 1.7930397987365723,
      "learning_rate": 0.0001817487590683467,
      "loss": 0.9811,
      "step": 722
    },
    {
      "epoch": 0.09195548489666137,
      "grad_norm": 2.075040817260742,
      "learning_rate": 0.00018172330406007384,
      "loss": 0.8685,
      "step": 723
    },
    {
      "epoch": 0.09208267090620031,
      "grad_norm": 1.7967315912246704,
      "learning_rate": 0.00018169784905180096,
      "loss": 0.7808,
      "step": 724
    },
    {
      "epoch": 0.09220985691573927,
      "grad_norm": 2.561188220977783,
      "learning_rate": 0.00018167239404352808,
      "loss": 1.1022,
      "step": 725
    },
    {
      "epoch": 0.09233704292527822,
      "grad_norm": 2.339765787124634,
      "learning_rate": 0.0001816469390352552,
      "loss": 1.0109,
      "step": 726
    },
    {
      "epoch": 0.09246422893481716,
      "grad_norm": 1.8062313795089722,
      "learning_rate": 0.00018162148402698231,
      "loss": 1.0288,
      "step": 727
    },
    {
      "epoch": 0.09259141494435612,
      "grad_norm": 1.8471782207489014,
      "learning_rate": 0.00018159602901870943,
      "loss": 1.0534,
      "step": 728
    },
    {
      "epoch": 0.09271860095389507,
      "grad_norm": 1.752877950668335,
      "learning_rate": 0.00018157057401043655,
      "loss": 0.7067,
      "step": 729
    },
    {
      "epoch": 0.09284578696343403,
      "grad_norm": 1.3808865547180176,
      "learning_rate": 0.0001815451190021637,
      "loss": 0.729,
      "step": 730
    },
    {
      "epoch": 0.09297297297297297,
      "grad_norm": 1.812738060951233,
      "learning_rate": 0.0001815196639938908,
      "loss": 1.2603,
      "step": 731
    },
    {
      "epoch": 0.09310015898251192,
      "grad_norm": 1.7349671125411987,
      "learning_rate": 0.00018149420898561794,
      "loss": 0.7982,
      "step": 732
    },
    {
      "epoch": 0.09322734499205088,
      "grad_norm": 1.4705383777618408,
      "learning_rate": 0.00018146875397734505,
      "loss": 0.652,
      "step": 733
    },
    {
      "epoch": 0.09335453100158983,
      "grad_norm": 1.5888293981552124,
      "learning_rate": 0.00018144329896907217,
      "loss": 0.7372,
      "step": 734
    },
    {
      "epoch": 0.09348171701112877,
      "grad_norm": 1.6410011053085327,
      "learning_rate": 0.00018141784396079932,
      "loss": 0.8117,
      "step": 735
    },
    {
      "epoch": 0.09360890302066772,
      "grad_norm": 2.1319127082824707,
      "learning_rate": 0.0001813923889525264,
      "loss": 1.0341,
      "step": 736
    },
    {
      "epoch": 0.09373608903020668,
      "grad_norm": 1.5467242002487183,
      "learning_rate": 0.00018136693394425356,
      "loss": 0.8227,
      "step": 737
    },
    {
      "epoch": 0.09386327503974563,
      "grad_norm": 1.6042039394378662,
      "learning_rate": 0.00018134147893598065,
      "loss": 0.7354,
      "step": 738
    },
    {
      "epoch": 0.09399046104928457,
      "grad_norm": 2.0383031368255615,
      "learning_rate": 0.0001813160239277078,
      "loss": 0.8095,
      "step": 739
    },
    {
      "epoch": 0.09411764705882353,
      "grad_norm": 1.684441089630127,
      "learning_rate": 0.0001812905689194349,
      "loss": 0.7152,
      "step": 740
    },
    {
      "epoch": 0.09424483306836248,
      "grad_norm": 1.5561739206314087,
      "learning_rate": 0.00018126511391116203,
      "loss": 0.7756,
      "step": 741
    },
    {
      "epoch": 0.09437201907790144,
      "grad_norm": 1.8663972616195679,
      "learning_rate": 0.00018123965890288915,
      "loss": 0.8833,
      "step": 742
    },
    {
      "epoch": 0.09449920508744038,
      "grad_norm": 1.8918776512145996,
      "learning_rate": 0.00018121420389461627,
      "loss": 0.9927,
      "step": 743
    },
    {
      "epoch": 0.09462639109697933,
      "grad_norm": 1.8995426893234253,
      "learning_rate": 0.00018118874888634341,
      "loss": 0.9974,
      "step": 744
    },
    {
      "epoch": 0.09475357710651829,
      "grad_norm": 1.4392770528793335,
      "learning_rate": 0.0001811632938780705,
      "loss": 0.9262,
      "step": 745
    },
    {
      "epoch": 0.09488076311605724,
      "grad_norm": 1.9420428276062012,
      "learning_rate": 0.00018113783886979765,
      "loss": 0.8128,
      "step": 746
    },
    {
      "epoch": 0.09500794912559618,
      "grad_norm": 1.627427339553833,
      "learning_rate": 0.00018111238386152477,
      "loss": 0.7601,
      "step": 747
    },
    {
      "epoch": 0.09513513513513513,
      "grad_norm": 2.089186668395996,
      "learning_rate": 0.0001810869288532519,
      "loss": 1.1043,
      "step": 748
    },
    {
      "epoch": 0.09526232114467409,
      "grad_norm": 1.6621811389923096,
      "learning_rate": 0.000181061473844979,
      "loss": 0.7864,
      "step": 749
    },
    {
      "epoch": 0.09538950715421304,
      "grad_norm": 1.688393473625183,
      "learning_rate": 0.00018103601883670613,
      "loss": 0.8064,
      "step": 750
    },
    {
      "epoch": 0.09551669316375198,
      "grad_norm": 2.0355546474456787,
      "learning_rate": 0.00018101056382843325,
      "loss": 0.8133,
      "step": 751
    },
    {
      "epoch": 0.09564387917329094,
      "grad_norm": 2.0368385314941406,
      "learning_rate": 0.00018098510882016036,
      "loss": 0.858,
      "step": 752
    },
    {
      "epoch": 0.09577106518282989,
      "grad_norm": 2.461181402206421,
      "learning_rate": 0.0001809596538118875,
      "loss": 0.8565,
      "step": 753
    },
    {
      "epoch": 0.09589825119236883,
      "grad_norm": 2.389568567276001,
      "learning_rate": 0.00018093419880361463,
      "loss": 0.6479,
      "step": 754
    },
    {
      "epoch": 0.09602543720190779,
      "grad_norm": 2.5740549564361572,
      "learning_rate": 0.00018090874379534175,
      "loss": 0.9095,
      "step": 755
    },
    {
      "epoch": 0.09615262321144674,
      "grad_norm": 1.8627325296401978,
      "learning_rate": 0.00018088328878706887,
      "loss": 0.6936,
      "step": 756
    },
    {
      "epoch": 0.0962798092209857,
      "grad_norm": 2.0952908992767334,
      "learning_rate": 0.00018085783377879599,
      "loss": 0.7822,
      "step": 757
    },
    {
      "epoch": 0.09640699523052464,
      "grad_norm": 2.991616725921631,
      "learning_rate": 0.0001808323787705231,
      "loss": 1.1995,
      "step": 758
    },
    {
      "epoch": 0.09653418124006359,
      "grad_norm": 2.54783034324646,
      "learning_rate": 0.00018080692376225022,
      "loss": 1.0972,
      "step": 759
    },
    {
      "epoch": 0.09666136724960255,
      "grad_norm": 2.6048760414123535,
      "learning_rate": 0.00018078146875397734,
      "loss": 0.9857,
      "step": 760
    },
    {
      "epoch": 0.0967885532591415,
      "grad_norm": 1.4159029722213745,
      "learning_rate": 0.0001807560137457045,
      "loss": 0.5311,
      "step": 761
    },
    {
      "epoch": 0.09691573926868044,
      "grad_norm": 1.862510323524475,
      "learning_rate": 0.00018073055873743158,
      "loss": 1.0184,
      "step": 762
    },
    {
      "epoch": 0.0970429252782194,
      "grad_norm": 1.5896875858306885,
      "learning_rate": 0.00018070510372915872,
      "loss": 0.681,
      "step": 763
    },
    {
      "epoch": 0.09717011128775835,
      "grad_norm": 1.7765287160873413,
      "learning_rate": 0.00018067964872088584,
      "loss": 0.8524,
      "step": 764
    },
    {
      "epoch": 0.0972972972972973,
      "grad_norm": 2.1852781772613525,
      "learning_rate": 0.00018065419371261296,
      "loss": 0.7871,
      "step": 765
    },
    {
      "epoch": 0.09742448330683624,
      "grad_norm": 2.0057055950164795,
      "learning_rate": 0.0001806287387043401,
      "loss": 0.8414,
      "step": 766
    },
    {
      "epoch": 0.0975516693163752,
      "grad_norm": 2.104447364807129,
      "learning_rate": 0.0001806032836960672,
      "loss": 1.2652,
      "step": 767
    },
    {
      "epoch": 0.09767885532591415,
      "grad_norm": 1.6678855419158936,
      "learning_rate": 0.00018057782868779435,
      "loss": 0.9808,
      "step": 768
    },
    {
      "epoch": 0.0978060413354531,
      "grad_norm": 1.518458604812622,
      "learning_rate": 0.00018055237367952144,
      "loss": 0.8156,
      "step": 769
    },
    {
      "epoch": 0.09793322734499205,
      "grad_norm": 1.6010810136795044,
      "learning_rate": 0.00018052691867124858,
      "loss": 0.6287,
      "step": 770
    },
    {
      "epoch": 0.098060413354531,
      "grad_norm": 2.0443875789642334,
      "learning_rate": 0.0001805014636629757,
      "loss": 1.0089,
      "step": 771
    },
    {
      "epoch": 0.09818759936406996,
      "grad_norm": 2.0275752544403076,
      "learning_rate": 0.00018047600865470282,
      "loss": 1.3372,
      "step": 772
    },
    {
      "epoch": 0.09831478537360891,
      "grad_norm": 1.8870928287506104,
      "learning_rate": 0.00018045055364642997,
      "loss": 0.6746,
      "step": 773
    },
    {
      "epoch": 0.09844197138314785,
      "grad_norm": 2.1580183506011963,
      "learning_rate": 0.00018042509863815706,
      "loss": 1.0143,
      "step": 774
    },
    {
      "epoch": 0.0985691573926868,
      "grad_norm": 2.2370688915252686,
      "learning_rate": 0.0001803996436298842,
      "loss": 0.7986,
      "step": 775
    },
    {
      "epoch": 0.09869634340222576,
      "grad_norm": 1.5317121744155884,
      "learning_rate": 0.0001803741886216113,
      "loss": 0.6536,
      "step": 776
    },
    {
      "epoch": 0.0988235294117647,
      "grad_norm": 1.8823370933532715,
      "learning_rate": 0.00018034873361333844,
      "loss": 0.9195,
      "step": 777
    },
    {
      "epoch": 0.09895071542130365,
      "grad_norm": 1.7894542217254639,
      "learning_rate": 0.00018032327860506556,
      "loss": 0.9983,
      "step": 778
    },
    {
      "epoch": 0.09907790143084261,
      "grad_norm": 2.1565494537353516,
      "learning_rate": 0.00018029782359679268,
      "loss": 0.8417,
      "step": 779
    },
    {
      "epoch": 0.09920508744038156,
      "grad_norm": 1.8367639780044556,
      "learning_rate": 0.0001802723685885198,
      "loss": 0.9891,
      "step": 780
    },
    {
      "epoch": 0.0993322734499205,
      "grad_norm": 2.049422025680542,
      "learning_rate": 0.00018024691358024692,
      "loss": 0.9066,
      "step": 781
    },
    {
      "epoch": 0.09945945945945946,
      "grad_norm": 2.8088014125823975,
      "learning_rate": 0.00018022145857197406,
      "loss": 1.2988,
      "step": 782
    },
    {
      "epoch": 0.09958664546899841,
      "grad_norm": 2.3270745277404785,
      "learning_rate": 0.00018019600356370115,
      "loss": 1.0176,
      "step": 783
    },
    {
      "epoch": 0.09971383147853737,
      "grad_norm": 2.1335947513580322,
      "learning_rate": 0.0001801705485554283,
      "loss": 0.7215,
      "step": 784
    },
    {
      "epoch": 0.0998410174880763,
      "grad_norm": 1.7683545351028442,
      "learning_rate": 0.00018014509354715542,
      "loss": 0.692,
      "step": 785
    },
    {
      "epoch": 0.09996820349761526,
      "grad_norm": 1.2943179607391357,
      "learning_rate": 0.00018011963853888254,
      "loss": 0.5955,
      "step": 786
    },
    {
      "epoch": 0.10009538950715421,
      "grad_norm": 1.494871973991394,
      "learning_rate": 0.00018009418353060966,
      "loss": 0.8608,
      "step": 787
    },
    {
      "epoch": 0.10022257551669317,
      "grad_norm": 1.6612956523895264,
      "learning_rate": 0.00018006872852233677,
      "loss": 0.7635,
      "step": 788
    },
    {
      "epoch": 0.10034976152623211,
      "grad_norm": 1.4522137641906738,
      "learning_rate": 0.0001800432735140639,
      "loss": 0.8016,
      "step": 789
    },
    {
      "epoch": 0.10047694753577106,
      "grad_norm": 1.737343668937683,
      "learning_rate": 0.000180017818505791,
      "loss": 0.911,
      "step": 790
    },
    {
      "epoch": 0.10060413354531002,
      "grad_norm": 5.465054512023926,
      "learning_rate": 0.00017999236349751813,
      "loss": 0.8061,
      "step": 791
    },
    {
      "epoch": 0.10073131955484897,
      "grad_norm": 1.4320682287216187,
      "learning_rate": 0.00017996690848924528,
      "loss": 0.7458,
      "step": 792
    },
    {
      "epoch": 0.10085850556438791,
      "grad_norm": 1.6596940755844116,
      "learning_rate": 0.0001799414534809724,
      "loss": 0.5229,
      "step": 793
    },
    {
      "epoch": 0.10098569157392687,
      "grad_norm": 1.631056308746338,
      "learning_rate": 0.00017991599847269951,
      "loss": 0.7958,
      "step": 794
    },
    {
      "epoch": 0.10111287758346582,
      "grad_norm": 1.8372715711593628,
      "learning_rate": 0.00017989054346442663,
      "loss": 1.0074,
      "step": 795
    },
    {
      "epoch": 0.10124006359300478,
      "grad_norm": 1.9239633083343506,
      "learning_rate": 0.00017986508845615375,
      "loss": 0.7898,
      "step": 796
    },
    {
      "epoch": 0.10136724960254372,
      "grad_norm": 1.560275912284851,
      "learning_rate": 0.0001798396334478809,
      "loss": 0.6623,
      "step": 797
    },
    {
      "epoch": 0.10149443561208267,
      "grad_norm": 2.549532413482666,
      "learning_rate": 0.000179814178439608,
      "loss": 1.0388,
      "step": 798
    },
    {
      "epoch": 0.10162162162162162,
      "grad_norm": 2.7570431232452393,
      "learning_rate": 0.00017978872343133513,
      "loss": 1.0927,
      "step": 799
    },
    {
      "epoch": 0.10174880763116058,
      "grad_norm": 1.9310414791107178,
      "learning_rate": 0.00017976326842306223,
      "loss": 0.667,
      "step": 800
    },
    {
      "epoch": 0.10187599364069952,
      "grad_norm": 2.07765793800354,
      "learning_rate": 0.00017973781341478937,
      "loss": 0.9023,
      "step": 801
    },
    {
      "epoch": 0.10200317965023847,
      "grad_norm": 3.593785285949707,
      "learning_rate": 0.0001797123584065165,
      "loss": 0.9735,
      "step": 802
    },
    {
      "epoch": 0.10213036565977743,
      "grad_norm": 1.8999793529510498,
      "learning_rate": 0.0001796869033982436,
      "loss": 0.8376,
      "step": 803
    },
    {
      "epoch": 0.10225755166931637,
      "grad_norm": 1.7051584720611572,
      "learning_rate": 0.00017966144838997076,
      "loss": 0.708,
      "step": 804
    },
    {
      "epoch": 0.10238473767885532,
      "grad_norm": 2.4136059284210205,
      "learning_rate": 0.00017963599338169785,
      "loss": 0.9255,
      "step": 805
    },
    {
      "epoch": 0.10251192368839428,
      "grad_norm": 2.1728017330169678,
      "learning_rate": 0.000179610538373425,
      "loss": 0.7371,
      "step": 806
    },
    {
      "epoch": 0.10263910969793323,
      "grad_norm": 2.0383753776550293,
      "learning_rate": 0.00017958508336515208,
      "loss": 0.7339,
      "step": 807
    },
    {
      "epoch": 0.10276629570747217,
      "grad_norm": 2.009108781814575,
      "learning_rate": 0.00017955962835687923,
      "loss": 0.882,
      "step": 808
    },
    {
      "epoch": 0.10289348171701113,
      "grad_norm": 2.311161518096924,
      "learning_rate": 0.00017953417334860635,
      "loss": 1.092,
      "step": 809
    },
    {
      "epoch": 0.10302066772655008,
      "grad_norm": 1.8708515167236328,
      "learning_rate": 0.00017950871834033347,
      "loss": 0.7174,
      "step": 810
    },
    {
      "epoch": 0.10314785373608903,
      "grad_norm": 2.721389055252075,
      "learning_rate": 0.0001794832633320606,
      "loss": 1.201,
      "step": 811
    },
    {
      "epoch": 0.10327503974562798,
      "grad_norm": 1.9980052709579468,
      "learning_rate": 0.0001794578083237877,
      "loss": 0.7253,
      "step": 812
    },
    {
      "epoch": 0.10340222575516693,
      "grad_norm": 2.8928754329681396,
      "learning_rate": 0.00017943235331551485,
      "loss": 0.9957,
      "step": 813
    },
    {
      "epoch": 0.10352941176470588,
      "grad_norm": 1.6487842798233032,
      "learning_rate": 0.00017940689830724194,
      "loss": 0.732,
      "step": 814
    },
    {
      "epoch": 0.10365659777424484,
      "grad_norm": 2.0936222076416016,
      "learning_rate": 0.0001793814432989691,
      "loss": 0.7835,
      "step": 815
    },
    {
      "epoch": 0.10378378378378378,
      "grad_norm": 2.542114496231079,
      "learning_rate": 0.0001793559882906962,
      "loss": 0.7241,
      "step": 816
    },
    {
      "epoch": 0.10391096979332273,
      "grad_norm": 1.7347732782363892,
      "learning_rate": 0.00017933053328242333,
      "loss": 0.7302,
      "step": 817
    },
    {
      "epoch": 0.10403815580286169,
      "grad_norm": 2.415520429611206,
      "learning_rate": 0.00017930507827415045,
      "loss": 0.7237,
      "step": 818
    },
    {
      "epoch": 0.10416534181240064,
      "grad_norm": 1.6599689722061157,
      "learning_rate": 0.00017927962326587756,
      "loss": 0.7682,
      "step": 819
    },
    {
      "epoch": 0.10429252782193958,
      "grad_norm": 2.0597434043884277,
      "learning_rate": 0.00017925416825760468,
      "loss": 0.6866,
      "step": 820
    },
    {
      "epoch": 0.10441971383147854,
      "grad_norm": 1.9076545238494873,
      "learning_rate": 0.0001792287132493318,
      "loss": 0.8458,
      "step": 821
    },
    {
      "epoch": 0.10454689984101749,
      "grad_norm": 1.658654808998108,
      "learning_rate": 0.00017920325824105895,
      "loss": 0.9234,
      "step": 822
    },
    {
      "epoch": 0.10467408585055644,
      "grad_norm": 2.1414082050323486,
      "learning_rate": 0.00017917780323278607,
      "loss": 0.9512,
      "step": 823
    },
    {
      "epoch": 0.10480127186009539,
      "grad_norm": 1.6120915412902832,
      "learning_rate": 0.00017915234822451318,
      "loss": 0.8288,
      "step": 824
    },
    {
      "epoch": 0.10492845786963434,
      "grad_norm": 2.041515588760376,
      "learning_rate": 0.0001791268932162403,
      "loss": 1.1165,
      "step": 825
    },
    {
      "epoch": 0.1050556438791733,
      "grad_norm": 2.1555111408233643,
      "learning_rate": 0.00017910143820796742,
      "loss": 0.7999,
      "step": 826
    },
    {
      "epoch": 0.10518282988871225,
      "grad_norm": 1.7541097402572632,
      "learning_rate": 0.00017907598319969454,
      "loss": 0.8821,
      "step": 827
    },
    {
      "epoch": 0.10531001589825119,
      "grad_norm": 1.756333827972412,
      "learning_rate": 0.0001790505281914217,
      "loss": 0.5574,
      "step": 828
    },
    {
      "epoch": 0.10543720190779014,
      "grad_norm": 2.3740763664245605,
      "learning_rate": 0.00017902507318314878,
      "loss": 0.8657,
      "step": 829
    },
    {
      "epoch": 0.1055643879173291,
      "grad_norm": 1.850071668624878,
      "learning_rate": 0.00017899961817487592,
      "loss": 0.9121,
      "step": 830
    },
    {
      "epoch": 0.10569157392686804,
      "grad_norm": 2.9372057914733887,
      "learning_rate": 0.00017897416316660304,
      "loss": 0.841,
      "step": 831
    },
    {
      "epoch": 0.10581875993640699,
      "grad_norm": 1.9472923278808594,
      "learning_rate": 0.00017894870815833016,
      "loss": 0.7405,
      "step": 832
    },
    {
      "epoch": 0.10594594594594595,
      "grad_norm": 1.6686780452728271,
      "learning_rate": 0.00017892325315005728,
      "loss": 0.5923,
      "step": 833
    },
    {
      "epoch": 0.1060731319554849,
      "grad_norm": 1.9357776641845703,
      "learning_rate": 0.0001788977981417844,
      "loss": 0.8655,
      "step": 834
    },
    {
      "epoch": 0.10620031796502384,
      "grad_norm": 2.0332281589508057,
      "learning_rate": 0.00017887234313351154,
      "loss": 0.9588,
      "step": 835
    },
    {
      "epoch": 0.1063275039745628,
      "grad_norm": 1.3523690700531006,
      "learning_rate": 0.00017884688812523864,
      "loss": 0.6282,
      "step": 836
    },
    {
      "epoch": 0.10645468998410175,
      "grad_norm": 1.419893503189087,
      "learning_rate": 0.00017882143311696578,
      "loss": 0.8134,
      "step": 837
    },
    {
      "epoch": 0.1065818759936407,
      "grad_norm": 2.857835054397583,
      "learning_rate": 0.00017879597810869287,
      "loss": 1.1835,
      "step": 838
    },
    {
      "epoch": 0.10670906200317964,
      "grad_norm": 1.4327043294906616,
      "learning_rate": 0.00017877052310042002,
      "loss": 0.7362,
      "step": 839
    },
    {
      "epoch": 0.1068362480127186,
      "grad_norm": 1.8577895164489746,
      "learning_rate": 0.00017874506809214714,
      "loss": 1.0862,
      "step": 840
    },
    {
      "epoch": 0.10696343402225755,
      "grad_norm": 1.5443552732467651,
      "learning_rate": 0.00017871961308387426,
      "loss": 0.7581,
      "step": 841
    },
    {
      "epoch": 0.10709062003179651,
      "grad_norm": 2.33658504486084,
      "learning_rate": 0.0001786941580756014,
      "loss": 0.8672,
      "step": 842
    },
    {
      "epoch": 0.10721780604133545,
      "grad_norm": 2.011775255203247,
      "learning_rate": 0.0001786687030673285,
      "loss": 0.853,
      "step": 843
    },
    {
      "epoch": 0.1073449920508744,
      "grad_norm": 1.6406625509262085,
      "learning_rate": 0.00017864324805905564,
      "loss": 0.8276,
      "step": 844
    },
    {
      "epoch": 0.10747217806041336,
      "grad_norm": 1.9618607759475708,
      "learning_rate": 0.00017861779305078273,
      "loss": 0.6386,
      "step": 845
    },
    {
      "epoch": 0.10759936406995231,
      "grad_norm": 2.110407590866089,
      "learning_rate": 0.00017859233804250988,
      "loss": 1.3882,
      "step": 846
    },
    {
      "epoch": 0.10772655007949125,
      "grad_norm": 2.3873660564422607,
      "learning_rate": 0.000178566883034237,
      "loss": 1.2998,
      "step": 847
    },
    {
      "epoch": 0.1078537360890302,
      "grad_norm": 1.3125865459442139,
      "learning_rate": 0.00017854142802596412,
      "loss": 0.7808,
      "step": 848
    },
    {
      "epoch": 0.10798092209856916,
      "grad_norm": 1.5991896390914917,
      "learning_rate": 0.00017851597301769123,
      "loss": 0.8489,
      "step": 849
    },
    {
      "epoch": 0.10810810810810811,
      "grad_norm": 1.9024628400802612,
      "learning_rate": 0.00017849051800941835,
      "loss": 0.9567,
      "step": 850
    },
    {
      "epoch": 0.10823529411764705,
      "grad_norm": 1.6359100341796875,
      "learning_rate": 0.0001784650630011455,
      "loss": 0.8408,
      "step": 851
    },
    {
      "epoch": 0.10836248012718601,
      "grad_norm": 3.4689948558807373,
      "learning_rate": 0.0001784396079928726,
      "loss": 0.6659,
      "step": 852
    },
    {
      "epoch": 0.10848966613672496,
      "grad_norm": 1.562410831451416,
      "learning_rate": 0.00017841415298459974,
      "loss": 0.6013,
      "step": 853
    },
    {
      "epoch": 0.10861685214626392,
      "grad_norm": 1.8248677253723145,
      "learning_rate": 0.00017838869797632686,
      "loss": 0.7728,
      "step": 854
    },
    {
      "epoch": 0.10874403815580286,
      "grad_norm": 1.4408107995986938,
      "learning_rate": 0.00017836324296805397,
      "loss": 0.7573,
      "step": 855
    },
    {
      "epoch": 0.10887122416534181,
      "grad_norm": 2.2176785469055176,
      "learning_rate": 0.0001783377879597811,
      "loss": 0.965,
      "step": 856
    },
    {
      "epoch": 0.10899841017488077,
      "grad_norm": 1.808699369430542,
      "learning_rate": 0.0001783123329515082,
      "loss": 0.8495,
      "step": 857
    },
    {
      "epoch": 0.10912559618441971,
      "grad_norm": 3.023176908493042,
      "learning_rate": 0.00017828687794323533,
      "loss": 0.8264,
      "step": 858
    },
    {
      "epoch": 0.10925278219395866,
      "grad_norm": 1.6722491979599,
      "learning_rate": 0.00017826142293496248,
      "loss": 1.0926,
      "step": 859
    },
    {
      "epoch": 0.10937996820349762,
      "grad_norm": 1.6186678409576416,
      "learning_rate": 0.0001782359679266896,
      "loss": 0.9218,
      "step": 860
    },
    {
      "epoch": 0.10950715421303657,
      "grad_norm": 1.8915296792984009,
      "learning_rate": 0.0001782105129184167,
      "loss": 0.7357,
      "step": 861
    },
    {
      "epoch": 0.10963434022257551,
      "grad_norm": 1.5536390542984009,
      "learning_rate": 0.00017818505791014383,
      "loss": 0.9071,
      "step": 862
    },
    {
      "epoch": 0.10976152623211446,
      "grad_norm": 1.6279975175857544,
      "learning_rate": 0.00017815960290187095,
      "loss": 0.7304,
      "step": 863
    },
    {
      "epoch": 0.10988871224165342,
      "grad_norm": 1.734399437904358,
      "learning_rate": 0.00017813414789359807,
      "loss": 1.0666,
      "step": 864
    },
    {
      "epoch": 0.11001589825119237,
      "grad_norm": 1.7225017547607422,
      "learning_rate": 0.0001781086928853252,
      "loss": 0.6466,
      "step": 865
    },
    {
      "epoch": 0.11014308426073131,
      "grad_norm": 1.570919156074524,
      "learning_rate": 0.00017808323787705233,
      "loss": 0.8726,
      "step": 866
    },
    {
      "epoch": 0.11027027027027027,
      "grad_norm": 2.113009214401245,
      "learning_rate": 0.00017805778286877943,
      "loss": 0.8635,
      "step": 867
    },
    {
      "epoch": 0.11039745627980922,
      "grad_norm": 1.924668312072754,
      "learning_rate": 0.00017803232786050657,
      "loss": 0.8225,
      "step": 868
    },
    {
      "epoch": 0.11052464228934818,
      "grad_norm": 1.9193750619888306,
      "learning_rate": 0.00017800687285223366,
      "loss": 1.0307,
      "step": 869
    },
    {
      "epoch": 0.11065182829888712,
      "grad_norm": 2.5634753704071045,
      "learning_rate": 0.0001779814178439608,
      "loss": 0.8099,
      "step": 870
    },
    {
      "epoch": 0.11077901430842607,
      "grad_norm": 1.9943119287490845,
      "learning_rate": 0.00017795596283568793,
      "loss": 0.7485,
      "step": 871
    },
    {
      "epoch": 0.11090620031796503,
      "grad_norm": 2.1346435546875,
      "learning_rate": 0.00017793050782741505,
      "loss": 0.7028,
      "step": 872
    },
    {
      "epoch": 0.11103338632750398,
      "grad_norm": 2.25410795211792,
      "learning_rate": 0.0001779050528191422,
      "loss": 1.0166,
      "step": 873
    },
    {
      "epoch": 0.11116057233704292,
      "grad_norm": 3.010662317276001,
      "learning_rate": 0.00017787959781086928,
      "loss": 1.0657,
      "step": 874
    },
    {
      "epoch": 0.11128775834658187,
      "grad_norm": 2.1154675483703613,
      "learning_rate": 0.00017785414280259643,
      "loss": 1.0129,
      "step": 875
    },
    {
      "epoch": 0.11141494435612083,
      "grad_norm": 2.427290916442871,
      "learning_rate": 0.00017782868779432352,
      "loss": 1.1942,
      "step": 876
    },
    {
      "epoch": 0.11154213036565978,
      "grad_norm": 1.9398947954177856,
      "learning_rate": 0.00017780323278605067,
      "loss": 0.8299,
      "step": 877
    },
    {
      "epoch": 0.11166931637519872,
      "grad_norm": 1.7195340394973755,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.6418,
      "step": 878
    },
    {
      "epoch": 0.11179650238473768,
      "grad_norm": 1.4350380897521973,
      "learning_rate": 0.0001777523227695049,
      "loss": 0.8073,
      "step": 879
    },
    {
      "epoch": 0.11192368839427663,
      "grad_norm": 2.045085906982422,
      "learning_rate": 0.00017772686776123205,
      "loss": 0.6615,
      "step": 880
    },
    {
      "epoch": 0.11205087440381559,
      "grad_norm": 2.7594761848449707,
      "learning_rate": 0.00017770141275295914,
      "loss": 1.0418,
      "step": 881
    },
    {
      "epoch": 0.11217806041335453,
      "grad_norm": 1.6516320705413818,
      "learning_rate": 0.0001776759577446863,
      "loss": 0.9929,
      "step": 882
    },
    {
      "epoch": 0.11230524642289348,
      "grad_norm": 1.7205774784088135,
      "learning_rate": 0.00017765050273641338,
      "loss": 0.7592,
      "step": 883
    },
    {
      "epoch": 0.11243243243243244,
      "grad_norm": 1.7435152530670166,
      "learning_rate": 0.00017762504772814053,
      "loss": 0.8248,
      "step": 884
    },
    {
      "epoch": 0.11255961844197138,
      "grad_norm": 1.6180016994476318,
      "learning_rate": 0.00017759959271986764,
      "loss": 0.9264,
      "step": 885
    },
    {
      "epoch": 0.11268680445151033,
      "grad_norm": 1.6781251430511475,
      "learning_rate": 0.00017757413771159476,
      "loss": 0.9411,
      "step": 886
    },
    {
      "epoch": 0.11281399046104928,
      "grad_norm": 1.6954269409179688,
      "learning_rate": 0.00017754868270332188,
      "loss": 1.01,
      "step": 887
    },
    {
      "epoch": 0.11294117647058824,
      "grad_norm": 1.807302713394165,
      "learning_rate": 0.000177523227695049,
      "loss": 0.8218,
      "step": 888
    },
    {
      "epoch": 0.11306836248012718,
      "grad_norm": 1.6269065141677856,
      "learning_rate": 0.00017749777268677615,
      "loss": 0.6944,
      "step": 889
    },
    {
      "epoch": 0.11319554848966613,
      "grad_norm": 1.5498859882354736,
      "learning_rate": 0.00017747231767850327,
      "loss": 0.7327,
      "step": 890
    },
    {
      "epoch": 0.11332273449920509,
      "grad_norm": 1.8988397121429443,
      "learning_rate": 0.00017744686267023038,
      "loss": 0.7401,
      "step": 891
    },
    {
      "epoch": 0.11344992050874404,
      "grad_norm": 1.7831379175186157,
      "learning_rate": 0.0001774214076619575,
      "loss": 0.7316,
      "step": 892
    },
    {
      "epoch": 0.11357710651828298,
      "grad_norm": 2.1874783039093018,
      "learning_rate": 0.00017739595265368462,
      "loss": 0.726,
      "step": 893
    },
    {
      "epoch": 0.11370429252782194,
      "grad_norm": 2.136406183242798,
      "learning_rate": 0.00017737049764541174,
      "loss": 0.7422,
      "step": 894
    },
    {
      "epoch": 0.11383147853736089,
      "grad_norm": 2.6550583839416504,
      "learning_rate": 0.00017734504263713886,
      "loss": 1.3279,
      "step": 895
    },
    {
      "epoch": 0.11395866454689985,
      "grad_norm": 1.7194713354110718,
      "learning_rate": 0.00017731958762886598,
      "loss": 0.8071,
      "step": 896
    },
    {
      "epoch": 0.11408585055643879,
      "grad_norm": 2.267991781234741,
      "learning_rate": 0.00017729413262059312,
      "loss": 0.9366,
      "step": 897
    },
    {
      "epoch": 0.11421303656597774,
      "grad_norm": 1.8338243961334229,
      "learning_rate": 0.00017726867761232022,
      "loss": 0.6822,
      "step": 898
    },
    {
      "epoch": 0.1143402225755167,
      "grad_norm": 1.9954756498336792,
      "learning_rate": 0.00017724322260404736,
      "loss": 1.0266,
      "step": 899
    },
    {
      "epoch": 0.11446740858505565,
      "grad_norm": 1.7988877296447754,
      "learning_rate": 0.00017721776759577448,
      "loss": 0.878,
      "step": 900
    },
    {
      "epoch": 0.11459459459459459,
      "grad_norm": 1.5372730493545532,
      "learning_rate": 0.0001771923125875016,
      "loss": 0.7313,
      "step": 901
    },
    {
      "epoch": 0.11472178060413354,
      "grad_norm": 1.5794538259506226,
      "learning_rate": 0.00017716685757922872,
      "loss": 0.9859,
      "step": 902
    },
    {
      "epoch": 0.1148489666136725,
      "grad_norm": 1.6226345300674438,
      "learning_rate": 0.00017714140257095584,
      "loss": 0.9344,
      "step": 903
    },
    {
      "epoch": 0.11497615262321145,
      "grad_norm": 1.9989032745361328,
      "learning_rate": 0.00017711594756268298,
      "loss": 0.8083,
      "step": 904
    },
    {
      "epoch": 0.1151033386327504,
      "grad_norm": 1.6901828050613403,
      "learning_rate": 0.00017709049255441007,
      "loss": 0.8506,
      "step": 905
    },
    {
      "epoch": 0.11523052464228935,
      "grad_norm": 2.3046391010284424,
      "learning_rate": 0.00017706503754613722,
      "loss": 0.9048,
      "step": 906
    },
    {
      "epoch": 0.1153577106518283,
      "grad_norm": 2.0396506786346436,
      "learning_rate": 0.0001770395825378643,
      "loss": 0.9259,
      "step": 907
    },
    {
      "epoch": 0.11548489666136726,
      "grad_norm": 2.3717353343963623,
      "learning_rate": 0.00017701412752959146,
      "loss": 1.0498,
      "step": 908
    },
    {
      "epoch": 0.1156120826709062,
      "grad_norm": 2.4010257720947266,
      "learning_rate": 0.00017698867252131858,
      "loss": 0.8852,
      "step": 909
    },
    {
      "epoch": 0.11573926868044515,
      "grad_norm": 1.5807708501815796,
      "learning_rate": 0.0001769632175130457,
      "loss": 0.7054,
      "step": 910
    },
    {
      "epoch": 0.1158664546899841,
      "grad_norm": 1.5957032442092896,
      "learning_rate": 0.00017693776250477284,
      "loss": 0.7927,
      "step": 911
    },
    {
      "epoch": 0.11599364069952305,
      "grad_norm": 2.134082317352295,
      "learning_rate": 0.00017691230749649993,
      "loss": 1.0341,
      "step": 912
    },
    {
      "epoch": 0.116120826709062,
      "grad_norm": 1.997948408126831,
      "learning_rate": 0.00017688685248822708,
      "loss": 0.7841,
      "step": 913
    },
    {
      "epoch": 0.11624801271860095,
      "grad_norm": 2.135218620300293,
      "learning_rate": 0.00017686139747995417,
      "loss": 0.7539,
      "step": 914
    },
    {
      "epoch": 0.11637519872813991,
      "grad_norm": 2.177283525466919,
      "learning_rate": 0.00017683594247168132,
      "loss": 0.8854,
      "step": 915
    },
    {
      "epoch": 0.11650238473767885,
      "grad_norm": 1.6669766902923584,
      "learning_rate": 0.00017681048746340843,
      "loss": 0.7677,
      "step": 916
    },
    {
      "epoch": 0.1166295707472178,
      "grad_norm": 1.982056975364685,
      "learning_rate": 0.00017678503245513555,
      "loss": 0.5435,
      "step": 917
    },
    {
      "epoch": 0.11675675675675676,
      "grad_norm": 2.193840980529785,
      "learning_rate": 0.0001767595774468627,
      "loss": 0.8014,
      "step": 918
    },
    {
      "epoch": 0.11688394276629571,
      "grad_norm": 1.692781925201416,
      "learning_rate": 0.0001767341224385898,
      "loss": 0.7485,
      "step": 919
    },
    {
      "epoch": 0.11701112877583465,
      "grad_norm": 2.5152902603149414,
      "learning_rate": 0.00017670866743031694,
      "loss": 0.9154,
      "step": 920
    },
    {
      "epoch": 0.1171383147853736,
      "grad_norm": 1.9475412368774414,
      "learning_rate": 0.00017668321242204405,
      "loss": 1.0982,
      "step": 921
    },
    {
      "epoch": 0.11726550079491256,
      "grad_norm": 2.2277677059173584,
      "learning_rate": 0.00017665775741377117,
      "loss": 0.9001,
      "step": 922
    },
    {
      "epoch": 0.11739268680445152,
      "grad_norm": 2.240485429763794,
      "learning_rate": 0.0001766323024054983,
      "loss": 0.8379,
      "step": 923
    },
    {
      "epoch": 0.11751987281399046,
      "grad_norm": 1.929858684539795,
      "learning_rate": 0.0001766068473972254,
      "loss": 0.7292,
      "step": 924
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 1.36758291721344,
      "learning_rate": 0.00017658139238895253,
      "loss": 0.8437,
      "step": 925
    },
    {
      "epoch": 0.11777424483306836,
      "grad_norm": 1.8179242610931396,
      "learning_rate": 0.00017655593738067965,
      "loss": 0.8921,
      "step": 926
    },
    {
      "epoch": 0.11790143084260732,
      "grad_norm": 1.6687746047973633,
      "learning_rate": 0.00017653048237240677,
      "loss": 1.0459,
      "step": 927
    },
    {
      "epoch": 0.11802861685214626,
      "grad_norm": 2.0365960597991943,
      "learning_rate": 0.0001765050273641339,
      "loss": 0.9035,
      "step": 928
    },
    {
      "epoch": 0.11815580286168521,
      "grad_norm": 1.803551197052002,
      "learning_rate": 0.00017647957235586103,
      "loss": 0.9156,
      "step": 929
    },
    {
      "epoch": 0.11828298887122417,
      "grad_norm": 1.654590368270874,
      "learning_rate": 0.00017645411734758815,
      "loss": 0.7637,
      "step": 930
    },
    {
      "epoch": 0.11841017488076312,
      "grad_norm": 1.4281480312347412,
      "learning_rate": 0.00017642866233931527,
      "loss": 0.7656,
      "step": 931
    },
    {
      "epoch": 0.11853736089030206,
      "grad_norm": 2.1925466060638428,
      "learning_rate": 0.0001764032073310424,
      "loss": 1.1449,
      "step": 932
    },
    {
      "epoch": 0.11866454689984102,
      "grad_norm": 1.5371214151382446,
      "learning_rate": 0.0001763777523227695,
      "loss": 0.8829,
      "step": 933
    },
    {
      "epoch": 0.11879173290937997,
      "grad_norm": 1.5513653755187988,
      "learning_rate": 0.00017635229731449663,
      "loss": 0.8721,
      "step": 934
    },
    {
      "epoch": 0.11891891891891893,
      "grad_norm": 1.7885630130767822,
      "learning_rate": 0.00017632684230622377,
      "loss": 0.8056,
      "step": 935
    },
    {
      "epoch": 0.11904610492845787,
      "grad_norm": 1.5686172246932983,
      "learning_rate": 0.00017630138729795086,
      "loss": 0.7726,
      "step": 936
    },
    {
      "epoch": 0.11917329093799682,
      "grad_norm": 1.5162087678909302,
      "learning_rate": 0.000176275932289678,
      "loss": 0.831,
      "step": 937
    },
    {
      "epoch": 0.11930047694753577,
      "grad_norm": 1.5817359685897827,
      "learning_rate": 0.00017625047728140513,
      "loss": 0.7662,
      "step": 938
    },
    {
      "epoch": 0.11942766295707472,
      "grad_norm": 2.315087080001831,
      "learning_rate": 0.00017622502227313225,
      "loss": 0.6368,
      "step": 939
    },
    {
      "epoch": 0.11955484896661367,
      "grad_norm": 2.0666332244873047,
      "learning_rate": 0.00017619956726485936,
      "loss": 0.8504,
      "step": 940
    },
    {
      "epoch": 0.11968203497615262,
      "grad_norm": 1.877300500869751,
      "learning_rate": 0.00017617411225658648,
      "loss": 1.0647,
      "step": 941
    },
    {
      "epoch": 0.11980922098569158,
      "grad_norm": 1.5666522979736328,
      "learning_rate": 0.00017614865724831363,
      "loss": 0.9866,
      "step": 942
    },
    {
      "epoch": 0.11993640699523052,
      "grad_norm": 1.6080610752105713,
      "learning_rate": 0.00017612320224004072,
      "loss": 0.6883,
      "step": 943
    },
    {
      "epoch": 0.12006359300476947,
      "grad_norm": 1.470556616783142,
      "learning_rate": 0.00017609774723176787,
      "loss": 0.9363,
      "step": 944
    },
    {
      "epoch": 0.12019077901430843,
      "grad_norm": 2.057988405227661,
      "learning_rate": 0.00017607229222349499,
      "loss": 0.8826,
      "step": 945
    },
    {
      "epoch": 0.12031796502384738,
      "grad_norm": 2.3356165885925293,
      "learning_rate": 0.0001760468372152221,
      "loss": 1.0527,
      "step": 946
    },
    {
      "epoch": 0.12044515103338632,
      "grad_norm": 1.611711859703064,
      "learning_rate": 0.00017602138220694922,
      "loss": 0.8007,
      "step": 947
    },
    {
      "epoch": 0.12057233704292528,
      "grad_norm": 1.5629299879074097,
      "learning_rate": 0.00017599592719867634,
      "loss": 0.8458,
      "step": 948
    },
    {
      "epoch": 0.12069952305246423,
      "grad_norm": 1.6148016452789307,
      "learning_rate": 0.0001759704721904035,
      "loss": 0.9415,
      "step": 949
    },
    {
      "epoch": 0.12082670906200318,
      "grad_norm": 2.154759645462036,
      "learning_rate": 0.00017594501718213058,
      "loss": 0.7167,
      "step": 950
    },
    {
      "epoch": 0.12095389507154213,
      "grad_norm": 2.2193901538848877,
      "learning_rate": 0.00017591956217385773,
      "loss": 1.0457,
      "step": 951
    },
    {
      "epoch": 0.12108108108108108,
      "grad_norm": 2.174123525619507,
      "learning_rate": 0.00017589410716558484,
      "loss": 0.8597,
      "step": 952
    },
    {
      "epoch": 0.12120826709062003,
      "grad_norm": 1.8990693092346191,
      "learning_rate": 0.00017586865215731196,
      "loss": 0.8084,
      "step": 953
    },
    {
      "epoch": 0.12133545310015899,
      "grad_norm": 2.5254106521606445,
      "learning_rate": 0.00017584319714903908,
      "loss": 1.2044,
      "step": 954
    },
    {
      "epoch": 0.12146263910969793,
      "grad_norm": 1.9077160358428955,
      "learning_rate": 0.0001758177421407662,
      "loss": 0.9541,
      "step": 955
    },
    {
      "epoch": 0.12158982511923688,
      "grad_norm": 1.7752487659454346,
      "learning_rate": 0.00017579228713249332,
      "loss": 0.7435,
      "step": 956
    },
    {
      "epoch": 0.12171701112877584,
      "grad_norm": 1.6426091194152832,
      "learning_rate": 0.00017576683212422044,
      "loss": 0.6566,
      "step": 957
    },
    {
      "epoch": 0.12184419713831479,
      "grad_norm": 2.545290946960449,
      "learning_rate": 0.00017574137711594758,
      "loss": 0.7995,
      "step": 958
    },
    {
      "epoch": 0.12197138314785373,
      "grad_norm": 2.740185499191284,
      "learning_rate": 0.0001757159221076747,
      "loss": 0.7419,
      "step": 959
    },
    {
      "epoch": 0.12209856915739269,
      "grad_norm": 1.7161684036254883,
      "learning_rate": 0.00017569046709940182,
      "loss": 0.8314,
      "step": 960
    },
    {
      "epoch": 0.12222575516693164,
      "grad_norm": 2.0272367000579834,
      "learning_rate": 0.00017566501209112894,
      "loss": 0.8416,
      "step": 961
    },
    {
      "epoch": 0.1223529411764706,
      "grad_norm": 2.5588512420654297,
      "learning_rate": 0.00017563955708285606,
      "loss": 0.7786,
      "step": 962
    },
    {
      "epoch": 0.12248012718600954,
      "grad_norm": 1.7744543552398682,
      "learning_rate": 0.00017561410207458318,
      "loss": 0.8387,
      "step": 963
    },
    {
      "epoch": 0.12260731319554849,
      "grad_norm": 2.463991165161133,
      "learning_rate": 0.0001755886470663103,
      "loss": 1.0566,
      "step": 964
    },
    {
      "epoch": 0.12273449920508744,
      "grad_norm": 2.1347668170928955,
      "learning_rate": 0.00017556319205803741,
      "loss": 0.7175,
      "step": 965
    },
    {
      "epoch": 0.12286168521462638,
      "grad_norm": 1.8243441581726074,
      "learning_rate": 0.00017553773704976456,
      "loss": 0.798,
      "step": 966
    },
    {
      "epoch": 0.12298887122416534,
      "grad_norm": 1.9797650575637817,
      "learning_rate": 0.00017551228204149168,
      "loss": 0.977,
      "step": 967
    },
    {
      "epoch": 0.12311605723370429,
      "grad_norm": 2.264406442642212,
      "learning_rate": 0.0001754868270332188,
      "loss": 0.9249,
      "step": 968
    },
    {
      "epoch": 0.12324324324324325,
      "grad_norm": 1.9396953582763672,
      "learning_rate": 0.00017546137202494592,
      "loss": 1.0523,
      "step": 969
    },
    {
      "epoch": 0.12337042925278219,
      "grad_norm": 1.5591938495635986,
      "learning_rate": 0.00017543591701667304,
      "loss": 0.832,
      "step": 970
    },
    {
      "epoch": 0.12349761526232114,
      "grad_norm": 2.1013972759246826,
      "learning_rate": 0.00017541046200840015,
      "loss": 0.8263,
      "step": 971
    },
    {
      "epoch": 0.1236248012718601,
      "grad_norm": 2.0462725162506104,
      "learning_rate": 0.00017538500700012727,
      "loss": 1.0135,
      "step": 972
    },
    {
      "epoch": 0.12375198728139905,
      "grad_norm": 2.3346939086914062,
      "learning_rate": 0.00017535955199185442,
      "loss": 0.8381,
      "step": 973
    },
    {
      "epoch": 0.12387917329093799,
      "grad_norm": 2.5823819637298584,
      "learning_rate": 0.0001753340969835815,
      "loss": 0.9775,
      "step": 974
    },
    {
      "epoch": 0.12400635930047695,
      "grad_norm": 2.0671796798706055,
      "learning_rate": 0.00017530864197530866,
      "loss": 0.8121,
      "step": 975
    },
    {
      "epoch": 0.1241335453100159,
      "grad_norm": 2.1708781719207764,
      "learning_rate": 0.00017528318696703578,
      "loss": 0.7884,
      "step": 976
    },
    {
      "epoch": 0.12426073131955485,
      "grad_norm": 1.8876335620880127,
      "learning_rate": 0.0001752577319587629,
      "loss": 0.5895,
      "step": 977
    },
    {
      "epoch": 0.1243879173290938,
      "grad_norm": 1.5610336065292358,
      "learning_rate": 0.00017523227695049004,
      "loss": 0.7714,
      "step": 978
    },
    {
      "epoch": 0.12451510333863275,
      "grad_norm": 1.8306242227554321,
      "learning_rate": 0.00017520682194221713,
      "loss": 0.771,
      "step": 979
    },
    {
      "epoch": 0.1246422893481717,
      "grad_norm": 1.444419503211975,
      "learning_rate": 0.00017518136693394428,
      "loss": 0.6002,
      "step": 980
    },
    {
      "epoch": 0.12476947535771066,
      "grad_norm": 2.2249252796173096,
      "learning_rate": 0.00017515591192567137,
      "loss": 1.1505,
      "step": 981
    },
    {
      "epoch": 0.1248966613672496,
      "grad_norm": 2.2260549068450928,
      "learning_rate": 0.00017513045691739851,
      "loss": 0.8889,
      "step": 982
    },
    {
      "epoch": 0.12502384737678857,
      "grad_norm": 1.6560839414596558,
      "learning_rate": 0.00017510500190912563,
      "loss": 0.7856,
      "step": 983
    },
    {
      "epoch": 0.1251510333863275,
      "grad_norm": 1.9041574001312256,
      "learning_rate": 0.00017507954690085275,
      "loss": 0.7627,
      "step": 984
    },
    {
      "epoch": 0.12527821939586645,
      "grad_norm": 2.226363182067871,
      "learning_rate": 0.00017505409189257987,
      "loss": 0.7912,
      "step": 985
    },
    {
      "epoch": 0.1254054054054054,
      "grad_norm": 1.8643243312835693,
      "learning_rate": 0.000175028636884307,
      "loss": 0.7215,
      "step": 986
    },
    {
      "epoch": 0.12553259141494436,
      "grad_norm": 1.7501931190490723,
      "learning_rate": 0.00017500318187603414,
      "loss": 0.6854,
      "step": 987
    },
    {
      "epoch": 0.1256597774244833,
      "grad_norm": 3.191596269607544,
      "learning_rate": 0.00017497772686776123,
      "loss": 0.7274,
      "step": 988
    },
    {
      "epoch": 0.12578696343402226,
      "grad_norm": 2.1655068397521973,
      "learning_rate": 0.00017495227185948837,
      "loss": 0.6427,
      "step": 989
    },
    {
      "epoch": 0.12591414944356122,
      "grad_norm": 2.3153507709503174,
      "learning_rate": 0.0001749268168512155,
      "loss": 0.5429,
      "step": 990
    },
    {
      "epoch": 0.12604133545310017,
      "grad_norm": 1.636404275894165,
      "learning_rate": 0.0001749013618429426,
      "loss": 0.9494,
      "step": 991
    },
    {
      "epoch": 0.1261685214626391,
      "grad_norm": 2.5579020977020264,
      "learning_rate": 0.00017487590683466973,
      "loss": 0.8483,
      "step": 992
    },
    {
      "epoch": 0.12629570747217805,
      "grad_norm": 2.099142551422119,
      "learning_rate": 0.00017485045182639685,
      "loss": 0.7141,
      "step": 993
    },
    {
      "epoch": 0.126422893481717,
      "grad_norm": 1.5231690406799316,
      "learning_rate": 0.00017482499681812397,
      "loss": 0.7169,
      "step": 994
    },
    {
      "epoch": 0.12655007949125596,
      "grad_norm": 1.8536990880966187,
      "learning_rate": 0.00017479954180985109,
      "loss": 0.6662,
      "step": 995
    },
    {
      "epoch": 0.12667726550079492,
      "grad_norm": 2.043874740600586,
      "learning_rate": 0.00017477408680157823,
      "loss": 0.9797,
      "step": 996
    },
    {
      "epoch": 0.12680445151033387,
      "grad_norm": 1.7361358404159546,
      "learning_rate": 0.00017474863179330535,
      "loss": 0.7778,
      "step": 997
    },
    {
      "epoch": 0.12693163751987283,
      "grad_norm": 1.4523844718933105,
      "learning_rate": 0.00017472317678503247,
      "loss": 0.6371,
      "step": 998
    },
    {
      "epoch": 0.12705882352941175,
      "grad_norm": 2.305946111679077,
      "learning_rate": 0.0001746977217767596,
      "loss": 0.9769,
      "step": 999
    },
    {
      "epoch": 0.1271860095389507,
      "grad_norm": 1.6527637243270874,
      "learning_rate": 0.0001746722667684867,
      "loss": 0.8962,
      "step": 1000
    },
    {
      "epoch": 0.12731319554848966,
      "grad_norm": 2.0471036434173584,
      "learning_rate": 0.00017464681176021382,
      "loss": 1.0048,
      "step": 1001
    },
    {
      "epoch": 0.12744038155802861,
      "grad_norm": 2.406928062438965,
      "learning_rate": 0.00017462135675194094,
      "loss": 0.8539,
      "step": 1002
    },
    {
      "epoch": 0.12756756756756757,
      "grad_norm": 2.076763391494751,
      "learning_rate": 0.00017459590174366806,
      "loss": 0.8891,
      "step": 1003
    },
    {
      "epoch": 0.12769475357710652,
      "grad_norm": 2.4455296993255615,
      "learning_rate": 0.0001745704467353952,
      "loss": 0.9266,
      "step": 1004
    },
    {
      "epoch": 0.12782193958664548,
      "grad_norm": 2.0748817920684814,
      "learning_rate": 0.0001745449917271223,
      "loss": 0.8494,
      "step": 1005
    },
    {
      "epoch": 0.12794912559618443,
      "grad_norm": 1.491209864616394,
      "learning_rate": 0.00017451953671884945,
      "loss": 0.7248,
      "step": 1006
    },
    {
      "epoch": 0.12807631160572336,
      "grad_norm": 2.028836250305176,
      "learning_rate": 0.00017449408171057656,
      "loss": 0.8717,
      "step": 1007
    },
    {
      "epoch": 0.1282034976152623,
      "grad_norm": 1.4544121026992798,
      "learning_rate": 0.00017446862670230368,
      "loss": 0.6924,
      "step": 1008
    },
    {
      "epoch": 0.12833068362480127,
      "grad_norm": 1.7856700420379639,
      "learning_rate": 0.00017444317169403083,
      "loss": 0.7248,
      "step": 1009
    },
    {
      "epoch": 0.12845786963434022,
      "grad_norm": 2.1207449436187744,
      "learning_rate": 0.00017441771668575792,
      "loss": 0.7365,
      "step": 1010
    },
    {
      "epoch": 0.12858505564387918,
      "grad_norm": 1.4096330404281616,
      "learning_rate": 0.00017439226167748507,
      "loss": 0.8176,
      "step": 1011
    },
    {
      "epoch": 0.12871224165341813,
      "grad_norm": 2.371020793914795,
      "learning_rate": 0.00017436680666921216,
      "loss": 0.8892,
      "step": 1012
    },
    {
      "epoch": 0.12883942766295708,
      "grad_norm": 2.2031116485595703,
      "learning_rate": 0.0001743413516609393,
      "loss": 0.9764,
      "step": 1013
    },
    {
      "epoch": 0.12896661367249604,
      "grad_norm": 2.051949977874756,
      "learning_rate": 0.00017431589665266642,
      "loss": 0.9085,
      "step": 1014
    },
    {
      "epoch": 0.12909379968203497,
      "grad_norm": 2.0499086380004883,
      "learning_rate": 0.00017429044164439354,
      "loss": 0.7735,
      "step": 1015
    },
    {
      "epoch": 0.12922098569157392,
      "grad_norm": 1.9957443475723267,
      "learning_rate": 0.0001742649866361207,
      "loss": 0.8172,
      "step": 1016
    },
    {
      "epoch": 0.12934817170111287,
      "grad_norm": 1.6233725547790527,
      "learning_rate": 0.00017423953162784778,
      "loss": 0.5109,
      "step": 1017
    },
    {
      "epoch": 0.12947535771065183,
      "grad_norm": 3.075103282928467,
      "learning_rate": 0.00017421407661957492,
      "loss": 0.8995,
      "step": 1018
    },
    {
      "epoch": 0.12960254372019078,
      "grad_norm": 2.781113624572754,
      "learning_rate": 0.00017418862161130202,
      "loss": 1.0103,
      "step": 1019
    },
    {
      "epoch": 0.12972972972972974,
      "grad_norm": 1.744136095046997,
      "learning_rate": 0.00017416316660302916,
      "loss": 0.8132,
      "step": 1020
    },
    {
      "epoch": 0.1298569157392687,
      "grad_norm": 1.990187168121338,
      "learning_rate": 0.00017413771159475628,
      "loss": 0.6822,
      "step": 1021
    },
    {
      "epoch": 0.12998410174880762,
      "grad_norm": 1.4837801456451416,
      "learning_rate": 0.0001741122565864834,
      "loss": 0.5383,
      "step": 1022
    },
    {
      "epoch": 0.13011128775834657,
      "grad_norm": 1.8944153785705566,
      "learning_rate": 0.00017408680157821052,
      "loss": 0.6686,
      "step": 1023
    },
    {
      "epoch": 0.13023847376788553,
      "grad_norm": 2.536965847015381,
      "learning_rate": 0.00017406134656993764,
      "loss": 1.0812,
      "step": 1024
    },
    {
      "epoch": 0.13036565977742448,
      "grad_norm": 1.9313091039657593,
      "learning_rate": 0.00017403589156166478,
      "loss": 0.6281,
      "step": 1025
    },
    {
      "epoch": 0.13049284578696344,
      "grad_norm": 2.3644185066223145,
      "learning_rate": 0.00017401043655339187,
      "loss": 0.9812,
      "step": 1026
    },
    {
      "epoch": 0.1306200317965024,
      "grad_norm": 1.5516796112060547,
      "learning_rate": 0.00017398498154511902,
      "loss": 0.7378,
      "step": 1027
    },
    {
      "epoch": 0.13074721780604134,
      "grad_norm": 2.02728009223938,
      "learning_rate": 0.00017395952653684614,
      "loss": 0.8752,
      "step": 1028
    },
    {
      "epoch": 0.1308744038155803,
      "grad_norm": 2.5510287284851074,
      "learning_rate": 0.00017393407152857326,
      "loss": 0.7247,
      "step": 1029
    },
    {
      "epoch": 0.13100158982511922,
      "grad_norm": 1.8360090255737305,
      "learning_rate": 0.00017390861652030038,
      "loss": 0.8818,
      "step": 1030
    },
    {
      "epoch": 0.13112877583465818,
      "grad_norm": 1.7769057750701904,
      "learning_rate": 0.0001738831615120275,
      "loss": 0.8974,
      "step": 1031
    },
    {
      "epoch": 0.13125596184419713,
      "grad_norm": 2.0013630390167236,
      "learning_rate": 0.00017385770650375461,
      "loss": 0.8338,
      "step": 1032
    },
    {
      "epoch": 0.1313831478537361,
      "grad_norm": 2.136230230331421,
      "learning_rate": 0.00017383225149548173,
      "loss": 1.113,
      "step": 1033
    },
    {
      "epoch": 0.13151033386327504,
      "grad_norm": 1.976375937461853,
      "learning_rate": 0.00017380679648720885,
      "loss": 0.8518,
      "step": 1034
    },
    {
      "epoch": 0.131637519872814,
      "grad_norm": 1.695408582687378,
      "learning_rate": 0.000173781341478936,
      "loss": 0.9903,
      "step": 1035
    },
    {
      "epoch": 0.13176470588235295,
      "grad_norm": 1.5010944604873657,
      "learning_rate": 0.00017375588647066312,
      "loss": 0.6464,
      "step": 1036
    },
    {
      "epoch": 0.1318918918918919,
      "grad_norm": 2.2865192890167236,
      "learning_rate": 0.00017373043146239024,
      "loss": 1.0012,
      "step": 1037
    },
    {
      "epoch": 0.13201907790143083,
      "grad_norm": 1.975002408027649,
      "learning_rate": 0.00017370497645411735,
      "loss": 0.8192,
      "step": 1038
    },
    {
      "epoch": 0.13214626391096979,
      "grad_norm": 2.3868062496185303,
      "learning_rate": 0.00017367952144584447,
      "loss": 0.8028,
      "step": 1039
    },
    {
      "epoch": 0.13227344992050874,
      "grad_norm": 1.541156530380249,
      "learning_rate": 0.00017365406643757162,
      "loss": 0.6518,
      "step": 1040
    },
    {
      "epoch": 0.1324006359300477,
      "grad_norm": 2.2385454177856445,
      "learning_rate": 0.0001736286114292987,
      "loss": 0.8632,
      "step": 1041
    },
    {
      "epoch": 0.13252782193958665,
      "grad_norm": 2.184934377670288,
      "learning_rate": 0.00017360315642102586,
      "loss": 0.9636,
      "step": 1042
    },
    {
      "epoch": 0.1326550079491256,
      "grad_norm": 1.9422636032104492,
      "learning_rate": 0.00017357770141275295,
      "loss": 0.8546,
      "step": 1043
    },
    {
      "epoch": 0.13278219395866456,
      "grad_norm": 1.7487114667892456,
      "learning_rate": 0.0001735522464044801,
      "loss": 0.8608,
      "step": 1044
    },
    {
      "epoch": 0.13290937996820348,
      "grad_norm": 1.9602361917495728,
      "learning_rate": 0.0001735267913962072,
      "loss": 0.5289,
      "step": 1045
    },
    {
      "epoch": 0.13303656597774244,
      "grad_norm": 2.2148516178131104,
      "learning_rate": 0.00017350133638793433,
      "loss": 0.7974,
      "step": 1046
    },
    {
      "epoch": 0.1331637519872814,
      "grad_norm": 1.770618200302124,
      "learning_rate": 0.00017347588137966148,
      "loss": 0.8229,
      "step": 1047
    },
    {
      "epoch": 0.13329093799682035,
      "grad_norm": 1.731166124343872,
      "learning_rate": 0.00017345042637138857,
      "loss": 0.6827,
      "step": 1048
    },
    {
      "epoch": 0.1334181240063593,
      "grad_norm": 1.5078825950622559,
      "learning_rate": 0.00017342497136311571,
      "loss": 0.6561,
      "step": 1049
    },
    {
      "epoch": 0.13354531001589826,
      "grad_norm": 2.0141725540161133,
      "learning_rate": 0.0001733995163548428,
      "loss": 0.8727,
      "step": 1050
    },
    {
      "epoch": 0.1336724960254372,
      "grad_norm": 2.1502737998962402,
      "learning_rate": 0.00017337406134656995,
      "loss": 1.0072,
      "step": 1051
    },
    {
      "epoch": 0.13379968203497616,
      "grad_norm": 2.4098522663116455,
      "learning_rate": 0.00017334860633829707,
      "loss": 1.26,
      "step": 1052
    },
    {
      "epoch": 0.1339268680445151,
      "grad_norm": 2.6166939735412598,
      "learning_rate": 0.0001733231513300242,
      "loss": 0.8735,
      "step": 1053
    },
    {
      "epoch": 0.13405405405405404,
      "grad_norm": 2.2019193172454834,
      "learning_rate": 0.00017329769632175133,
      "loss": 0.6589,
      "step": 1054
    },
    {
      "epoch": 0.134181240063593,
      "grad_norm": 2.1437082290649414,
      "learning_rate": 0.00017327224131347843,
      "loss": 0.8615,
      "step": 1055
    },
    {
      "epoch": 0.13430842607313195,
      "grad_norm": 2.6677353382110596,
      "learning_rate": 0.00017324678630520557,
      "loss": 1.0992,
      "step": 1056
    },
    {
      "epoch": 0.1344356120826709,
      "grad_norm": 2.5301244258880615,
      "learning_rate": 0.00017322133129693266,
      "loss": 0.8909,
      "step": 1057
    },
    {
      "epoch": 0.13456279809220986,
      "grad_norm": 2.0545895099639893,
      "learning_rate": 0.0001731958762886598,
      "loss": 0.6374,
      "step": 1058
    },
    {
      "epoch": 0.13468998410174882,
      "grad_norm": 2.1832973957061768,
      "learning_rate": 0.00017317042128038693,
      "loss": 0.7747,
      "step": 1059
    },
    {
      "epoch": 0.13481717011128777,
      "grad_norm": 1.8834962844848633,
      "learning_rate": 0.00017314496627211405,
      "loss": 0.6772,
      "step": 1060
    },
    {
      "epoch": 0.1349443561208267,
      "grad_norm": 2.331242084503174,
      "learning_rate": 0.00017311951126384117,
      "loss": 0.8176,
      "step": 1061
    },
    {
      "epoch": 0.13507154213036565,
      "grad_norm": 2.068459987640381,
      "learning_rate": 0.00017309405625556828,
      "loss": 0.6537,
      "step": 1062
    },
    {
      "epoch": 0.1351987281399046,
      "grad_norm": 1.485717535018921,
      "learning_rate": 0.0001730686012472954,
      "loss": 1.021,
      "step": 1063
    },
    {
      "epoch": 0.13532591414944356,
      "grad_norm": 1.8074085712432861,
      "learning_rate": 0.00017304314623902252,
      "loss": 0.6757,
      "step": 1064
    },
    {
      "epoch": 0.13545310015898251,
      "grad_norm": 1.7720953226089478,
      "learning_rate": 0.00017301769123074967,
      "loss": 0.9204,
      "step": 1065
    },
    {
      "epoch": 0.13558028616852147,
      "grad_norm": 2.52553653717041,
      "learning_rate": 0.0001729922362224768,
      "loss": 0.7901,
      "step": 1066
    },
    {
      "epoch": 0.13570747217806042,
      "grad_norm": 2.45396089553833,
      "learning_rate": 0.0001729667812142039,
      "loss": 1.0836,
      "step": 1067
    },
    {
      "epoch": 0.13583465818759938,
      "grad_norm": 1.6516584157943726,
      "learning_rate": 0.00017294132620593102,
      "loss": 0.8825,
      "step": 1068
    },
    {
      "epoch": 0.1359618441971383,
      "grad_norm": 2.2730300426483154,
      "learning_rate": 0.00017291587119765814,
      "loss": 0.9896,
      "step": 1069
    },
    {
      "epoch": 0.13608903020667726,
      "grad_norm": 2.0429086685180664,
      "learning_rate": 0.00017289041618938526,
      "loss": 0.7835,
      "step": 1070
    },
    {
      "epoch": 0.1362162162162162,
      "grad_norm": 2.2613887786865234,
      "learning_rate": 0.0001728649611811124,
      "loss": 0.9016,
      "step": 1071
    },
    {
      "epoch": 0.13634340222575517,
      "grad_norm": 1.6117897033691406,
      "learning_rate": 0.0001728395061728395,
      "loss": 0.894,
      "step": 1072
    },
    {
      "epoch": 0.13647058823529412,
      "grad_norm": 1.7039518356323242,
      "learning_rate": 0.00017281405116456665,
      "loss": 0.7719,
      "step": 1073
    },
    {
      "epoch": 0.13659777424483308,
      "grad_norm": 1.3742048740386963,
      "learning_rate": 0.00017278859615629376,
      "loss": 0.8623,
      "step": 1074
    },
    {
      "epoch": 0.13672496025437203,
      "grad_norm": 1.9682343006134033,
      "learning_rate": 0.00017276314114802088,
      "loss": 0.6767,
      "step": 1075
    },
    {
      "epoch": 0.13685214626391096,
      "grad_norm": 1.705057978630066,
      "learning_rate": 0.000172737686139748,
      "loss": 0.6435,
      "step": 1076
    },
    {
      "epoch": 0.1369793322734499,
      "grad_norm": 1.1887552738189697,
      "learning_rate": 0.00017271223113147512,
      "loss": 0.7009,
      "step": 1077
    },
    {
      "epoch": 0.13710651828298887,
      "grad_norm": 2.3032889366149902,
      "learning_rate": 0.00017268677612320227,
      "loss": 0.7573,
      "step": 1078
    },
    {
      "epoch": 0.13723370429252782,
      "grad_norm": 2.099499464035034,
      "learning_rate": 0.00017266132111492936,
      "loss": 0.9237,
      "step": 1079
    },
    {
      "epoch": 0.13736089030206677,
      "grad_norm": 3.0206642150878906,
      "learning_rate": 0.0001726358661066565,
      "loss": 1.1305,
      "step": 1080
    },
    {
      "epoch": 0.13748807631160573,
      "grad_norm": 2.174569606781006,
      "learning_rate": 0.0001726104110983836,
      "loss": 1.0097,
      "step": 1081
    },
    {
      "epoch": 0.13761526232114468,
      "grad_norm": 2.243607759475708,
      "learning_rate": 0.00017258495609011074,
      "loss": 0.7285,
      "step": 1082
    },
    {
      "epoch": 0.13774244833068364,
      "grad_norm": 2.0941085815429688,
      "learning_rate": 0.00017255950108183786,
      "loss": 0.7549,
      "step": 1083
    },
    {
      "epoch": 0.13786963434022256,
      "grad_norm": 2.4924139976501465,
      "learning_rate": 0.00017253404607356498,
      "loss": 0.8182,
      "step": 1084
    },
    {
      "epoch": 0.13799682034976152,
      "grad_norm": 2.44329571723938,
      "learning_rate": 0.00017250859106529212,
      "loss": 0.8676,
      "step": 1085
    },
    {
      "epoch": 0.13812400635930047,
      "grad_norm": 1.8892550468444824,
      "learning_rate": 0.00017248313605701922,
      "loss": 0.5259,
      "step": 1086
    },
    {
      "epoch": 0.13825119236883943,
      "grad_norm": 1.7758749723434448,
      "learning_rate": 0.00017245768104874636,
      "loss": 0.51,
      "step": 1087
    },
    {
      "epoch": 0.13837837837837838,
      "grad_norm": 2.789802074432373,
      "learning_rate": 0.00017243222604047345,
      "loss": 0.6418,
      "step": 1088
    },
    {
      "epoch": 0.13850556438791733,
      "grad_norm": 2.264965772628784,
      "learning_rate": 0.0001724067710322006,
      "loss": 0.6754,
      "step": 1089
    },
    {
      "epoch": 0.1386327503974563,
      "grad_norm": 2.2182834148406982,
      "learning_rate": 0.00017238131602392772,
      "loss": 0.7011,
      "step": 1090
    },
    {
      "epoch": 0.13875993640699524,
      "grad_norm": 2.093940496444702,
      "learning_rate": 0.00017235586101565484,
      "loss": 1.1062,
      "step": 1091
    },
    {
      "epoch": 0.13888712241653417,
      "grad_norm": 2.0405995845794678,
      "learning_rate": 0.00017233040600738196,
      "loss": 0.673,
      "step": 1092
    },
    {
      "epoch": 0.13901430842607312,
      "grad_norm": 1.5225067138671875,
      "learning_rate": 0.00017230495099910907,
      "loss": 0.9366,
      "step": 1093
    },
    {
      "epoch": 0.13914149443561208,
      "grad_norm": 2.1508333683013916,
      "learning_rate": 0.00017227949599083622,
      "loss": 0.7986,
      "step": 1094
    },
    {
      "epoch": 0.13926868044515103,
      "grad_norm": 1.6339232921600342,
      "learning_rate": 0.0001722540409825633,
      "loss": 0.8046,
      "step": 1095
    },
    {
      "epoch": 0.13939586645469,
      "grad_norm": 1.415099024772644,
      "learning_rate": 0.00017222858597429046,
      "loss": 0.6939,
      "step": 1096
    },
    {
      "epoch": 0.13952305246422894,
      "grad_norm": 1.7184295654296875,
      "learning_rate": 0.00017220313096601758,
      "loss": 0.8476,
      "step": 1097
    },
    {
      "epoch": 0.1396502384737679,
      "grad_norm": 1.6760863065719604,
      "learning_rate": 0.0001721776759577447,
      "loss": 0.6991,
      "step": 1098
    },
    {
      "epoch": 0.13977742448330682,
      "grad_norm": 1.764082908630371,
      "learning_rate": 0.00017215222094947181,
      "loss": 0.6672,
      "step": 1099
    },
    {
      "epoch": 0.13990461049284578,
      "grad_norm": 1.8139572143554688,
      "learning_rate": 0.00017212676594119893,
      "loss": 0.7605,
      "step": 1100
    },
    {
      "epoch": 0.14003179650238473,
      "grad_norm": 2.4966092109680176,
      "learning_rate": 0.00017210131093292605,
      "loss": 1.1603,
      "step": 1101
    },
    {
      "epoch": 0.14015898251192369,
      "grad_norm": 1.625032663345337,
      "learning_rate": 0.0001720758559246532,
      "loss": 1.0071,
      "step": 1102
    },
    {
      "epoch": 0.14028616852146264,
      "grad_norm": 1.8295241594314575,
      "learning_rate": 0.00017205040091638032,
      "loss": 0.8192,
      "step": 1103
    },
    {
      "epoch": 0.1404133545310016,
      "grad_norm": 1.944549322128296,
      "learning_rate": 0.00017202494590810743,
      "loss": 1.0006,
      "step": 1104
    },
    {
      "epoch": 0.14054054054054055,
      "grad_norm": 1.7398632764816284,
      "learning_rate": 0.00017199949089983455,
      "loss": 0.7646,
      "step": 1105
    },
    {
      "epoch": 0.1406677265500795,
      "grad_norm": 1.5175443887710571,
      "learning_rate": 0.00017197403589156167,
      "loss": 0.6422,
      "step": 1106
    },
    {
      "epoch": 0.14079491255961843,
      "grad_norm": 1.9441274404525757,
      "learning_rate": 0.0001719485808832888,
      "loss": 0.7587,
      "step": 1107
    },
    {
      "epoch": 0.14092209856915738,
      "grad_norm": 1.714933156967163,
      "learning_rate": 0.0001719231258750159,
      "loss": 0.8223,
      "step": 1108
    },
    {
      "epoch": 0.14104928457869634,
      "grad_norm": 1.8613351583480835,
      "learning_rate": 0.00017189767086674306,
      "loss": 0.857,
      "step": 1109
    },
    {
      "epoch": 0.1411764705882353,
      "grad_norm": 2.3789772987365723,
      "learning_rate": 0.00017187221585847015,
      "loss": 0.9816,
      "step": 1110
    },
    {
      "epoch": 0.14130365659777425,
      "grad_norm": 2.748244524002075,
      "learning_rate": 0.0001718467608501973,
      "loss": 0.7943,
      "step": 1111
    },
    {
      "epoch": 0.1414308426073132,
      "grad_norm": 1.965403437614441,
      "learning_rate": 0.00017182130584192438,
      "loss": 0.6318,
      "step": 1112
    },
    {
      "epoch": 0.14155802861685216,
      "grad_norm": 1.902220368385315,
      "learning_rate": 0.00017179585083365153,
      "loss": 0.9236,
      "step": 1113
    },
    {
      "epoch": 0.1416852146263911,
      "grad_norm": 2.648482322692871,
      "learning_rate": 0.00017177039582537865,
      "loss": 1.2445,
      "step": 1114
    },
    {
      "epoch": 0.14181240063593004,
      "grad_norm": 2.344597816467285,
      "learning_rate": 0.00017174494081710577,
      "loss": 0.8018,
      "step": 1115
    },
    {
      "epoch": 0.141939586645469,
      "grad_norm": 1.7384804487228394,
      "learning_rate": 0.00017171948580883291,
      "loss": 0.7872,
      "step": 1116
    },
    {
      "epoch": 0.14206677265500794,
      "grad_norm": 2.1986992359161377,
      "learning_rate": 0.00017169403080056,
      "loss": 1.0693,
      "step": 1117
    },
    {
      "epoch": 0.1421939586645469,
      "grad_norm": 2.2408764362335205,
      "learning_rate": 0.00017166857579228715,
      "loss": 0.8833,
      "step": 1118
    },
    {
      "epoch": 0.14232114467408585,
      "grad_norm": 1.9457058906555176,
      "learning_rate": 0.00017164312078401424,
      "loss": 0.8068,
      "step": 1119
    },
    {
      "epoch": 0.1424483306836248,
      "grad_norm": 1.7417895793914795,
      "learning_rate": 0.0001716176657757414,
      "loss": 0.6457,
      "step": 1120
    },
    {
      "epoch": 0.14257551669316376,
      "grad_norm": 1.7647069692611694,
      "learning_rate": 0.0001715922107674685,
      "loss": 0.7326,
      "step": 1121
    },
    {
      "epoch": 0.14270270270270272,
      "grad_norm": 1.5944961309432983,
      "learning_rate": 0.00017156675575919563,
      "loss": 0.8096,
      "step": 1122
    },
    {
      "epoch": 0.14282988871224164,
      "grad_norm": 2.038879871368408,
      "learning_rate": 0.00017154130075092277,
      "loss": 0.7908,
      "step": 1123
    },
    {
      "epoch": 0.1429570747217806,
      "grad_norm": 1.4862074851989746,
      "learning_rate": 0.00017151584574264986,
      "loss": 0.7122,
      "step": 1124
    },
    {
      "epoch": 0.14308426073131955,
      "grad_norm": 1.4230961799621582,
      "learning_rate": 0.000171490390734377,
      "loss": 0.6702,
      "step": 1125
    },
    {
      "epoch": 0.1432114467408585,
      "grad_norm": 1.782829761505127,
      "learning_rate": 0.0001714649357261041,
      "loss": 0.7484,
      "step": 1126
    },
    {
      "epoch": 0.14333863275039746,
      "grad_norm": 2.5223474502563477,
      "learning_rate": 0.00017143948071783125,
      "loss": 1.0017,
      "step": 1127
    },
    {
      "epoch": 0.14346581875993641,
      "grad_norm": 1.8846557140350342,
      "learning_rate": 0.00017141402570955837,
      "loss": 0.9196,
      "step": 1128
    },
    {
      "epoch": 0.14359300476947537,
      "grad_norm": 2.053346633911133,
      "learning_rate": 0.00017138857070128548,
      "loss": 0.7713,
      "step": 1129
    },
    {
      "epoch": 0.1437201907790143,
      "grad_norm": 1.3026715517044067,
      "learning_rate": 0.0001713631156930126,
      "loss": 0.6416,
      "step": 1130
    },
    {
      "epoch": 0.14384737678855325,
      "grad_norm": 1.8913956880569458,
      "learning_rate": 0.00017133766068473972,
      "loss": 0.8272,
      "step": 1131
    },
    {
      "epoch": 0.1439745627980922,
      "grad_norm": 2.0221261978149414,
      "learning_rate": 0.00017131220567646687,
      "loss": 0.7644,
      "step": 1132
    },
    {
      "epoch": 0.14410174880763116,
      "grad_norm": 1.865954041481018,
      "learning_rate": 0.000171286750668194,
      "loss": 0.7833,
      "step": 1133
    },
    {
      "epoch": 0.1442289348171701,
      "grad_norm": 1.68902587890625,
      "learning_rate": 0.0001712612956599211,
      "loss": 0.6074,
      "step": 1134
    },
    {
      "epoch": 0.14435612082670907,
      "grad_norm": 1.4845285415649414,
      "learning_rate": 0.00017123584065164822,
      "loss": 0.8733,
      "step": 1135
    },
    {
      "epoch": 0.14448330683624802,
      "grad_norm": 2.2018511295318604,
      "learning_rate": 0.00017121038564337534,
      "loss": 0.7217,
      "step": 1136
    },
    {
      "epoch": 0.14461049284578698,
      "grad_norm": 2.0606915950775146,
      "learning_rate": 0.00017118493063510246,
      "loss": 0.8983,
      "step": 1137
    },
    {
      "epoch": 0.1447376788553259,
      "grad_norm": 1.6123042106628418,
      "learning_rate": 0.00017115947562682958,
      "loss": 0.6776,
      "step": 1138
    },
    {
      "epoch": 0.14486486486486486,
      "grad_norm": 1.7228162288665771,
      "learning_rate": 0.0001711340206185567,
      "loss": 0.5944,
      "step": 1139
    },
    {
      "epoch": 0.1449920508744038,
      "grad_norm": 2.1346585750579834,
      "learning_rate": 0.00017110856561028384,
      "loss": 0.9554,
      "step": 1140
    },
    {
      "epoch": 0.14511923688394276,
      "grad_norm": 2.0120038986206055,
      "learning_rate": 0.00017108311060201094,
      "loss": 0.7831,
      "step": 1141
    },
    {
      "epoch": 0.14524642289348172,
      "grad_norm": 2.2250137329101562,
      "learning_rate": 0.00017105765559373808,
      "loss": 0.9067,
      "step": 1142
    },
    {
      "epoch": 0.14537360890302067,
      "grad_norm": 1.6738032102584839,
      "learning_rate": 0.0001710322005854652,
      "loss": 0.8142,
      "step": 1143
    },
    {
      "epoch": 0.14550079491255963,
      "grad_norm": 1.7426105737686157,
      "learning_rate": 0.00017100674557719232,
      "loss": 0.6864,
      "step": 1144
    },
    {
      "epoch": 0.14562798092209858,
      "grad_norm": 2.742095947265625,
      "learning_rate": 0.00017098129056891944,
      "loss": 0.8019,
      "step": 1145
    },
    {
      "epoch": 0.1457551669316375,
      "grad_norm": 1.6968482732772827,
      "learning_rate": 0.00017095583556064656,
      "loss": 0.7016,
      "step": 1146
    },
    {
      "epoch": 0.14588235294117646,
      "grad_norm": 1.6646625995635986,
      "learning_rate": 0.0001709303805523737,
      "loss": 0.7358,
      "step": 1147
    },
    {
      "epoch": 0.14600953895071542,
      "grad_norm": 1.8271894454956055,
      "learning_rate": 0.0001709049255441008,
      "loss": 0.5496,
      "step": 1148
    },
    {
      "epoch": 0.14613672496025437,
      "grad_norm": 2.100104570388794,
      "learning_rate": 0.00017087947053582794,
      "loss": 0.8912,
      "step": 1149
    },
    {
      "epoch": 0.14626391096979333,
      "grad_norm": 2.8129403591156006,
      "learning_rate": 0.00017085401552755503,
      "loss": 0.9342,
      "step": 1150
    },
    {
      "epoch": 0.14639109697933228,
      "grad_norm": 2.4904682636260986,
      "learning_rate": 0.00017082856051928218,
      "loss": 0.9194,
      "step": 1151
    },
    {
      "epoch": 0.14651828298887123,
      "grad_norm": 1.5680525302886963,
      "learning_rate": 0.0001708031055110093,
      "loss": 0.7729,
      "step": 1152
    },
    {
      "epoch": 0.14664546899841016,
      "grad_norm": 2.4420454502105713,
      "learning_rate": 0.00017077765050273642,
      "loss": 0.6205,
      "step": 1153
    },
    {
      "epoch": 0.14677265500794912,
      "grad_norm": 1.8697677850723267,
      "learning_rate": 0.00017075219549446356,
      "loss": 0.8657,
      "step": 1154
    },
    {
      "epoch": 0.14689984101748807,
      "grad_norm": 2.2037172317504883,
      "learning_rate": 0.00017072674048619065,
      "loss": 0.8866,
      "step": 1155
    },
    {
      "epoch": 0.14702702702702702,
      "grad_norm": 2.0933609008789062,
      "learning_rate": 0.0001707012854779178,
      "loss": 0.8926,
      "step": 1156
    },
    {
      "epoch": 0.14715421303656598,
      "grad_norm": 2.0920653343200684,
      "learning_rate": 0.00017067583046964492,
      "loss": 1.0229,
      "step": 1157
    },
    {
      "epoch": 0.14728139904610493,
      "grad_norm": 1.4887118339538574,
      "learning_rate": 0.00017065037546137204,
      "loss": 0.7017,
      "step": 1158
    },
    {
      "epoch": 0.1474085850556439,
      "grad_norm": 2.059199333190918,
      "learning_rate": 0.00017062492045309915,
      "loss": 0.6649,
      "step": 1159
    },
    {
      "epoch": 0.14753577106518284,
      "grad_norm": 3.04118275642395,
      "learning_rate": 0.00017059946544482627,
      "loss": 0.824,
      "step": 1160
    },
    {
      "epoch": 0.14766295707472177,
      "grad_norm": 1.9154967069625854,
      "learning_rate": 0.00017057401043655342,
      "loss": 0.6942,
      "step": 1161
    },
    {
      "epoch": 0.14779014308426072,
      "grad_norm": 4.76615047454834,
      "learning_rate": 0.0001705485554282805,
      "loss": 0.9069,
      "step": 1162
    },
    {
      "epoch": 0.14791732909379968,
      "grad_norm": 1.9751813411712646,
      "learning_rate": 0.00017052310042000766,
      "loss": 0.7637,
      "step": 1163
    },
    {
      "epoch": 0.14804451510333863,
      "grad_norm": 2.5209197998046875,
      "learning_rate": 0.00017049764541173478,
      "loss": 0.9168,
      "step": 1164
    },
    {
      "epoch": 0.14817170111287759,
      "grad_norm": 2.3149969577789307,
      "learning_rate": 0.0001704721904034619,
      "loss": 0.8163,
      "step": 1165
    },
    {
      "epoch": 0.14829888712241654,
      "grad_norm": 1.7768385410308838,
      "learning_rate": 0.000170446735395189,
      "loss": 0.6303,
      "step": 1166
    },
    {
      "epoch": 0.1484260731319555,
      "grad_norm": 2.1585566997528076,
      "learning_rate": 0.00017042128038691613,
      "loss": 0.6352,
      "step": 1167
    },
    {
      "epoch": 0.14855325914149445,
      "grad_norm": 1.3194421529769897,
      "learning_rate": 0.00017039582537864325,
      "loss": 0.5637,
      "step": 1168
    },
    {
      "epoch": 0.14868044515103337,
      "grad_norm": 2.2757391929626465,
      "learning_rate": 0.00017037037037037037,
      "loss": 0.832,
      "step": 1169
    },
    {
      "epoch": 0.14880763116057233,
      "grad_norm": 2.0045363903045654,
      "learning_rate": 0.0001703449153620975,
      "loss": 0.7024,
      "step": 1170
    },
    {
      "epoch": 0.14893481717011128,
      "grad_norm": 2.6324071884155273,
      "learning_rate": 0.00017031946035382463,
      "loss": 0.72,
      "step": 1171
    },
    {
      "epoch": 0.14906200317965024,
      "grad_norm": 3.417409658432007,
      "learning_rate": 0.00017029400534555175,
      "loss": 0.8041,
      "step": 1172
    },
    {
      "epoch": 0.1491891891891892,
      "grad_norm": 1.6789530515670776,
      "learning_rate": 0.00017026855033727887,
      "loss": 0.6054,
      "step": 1173
    },
    {
      "epoch": 0.14931637519872815,
      "grad_norm": 2.9492475986480713,
      "learning_rate": 0.000170243095329006,
      "loss": 1.1046,
      "step": 1174
    },
    {
      "epoch": 0.1494435612082671,
      "grad_norm": 2.4318718910217285,
      "learning_rate": 0.0001702176403207331,
      "loss": 0.9127,
      "step": 1175
    },
    {
      "epoch": 0.14957074721780605,
      "grad_norm": 2.7231645584106445,
      "learning_rate": 0.00017019218531246023,
      "loss": 1.0723,
      "step": 1176
    },
    {
      "epoch": 0.14969793322734498,
      "grad_norm": 2.249481439590454,
      "learning_rate": 0.00017016673030418735,
      "loss": 0.5478,
      "step": 1177
    },
    {
      "epoch": 0.14982511923688394,
      "grad_norm": 2.7911531925201416,
      "learning_rate": 0.0001701412752959145,
      "loss": 0.7847,
      "step": 1178
    },
    {
      "epoch": 0.1499523052464229,
      "grad_norm": 2.044837474822998,
      "learning_rate": 0.00017011582028764158,
      "loss": 0.6706,
      "step": 1179
    },
    {
      "epoch": 0.15007949125596184,
      "grad_norm": 1.4884235858917236,
      "learning_rate": 0.00017009036527936873,
      "loss": 0.9078,
      "step": 1180
    },
    {
      "epoch": 0.1502066772655008,
      "grad_norm": 1.719786524772644,
      "learning_rate": 0.00017006491027109585,
      "loss": 0.9103,
      "step": 1181
    },
    {
      "epoch": 0.15033386327503975,
      "grad_norm": 2.4853765964508057,
      "learning_rate": 0.00017003945526282297,
      "loss": 0.7671,
      "step": 1182
    },
    {
      "epoch": 0.1504610492845787,
      "grad_norm": 1.3859868049621582,
      "learning_rate": 0.00017001400025455009,
      "loss": 0.7441,
      "step": 1183
    },
    {
      "epoch": 0.15058823529411763,
      "grad_norm": 2.3580715656280518,
      "learning_rate": 0.0001699885452462772,
      "loss": 0.7803,
      "step": 1184
    },
    {
      "epoch": 0.1507154213036566,
      "grad_norm": 1.6627862453460693,
      "learning_rate": 0.00016996309023800435,
      "loss": 0.7536,
      "step": 1185
    },
    {
      "epoch": 0.15084260731319554,
      "grad_norm": 2.0478928089141846,
      "learning_rate": 0.00016993763522973144,
      "loss": 0.7218,
      "step": 1186
    },
    {
      "epoch": 0.1509697933227345,
      "grad_norm": 1.9303275346755981,
      "learning_rate": 0.0001699121802214586,
      "loss": 0.7989,
      "step": 1187
    },
    {
      "epoch": 0.15109697933227345,
      "grad_norm": 2.2542736530303955,
      "learning_rate": 0.0001698867252131857,
      "loss": 1.1037,
      "step": 1188
    },
    {
      "epoch": 0.1512241653418124,
      "grad_norm": 1.8320481777191162,
      "learning_rate": 0.00016986127020491283,
      "loss": 0.842,
      "step": 1189
    },
    {
      "epoch": 0.15135135135135136,
      "grad_norm": 2.2083425521850586,
      "learning_rate": 0.00016983581519663997,
      "loss": 0.6326,
      "step": 1190
    },
    {
      "epoch": 0.15147853736089031,
      "grad_norm": 2.547985553741455,
      "learning_rate": 0.00016981036018836706,
      "loss": 0.8655,
      "step": 1191
    },
    {
      "epoch": 0.15160572337042924,
      "grad_norm": 1.6576193571090698,
      "learning_rate": 0.0001697849051800942,
      "loss": 0.6775,
      "step": 1192
    },
    {
      "epoch": 0.1517329093799682,
      "grad_norm": 2.0467915534973145,
      "learning_rate": 0.0001697594501718213,
      "loss": 0.7378,
      "step": 1193
    },
    {
      "epoch": 0.15186009538950715,
      "grad_norm": 1.4720818996429443,
      "learning_rate": 0.00016973399516354845,
      "loss": 0.5984,
      "step": 1194
    },
    {
      "epoch": 0.1519872813990461,
      "grad_norm": 1.831188678741455,
      "learning_rate": 0.00016970854015527557,
      "loss": 0.8964,
      "step": 1195
    },
    {
      "epoch": 0.15211446740858506,
      "grad_norm": 1.894740343093872,
      "learning_rate": 0.00016968308514700268,
      "loss": 0.8345,
      "step": 1196
    },
    {
      "epoch": 0.152241653418124,
      "grad_norm": 2.4150683879852295,
      "learning_rate": 0.0001696576301387298,
      "loss": 0.6845,
      "step": 1197
    },
    {
      "epoch": 0.15236883942766297,
      "grad_norm": 2.181548595428467,
      "learning_rate": 0.00016963217513045692,
      "loss": 0.7466,
      "step": 1198
    },
    {
      "epoch": 0.15249602543720192,
      "grad_norm": 2.1699490547180176,
      "learning_rate": 0.00016960672012218404,
      "loss": 0.8206,
      "step": 1199
    },
    {
      "epoch": 0.15262321144674085,
      "grad_norm": 1.8111133575439453,
      "learning_rate": 0.00016958126511391116,
      "loss": 0.7986,
      "step": 1200
    },
    {
      "epoch": 0.1527503974562798,
      "grad_norm": 1.6417068243026733,
      "learning_rate": 0.0001695558101056383,
      "loss": 0.7335,
      "step": 1201
    },
    {
      "epoch": 0.15287758346581876,
      "grad_norm": 1.7597920894622803,
      "learning_rate": 0.00016953035509736542,
      "loss": 0.9203,
      "step": 1202
    },
    {
      "epoch": 0.1530047694753577,
      "grad_norm": 2.249229907989502,
      "learning_rate": 0.00016950490008909254,
      "loss": 0.8362,
      "step": 1203
    },
    {
      "epoch": 0.15313195548489666,
      "grad_norm": 1.5798060894012451,
      "learning_rate": 0.00016947944508081966,
      "loss": 0.746,
      "step": 1204
    },
    {
      "epoch": 0.15325914149443562,
      "grad_norm": 1.4947052001953125,
      "learning_rate": 0.00016945399007254678,
      "loss": 0.5101,
      "step": 1205
    },
    {
      "epoch": 0.15338632750397457,
      "grad_norm": 3.3746683597564697,
      "learning_rate": 0.0001694285350642739,
      "loss": 0.8001,
      "step": 1206
    },
    {
      "epoch": 0.1535135135135135,
      "grad_norm": 1.8899158239364624,
      "learning_rate": 0.00016940308005600102,
      "loss": 1.0677,
      "step": 1207
    },
    {
      "epoch": 0.15364069952305245,
      "grad_norm": 2.5050466060638428,
      "learning_rate": 0.00016937762504772814,
      "loss": 1.0455,
      "step": 1208
    },
    {
      "epoch": 0.1537678855325914,
      "grad_norm": 1.9435200691223145,
      "learning_rate": 0.00016935217003945528,
      "loss": 0.936,
      "step": 1209
    },
    {
      "epoch": 0.15389507154213036,
      "grad_norm": 2.26127552986145,
      "learning_rate": 0.0001693267150311824,
      "loss": 0.8293,
      "step": 1210
    },
    {
      "epoch": 0.15402225755166932,
      "grad_norm": 1.4471393823623657,
      "learning_rate": 0.00016930126002290952,
      "loss": 0.5608,
      "step": 1211
    },
    {
      "epoch": 0.15414944356120827,
      "grad_norm": 1.8686652183532715,
      "learning_rate": 0.00016927580501463664,
      "loss": 0.7627,
      "step": 1212
    },
    {
      "epoch": 0.15427662957074723,
      "grad_norm": 1.6972393989562988,
      "learning_rate": 0.00016925035000636376,
      "loss": 0.5908,
      "step": 1213
    },
    {
      "epoch": 0.15440381558028618,
      "grad_norm": 1.6975337266921997,
      "learning_rate": 0.00016922489499809088,
      "loss": 0.7104,
      "step": 1214
    },
    {
      "epoch": 0.1545310015898251,
      "grad_norm": 1.8987146615982056,
      "learning_rate": 0.000169199439989818,
      "loss": 0.8262,
      "step": 1215
    },
    {
      "epoch": 0.15465818759936406,
      "grad_norm": 1.3926318883895874,
      "learning_rate": 0.00016917398498154514,
      "loss": 0.8998,
      "step": 1216
    },
    {
      "epoch": 0.15478537360890302,
      "grad_norm": 1.8709492683410645,
      "learning_rate": 0.00016914852997327223,
      "loss": 0.7154,
      "step": 1217
    },
    {
      "epoch": 0.15491255961844197,
      "grad_norm": 2.392592191696167,
      "learning_rate": 0.00016912307496499938,
      "loss": 0.8923,
      "step": 1218
    },
    {
      "epoch": 0.15503974562798092,
      "grad_norm": 1.9064264297485352,
      "learning_rate": 0.0001690976199567265,
      "loss": 0.8358,
      "step": 1219
    },
    {
      "epoch": 0.15516693163751988,
      "grad_norm": 1.8845504522323608,
      "learning_rate": 0.00016907216494845361,
      "loss": 0.8748,
      "step": 1220
    },
    {
      "epoch": 0.15529411764705883,
      "grad_norm": 3.8024795055389404,
      "learning_rate": 0.00016904670994018076,
      "loss": 0.8509,
      "step": 1221
    },
    {
      "epoch": 0.1554213036565978,
      "grad_norm": 2.0762174129486084,
      "learning_rate": 0.00016902125493190785,
      "loss": 0.879,
      "step": 1222
    },
    {
      "epoch": 0.1555484896661367,
      "grad_norm": 1.655976414680481,
      "learning_rate": 0.000168995799923635,
      "loss": 0.7051,
      "step": 1223
    },
    {
      "epoch": 0.15567567567567567,
      "grad_norm": 2.410954475402832,
      "learning_rate": 0.0001689703449153621,
      "loss": 0.8585,
      "step": 1224
    },
    {
      "epoch": 0.15580286168521462,
      "grad_norm": 2.062889814376831,
      "learning_rate": 0.00016894488990708924,
      "loss": 0.8923,
      "step": 1225
    },
    {
      "epoch": 0.15593004769475358,
      "grad_norm": 2.603330135345459,
      "learning_rate": 0.00016891943489881635,
      "loss": 0.8452,
      "step": 1226
    },
    {
      "epoch": 0.15605723370429253,
      "grad_norm": 1.5087687969207764,
      "learning_rate": 0.00016889397989054347,
      "loss": 0.651,
      "step": 1227
    },
    {
      "epoch": 0.15618441971383148,
      "grad_norm": 1.9414676427841187,
      "learning_rate": 0.0001688685248822706,
      "loss": 0.7509,
      "step": 1228
    },
    {
      "epoch": 0.15631160572337044,
      "grad_norm": 2.140303373336792,
      "learning_rate": 0.0001688430698739977,
      "loss": 1.0201,
      "step": 1229
    },
    {
      "epoch": 0.1564387917329094,
      "grad_norm": 2.7929131984710693,
      "learning_rate": 0.00016881761486572486,
      "loss": 0.8015,
      "step": 1230
    },
    {
      "epoch": 0.15656597774244832,
      "grad_norm": 2.3266212940216064,
      "learning_rate": 0.00016879215985745195,
      "loss": 0.8475,
      "step": 1231
    },
    {
      "epoch": 0.15669316375198727,
      "grad_norm": 2.244032382965088,
      "learning_rate": 0.0001687667048491791,
      "loss": 0.9492,
      "step": 1232
    },
    {
      "epoch": 0.15682034976152623,
      "grad_norm": 1.581815481185913,
      "learning_rate": 0.0001687412498409062,
      "loss": 0.8622,
      "step": 1233
    },
    {
      "epoch": 0.15694753577106518,
      "grad_norm": 2.2750465869903564,
      "learning_rate": 0.00016871579483263333,
      "loss": 0.8193,
      "step": 1234
    },
    {
      "epoch": 0.15707472178060414,
      "grad_norm": 2.4111759662628174,
      "learning_rate": 0.00016869033982436045,
      "loss": 0.6655,
      "step": 1235
    },
    {
      "epoch": 0.1572019077901431,
      "grad_norm": 1.3873398303985596,
      "learning_rate": 0.00016866488481608757,
      "loss": 0.6602,
      "step": 1236
    },
    {
      "epoch": 0.15732909379968205,
      "grad_norm": 2.30853271484375,
      "learning_rate": 0.0001686394298078147,
      "loss": 0.7463,
      "step": 1237
    },
    {
      "epoch": 0.15745627980922097,
      "grad_norm": 1.7725270986557007,
      "learning_rate": 0.0001686139747995418,
      "loss": 0.8219,
      "step": 1238
    },
    {
      "epoch": 0.15758346581875993,
      "grad_norm": 2.0906665325164795,
      "learning_rate": 0.00016858851979126895,
      "loss": 0.7037,
      "step": 1239
    },
    {
      "epoch": 0.15771065182829888,
      "grad_norm": 2.1508758068084717,
      "learning_rate": 0.00016856306478299607,
      "loss": 0.7657,
      "step": 1240
    },
    {
      "epoch": 0.15783783783783784,
      "grad_norm": 2.1971676349639893,
      "learning_rate": 0.0001685376097747232,
      "loss": 0.9057,
      "step": 1241
    },
    {
      "epoch": 0.1579650238473768,
      "grad_norm": 2.1571056842803955,
      "learning_rate": 0.0001685121547664503,
      "loss": 1.1498,
      "step": 1242
    },
    {
      "epoch": 0.15809220985691574,
      "grad_norm": 2.0631954669952393,
      "learning_rate": 0.00016848669975817743,
      "loss": 0.6059,
      "step": 1243
    },
    {
      "epoch": 0.1582193958664547,
      "grad_norm": 1.7770118713378906,
      "learning_rate": 0.00016846124474990455,
      "loss": 0.6899,
      "step": 1244
    },
    {
      "epoch": 0.15834658187599365,
      "grad_norm": 2.190268039703369,
      "learning_rate": 0.00016843578974163166,
      "loss": 0.599,
      "step": 1245
    },
    {
      "epoch": 0.15847376788553258,
      "grad_norm": 1.863402009010315,
      "learning_rate": 0.00016841033473335878,
      "loss": 0.9937,
      "step": 1246
    },
    {
      "epoch": 0.15860095389507153,
      "grad_norm": 2.3612220287323,
      "learning_rate": 0.00016838487972508593,
      "loss": 1.017,
      "step": 1247
    },
    {
      "epoch": 0.1587281399046105,
      "grad_norm": 2.256889820098877,
      "learning_rate": 0.00016835942471681302,
      "loss": 0.8395,
      "step": 1248
    },
    {
      "epoch": 0.15885532591414944,
      "grad_norm": 2.5696606636047363,
      "learning_rate": 0.00016833396970854017,
      "loss": 1.0313,
      "step": 1249
    },
    {
      "epoch": 0.1589825119236884,
      "grad_norm": 2.3084335327148438,
      "learning_rate": 0.00016830851470026729,
      "loss": 0.702,
      "step": 1250
    },
    {
      "epoch": 0.15910969793322735,
      "grad_norm": 2.294213056564331,
      "learning_rate": 0.0001682830596919944,
      "loss": 0.9491,
      "step": 1251
    },
    {
      "epoch": 0.1592368839427663,
      "grad_norm": 1.265534520149231,
      "learning_rate": 0.00016825760468372155,
      "loss": 0.7588,
      "step": 1252
    },
    {
      "epoch": 0.15936406995230526,
      "grad_norm": 1.5605465173721313,
      "learning_rate": 0.00016823214967544864,
      "loss": 0.8231,
      "step": 1253
    },
    {
      "epoch": 0.15949125596184419,
      "grad_norm": 1.8069653511047363,
      "learning_rate": 0.0001682066946671758,
      "loss": 0.7545,
      "step": 1254
    },
    {
      "epoch": 0.15961844197138314,
      "grad_norm": 1.8989496231079102,
      "learning_rate": 0.00016818123965890288,
      "loss": 0.7907,
      "step": 1255
    },
    {
      "epoch": 0.1597456279809221,
      "grad_norm": 1.9500013589859009,
      "learning_rate": 0.00016815578465063003,
      "loss": 0.8846,
      "step": 1256
    },
    {
      "epoch": 0.15987281399046105,
      "grad_norm": 1.8664320707321167,
      "learning_rate": 0.00016813032964235714,
      "loss": 0.9124,
      "step": 1257
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.3979915380477905,
      "learning_rate": 0.00016810487463408426,
      "loss": 0.6816,
      "step": 1258
    },
    {
      "epoch": 0.16012718600953896,
      "grad_norm": 1.8108460903167725,
      "learning_rate": 0.0001680794196258114,
      "loss": 0.78,
      "step": 1259
    },
    {
      "epoch": 0.1602543720190779,
      "grad_norm": 1.5797686576843262,
      "learning_rate": 0.0001680539646175385,
      "loss": 0.6474,
      "step": 1260
    },
    {
      "epoch": 0.16038155802861684,
      "grad_norm": 2.343923568725586,
      "learning_rate": 0.00016802850960926565,
      "loss": 1.01,
      "step": 1261
    },
    {
      "epoch": 0.1605087440381558,
      "grad_norm": 1.707340121269226,
      "learning_rate": 0.00016800305460099274,
      "loss": 0.6209,
      "step": 1262
    },
    {
      "epoch": 0.16063593004769475,
      "grad_norm": 2.061635971069336,
      "learning_rate": 0.00016797759959271988,
      "loss": 0.9256,
      "step": 1263
    },
    {
      "epoch": 0.1607631160572337,
      "grad_norm": 1.379172444343567,
      "learning_rate": 0.000167952144584447,
      "loss": 0.8319,
      "step": 1264
    },
    {
      "epoch": 0.16089030206677266,
      "grad_norm": 1.8034965991973877,
      "learning_rate": 0.00016792668957617412,
      "loss": 0.8786,
      "step": 1265
    },
    {
      "epoch": 0.1610174880763116,
      "grad_norm": 1.7047271728515625,
      "learning_rate": 0.00016790123456790124,
      "loss": 0.7215,
      "step": 1266
    },
    {
      "epoch": 0.16114467408585056,
      "grad_norm": 1.9367213249206543,
      "learning_rate": 0.00016787577955962836,
      "loss": 0.8345,
      "step": 1267
    },
    {
      "epoch": 0.16127186009538952,
      "grad_norm": 2.173065185546875,
      "learning_rate": 0.0001678503245513555,
      "loss": 0.9833,
      "step": 1268
    },
    {
      "epoch": 0.16139904610492845,
      "grad_norm": 2.25630259513855,
      "learning_rate": 0.0001678248695430826,
      "loss": 0.7631,
      "step": 1269
    },
    {
      "epoch": 0.1615262321144674,
      "grad_norm": 1.8119680881500244,
      "learning_rate": 0.00016779941453480974,
      "loss": 0.8068,
      "step": 1270
    },
    {
      "epoch": 0.16165341812400635,
      "grad_norm": 1.6508688926696777,
      "learning_rate": 0.00016777395952653686,
      "loss": 0.7561,
      "step": 1271
    },
    {
      "epoch": 0.1617806041335453,
      "grad_norm": 1.6032193899154663,
      "learning_rate": 0.00016774850451826398,
      "loss": 0.7143,
      "step": 1272
    },
    {
      "epoch": 0.16190779014308426,
      "grad_norm": 1.8659836053848267,
      "learning_rate": 0.0001677230495099911,
      "loss": 0.6964,
      "step": 1273
    },
    {
      "epoch": 0.16203497615262322,
      "grad_norm": 2.0045833587646484,
      "learning_rate": 0.00016769759450171822,
      "loss": 0.6916,
      "step": 1274
    },
    {
      "epoch": 0.16216216216216217,
      "grad_norm": 1.870491623878479,
      "learning_rate": 0.00016767213949344534,
      "loss": 0.6902,
      "step": 1275
    },
    {
      "epoch": 0.16228934817170113,
      "grad_norm": 2.183398962020874,
      "learning_rate": 0.00016764668448517245,
      "loss": 0.8564,
      "step": 1276
    },
    {
      "epoch": 0.16241653418124005,
      "grad_norm": 1.826348066329956,
      "learning_rate": 0.00016762122947689957,
      "loss": 1.0358,
      "step": 1277
    },
    {
      "epoch": 0.162543720190779,
      "grad_norm": 2.2437078952789307,
      "learning_rate": 0.00016759577446862672,
      "loss": 0.7889,
      "step": 1278
    },
    {
      "epoch": 0.16267090620031796,
      "grad_norm": 2.033290386199951,
      "learning_rate": 0.00016757031946035384,
      "loss": 0.8611,
      "step": 1279
    },
    {
      "epoch": 0.16279809220985691,
      "grad_norm": 1.98073410987854,
      "learning_rate": 0.00016754486445208096,
      "loss": 0.9849,
      "step": 1280
    },
    {
      "epoch": 0.16292527821939587,
      "grad_norm": 2.166722536087036,
      "learning_rate": 0.00016751940944380807,
      "loss": 0.8989,
      "step": 1281
    },
    {
      "epoch": 0.16305246422893482,
      "grad_norm": 2.300499439239502,
      "learning_rate": 0.0001674939544355352,
      "loss": 1.0346,
      "step": 1282
    },
    {
      "epoch": 0.16317965023847378,
      "grad_norm": 1.7687069177627563,
      "learning_rate": 0.00016746849942726234,
      "loss": 0.7756,
      "step": 1283
    },
    {
      "epoch": 0.16330683624801273,
      "grad_norm": 2.6421592235565186,
      "learning_rate": 0.00016744304441898943,
      "loss": 0.7923,
      "step": 1284
    },
    {
      "epoch": 0.16343402225755166,
      "grad_norm": 2.0123260021209717,
      "learning_rate": 0.00016741758941071658,
      "loss": 0.8061,
      "step": 1285
    },
    {
      "epoch": 0.1635612082670906,
      "grad_norm": 1.7864760160446167,
      "learning_rate": 0.00016739213440244367,
      "loss": 0.8396,
      "step": 1286
    },
    {
      "epoch": 0.16368839427662957,
      "grad_norm": 2.357337474822998,
      "learning_rate": 0.00016736667939417081,
      "loss": 0.848,
      "step": 1287
    },
    {
      "epoch": 0.16381558028616852,
      "grad_norm": 2.2213923931121826,
      "learning_rate": 0.00016734122438589793,
      "loss": 0.8306,
      "step": 1288
    },
    {
      "epoch": 0.16394276629570748,
      "grad_norm": 1.924025058746338,
      "learning_rate": 0.00016731576937762505,
      "loss": 0.7462,
      "step": 1289
    },
    {
      "epoch": 0.16406995230524643,
      "grad_norm": 2.3203234672546387,
      "learning_rate": 0.0001672903143693522,
      "loss": 0.8664,
      "step": 1290
    },
    {
      "epoch": 0.16419713831478538,
      "grad_norm": 1.622164249420166,
      "learning_rate": 0.0001672648593610793,
      "loss": 0.6514,
      "step": 1291
    },
    {
      "epoch": 0.1643243243243243,
      "grad_norm": 1.522618293762207,
      "learning_rate": 0.00016723940435280644,
      "loss": 0.6741,
      "step": 1292
    },
    {
      "epoch": 0.16445151033386327,
      "grad_norm": 2.0162572860717773,
      "learning_rate": 0.00016721394934453353,
      "loss": 0.9401,
      "step": 1293
    },
    {
      "epoch": 0.16457869634340222,
      "grad_norm": 1.838271975517273,
      "learning_rate": 0.00016718849433626067,
      "loss": 0.8203,
      "step": 1294
    },
    {
      "epoch": 0.16470588235294117,
      "grad_norm": 1.2514231204986572,
      "learning_rate": 0.0001671630393279878,
      "loss": 0.632,
      "step": 1295
    },
    {
      "epoch": 0.16483306836248013,
      "grad_norm": 2.124479293823242,
      "learning_rate": 0.0001671375843197149,
      "loss": 0.7566,
      "step": 1296
    },
    {
      "epoch": 0.16496025437201908,
      "grad_norm": 1.748331069946289,
      "learning_rate": 0.00016711212931144206,
      "loss": 0.7425,
      "step": 1297
    },
    {
      "epoch": 0.16508744038155804,
      "grad_norm": 1.9185435771942139,
      "learning_rate": 0.00016708667430316915,
      "loss": 0.776,
      "step": 1298
    },
    {
      "epoch": 0.165214626391097,
      "grad_norm": 1.9585514068603516,
      "learning_rate": 0.0001670612192948963,
      "loss": 0.7243,
      "step": 1299
    },
    {
      "epoch": 0.16534181240063592,
      "grad_norm": 2.286190986633301,
      "learning_rate": 0.00016703576428662339,
      "loss": 0.8431,
      "step": 1300
    },
    {
      "epoch": 0.16546899841017487,
      "grad_norm": 2.319812536239624,
      "learning_rate": 0.00016701030927835053,
      "loss": 0.9905,
      "step": 1301
    },
    {
      "epoch": 0.16559618441971383,
      "grad_norm": 2.0000851154327393,
      "learning_rate": 0.00016698485427007765,
      "loss": 0.7497,
      "step": 1302
    },
    {
      "epoch": 0.16572337042925278,
      "grad_norm": 2.7064244747161865,
      "learning_rate": 0.00016695939926180477,
      "loss": 0.7377,
      "step": 1303
    },
    {
      "epoch": 0.16585055643879174,
      "grad_norm": 2.126427412033081,
      "learning_rate": 0.0001669339442535319,
      "loss": 0.592,
      "step": 1304
    },
    {
      "epoch": 0.1659777424483307,
      "grad_norm": 2.434783935546875,
      "learning_rate": 0.000166908489245259,
      "loss": 0.7052,
      "step": 1305
    },
    {
      "epoch": 0.16610492845786964,
      "grad_norm": 2.3045458793640137,
      "learning_rate": 0.00016688303423698612,
      "loss": 0.709,
      "step": 1306
    },
    {
      "epoch": 0.1662321144674086,
      "grad_norm": 2.2708938121795654,
      "learning_rate": 0.00016685757922871324,
      "loss": 0.7645,
      "step": 1307
    },
    {
      "epoch": 0.16635930047694752,
      "grad_norm": 2.0907437801361084,
      "learning_rate": 0.0001668321242204404,
      "loss": 0.7521,
      "step": 1308
    },
    {
      "epoch": 0.16648648648648648,
      "grad_norm": 1.528581142425537,
      "learning_rate": 0.0001668066692121675,
      "loss": 0.5617,
      "step": 1309
    },
    {
      "epoch": 0.16661367249602543,
      "grad_norm": 1.7333571910858154,
      "learning_rate": 0.00016678121420389463,
      "loss": 0.5993,
      "step": 1310
    },
    {
      "epoch": 0.1667408585055644,
      "grad_norm": 1.96310555934906,
      "learning_rate": 0.00016675575919562175,
      "loss": 0.9391,
      "step": 1311
    },
    {
      "epoch": 0.16686804451510334,
      "grad_norm": 2.0303170680999756,
      "learning_rate": 0.00016673030418734886,
      "loss": 0.9796,
      "step": 1312
    },
    {
      "epoch": 0.1669952305246423,
      "grad_norm": 1.4225056171417236,
      "learning_rate": 0.00016670484917907598,
      "loss": 0.7336,
      "step": 1313
    },
    {
      "epoch": 0.16712241653418125,
      "grad_norm": 2.4649994373321533,
      "learning_rate": 0.00016667939417080313,
      "loss": 0.8997,
      "step": 1314
    },
    {
      "epoch": 0.16724960254372018,
      "grad_norm": 2.712602138519287,
      "learning_rate": 0.00016665393916253022,
      "loss": 0.9035,
      "step": 1315
    },
    {
      "epoch": 0.16737678855325913,
      "grad_norm": 2.227163791656494,
      "learning_rate": 0.00016662848415425737,
      "loss": 0.9673,
      "step": 1316
    },
    {
      "epoch": 0.16750397456279809,
      "grad_norm": 1.7511460781097412,
      "learning_rate": 0.00016660302914598449,
      "loss": 0.8321,
      "step": 1317
    },
    {
      "epoch": 0.16763116057233704,
      "grad_norm": 1.8457136154174805,
      "learning_rate": 0.0001665775741377116,
      "loss": 0.7586,
      "step": 1318
    },
    {
      "epoch": 0.167758346581876,
      "grad_norm": 1.4462664127349854,
      "learning_rate": 0.00016655211912943872,
      "loss": 0.6129,
      "step": 1319
    },
    {
      "epoch": 0.16788553259141495,
      "grad_norm": 2.4501705169677734,
      "learning_rate": 0.00016652666412116584,
      "loss": 0.9623,
      "step": 1320
    },
    {
      "epoch": 0.1680127186009539,
      "grad_norm": 1.4780555963516235,
      "learning_rate": 0.000166501209112893,
      "loss": 0.6886,
      "step": 1321
    },
    {
      "epoch": 0.16813990461049286,
      "grad_norm": 3.399338722229004,
      "learning_rate": 0.00016647575410462008,
      "loss": 0.7521,
      "step": 1322
    },
    {
      "epoch": 0.16826709062003178,
      "grad_norm": 1.8355399370193481,
      "learning_rate": 0.00016645029909634722,
      "loss": 0.7999,
      "step": 1323
    },
    {
      "epoch": 0.16839427662957074,
      "grad_norm": 1.7427871227264404,
      "learning_rate": 0.00016642484408807432,
      "loss": 0.9227,
      "step": 1324
    },
    {
      "epoch": 0.1685214626391097,
      "grad_norm": 2.073930263519287,
      "learning_rate": 0.00016639938907980146,
      "loss": 0.7159,
      "step": 1325
    },
    {
      "epoch": 0.16864864864864865,
      "grad_norm": 2.136690378189087,
      "learning_rate": 0.00016637393407152858,
      "loss": 0.899,
      "step": 1326
    },
    {
      "epoch": 0.1687758346581876,
      "grad_norm": 1.7114394903182983,
      "learning_rate": 0.0001663484790632557,
      "loss": 0.7985,
      "step": 1327
    },
    {
      "epoch": 0.16890302066772656,
      "grad_norm": 2.2798264026641846,
      "learning_rate": 0.00016632302405498285,
      "loss": 1.055,
      "step": 1328
    },
    {
      "epoch": 0.1690302066772655,
      "grad_norm": 1.8483835458755493,
      "learning_rate": 0.00016629756904670994,
      "loss": 0.7916,
      "step": 1329
    },
    {
      "epoch": 0.16915739268680446,
      "grad_norm": 2.264078140258789,
      "learning_rate": 0.00016627211403843708,
      "loss": 0.8004,
      "step": 1330
    },
    {
      "epoch": 0.1692845786963434,
      "grad_norm": 2.5229225158691406,
      "learning_rate": 0.00016624665903016417,
      "loss": 0.8395,
      "step": 1331
    },
    {
      "epoch": 0.16941176470588235,
      "grad_norm": 1.1487884521484375,
      "learning_rate": 0.00016622120402189132,
      "loss": 0.6345,
      "step": 1332
    },
    {
      "epoch": 0.1695389507154213,
      "grad_norm": 1.6196192502975464,
      "learning_rate": 0.00016619574901361844,
      "loss": 0.5102,
      "step": 1333
    },
    {
      "epoch": 0.16966613672496025,
      "grad_norm": 1.5066066980361938,
      "learning_rate": 0.00016617029400534556,
      "loss": 0.5157,
      "step": 1334
    },
    {
      "epoch": 0.1697933227344992,
      "grad_norm": 2.0269906520843506,
      "learning_rate": 0.00016614483899707268,
      "loss": 1.0914,
      "step": 1335
    },
    {
      "epoch": 0.16992050874403816,
      "grad_norm": 1.7410084009170532,
      "learning_rate": 0.0001661193839887998,
      "loss": 0.6732,
      "step": 1336
    },
    {
      "epoch": 0.17004769475357712,
      "grad_norm": 2.3434016704559326,
      "learning_rate": 0.00016609392898052694,
      "loss": 1.0217,
      "step": 1337
    },
    {
      "epoch": 0.17017488076311607,
      "grad_norm": 2.152071952819824,
      "learning_rate": 0.00016606847397225403,
      "loss": 0.7697,
      "step": 1338
    },
    {
      "epoch": 0.170302066772655,
      "grad_norm": 2.29228138923645,
      "learning_rate": 0.00016604301896398118,
      "loss": 1.0193,
      "step": 1339
    },
    {
      "epoch": 0.17042925278219395,
      "grad_norm": 1.6502864360809326,
      "learning_rate": 0.0001660175639557083,
      "loss": 0.6887,
      "step": 1340
    },
    {
      "epoch": 0.1705564387917329,
      "grad_norm": 1.9194056987762451,
      "learning_rate": 0.00016599210894743542,
      "loss": 0.8276,
      "step": 1341
    },
    {
      "epoch": 0.17068362480127186,
      "grad_norm": 1.4968600273132324,
      "learning_rate": 0.00016596665393916253,
      "loss": 0.6627,
      "step": 1342
    },
    {
      "epoch": 0.17081081081081081,
      "grad_norm": 2.2854559421539307,
      "learning_rate": 0.00016594119893088965,
      "loss": 0.9389,
      "step": 1343
    },
    {
      "epoch": 0.17093799682034977,
      "grad_norm": 2.3779680728912354,
      "learning_rate": 0.00016591574392261677,
      "loss": 0.7292,
      "step": 1344
    },
    {
      "epoch": 0.17106518282988872,
      "grad_norm": 2.2254772186279297,
      "learning_rate": 0.00016589028891434392,
      "loss": 1.0518,
      "step": 1345
    },
    {
      "epoch": 0.17119236883942765,
      "grad_norm": 1.8986233472824097,
      "learning_rate": 0.00016586483390607104,
      "loss": 0.7602,
      "step": 1346
    },
    {
      "epoch": 0.1713195548489666,
      "grad_norm": 1.6541954278945923,
      "learning_rate": 0.00016583937889779816,
      "loss": 0.8516,
      "step": 1347
    },
    {
      "epoch": 0.17144674085850556,
      "grad_norm": 1.9188075065612793,
      "learning_rate": 0.00016581392388952527,
      "loss": 0.7911,
      "step": 1348
    },
    {
      "epoch": 0.1715739268680445,
      "grad_norm": 1.795929193496704,
      "learning_rate": 0.0001657884688812524,
      "loss": 0.7195,
      "step": 1349
    },
    {
      "epoch": 0.17170111287758347,
      "grad_norm": 1.707586646080017,
      "learning_rate": 0.0001657630138729795,
      "loss": 0.8619,
      "step": 1350
    },
    {
      "epoch": 0.17182829888712242,
      "grad_norm": 1.8271316289901733,
      "learning_rate": 0.00016573755886470663,
      "loss": 0.5868,
      "step": 1351
    },
    {
      "epoch": 0.17195548489666138,
      "grad_norm": 2.5858895778656006,
      "learning_rate": 0.00016571210385643378,
      "loss": 0.7239,
      "step": 1352
    },
    {
      "epoch": 0.17208267090620033,
      "grad_norm": 1.7440054416656494,
      "learning_rate": 0.00016568664884816087,
      "loss": 0.8331,
      "step": 1353
    },
    {
      "epoch": 0.17220985691573926,
      "grad_norm": 2.050935983657837,
      "learning_rate": 0.00016566119383988801,
      "loss": 0.7847,
      "step": 1354
    },
    {
      "epoch": 0.1723370429252782,
      "grad_norm": 2.3446767330169678,
      "learning_rate": 0.00016563573883161513,
      "loss": 0.9673,
      "step": 1355
    },
    {
      "epoch": 0.17246422893481717,
      "grad_norm": 1.6136313676834106,
      "learning_rate": 0.00016561028382334225,
      "loss": 0.7174,
      "step": 1356
    },
    {
      "epoch": 0.17259141494435612,
      "grad_norm": 1.6712725162506104,
      "learning_rate": 0.00016558482881506937,
      "loss": 0.7516,
      "step": 1357
    },
    {
      "epoch": 0.17271860095389507,
      "grad_norm": 1.6241551637649536,
      "learning_rate": 0.0001655593738067965,
      "loss": 0.7723,
      "step": 1358
    },
    {
      "epoch": 0.17284578696343403,
      "grad_norm": 2.3964593410491943,
      "learning_rate": 0.00016553391879852363,
      "loss": 0.5764,
      "step": 1359
    },
    {
      "epoch": 0.17297297297297298,
      "grad_norm": 2.278698444366455,
      "learning_rate": 0.00016550846379025073,
      "loss": 0.6483,
      "step": 1360
    },
    {
      "epoch": 0.17310015898251194,
      "grad_norm": 1.8185988664627075,
      "learning_rate": 0.00016548300878197787,
      "loss": 0.8524,
      "step": 1361
    },
    {
      "epoch": 0.17322734499205086,
      "grad_norm": 2.173217535018921,
      "learning_rate": 0.00016545755377370496,
      "loss": 0.9068,
      "step": 1362
    },
    {
      "epoch": 0.17335453100158982,
      "grad_norm": 1.7094353437423706,
      "learning_rate": 0.0001654320987654321,
      "loss": 1.0178,
      "step": 1363
    },
    {
      "epoch": 0.17348171701112877,
      "grad_norm": 1.3180121183395386,
      "learning_rate": 0.00016540664375715923,
      "loss": 0.8778,
      "step": 1364
    },
    {
      "epoch": 0.17360890302066773,
      "grad_norm": 1.369821548461914,
      "learning_rate": 0.00016538118874888635,
      "loss": 0.4878,
      "step": 1365
    },
    {
      "epoch": 0.17373608903020668,
      "grad_norm": 2.8384406566619873,
      "learning_rate": 0.0001653557337406135,
      "loss": 0.9264,
      "step": 1366
    },
    {
      "epoch": 0.17386327503974564,
      "grad_norm": 2.144784927368164,
      "learning_rate": 0.00016533027873234058,
      "loss": 0.7236,
      "step": 1367
    },
    {
      "epoch": 0.1739904610492846,
      "grad_norm": 2.0817134380340576,
      "learning_rate": 0.00016530482372406773,
      "loss": 0.6205,
      "step": 1368
    },
    {
      "epoch": 0.17411764705882352,
      "grad_norm": 1.745405673980713,
      "learning_rate": 0.00016527936871579482,
      "loss": 0.683,
      "step": 1369
    },
    {
      "epoch": 0.17424483306836247,
      "grad_norm": 1.745408296585083,
      "learning_rate": 0.00016525391370752197,
      "loss": 0.7211,
      "step": 1370
    },
    {
      "epoch": 0.17437201907790142,
      "grad_norm": 2.182204008102417,
      "learning_rate": 0.0001652284586992491,
      "loss": 1.1085,
      "step": 1371
    },
    {
      "epoch": 0.17449920508744038,
      "grad_norm": 2.137171506881714,
      "learning_rate": 0.0001652030036909762,
      "loss": 0.8994,
      "step": 1372
    },
    {
      "epoch": 0.17462639109697933,
      "grad_norm": 1.8236356973648071,
      "learning_rate": 0.00016517754868270332,
      "loss": 0.5783,
      "step": 1373
    },
    {
      "epoch": 0.1747535771065183,
      "grad_norm": 2.190721273422241,
      "learning_rate": 0.00016515209367443044,
      "loss": 0.654,
      "step": 1374
    },
    {
      "epoch": 0.17488076311605724,
      "grad_norm": 1.921207308769226,
      "learning_rate": 0.0001651266386661576,
      "loss": 0.6402,
      "step": 1375
    },
    {
      "epoch": 0.1750079491255962,
      "grad_norm": 1.7919282913208008,
      "learning_rate": 0.0001651011836578847,
      "loss": 0.7298,
      "step": 1376
    },
    {
      "epoch": 0.17513513513513512,
      "grad_norm": 2.455288887023926,
      "learning_rate": 0.00016507572864961183,
      "loss": 0.6962,
      "step": 1377
    },
    {
      "epoch": 0.17526232114467408,
      "grad_norm": 1.9355424642562866,
      "learning_rate": 0.00016505027364133895,
      "loss": 0.7216,
      "step": 1378
    },
    {
      "epoch": 0.17538950715421303,
      "grad_norm": 2.996206521987915,
      "learning_rate": 0.00016502481863306606,
      "loss": 0.7271,
      "step": 1379
    },
    {
      "epoch": 0.17551669316375199,
      "grad_norm": 2.348874092102051,
      "learning_rate": 0.00016499936362479318,
      "loss": 0.8187,
      "step": 1380
    },
    {
      "epoch": 0.17564387917329094,
      "grad_norm": 1.5916386842727661,
      "learning_rate": 0.0001649739086165203,
      "loss": 0.7878,
      "step": 1381
    },
    {
      "epoch": 0.1757710651828299,
      "grad_norm": 2.216163158416748,
      "learning_rate": 0.00016494845360824742,
      "loss": 0.9063,
      "step": 1382
    },
    {
      "epoch": 0.17589825119236885,
      "grad_norm": 2.18070125579834,
      "learning_rate": 0.00016492299859997457,
      "loss": 0.8887,
      "step": 1383
    },
    {
      "epoch": 0.1760254372019078,
      "grad_norm": 2.508234977722168,
      "learning_rate": 0.00016489754359170166,
      "loss": 0.9366,
      "step": 1384
    },
    {
      "epoch": 0.17615262321144673,
      "grad_norm": 2.5261356830596924,
      "learning_rate": 0.0001648720885834288,
      "loss": 0.801,
      "step": 1385
    },
    {
      "epoch": 0.17627980922098568,
      "grad_norm": 2.299450397491455,
      "learning_rate": 0.00016484663357515592,
      "loss": 0.7979,
      "step": 1386
    },
    {
      "epoch": 0.17640699523052464,
      "grad_norm": 1.701047658920288,
      "learning_rate": 0.00016482117856688304,
      "loss": 0.5581,
      "step": 1387
    },
    {
      "epoch": 0.1765341812400636,
      "grad_norm": 2.4849026203155518,
      "learning_rate": 0.00016479572355861016,
      "loss": 0.8445,
      "step": 1388
    },
    {
      "epoch": 0.17666136724960255,
      "grad_norm": 1.5342698097229004,
      "learning_rate": 0.00016477026855033728,
      "loss": 0.8192,
      "step": 1389
    },
    {
      "epoch": 0.1767885532591415,
      "grad_norm": 3.18929123878479,
      "learning_rate": 0.00016474481354206442,
      "loss": 0.8683,
      "step": 1390
    },
    {
      "epoch": 0.17691573926868046,
      "grad_norm": 1.9630613327026367,
      "learning_rate": 0.00016471935853379152,
      "loss": 1.0196,
      "step": 1391
    },
    {
      "epoch": 0.1770429252782194,
      "grad_norm": 1.8381099700927734,
      "learning_rate": 0.00016469390352551866,
      "loss": 0.7179,
      "step": 1392
    },
    {
      "epoch": 0.17717011128775834,
      "grad_norm": 1.6254165172576904,
      "learning_rate": 0.00016466844851724575,
      "loss": 0.5938,
      "step": 1393
    },
    {
      "epoch": 0.1772972972972973,
      "grad_norm": 1.8091191053390503,
      "learning_rate": 0.0001646429935089729,
      "loss": 0.8319,
      "step": 1394
    },
    {
      "epoch": 0.17742448330683624,
      "grad_norm": 1.4501398801803589,
      "learning_rate": 0.00016461753850070002,
      "loss": 0.6856,
      "step": 1395
    },
    {
      "epoch": 0.1775516693163752,
      "grad_norm": 1.9702482223510742,
      "learning_rate": 0.00016459208349242714,
      "loss": 0.7321,
      "step": 1396
    },
    {
      "epoch": 0.17767885532591415,
      "grad_norm": 2.683265209197998,
      "learning_rate": 0.00016456662848415428,
      "loss": 0.6071,
      "step": 1397
    },
    {
      "epoch": 0.1778060413354531,
      "grad_norm": 2.296952962875366,
      "learning_rate": 0.00016454117347588137,
      "loss": 0.8496,
      "step": 1398
    },
    {
      "epoch": 0.17793322734499206,
      "grad_norm": 1.9747300148010254,
      "learning_rate": 0.00016451571846760852,
      "loss": 0.8592,
      "step": 1399
    },
    {
      "epoch": 0.178060413354531,
      "grad_norm": 2.6097426414489746,
      "learning_rate": 0.00016449026345933564,
      "loss": 0.8165,
      "step": 1400
    },
    {
      "epoch": 0.17818759936406994,
      "grad_norm": 2.0717580318450928,
      "learning_rate": 0.00016446480845106276,
      "loss": 0.8176,
      "step": 1401
    },
    {
      "epoch": 0.1783147853736089,
      "grad_norm": 1.938164472579956,
      "learning_rate": 0.00016443935344278988,
      "loss": 0.7375,
      "step": 1402
    },
    {
      "epoch": 0.17844197138314785,
      "grad_norm": 6.649278163909912,
      "learning_rate": 0.000164413898434517,
      "loss": 0.6342,
      "step": 1403
    },
    {
      "epoch": 0.1785691573926868,
      "grad_norm": 1.6701472997665405,
      "learning_rate": 0.00016438844342624414,
      "loss": 0.6798,
      "step": 1404
    },
    {
      "epoch": 0.17869634340222576,
      "grad_norm": 1.7382313013076782,
      "learning_rate": 0.00016436298841797123,
      "loss": 0.6749,
      "step": 1405
    },
    {
      "epoch": 0.17882352941176471,
      "grad_norm": 1.582380771636963,
      "learning_rate": 0.00016433753340969838,
      "loss": 0.6691,
      "step": 1406
    },
    {
      "epoch": 0.17895071542130367,
      "grad_norm": 2.033525228500366,
      "learning_rate": 0.0001643120784014255,
      "loss": 0.6492,
      "step": 1407
    },
    {
      "epoch": 0.1790779014308426,
      "grad_norm": 2.632876396179199,
      "learning_rate": 0.00016428662339315262,
      "loss": 0.9771,
      "step": 1408
    },
    {
      "epoch": 0.17920508744038155,
      "grad_norm": 1.5929738283157349,
      "learning_rate": 0.00016426116838487973,
      "loss": 0.9116,
      "step": 1409
    },
    {
      "epoch": 0.1793322734499205,
      "grad_norm": 1.7409138679504395,
      "learning_rate": 0.00016423571337660685,
      "loss": 0.8611,
      "step": 1410
    },
    {
      "epoch": 0.17945945945945946,
      "grad_norm": 2.2620842456817627,
      "learning_rate": 0.00016421025836833397,
      "loss": 0.834,
      "step": 1411
    },
    {
      "epoch": 0.1795866454689984,
      "grad_norm": 1.6237592697143555,
      "learning_rate": 0.0001641848033600611,
      "loss": 0.5546,
      "step": 1412
    },
    {
      "epoch": 0.17971383147853737,
      "grad_norm": 1.9431735277175903,
      "learning_rate": 0.0001641593483517882,
      "loss": 0.8351,
      "step": 1413
    },
    {
      "epoch": 0.17984101748807632,
      "grad_norm": 2.234093189239502,
      "learning_rate": 0.00016413389334351536,
      "loss": 0.7483,
      "step": 1414
    },
    {
      "epoch": 0.17996820349761528,
      "grad_norm": 1.6326136589050293,
      "learning_rate": 0.00016410843833524247,
      "loss": 0.8831,
      "step": 1415
    },
    {
      "epoch": 0.1800953895071542,
      "grad_norm": 1.4317787885665894,
      "learning_rate": 0.0001640829833269696,
      "loss": 0.554,
      "step": 1416
    },
    {
      "epoch": 0.18022257551669316,
      "grad_norm": 2.660360336303711,
      "learning_rate": 0.0001640575283186967,
      "loss": 0.8516,
      "step": 1417
    },
    {
      "epoch": 0.1803497615262321,
      "grad_norm": 1.7108181715011597,
      "learning_rate": 0.00016403207331042383,
      "loss": 0.772,
      "step": 1418
    },
    {
      "epoch": 0.18047694753577107,
      "grad_norm": 2.2215182781219482,
      "learning_rate": 0.00016400661830215095,
      "loss": 0.6565,
      "step": 1419
    },
    {
      "epoch": 0.18060413354531002,
      "grad_norm": 2.1765120029449463,
      "learning_rate": 0.00016398116329387807,
      "loss": 0.8134,
      "step": 1420
    },
    {
      "epoch": 0.18073131955484897,
      "grad_norm": 2.060020923614502,
      "learning_rate": 0.0001639557082856052,
      "loss": 0.6999,
      "step": 1421
    },
    {
      "epoch": 0.18085850556438793,
      "grad_norm": 1.8131206035614014,
      "learning_rate": 0.0001639302532773323,
      "loss": 0.6543,
      "step": 1422
    },
    {
      "epoch": 0.18098569157392685,
      "grad_norm": 1.9902726411819458,
      "learning_rate": 0.00016390479826905945,
      "loss": 0.7803,
      "step": 1423
    },
    {
      "epoch": 0.1811128775834658,
      "grad_norm": 2.102994918823242,
      "learning_rate": 0.00016387934326078657,
      "loss": 0.6434,
      "step": 1424
    },
    {
      "epoch": 0.18124006359300476,
      "grad_norm": 1.6422463655471802,
      "learning_rate": 0.0001638538882525137,
      "loss": 0.537,
      "step": 1425
    },
    {
      "epoch": 0.18136724960254372,
      "grad_norm": 1.5628970861434937,
      "learning_rate": 0.0001638284332442408,
      "loss": 0.6566,
      "step": 1426
    },
    {
      "epoch": 0.18149443561208267,
      "grad_norm": 1.6558971405029297,
      "learning_rate": 0.00016380297823596793,
      "loss": 0.7833,
      "step": 1427
    },
    {
      "epoch": 0.18162162162162163,
      "grad_norm": 2.151135206222534,
      "learning_rate": 0.00016377752322769507,
      "loss": 1.1215,
      "step": 1428
    },
    {
      "epoch": 0.18174880763116058,
      "grad_norm": 2.767796516418457,
      "learning_rate": 0.00016375206821942216,
      "loss": 1.0361,
      "step": 1429
    },
    {
      "epoch": 0.18187599364069953,
      "grad_norm": 1.6218832731246948,
      "learning_rate": 0.0001637266132111493,
      "loss": 0.5356,
      "step": 1430
    },
    {
      "epoch": 0.18200317965023846,
      "grad_norm": 2.569735050201416,
      "learning_rate": 0.00016370115820287643,
      "loss": 0.6921,
      "step": 1431
    },
    {
      "epoch": 0.18213036565977742,
      "grad_norm": 2.01383376121521,
      "learning_rate": 0.00016367570319460355,
      "loss": 0.6646,
      "step": 1432
    },
    {
      "epoch": 0.18225755166931637,
      "grad_norm": 2.037114381790161,
      "learning_rate": 0.0001636502481863307,
      "loss": 0.7889,
      "step": 1433
    },
    {
      "epoch": 0.18238473767885532,
      "grad_norm": 2.2065958976745605,
      "learning_rate": 0.00016362479317805778,
      "loss": 0.7339,
      "step": 1434
    },
    {
      "epoch": 0.18251192368839428,
      "grad_norm": 1.7691624164581299,
      "learning_rate": 0.00016359933816978493,
      "loss": 0.6116,
      "step": 1435
    },
    {
      "epoch": 0.18263910969793323,
      "grad_norm": 1.884565830230713,
      "learning_rate": 0.00016357388316151202,
      "loss": 0.5878,
      "step": 1436
    },
    {
      "epoch": 0.1827662957074722,
      "grad_norm": 1.7977027893066406,
      "learning_rate": 0.00016354842815323917,
      "loss": 0.8695,
      "step": 1437
    },
    {
      "epoch": 0.18289348171701114,
      "grad_norm": 2.2548038959503174,
      "learning_rate": 0.00016352297314496629,
      "loss": 0.6692,
      "step": 1438
    },
    {
      "epoch": 0.18302066772655007,
      "grad_norm": 1.362357258796692,
      "learning_rate": 0.0001634975181366934,
      "loss": 0.5334,
      "step": 1439
    },
    {
      "epoch": 0.18314785373608902,
      "grad_norm": 1.8092137575149536,
      "learning_rate": 0.00016347206312842052,
      "loss": 0.8666,
      "step": 1440
    },
    {
      "epoch": 0.18327503974562798,
      "grad_norm": 2.2633614540100098,
      "learning_rate": 0.00016344660812014764,
      "loss": 0.9938,
      "step": 1441
    },
    {
      "epoch": 0.18340222575516693,
      "grad_norm": 2.105865001678467,
      "learning_rate": 0.00016342115311187476,
      "loss": 0.6758,
      "step": 1442
    },
    {
      "epoch": 0.18352941176470589,
      "grad_norm": 3.1399054527282715,
      "learning_rate": 0.00016339569810360188,
      "loss": 1.123,
      "step": 1443
    },
    {
      "epoch": 0.18365659777424484,
      "grad_norm": 1.7823433876037598,
      "learning_rate": 0.00016337024309532903,
      "loss": 0.6537,
      "step": 1444
    },
    {
      "epoch": 0.1837837837837838,
      "grad_norm": 1.8944426774978638,
      "learning_rate": 0.00016334478808705614,
      "loss": 0.6831,
      "step": 1445
    },
    {
      "epoch": 0.18391096979332275,
      "grad_norm": 2.2430548667907715,
      "learning_rate": 0.00016331933307878326,
      "loss": 0.6508,
      "step": 1446
    },
    {
      "epoch": 0.18403815580286167,
      "grad_norm": 2.5388853549957275,
      "learning_rate": 0.00016329387807051038,
      "loss": 0.9677,
      "step": 1447
    },
    {
      "epoch": 0.18416534181240063,
      "grad_norm": 2.284351110458374,
      "learning_rate": 0.0001632684230622375,
      "loss": 0.6814,
      "step": 1448
    },
    {
      "epoch": 0.18429252782193958,
      "grad_norm": 2.022242307662964,
      "learning_rate": 0.00016324296805396462,
      "loss": 0.8116,
      "step": 1449
    },
    {
      "epoch": 0.18441971383147854,
      "grad_norm": 2.445453405380249,
      "learning_rate": 0.00016321751304569174,
      "loss": 0.8,
      "step": 1450
    },
    {
      "epoch": 0.1845468998410175,
      "grad_norm": 1.7327866554260254,
      "learning_rate": 0.00016319205803741886,
      "loss": 0.7205,
      "step": 1451
    },
    {
      "epoch": 0.18467408585055645,
      "grad_norm": 2.011155605316162,
      "learning_rate": 0.000163166603029146,
      "loss": 0.8246,
      "step": 1452
    },
    {
      "epoch": 0.1848012718600954,
      "grad_norm": 1.9283778667449951,
      "learning_rate": 0.00016314114802087312,
      "loss": 0.8684,
      "step": 1453
    },
    {
      "epoch": 0.18492845786963433,
      "grad_norm": 2.179824113845825,
      "learning_rate": 0.00016311569301260024,
      "loss": 0.7738,
      "step": 1454
    },
    {
      "epoch": 0.18505564387917328,
      "grad_norm": 2.258556604385376,
      "learning_rate": 0.00016309023800432736,
      "loss": 0.6288,
      "step": 1455
    },
    {
      "epoch": 0.18518282988871224,
      "grad_norm": 2.4732935428619385,
      "learning_rate": 0.00016306478299605448,
      "loss": 0.9093,
      "step": 1456
    },
    {
      "epoch": 0.1853100158982512,
      "grad_norm": 2.3492319583892822,
      "learning_rate": 0.0001630393279877816,
      "loss": 0.9329,
      "step": 1457
    },
    {
      "epoch": 0.18543720190779014,
      "grad_norm": 6.163917541503906,
      "learning_rate": 0.00016301387297950872,
      "loss": 0.8005,
      "step": 1458
    },
    {
      "epoch": 0.1855643879173291,
      "grad_norm": 2.131605863571167,
      "learning_rate": 0.00016298841797123586,
      "loss": 0.9942,
      "step": 1459
    },
    {
      "epoch": 0.18569157392686805,
      "grad_norm": 1.820739507675171,
      "learning_rate": 0.00016296296296296295,
      "loss": 0.5753,
      "step": 1460
    },
    {
      "epoch": 0.185818759936407,
      "grad_norm": 2.2511494159698486,
      "learning_rate": 0.0001629375079546901,
      "loss": 0.7551,
      "step": 1461
    },
    {
      "epoch": 0.18594594594594593,
      "grad_norm": 1.716149091720581,
      "learning_rate": 0.00016291205294641722,
      "loss": 0.6765,
      "step": 1462
    },
    {
      "epoch": 0.1860731319554849,
      "grad_norm": 1.986559510231018,
      "learning_rate": 0.00016288659793814434,
      "loss": 0.5616,
      "step": 1463
    },
    {
      "epoch": 0.18620031796502384,
      "grad_norm": 1.8091298341751099,
      "learning_rate": 0.00016286114292987148,
      "loss": 0.635,
      "step": 1464
    },
    {
      "epoch": 0.1863275039745628,
      "grad_norm": 1.835533618927002,
      "learning_rate": 0.00016283568792159857,
      "loss": 0.8057,
      "step": 1465
    },
    {
      "epoch": 0.18645468998410175,
      "grad_norm": 1.4969000816345215,
      "learning_rate": 0.00016281023291332572,
      "loss": 0.512,
      "step": 1466
    },
    {
      "epoch": 0.1865818759936407,
      "grad_norm": 1.9753764867782593,
      "learning_rate": 0.0001627847779050528,
      "loss": 0.7734,
      "step": 1467
    },
    {
      "epoch": 0.18670906200317966,
      "grad_norm": 2.4908928871154785,
      "learning_rate": 0.00016275932289677996,
      "loss": 1.1285,
      "step": 1468
    },
    {
      "epoch": 0.18683624801271861,
      "grad_norm": 1.4938584566116333,
      "learning_rate": 0.00016273386788850708,
      "loss": 0.4486,
      "step": 1469
    },
    {
      "epoch": 0.18696343402225754,
      "grad_norm": 2.837013006210327,
      "learning_rate": 0.0001627084128802342,
      "loss": 0.8753,
      "step": 1470
    },
    {
      "epoch": 0.1870906200317965,
      "grad_norm": 2.285330295562744,
      "learning_rate": 0.0001626829578719613,
      "loss": 0.6812,
      "step": 1471
    },
    {
      "epoch": 0.18721780604133545,
      "grad_norm": 2.0616655349731445,
      "learning_rate": 0.00016265750286368843,
      "loss": 0.8895,
      "step": 1472
    },
    {
      "epoch": 0.1873449920508744,
      "grad_norm": 2.8954052925109863,
      "learning_rate": 0.00016263204785541558,
      "loss": 1.1164,
      "step": 1473
    },
    {
      "epoch": 0.18747217806041336,
      "grad_norm": 1.6700013875961304,
      "learning_rate": 0.00016260659284714267,
      "loss": 0.813,
      "step": 1474
    },
    {
      "epoch": 0.1875993640699523,
      "grad_norm": 2.1897125244140625,
      "learning_rate": 0.00016258113783886982,
      "loss": 0.7926,
      "step": 1475
    },
    {
      "epoch": 0.18772655007949127,
      "grad_norm": 1.7310086488723755,
      "learning_rate": 0.00016255568283059693,
      "loss": 0.6143,
      "step": 1476
    },
    {
      "epoch": 0.1878537360890302,
      "grad_norm": 2.2085227966308594,
      "learning_rate": 0.00016253022782232405,
      "loss": 0.8506,
      "step": 1477
    },
    {
      "epoch": 0.18798092209856915,
      "grad_norm": 2.411102056503296,
      "learning_rate": 0.00016250477281405117,
      "loss": 0.9333,
      "step": 1478
    },
    {
      "epoch": 0.1881081081081081,
      "grad_norm": 2.009592056274414,
      "learning_rate": 0.0001624793178057783,
      "loss": 0.6363,
      "step": 1479
    },
    {
      "epoch": 0.18823529411764706,
      "grad_norm": 2.29303240776062,
      "learning_rate": 0.0001624538627975054,
      "loss": 0.4006,
      "step": 1480
    },
    {
      "epoch": 0.188362480127186,
      "grad_norm": 1.9664386510849,
      "learning_rate": 0.00016242840778923253,
      "loss": 0.7645,
      "step": 1481
    },
    {
      "epoch": 0.18848966613672496,
      "grad_norm": 2.1010279655456543,
      "learning_rate": 0.00016240295278095967,
      "loss": 0.825,
      "step": 1482
    },
    {
      "epoch": 0.18861685214626392,
      "grad_norm": 2.455617904663086,
      "learning_rate": 0.0001623774977726868,
      "loss": 0.6324,
      "step": 1483
    },
    {
      "epoch": 0.18874403815580287,
      "grad_norm": 1.7640396356582642,
      "learning_rate": 0.0001623520427644139,
      "loss": 0.9049,
      "step": 1484
    },
    {
      "epoch": 0.1888712241653418,
      "grad_norm": 2.158252239227295,
      "learning_rate": 0.00016232658775614103,
      "loss": 1.0962,
      "step": 1485
    },
    {
      "epoch": 0.18899841017488075,
      "grad_norm": 2.2974510192871094,
      "learning_rate": 0.00016230113274786815,
      "loss": 1.1452,
      "step": 1486
    },
    {
      "epoch": 0.1891255961844197,
      "grad_norm": 1.7256323099136353,
      "learning_rate": 0.00016227567773959527,
      "loss": 0.8301,
      "step": 1487
    },
    {
      "epoch": 0.18925278219395866,
      "grad_norm": 1.7441389560699463,
      "learning_rate": 0.00016225022273132239,
      "loss": 0.6829,
      "step": 1488
    },
    {
      "epoch": 0.18937996820349762,
      "grad_norm": 2.5397698879241943,
      "learning_rate": 0.0001622247677230495,
      "loss": 0.9333,
      "step": 1489
    },
    {
      "epoch": 0.18950715421303657,
      "grad_norm": 2.235551118850708,
      "learning_rate": 0.00016219931271477665,
      "loss": 0.7561,
      "step": 1490
    },
    {
      "epoch": 0.18963434022257553,
      "grad_norm": 3.2965714931488037,
      "learning_rate": 0.00016217385770650377,
      "loss": 1.0424,
      "step": 1491
    },
    {
      "epoch": 0.18976152623211448,
      "grad_norm": 2.628730297088623,
      "learning_rate": 0.0001621484026982309,
      "loss": 1.0069,
      "step": 1492
    },
    {
      "epoch": 0.1898887122416534,
      "grad_norm": 1.7787768840789795,
      "learning_rate": 0.000162122947689958,
      "loss": 0.7272,
      "step": 1493
    },
    {
      "epoch": 0.19001589825119236,
      "grad_norm": 1.8663586378097534,
      "learning_rate": 0.00016209749268168513,
      "loss": 0.8204,
      "step": 1494
    },
    {
      "epoch": 0.19014308426073132,
      "grad_norm": 1.9719842672348022,
      "learning_rate": 0.00016207203767341227,
      "loss": 0.8377,
      "step": 1495
    },
    {
      "epoch": 0.19027027027027027,
      "grad_norm": 1.935287356376648,
      "learning_rate": 0.00016204658266513936,
      "loss": 0.7576,
      "step": 1496
    },
    {
      "epoch": 0.19039745627980922,
      "grad_norm": 1.986046314239502,
      "learning_rate": 0.0001620211276568665,
      "loss": 1.0199,
      "step": 1497
    },
    {
      "epoch": 0.19052464228934818,
      "grad_norm": 1.8691346645355225,
      "learning_rate": 0.0001619956726485936,
      "loss": 0.5724,
      "step": 1498
    },
    {
      "epoch": 0.19065182829888713,
      "grad_norm": 1.9906086921691895,
      "learning_rate": 0.00016197021764032075,
      "loss": 0.6949,
      "step": 1499
    },
    {
      "epoch": 0.1907790143084261,
      "grad_norm": 2.20993971824646,
      "learning_rate": 0.00016194476263204786,
      "loss": 0.7471,
      "step": 1500
    },
    {
      "epoch": 0.190906200317965,
      "grad_norm": 1.7932188510894775,
      "learning_rate": 0.00016191930762377498,
      "loss": 0.6676,
      "step": 1501
    },
    {
      "epoch": 0.19103338632750397,
      "grad_norm": 2.0013668537139893,
      "learning_rate": 0.00016189385261550213,
      "loss": 1.0116,
      "step": 1502
    },
    {
      "epoch": 0.19116057233704292,
      "grad_norm": 2.258415460586548,
      "learning_rate": 0.00016186839760722922,
      "loss": 0.7965,
      "step": 1503
    },
    {
      "epoch": 0.19128775834658188,
      "grad_norm": 1.465378999710083,
      "learning_rate": 0.00016184294259895637,
      "loss": 0.5658,
      "step": 1504
    },
    {
      "epoch": 0.19141494435612083,
      "grad_norm": 2.593446731567383,
      "learning_rate": 0.00016181748759068346,
      "loss": 0.8489,
      "step": 1505
    },
    {
      "epoch": 0.19154213036565979,
      "grad_norm": 1.8085228204727173,
      "learning_rate": 0.0001617920325824106,
      "loss": 0.4926,
      "step": 1506
    },
    {
      "epoch": 0.19166931637519874,
      "grad_norm": 1.8454962968826294,
      "learning_rate": 0.00016176657757413772,
      "loss": 0.8506,
      "step": 1507
    },
    {
      "epoch": 0.19179650238473767,
      "grad_norm": 2.1131937503814697,
      "learning_rate": 0.00016174112256586484,
      "loss": 0.8003,
      "step": 1508
    },
    {
      "epoch": 0.19192368839427662,
      "grad_norm": 2.4724559783935547,
      "learning_rate": 0.00016171566755759196,
      "loss": 0.7082,
      "step": 1509
    },
    {
      "epoch": 0.19205087440381557,
      "grad_norm": 1.7171956300735474,
      "learning_rate": 0.00016169021254931908,
      "loss": 0.8882,
      "step": 1510
    },
    {
      "epoch": 0.19217806041335453,
      "grad_norm": 1.8348584175109863,
      "learning_rate": 0.00016166475754104623,
      "loss": 0.5926,
      "step": 1511
    },
    {
      "epoch": 0.19230524642289348,
      "grad_norm": 2.60090708732605,
      "learning_rate": 0.00016163930253277332,
      "loss": 0.8318,
      "step": 1512
    },
    {
      "epoch": 0.19243243243243244,
      "grad_norm": 1.6721687316894531,
      "learning_rate": 0.00016161384752450046,
      "loss": 0.7746,
      "step": 1513
    },
    {
      "epoch": 0.1925596184419714,
      "grad_norm": 2.3786025047302246,
      "learning_rate": 0.00016158839251622758,
      "loss": 0.7853,
      "step": 1514
    },
    {
      "epoch": 0.19268680445151035,
      "grad_norm": 2.421670913696289,
      "learning_rate": 0.0001615629375079547,
      "loss": 1.281,
      "step": 1515
    },
    {
      "epoch": 0.19281399046104927,
      "grad_norm": 1.919572114944458,
      "learning_rate": 0.00016153748249968182,
      "loss": 0.6734,
      "step": 1516
    },
    {
      "epoch": 0.19294117647058823,
      "grad_norm": 2.042389392852783,
      "learning_rate": 0.00016151202749140894,
      "loss": 0.8535,
      "step": 1517
    },
    {
      "epoch": 0.19306836248012718,
      "grad_norm": 1.5717164278030396,
      "learning_rate": 0.00016148657248313606,
      "loss": 0.5506,
      "step": 1518
    },
    {
      "epoch": 0.19319554848966614,
      "grad_norm": 1.9456064701080322,
      "learning_rate": 0.00016146111747486318,
      "loss": 0.7717,
      "step": 1519
    },
    {
      "epoch": 0.1933227344992051,
      "grad_norm": 1.3001973628997803,
      "learning_rate": 0.0001614356624665903,
      "loss": 0.8287,
      "step": 1520
    },
    {
      "epoch": 0.19344992050874404,
      "grad_norm": 1.8148419857025146,
      "learning_rate": 0.00016141020745831744,
      "loss": 0.7907,
      "step": 1521
    },
    {
      "epoch": 0.193577106518283,
      "grad_norm": 2.3935863971710205,
      "learning_rate": 0.00016138475245004456,
      "loss": 0.8032,
      "step": 1522
    },
    {
      "epoch": 0.19370429252782195,
      "grad_norm": 1.863814353942871,
      "learning_rate": 0.00016135929744177168,
      "loss": 0.9468,
      "step": 1523
    },
    {
      "epoch": 0.19383147853736088,
      "grad_norm": 1.9351438283920288,
      "learning_rate": 0.0001613338424334988,
      "loss": 0.7832,
      "step": 1524
    },
    {
      "epoch": 0.19395866454689983,
      "grad_norm": 2.2351105213165283,
      "learning_rate": 0.00016130838742522591,
      "loss": 0.9065,
      "step": 1525
    },
    {
      "epoch": 0.1940858505564388,
      "grad_norm": 1.4239087104797363,
      "learning_rate": 0.00016128293241695306,
      "loss": 0.6083,
      "step": 1526
    },
    {
      "epoch": 0.19421303656597774,
      "grad_norm": 1.974910855293274,
      "learning_rate": 0.00016125747740868015,
      "loss": 0.7366,
      "step": 1527
    },
    {
      "epoch": 0.1943402225755167,
      "grad_norm": 1.6317039728164673,
      "learning_rate": 0.0001612320224004073,
      "loss": 0.7291,
      "step": 1528
    },
    {
      "epoch": 0.19446740858505565,
      "grad_norm": 3.328298330307007,
      "learning_rate": 0.0001612065673921344,
      "loss": 0.7584,
      "step": 1529
    },
    {
      "epoch": 0.1945945945945946,
      "grad_norm": 2.1958351135253906,
      "learning_rate": 0.00016118111238386154,
      "loss": 0.8317,
      "step": 1530
    },
    {
      "epoch": 0.19472178060413353,
      "grad_norm": 1.909923791885376,
      "learning_rate": 0.00016115565737558865,
      "loss": 0.6329,
      "step": 1531
    },
    {
      "epoch": 0.1948489666136725,
      "grad_norm": 1.6717982292175293,
      "learning_rate": 0.00016113020236731577,
      "loss": 0.5412,
      "step": 1532
    },
    {
      "epoch": 0.19497615262321144,
      "grad_norm": 1.703334927558899,
      "learning_rate": 0.00016110474735904292,
      "loss": 0.6645,
      "step": 1533
    },
    {
      "epoch": 0.1951033386327504,
      "grad_norm": 2.4702279567718506,
      "learning_rate": 0.00016107929235077,
      "loss": 0.7522,
      "step": 1534
    },
    {
      "epoch": 0.19523052464228935,
      "grad_norm": 2.391779899597168,
      "learning_rate": 0.00016105383734249716,
      "loss": 0.9428,
      "step": 1535
    },
    {
      "epoch": 0.1953577106518283,
      "grad_norm": 2.337380886077881,
      "learning_rate": 0.00016102838233422425,
      "loss": 0.8784,
      "step": 1536
    },
    {
      "epoch": 0.19548489666136726,
      "grad_norm": 2.3809521198272705,
      "learning_rate": 0.0001610029273259514,
      "loss": 0.8886,
      "step": 1537
    },
    {
      "epoch": 0.1956120826709062,
      "grad_norm": 3.035072088241577,
      "learning_rate": 0.0001609774723176785,
      "loss": 0.6116,
      "step": 1538
    },
    {
      "epoch": 0.19573926868044514,
      "grad_norm": 2.533930540084839,
      "learning_rate": 0.00016095201730940563,
      "loss": 1.1754,
      "step": 1539
    },
    {
      "epoch": 0.1958664546899841,
      "grad_norm": 2.2073562145233154,
      "learning_rate": 0.00016092656230113278,
      "loss": 0.8273,
      "step": 1540
    },
    {
      "epoch": 0.19599364069952305,
      "grad_norm": 2.56058931350708,
      "learning_rate": 0.00016090110729285987,
      "loss": 0.621,
      "step": 1541
    },
    {
      "epoch": 0.196120826709062,
      "grad_norm": 1.7641332149505615,
      "learning_rate": 0.00016087565228458701,
      "loss": 0.679,
      "step": 1542
    },
    {
      "epoch": 0.19624801271860096,
      "grad_norm": 2.2236552238464355,
      "learning_rate": 0.0001608501972763141,
      "loss": 0.831,
      "step": 1543
    },
    {
      "epoch": 0.1963751987281399,
      "grad_norm": 2.162163734436035,
      "learning_rate": 0.00016082474226804125,
      "loss": 0.8063,
      "step": 1544
    },
    {
      "epoch": 0.19650238473767886,
      "grad_norm": 1.6323988437652588,
      "learning_rate": 0.00016079928725976837,
      "loss": 0.6681,
      "step": 1545
    },
    {
      "epoch": 0.19662957074721782,
      "grad_norm": 2.357879161834717,
      "learning_rate": 0.0001607738322514955,
      "loss": 0.9831,
      "step": 1546
    },
    {
      "epoch": 0.19675675675675675,
      "grad_norm": 1.9026697874069214,
      "learning_rate": 0.0001607483772432226,
      "loss": 0.7172,
      "step": 1547
    },
    {
      "epoch": 0.1968839427662957,
      "grad_norm": 1.6615790128707886,
      "learning_rate": 0.00016072292223494973,
      "loss": 0.7376,
      "step": 1548
    },
    {
      "epoch": 0.19701112877583465,
      "grad_norm": 1.48552668094635,
      "learning_rate": 0.00016069746722667685,
      "loss": 0.7806,
      "step": 1549
    },
    {
      "epoch": 0.1971383147853736,
      "grad_norm": 1.7185454368591309,
      "learning_rate": 0.00016067201221840396,
      "loss": 0.8277,
      "step": 1550
    },
    {
      "epoch": 0.19726550079491256,
      "grad_norm": 1.9887641668319702,
      "learning_rate": 0.0001606465572101311,
      "loss": 0.8509,
      "step": 1551
    },
    {
      "epoch": 0.19739268680445152,
      "grad_norm": 1.7557226419448853,
      "learning_rate": 0.00016062110220185823,
      "loss": 1.0052,
      "step": 1552
    },
    {
      "epoch": 0.19751987281399047,
      "grad_norm": 2.0141260623931885,
      "learning_rate": 0.00016059564719358535,
      "loss": 0.7856,
      "step": 1553
    },
    {
      "epoch": 0.1976470588235294,
      "grad_norm": 2.098814010620117,
      "learning_rate": 0.00016057019218531247,
      "loss": 0.8529,
      "step": 1554
    },
    {
      "epoch": 0.19777424483306835,
      "grad_norm": 1.773672103881836,
      "learning_rate": 0.00016054473717703959,
      "loss": 0.625,
      "step": 1555
    },
    {
      "epoch": 0.1979014308426073,
      "grad_norm": 1.3681315183639526,
      "learning_rate": 0.0001605192821687667,
      "loss": 0.6564,
      "step": 1556
    },
    {
      "epoch": 0.19802861685214626,
      "grad_norm": 1.619394302368164,
      "learning_rate": 0.00016049382716049385,
      "loss": 0.5316,
      "step": 1557
    },
    {
      "epoch": 0.19815580286168522,
      "grad_norm": 2.806347608566284,
      "learning_rate": 0.00016046837215222094,
      "loss": 0.8403,
      "step": 1558
    },
    {
      "epoch": 0.19828298887122417,
      "grad_norm": 2.030384063720703,
      "learning_rate": 0.0001604429171439481,
      "loss": 0.7264,
      "step": 1559
    },
    {
      "epoch": 0.19841017488076312,
      "grad_norm": 2.292369842529297,
      "learning_rate": 0.0001604174621356752,
      "loss": 0.5948,
      "step": 1560
    },
    {
      "epoch": 0.19853736089030208,
      "grad_norm": 3.3190815448760986,
      "learning_rate": 0.00016039200712740232,
      "loss": 0.9827,
      "step": 1561
    },
    {
      "epoch": 0.198664546899841,
      "grad_norm": 1.875053882598877,
      "learning_rate": 0.00016036655211912944,
      "loss": 0.7553,
      "step": 1562
    },
    {
      "epoch": 0.19879173290937996,
      "grad_norm": 1.4641809463500977,
      "learning_rate": 0.00016034109711085656,
      "loss": 0.5285,
      "step": 1563
    },
    {
      "epoch": 0.1989189189189189,
      "grad_norm": 1.7950108051300049,
      "learning_rate": 0.0001603156421025837,
      "loss": 0.8321,
      "step": 1564
    },
    {
      "epoch": 0.19904610492845787,
      "grad_norm": 1.969596266746521,
      "learning_rate": 0.0001602901870943108,
      "loss": 0.93,
      "step": 1565
    },
    {
      "epoch": 0.19917329093799682,
      "grad_norm": 1.715429425239563,
      "learning_rate": 0.00016026473208603795,
      "loss": 0.8564,
      "step": 1566
    },
    {
      "epoch": 0.19930047694753578,
      "grad_norm": 2.4472758769989014,
      "learning_rate": 0.00016023927707776504,
      "loss": 0.7105,
      "step": 1567
    },
    {
      "epoch": 0.19942766295707473,
      "grad_norm": 1.840165376663208,
      "learning_rate": 0.00016021382206949218,
      "loss": 0.8289,
      "step": 1568
    },
    {
      "epoch": 0.19955484896661368,
      "grad_norm": 1.9663604497909546,
      "learning_rate": 0.0001601883670612193,
      "loss": 0.7358,
      "step": 1569
    },
    {
      "epoch": 0.1996820349761526,
      "grad_norm": 2.1794283390045166,
      "learning_rate": 0.00016016291205294642,
      "loss": 0.7426,
      "step": 1570
    },
    {
      "epoch": 0.19980922098569157,
      "grad_norm": 1.5114165544509888,
      "learning_rate": 0.00016013745704467357,
      "loss": 0.6833,
      "step": 1571
    },
    {
      "epoch": 0.19993640699523052,
      "grad_norm": 1.320109486579895,
      "learning_rate": 0.00016011200203640066,
      "loss": 0.5844,
      "step": 1572
    },
    {
      "epoch": 0.20006359300476947,
      "grad_norm": 2.0488321781158447,
      "learning_rate": 0.0001600865470281278,
      "loss": 0.8763,
      "step": 1573
    },
    {
      "epoch": 0.20019077901430843,
      "grad_norm": 2.2582969665527344,
      "learning_rate": 0.0001600610920198549,
      "loss": 0.8454,
      "step": 1574
    },
    {
      "epoch": 0.20031796502384738,
      "grad_norm": 2.0372262001037598,
      "learning_rate": 0.00016003563701158204,
      "loss": 0.976,
      "step": 1575
    },
    {
      "epoch": 0.20044515103338634,
      "grad_norm": 2.0384061336517334,
      "learning_rate": 0.00016001018200330916,
      "loss": 0.5053,
      "step": 1576
    },
    {
      "epoch": 0.2005723370429253,
      "grad_norm": 1.6601190567016602,
      "learning_rate": 0.00015998472699503628,
      "loss": 0.89,
      "step": 1577
    },
    {
      "epoch": 0.20069952305246422,
      "grad_norm": 1.9398114681243896,
      "learning_rate": 0.0001599592719867634,
      "loss": 0.4934,
      "step": 1578
    },
    {
      "epoch": 0.20082670906200317,
      "grad_norm": 2.4088406562805176,
      "learning_rate": 0.00015993381697849052,
      "loss": 0.829,
      "step": 1579
    },
    {
      "epoch": 0.20095389507154213,
      "grad_norm": 1.790175437927246,
      "learning_rate": 0.00015990836197021766,
      "loss": 0.5883,
      "step": 1580
    },
    {
      "epoch": 0.20108108108108108,
      "grad_norm": 1.790927767753601,
      "learning_rate": 0.00015988290696194475,
      "loss": 0.6077,
      "step": 1581
    },
    {
      "epoch": 0.20120826709062004,
      "grad_norm": 2.2880895137786865,
      "learning_rate": 0.0001598574519536719,
      "loss": 0.9264,
      "step": 1582
    },
    {
      "epoch": 0.201335453100159,
      "grad_norm": 2.356947898864746,
      "learning_rate": 0.00015983199694539902,
      "loss": 1.3598,
      "step": 1583
    },
    {
      "epoch": 0.20146263910969794,
      "grad_norm": 2.290501117706299,
      "learning_rate": 0.00015980654193712614,
      "loss": 0.7797,
      "step": 1584
    },
    {
      "epoch": 0.20158982511923687,
      "grad_norm": 2.0512630939483643,
      "learning_rate": 0.00015978108692885326,
      "loss": 0.4551,
      "step": 1585
    },
    {
      "epoch": 0.20171701112877582,
      "grad_norm": 2.209345817565918,
      "learning_rate": 0.00015975563192058037,
      "loss": 0.6446,
      "step": 1586
    },
    {
      "epoch": 0.20184419713831478,
      "grad_norm": 1.7553129196166992,
      "learning_rate": 0.0001597301769123075,
      "loss": 0.6798,
      "step": 1587
    },
    {
      "epoch": 0.20197138314785373,
      "grad_norm": 1.6882824897766113,
      "learning_rate": 0.00015970472190403464,
      "loss": 0.9146,
      "step": 1588
    },
    {
      "epoch": 0.2020985691573927,
      "grad_norm": 1.7905192375183105,
      "learning_rate": 0.00015967926689576176,
      "loss": 0.6429,
      "step": 1589
    },
    {
      "epoch": 0.20222575516693164,
      "grad_norm": 1.6072075366973877,
      "learning_rate": 0.00015965381188748888,
      "loss": 0.6555,
      "step": 1590
    },
    {
      "epoch": 0.2023529411764706,
      "grad_norm": 1.870392918586731,
      "learning_rate": 0.000159628356879216,
      "loss": 0.5953,
      "step": 1591
    },
    {
      "epoch": 0.20248012718600955,
      "grad_norm": 2.4586904048919678,
      "learning_rate": 0.00015960290187094311,
      "loss": 0.9393,
      "step": 1592
    },
    {
      "epoch": 0.20260731319554848,
      "grad_norm": 1.7641420364379883,
      "learning_rate": 0.00015957744686267023,
      "loss": 0.6344,
      "step": 1593
    },
    {
      "epoch": 0.20273449920508743,
      "grad_norm": 2.054896593093872,
      "learning_rate": 0.00015955199185439735,
      "loss": 0.6996,
      "step": 1594
    },
    {
      "epoch": 0.20286168521462639,
      "grad_norm": 1.6698259115219116,
      "learning_rate": 0.0001595265368461245,
      "loss": 0.5428,
      "step": 1595
    },
    {
      "epoch": 0.20298887122416534,
      "grad_norm": 2.0853729248046875,
      "learning_rate": 0.0001595010818378516,
      "loss": 0.9972,
      "step": 1596
    },
    {
      "epoch": 0.2031160572337043,
      "grad_norm": 2.1264679431915283,
      "learning_rate": 0.00015947562682957874,
      "loss": 0.8591,
      "step": 1597
    },
    {
      "epoch": 0.20324324324324325,
      "grad_norm": 1.8185631036758423,
      "learning_rate": 0.00015945017182130585,
      "loss": 0.7144,
      "step": 1598
    },
    {
      "epoch": 0.2033704292527822,
      "grad_norm": 2.067657709121704,
      "learning_rate": 0.00015942471681303297,
      "loss": 0.5649,
      "step": 1599
    },
    {
      "epoch": 0.20349761526232116,
      "grad_norm": 2.523561716079712,
      "learning_rate": 0.0001593992618047601,
      "loss": 0.8263,
      "step": 1600
    },
    {
      "epoch": 0.20362480127186008,
      "grad_norm": 1.4510446786880493,
      "learning_rate": 0.0001593738067964872,
      "loss": 0.6516,
      "step": 1601
    },
    {
      "epoch": 0.20375198728139904,
      "grad_norm": 1.8188210725784302,
      "learning_rate": 0.00015934835178821436,
      "loss": 0.6872,
      "step": 1602
    },
    {
      "epoch": 0.203879173290938,
      "grad_norm": 2.6353752613067627,
      "learning_rate": 0.00015932289677994145,
      "loss": 0.8024,
      "step": 1603
    },
    {
      "epoch": 0.20400635930047695,
      "grad_norm": 2.014181613922119,
      "learning_rate": 0.0001592974417716686,
      "loss": 0.6798,
      "step": 1604
    },
    {
      "epoch": 0.2041335453100159,
      "grad_norm": 1.9861246347427368,
      "learning_rate": 0.00015927198676339568,
      "loss": 0.6908,
      "step": 1605
    },
    {
      "epoch": 0.20426073131955486,
      "grad_norm": 1.632405400276184,
      "learning_rate": 0.00015924653175512283,
      "loss": 0.6493,
      "step": 1606
    },
    {
      "epoch": 0.2043879173290938,
      "grad_norm": 2.10461688041687,
      "learning_rate": 0.00015922107674684995,
      "loss": 0.7158,
      "step": 1607
    },
    {
      "epoch": 0.20451510333863274,
      "grad_norm": 1.6886494159698486,
      "learning_rate": 0.00015919562173857707,
      "loss": 0.5361,
      "step": 1608
    },
    {
      "epoch": 0.2046422893481717,
      "grad_norm": 1.5524022579193115,
      "learning_rate": 0.00015917016673030421,
      "loss": 0.7074,
      "step": 1609
    },
    {
      "epoch": 0.20476947535771065,
      "grad_norm": 2.024570941925049,
      "learning_rate": 0.0001591447117220313,
      "loss": 0.6541,
      "step": 1610
    },
    {
      "epoch": 0.2048966613672496,
      "grad_norm": 2.5587868690490723,
      "learning_rate": 0.00015911925671375845,
      "loss": 0.7511,
      "step": 1611
    },
    {
      "epoch": 0.20502384737678855,
      "grad_norm": 2.0535824298858643,
      "learning_rate": 0.00015909380170548554,
      "loss": 0.8772,
      "step": 1612
    },
    {
      "epoch": 0.2051510333863275,
      "grad_norm": 2.069967269897461,
      "learning_rate": 0.0001590683466972127,
      "loss": 0.8744,
      "step": 1613
    },
    {
      "epoch": 0.20527821939586646,
      "grad_norm": 1.935009241104126,
      "learning_rate": 0.0001590428916889398,
      "loss": 0.6726,
      "step": 1614
    },
    {
      "epoch": 0.20540540540540542,
      "grad_norm": 2.225480318069458,
      "learning_rate": 0.00015901743668066693,
      "loss": 0.9034,
      "step": 1615
    },
    {
      "epoch": 0.20553259141494434,
      "grad_norm": 2.8814516067504883,
      "learning_rate": 0.00015899198167239405,
      "loss": 0.6406,
      "step": 1616
    },
    {
      "epoch": 0.2056597774244833,
      "grad_norm": 1.7206929922103882,
      "learning_rate": 0.00015896652666412116,
      "loss": 0.6196,
      "step": 1617
    },
    {
      "epoch": 0.20578696343402225,
      "grad_norm": 1.5644590854644775,
      "learning_rate": 0.0001589410716558483,
      "loss": 0.5742,
      "step": 1618
    },
    {
      "epoch": 0.2059141494435612,
      "grad_norm": 2.746617078781128,
      "learning_rate": 0.00015891561664757543,
      "loss": 0.9584,
      "step": 1619
    },
    {
      "epoch": 0.20604133545310016,
      "grad_norm": 2.058124542236328,
      "learning_rate": 0.00015889016163930255,
      "loss": 0.7076,
      "step": 1620
    },
    {
      "epoch": 0.20616852146263911,
      "grad_norm": 3.036248207092285,
      "learning_rate": 0.00015886470663102967,
      "loss": 1.0235,
      "step": 1621
    },
    {
      "epoch": 0.20629570747217807,
      "grad_norm": 1.9226919412612915,
      "learning_rate": 0.00015883925162275678,
      "loss": 0.8222,
      "step": 1622
    },
    {
      "epoch": 0.20642289348171702,
      "grad_norm": 2.9654550552368164,
      "learning_rate": 0.0001588137966144839,
      "loss": 0.7819,
      "step": 1623
    },
    {
      "epoch": 0.20655007949125595,
      "grad_norm": 2.1719110012054443,
      "learning_rate": 0.00015878834160621102,
      "loss": 0.8058,
      "step": 1624
    },
    {
      "epoch": 0.2066772655007949,
      "grad_norm": 1.200056552886963,
      "learning_rate": 0.00015876288659793814,
      "loss": 0.5958,
      "step": 1625
    },
    {
      "epoch": 0.20680445151033386,
      "grad_norm": 3.0196564197540283,
      "learning_rate": 0.0001587374315896653,
      "loss": 0.8781,
      "step": 1626
    },
    {
      "epoch": 0.2069316375198728,
      "grad_norm": 2.351977825164795,
      "learning_rate": 0.0001587119765813924,
      "loss": 0.8453,
      "step": 1627
    },
    {
      "epoch": 0.20705882352941177,
      "grad_norm": 2.535388469696045,
      "learning_rate": 0.00015868652157311952,
      "loss": 0.7452,
      "step": 1628
    },
    {
      "epoch": 0.20718600953895072,
      "grad_norm": 2.1446006298065186,
      "learning_rate": 0.00015866106656484664,
      "loss": 1.1226,
      "step": 1629
    },
    {
      "epoch": 0.20731319554848968,
      "grad_norm": 1.865978479385376,
      "learning_rate": 0.00015863561155657376,
      "loss": 0.706,
      "step": 1630
    },
    {
      "epoch": 0.20744038155802863,
      "grad_norm": 1.9758563041687012,
      "learning_rate": 0.00015861015654830088,
      "loss": 0.9082,
      "step": 1631
    },
    {
      "epoch": 0.20756756756756756,
      "grad_norm": 2.5569846630096436,
      "learning_rate": 0.000158584701540028,
      "loss": 0.7503,
      "step": 1632
    },
    {
      "epoch": 0.2076947535771065,
      "grad_norm": 1.642138957977295,
      "learning_rate": 0.00015855924653175515,
      "loss": 0.6108,
      "step": 1633
    },
    {
      "epoch": 0.20782193958664547,
      "grad_norm": 2.05197811126709,
      "learning_rate": 0.00015853379152348224,
      "loss": 0.7271,
      "step": 1634
    },
    {
      "epoch": 0.20794912559618442,
      "grad_norm": 1.8979355096817017,
      "learning_rate": 0.00015850833651520938,
      "loss": 0.7984,
      "step": 1635
    },
    {
      "epoch": 0.20807631160572337,
      "grad_norm": 2.1893064975738525,
      "learning_rate": 0.00015848288150693647,
      "loss": 1.1371,
      "step": 1636
    },
    {
      "epoch": 0.20820349761526233,
      "grad_norm": 1.842578411102295,
      "learning_rate": 0.00015845742649866362,
      "loss": 0.542,
      "step": 1637
    },
    {
      "epoch": 0.20833068362480128,
      "grad_norm": 2.1785144805908203,
      "learning_rate": 0.00015843197149039074,
      "loss": 0.8925,
      "step": 1638
    },
    {
      "epoch": 0.2084578696343402,
      "grad_norm": 1.942337155342102,
      "learning_rate": 0.00015840651648211786,
      "loss": 0.8466,
      "step": 1639
    },
    {
      "epoch": 0.20858505564387916,
      "grad_norm": 2.4467880725860596,
      "learning_rate": 0.000158381061473845,
      "loss": 1.0836,
      "step": 1640
    },
    {
      "epoch": 0.20871224165341812,
      "grad_norm": 1.8139408826828003,
      "learning_rate": 0.0001583556064655721,
      "loss": 0.7971,
      "step": 1641
    },
    {
      "epoch": 0.20883942766295707,
      "grad_norm": 1.7622915506362915,
      "learning_rate": 0.00015833015145729924,
      "loss": 0.9002,
      "step": 1642
    },
    {
      "epoch": 0.20896661367249603,
      "grad_norm": 2.161132574081421,
      "learning_rate": 0.00015830469644902636,
      "loss": 0.7424,
      "step": 1643
    },
    {
      "epoch": 0.20909379968203498,
      "grad_norm": 2.202831983566284,
      "learning_rate": 0.00015827924144075348,
      "loss": 0.8461,
      "step": 1644
    },
    {
      "epoch": 0.20922098569157394,
      "grad_norm": 1.5122487545013428,
      "learning_rate": 0.0001582537864324806,
      "loss": 0.7108,
      "step": 1645
    },
    {
      "epoch": 0.2093481717011129,
      "grad_norm": 1.325972557067871,
      "learning_rate": 0.00015822833142420772,
      "loss": 0.7843,
      "step": 1646
    },
    {
      "epoch": 0.20947535771065182,
      "grad_norm": 2.1826648712158203,
      "learning_rate": 0.00015820287641593486,
      "loss": 0.8457,
      "step": 1647
    },
    {
      "epoch": 0.20960254372019077,
      "grad_norm": 1.840786099433899,
      "learning_rate": 0.00015817742140766195,
      "loss": 0.6698,
      "step": 1648
    },
    {
      "epoch": 0.20972972972972972,
      "grad_norm": 2.2277939319610596,
      "learning_rate": 0.0001581519663993891,
      "loss": 0.738,
      "step": 1649
    },
    {
      "epoch": 0.20985691573926868,
      "grad_norm": 2.081874132156372,
      "learning_rate": 0.00015812651139111622,
      "loss": 0.841,
      "step": 1650
    },
    {
      "epoch": 0.20998410174880763,
      "grad_norm": 2.009331464767456,
      "learning_rate": 0.00015810105638284334,
      "loss": 0.7347,
      "step": 1651
    },
    {
      "epoch": 0.2101112877583466,
      "grad_norm": 2.0552003383636475,
      "learning_rate": 0.00015807560137457046,
      "loss": 0.686,
      "step": 1652
    },
    {
      "epoch": 0.21023847376788554,
      "grad_norm": 3.0787508487701416,
      "learning_rate": 0.00015805014636629757,
      "loss": 0.7611,
      "step": 1653
    },
    {
      "epoch": 0.2103656597774245,
      "grad_norm": 2.469374179840088,
      "learning_rate": 0.0001580246913580247,
      "loss": 0.7876,
      "step": 1654
    },
    {
      "epoch": 0.21049284578696342,
      "grad_norm": 2.3810906410217285,
      "learning_rate": 0.0001579992363497518,
      "loss": 0.7209,
      "step": 1655
    },
    {
      "epoch": 0.21062003179650238,
      "grad_norm": 1.8917725086212158,
      "learning_rate": 0.00015797378134147893,
      "loss": 0.819,
      "step": 1656
    },
    {
      "epoch": 0.21074721780604133,
      "grad_norm": 2.688380718231201,
      "learning_rate": 0.00015794832633320608,
      "loss": 0.7609,
      "step": 1657
    },
    {
      "epoch": 0.21087440381558029,
      "grad_norm": 2.180307388305664,
      "learning_rate": 0.0001579228713249332,
      "loss": 0.7153,
      "step": 1658
    },
    {
      "epoch": 0.21100158982511924,
      "grad_norm": 2.2857069969177246,
      "learning_rate": 0.00015789741631666031,
      "loss": 0.6797,
      "step": 1659
    },
    {
      "epoch": 0.2111287758346582,
      "grad_norm": 1.6671420335769653,
      "learning_rate": 0.00015787196130838743,
      "loss": 0.5641,
      "step": 1660
    },
    {
      "epoch": 0.21125596184419715,
      "grad_norm": 1.9287359714508057,
      "learning_rate": 0.00015784650630011455,
      "loss": 0.7621,
      "step": 1661
    },
    {
      "epoch": 0.21138314785373608,
      "grad_norm": 2.342022180557251,
      "learning_rate": 0.00015782105129184167,
      "loss": 0.6533,
      "step": 1662
    },
    {
      "epoch": 0.21151033386327503,
      "grad_norm": 3.2187106609344482,
      "learning_rate": 0.0001577955962835688,
      "loss": 1.2081,
      "step": 1663
    },
    {
      "epoch": 0.21163751987281398,
      "grad_norm": 2.175271511077881,
      "learning_rate": 0.00015777014127529593,
      "loss": 0.7518,
      "step": 1664
    },
    {
      "epoch": 0.21176470588235294,
      "grad_norm": 2.4055721759796143,
      "learning_rate": 0.00015774468626702303,
      "loss": 0.8211,
      "step": 1665
    },
    {
      "epoch": 0.2118918918918919,
      "grad_norm": 2.6204049587249756,
      "learning_rate": 0.00015771923125875017,
      "loss": 0.6493,
      "step": 1666
    },
    {
      "epoch": 0.21201907790143085,
      "grad_norm": 2.273685932159424,
      "learning_rate": 0.0001576937762504773,
      "loss": 0.8962,
      "step": 1667
    },
    {
      "epoch": 0.2121462639109698,
      "grad_norm": 1.891311526298523,
      "learning_rate": 0.0001576683212422044,
      "loss": 0.7772,
      "step": 1668
    },
    {
      "epoch": 0.21227344992050876,
      "grad_norm": 2.997568130493164,
      "learning_rate": 0.00015764286623393153,
      "loss": 0.6988,
      "step": 1669
    },
    {
      "epoch": 0.21240063593004768,
      "grad_norm": 2.1634228229522705,
      "learning_rate": 0.00015761741122565865,
      "loss": 0.9658,
      "step": 1670
    },
    {
      "epoch": 0.21252782193958664,
      "grad_norm": 2.4580729007720947,
      "learning_rate": 0.0001575919562173858,
      "loss": 0.7703,
      "step": 1671
    },
    {
      "epoch": 0.2126550079491256,
      "grad_norm": 1.6178277730941772,
      "learning_rate": 0.00015756650120911288,
      "loss": 0.7537,
      "step": 1672
    },
    {
      "epoch": 0.21278219395866454,
      "grad_norm": 2.794295072555542,
      "learning_rate": 0.00015754104620084003,
      "loss": 0.6671,
      "step": 1673
    },
    {
      "epoch": 0.2129093799682035,
      "grad_norm": 1.6448498964309692,
      "learning_rate": 0.00015751559119256715,
      "loss": 0.8111,
      "step": 1674
    },
    {
      "epoch": 0.21303656597774245,
      "grad_norm": 2.0650103092193604,
      "learning_rate": 0.00015749013618429427,
      "loss": 0.7555,
      "step": 1675
    },
    {
      "epoch": 0.2131637519872814,
      "grad_norm": 1.8835031986236572,
      "learning_rate": 0.00015746468117602141,
      "loss": 0.8156,
      "step": 1676
    },
    {
      "epoch": 0.21329093799682036,
      "grad_norm": 1.9278459548950195,
      "learning_rate": 0.0001574392261677485,
      "loss": 0.7111,
      "step": 1677
    },
    {
      "epoch": 0.2134181240063593,
      "grad_norm": 2.0147135257720947,
      "learning_rate": 0.00015741377115947565,
      "loss": 0.9734,
      "step": 1678
    },
    {
      "epoch": 0.21354531001589824,
      "grad_norm": 1.5126268863677979,
      "learning_rate": 0.00015738831615120274,
      "loss": 0.9106,
      "step": 1679
    },
    {
      "epoch": 0.2136724960254372,
      "grad_norm": 1.8772242069244385,
      "learning_rate": 0.0001573628611429299,
      "loss": 0.8339,
      "step": 1680
    },
    {
      "epoch": 0.21379968203497615,
      "grad_norm": 1.7770060300827026,
      "learning_rate": 0.000157337406134657,
      "loss": 0.6013,
      "step": 1681
    },
    {
      "epoch": 0.2139268680445151,
      "grad_norm": 2.0994551181793213,
      "learning_rate": 0.00015731195112638413,
      "loss": 0.7991,
      "step": 1682
    },
    {
      "epoch": 0.21405405405405406,
      "grad_norm": 1.9991306066513062,
      "learning_rate": 0.00015728649611811124,
      "loss": 0.8656,
      "step": 1683
    },
    {
      "epoch": 0.21418124006359301,
      "grad_norm": 1.640662670135498,
      "learning_rate": 0.00015726104110983836,
      "loss": 0.7761,
      "step": 1684
    },
    {
      "epoch": 0.21430842607313197,
      "grad_norm": 1.7296088933944702,
      "learning_rate": 0.00015723558610156548,
      "loss": 0.7898,
      "step": 1685
    },
    {
      "epoch": 0.2144356120826709,
      "grad_norm": 2.2382304668426514,
      "learning_rate": 0.0001572101310932926,
      "loss": 0.8573,
      "step": 1686
    },
    {
      "epoch": 0.21456279809220985,
      "grad_norm": 2.4185051918029785,
      "learning_rate": 0.00015718467608501975,
      "loss": 0.7917,
      "step": 1687
    },
    {
      "epoch": 0.2146899841017488,
      "grad_norm": 3.133481740951538,
      "learning_rate": 0.00015715922107674687,
      "loss": 1.304,
      "step": 1688
    },
    {
      "epoch": 0.21481717011128776,
      "grad_norm": 1.7717171907424927,
      "learning_rate": 0.00015713376606847398,
      "loss": 0.5052,
      "step": 1689
    },
    {
      "epoch": 0.2149443561208267,
      "grad_norm": 2.3840606212615967,
      "learning_rate": 0.0001571083110602011,
      "loss": 0.7707,
      "step": 1690
    },
    {
      "epoch": 0.21507154213036567,
      "grad_norm": 1.950167179107666,
      "learning_rate": 0.00015708285605192822,
      "loss": 0.7942,
      "step": 1691
    },
    {
      "epoch": 0.21519872813990462,
      "grad_norm": 1.7359777688980103,
      "learning_rate": 0.00015705740104365534,
      "loss": 0.6751,
      "step": 1692
    },
    {
      "epoch": 0.21532591414944355,
      "grad_norm": 1.8102985620498657,
      "learning_rate": 0.00015703194603538246,
      "loss": 0.6999,
      "step": 1693
    },
    {
      "epoch": 0.2154531001589825,
      "grad_norm": 2.094810962677002,
      "learning_rate": 0.00015700649102710958,
      "loss": 0.7281,
      "step": 1694
    },
    {
      "epoch": 0.21558028616852146,
      "grad_norm": 2.108975410461426,
      "learning_rate": 0.00015698103601883672,
      "loss": 0.9767,
      "step": 1695
    },
    {
      "epoch": 0.2157074721780604,
      "grad_norm": 2.025547981262207,
      "learning_rate": 0.00015695558101056384,
      "loss": 0.6757,
      "step": 1696
    },
    {
      "epoch": 0.21583465818759937,
      "grad_norm": 2.112861156463623,
      "learning_rate": 0.00015693012600229096,
      "loss": 0.6768,
      "step": 1697
    },
    {
      "epoch": 0.21596184419713832,
      "grad_norm": 2.2523419857025146,
      "learning_rate": 0.00015690467099401808,
      "loss": 0.8258,
      "step": 1698
    },
    {
      "epoch": 0.21608903020667727,
      "grad_norm": 2.08766770362854,
      "learning_rate": 0.0001568792159857452,
      "loss": 0.8844,
      "step": 1699
    },
    {
      "epoch": 0.21621621621621623,
      "grad_norm": 1.516460657119751,
      "learning_rate": 0.00015685376097747232,
      "loss": 0.6614,
      "step": 1700
    },
    {
      "epoch": 0.21634340222575515,
      "grad_norm": 1.6880507469177246,
      "learning_rate": 0.00015682830596919944,
      "loss": 0.5792,
      "step": 1701
    },
    {
      "epoch": 0.2164705882352941,
      "grad_norm": 1.624143123626709,
      "learning_rate": 0.00015680285096092658,
      "loss": 0.8182,
      "step": 1702
    },
    {
      "epoch": 0.21659777424483306,
      "grad_norm": 1.6351568698883057,
      "learning_rate": 0.00015677739595265367,
      "loss": 0.8304,
      "step": 1703
    },
    {
      "epoch": 0.21672496025437202,
      "grad_norm": 2.3828418254852295,
      "learning_rate": 0.00015675194094438082,
      "loss": 0.8006,
      "step": 1704
    },
    {
      "epoch": 0.21685214626391097,
      "grad_norm": 2.4228696823120117,
      "learning_rate": 0.00015672648593610794,
      "loss": 0.8284,
      "step": 1705
    },
    {
      "epoch": 0.21697933227344993,
      "grad_norm": 2.0310583114624023,
      "learning_rate": 0.00015670103092783506,
      "loss": 0.6194,
      "step": 1706
    },
    {
      "epoch": 0.21710651828298888,
      "grad_norm": 1.8440749645233154,
      "learning_rate": 0.0001566755759195622,
      "loss": 0.8909,
      "step": 1707
    },
    {
      "epoch": 0.21723370429252783,
      "grad_norm": 1.5749353170394897,
      "learning_rate": 0.0001566501209112893,
      "loss": 0.9369,
      "step": 1708
    },
    {
      "epoch": 0.21736089030206676,
      "grad_norm": 2.2633652687072754,
      "learning_rate": 0.00015662466590301644,
      "loss": 0.9807,
      "step": 1709
    },
    {
      "epoch": 0.21748807631160572,
      "grad_norm": 1.569110631942749,
      "learning_rate": 0.00015659921089474353,
      "loss": 0.6847,
      "step": 1710
    },
    {
      "epoch": 0.21761526232114467,
      "grad_norm": 2.1075363159179688,
      "learning_rate": 0.00015657375588647068,
      "loss": 0.822,
      "step": 1711
    },
    {
      "epoch": 0.21774244833068362,
      "grad_norm": 2.4785361289978027,
      "learning_rate": 0.0001565483008781978,
      "loss": 0.7465,
      "step": 1712
    },
    {
      "epoch": 0.21786963434022258,
      "grad_norm": 2.1924846172332764,
      "learning_rate": 0.00015652284586992492,
      "loss": 0.9487,
      "step": 1713
    },
    {
      "epoch": 0.21799682034976153,
      "grad_norm": 1.897666096687317,
      "learning_rate": 0.00015649739086165203,
      "loss": 0.8488,
      "step": 1714
    },
    {
      "epoch": 0.2181240063593005,
      "grad_norm": 1.5380605459213257,
      "learning_rate": 0.00015647193585337915,
      "loss": 0.6562,
      "step": 1715
    },
    {
      "epoch": 0.21825119236883941,
      "grad_norm": 1.8495075702667236,
      "learning_rate": 0.0001564464808451063,
      "loss": 0.9539,
      "step": 1716
    },
    {
      "epoch": 0.21837837837837837,
      "grad_norm": 1.754266619682312,
      "learning_rate": 0.0001564210258368334,
      "loss": 0.7356,
      "step": 1717
    },
    {
      "epoch": 0.21850556438791732,
      "grad_norm": 2.2206125259399414,
      "learning_rate": 0.00015639557082856054,
      "loss": 0.7972,
      "step": 1718
    },
    {
      "epoch": 0.21863275039745628,
      "grad_norm": 1.8781042098999023,
      "learning_rate": 0.00015637011582028765,
      "loss": 0.7011,
      "step": 1719
    },
    {
      "epoch": 0.21875993640699523,
      "grad_norm": 1.719131588935852,
      "learning_rate": 0.00015634466081201477,
      "loss": 0.769,
      "step": 1720
    },
    {
      "epoch": 0.21888712241653419,
      "grad_norm": 5.5570220947265625,
      "learning_rate": 0.0001563192058037419,
      "loss": 0.5956,
      "step": 1721
    },
    {
      "epoch": 0.21901430842607314,
      "grad_norm": 2.028735637664795,
      "learning_rate": 0.000156293750795469,
      "loss": 0.5888,
      "step": 1722
    },
    {
      "epoch": 0.2191414944356121,
      "grad_norm": 2.1527719497680664,
      "learning_rate": 0.00015626829578719613,
      "loss": 0.8288,
      "step": 1723
    },
    {
      "epoch": 0.21926868044515102,
      "grad_norm": 1.7491031885147095,
      "learning_rate": 0.00015624284077892325,
      "loss": 0.6971,
      "step": 1724
    },
    {
      "epoch": 0.21939586645468998,
      "grad_norm": 2.1534605026245117,
      "learning_rate": 0.0001562173857706504,
      "loss": 0.5561,
      "step": 1725
    },
    {
      "epoch": 0.21952305246422893,
      "grad_norm": 2.203338384628296,
      "learning_rate": 0.0001561919307623775,
      "loss": 0.7561,
      "step": 1726
    },
    {
      "epoch": 0.21965023847376788,
      "grad_norm": 2.5609328746795654,
      "learning_rate": 0.00015616647575410463,
      "loss": 1.2219,
      "step": 1727
    },
    {
      "epoch": 0.21977742448330684,
      "grad_norm": 2.780762195587158,
      "learning_rate": 0.00015614102074583175,
      "loss": 1.0787,
      "step": 1728
    },
    {
      "epoch": 0.2199046104928458,
      "grad_norm": 1.9704937934875488,
      "learning_rate": 0.00015611556573755887,
      "loss": 0.8185,
      "step": 1729
    },
    {
      "epoch": 0.22003179650238475,
      "grad_norm": 2.512744903564453,
      "learning_rate": 0.000156090110729286,
      "loss": 0.8533,
      "step": 1730
    },
    {
      "epoch": 0.2201589825119237,
      "grad_norm": 1.9929707050323486,
      "learning_rate": 0.0001560646557210131,
      "loss": 0.6478,
      "step": 1731
    },
    {
      "epoch": 0.22028616852146263,
      "grad_norm": 1.4941319227218628,
      "learning_rate": 0.00015603920071274023,
      "loss": 0.5692,
      "step": 1732
    },
    {
      "epoch": 0.22041335453100158,
      "grad_norm": 1.8679149150848389,
      "learning_rate": 0.00015601374570446737,
      "loss": 0.7006,
      "step": 1733
    },
    {
      "epoch": 0.22054054054054054,
      "grad_norm": 1.7505499124526978,
      "learning_rate": 0.0001559882906961945,
      "loss": 0.6892,
      "step": 1734
    },
    {
      "epoch": 0.2206677265500795,
      "grad_norm": 1.6100300550460815,
      "learning_rate": 0.0001559628356879216,
      "loss": 0.7697,
      "step": 1735
    },
    {
      "epoch": 0.22079491255961844,
      "grad_norm": 2.0970115661621094,
      "learning_rate": 0.00015593738067964873,
      "loss": 0.798,
      "step": 1736
    },
    {
      "epoch": 0.2209220985691574,
      "grad_norm": 1.6039297580718994,
      "learning_rate": 0.00015591192567137585,
      "loss": 0.7045,
      "step": 1737
    },
    {
      "epoch": 0.22104928457869635,
      "grad_norm": 1.6687875986099243,
      "learning_rate": 0.000155886470663103,
      "loss": 0.671,
      "step": 1738
    },
    {
      "epoch": 0.2211764705882353,
      "grad_norm": 1.424251675605774,
      "learning_rate": 0.00015586101565483008,
      "loss": 0.5368,
      "step": 1739
    },
    {
      "epoch": 0.22130365659777423,
      "grad_norm": 2.058051347732544,
      "learning_rate": 0.00015583556064655723,
      "loss": 0.7177,
      "step": 1740
    },
    {
      "epoch": 0.2214308426073132,
      "grad_norm": 1.8529767990112305,
      "learning_rate": 0.00015581010563828432,
      "loss": 0.6659,
      "step": 1741
    },
    {
      "epoch": 0.22155802861685214,
      "grad_norm": 1.8125529289245605,
      "learning_rate": 0.00015578465063001147,
      "loss": 0.7564,
      "step": 1742
    },
    {
      "epoch": 0.2216852146263911,
      "grad_norm": 2.66641902923584,
      "learning_rate": 0.00015575919562173859,
      "loss": 1.017,
      "step": 1743
    },
    {
      "epoch": 0.22181240063593005,
      "grad_norm": 4.17881965637207,
      "learning_rate": 0.0001557337406134657,
      "loss": 0.6884,
      "step": 1744
    },
    {
      "epoch": 0.221939586645469,
      "grad_norm": 2.2382383346557617,
      "learning_rate": 0.00015570828560519285,
      "loss": 0.9828,
      "step": 1745
    },
    {
      "epoch": 0.22206677265500796,
      "grad_norm": 1.8344879150390625,
      "learning_rate": 0.00015568283059691994,
      "loss": 0.9746,
      "step": 1746
    },
    {
      "epoch": 0.2221939586645469,
      "grad_norm": 1.7717127799987793,
      "learning_rate": 0.0001556573755886471,
      "loss": 0.6049,
      "step": 1747
    },
    {
      "epoch": 0.22232114467408584,
      "grad_norm": 1.796099305152893,
      "learning_rate": 0.00015563192058037418,
      "loss": 0.8136,
      "step": 1748
    },
    {
      "epoch": 0.2224483306836248,
      "grad_norm": 2.5240285396575928,
      "learning_rate": 0.00015560646557210133,
      "loss": 0.8436,
      "step": 1749
    },
    {
      "epoch": 0.22257551669316375,
      "grad_norm": 1.788141131401062,
      "learning_rate": 0.00015558101056382844,
      "loss": 0.5572,
      "step": 1750
    },
    {
      "epoch": 0.2227027027027027,
      "grad_norm": 1.8439364433288574,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.717,
      "step": 1751
    },
    {
      "epoch": 0.22282988871224166,
      "grad_norm": 1.9176146984100342,
      "learning_rate": 0.00015553010054728268,
      "loss": 0.6694,
      "step": 1752
    },
    {
      "epoch": 0.2229570747217806,
      "grad_norm": 1.8307825326919556,
      "learning_rate": 0.0001555046455390098,
      "loss": 0.7784,
      "step": 1753
    },
    {
      "epoch": 0.22308426073131957,
      "grad_norm": 2.3177661895751953,
      "learning_rate": 0.00015547919053073695,
      "loss": 0.9284,
      "step": 1754
    },
    {
      "epoch": 0.2232114467408585,
      "grad_norm": 2.2481279373168945,
      "learning_rate": 0.00015545373552246404,
      "loss": 1.1257,
      "step": 1755
    },
    {
      "epoch": 0.22333863275039745,
      "grad_norm": 1.8726824522018433,
      "learning_rate": 0.00015542828051419118,
      "loss": 0.753,
      "step": 1756
    },
    {
      "epoch": 0.2234658187599364,
      "grad_norm": 2.696868658065796,
      "learning_rate": 0.0001554028255059183,
      "loss": 0.9256,
      "step": 1757
    },
    {
      "epoch": 0.22359300476947536,
      "grad_norm": 2.0342907905578613,
      "learning_rate": 0.00015537737049764542,
      "loss": 0.7114,
      "step": 1758
    },
    {
      "epoch": 0.2237201907790143,
      "grad_norm": 1.8213980197906494,
      "learning_rate": 0.00015535191548937254,
      "loss": 0.8389,
      "step": 1759
    },
    {
      "epoch": 0.22384737678855327,
      "grad_norm": 3.462963342666626,
      "learning_rate": 0.00015532646048109966,
      "loss": 1.3859,
      "step": 1760
    },
    {
      "epoch": 0.22397456279809222,
      "grad_norm": 2.6896305084228516,
      "learning_rate": 0.00015530100547282678,
      "loss": 0.7396,
      "step": 1761
    },
    {
      "epoch": 0.22410174880763117,
      "grad_norm": 2.374286651611328,
      "learning_rate": 0.0001552755504645539,
      "loss": 0.6296,
      "step": 1762
    },
    {
      "epoch": 0.2242289348171701,
      "grad_norm": 1.9775100946426392,
      "learning_rate": 0.00015525009545628104,
      "loss": 0.8869,
      "step": 1763
    },
    {
      "epoch": 0.22435612082670905,
      "grad_norm": 2.514641046524048,
      "learning_rate": 0.00015522464044800816,
      "loss": 0.7923,
      "step": 1764
    },
    {
      "epoch": 0.224483306836248,
      "grad_norm": 2.707402467727661,
      "learning_rate": 0.00015519918543973528,
      "loss": 0.7009,
      "step": 1765
    },
    {
      "epoch": 0.22461049284578696,
      "grad_norm": 2.27083683013916,
      "learning_rate": 0.0001551737304314624,
      "loss": 1.1065,
      "step": 1766
    },
    {
      "epoch": 0.22473767885532592,
      "grad_norm": 2.349975824356079,
      "learning_rate": 0.00015514827542318952,
      "loss": 0.8326,
      "step": 1767
    },
    {
      "epoch": 0.22486486486486487,
      "grad_norm": 1.744295358657837,
      "learning_rate": 0.00015512282041491664,
      "loss": 0.705,
      "step": 1768
    },
    {
      "epoch": 0.22499205087440383,
      "grad_norm": 2.0838379859924316,
      "learning_rate": 0.00015509736540664378,
      "loss": 0.5584,
      "step": 1769
    },
    {
      "epoch": 0.22511923688394275,
      "grad_norm": 2.5805485248565674,
      "learning_rate": 0.00015507191039837087,
      "loss": 0.8875,
      "step": 1770
    },
    {
      "epoch": 0.2252464228934817,
      "grad_norm": 1.8229175806045532,
      "learning_rate": 0.00015504645539009802,
      "loss": 0.5825,
      "step": 1771
    },
    {
      "epoch": 0.22537360890302066,
      "grad_norm": 2.5823655128479004,
      "learning_rate": 0.0001550210003818251,
      "loss": 0.7118,
      "step": 1772
    },
    {
      "epoch": 0.22550079491255962,
      "grad_norm": 1.5083638429641724,
      "learning_rate": 0.00015499554537355226,
      "loss": 0.7246,
      "step": 1773
    },
    {
      "epoch": 0.22562798092209857,
      "grad_norm": 1.747700810432434,
      "learning_rate": 0.00015497009036527938,
      "loss": 0.6635,
      "step": 1774
    },
    {
      "epoch": 0.22575516693163752,
      "grad_norm": 2.0143330097198486,
      "learning_rate": 0.0001549446353570065,
      "loss": 0.7847,
      "step": 1775
    },
    {
      "epoch": 0.22588235294117648,
      "grad_norm": 2.2458412647247314,
      "learning_rate": 0.00015491918034873364,
      "loss": 0.95,
      "step": 1776
    },
    {
      "epoch": 0.22600953895071543,
      "grad_norm": 2.246610641479492,
      "learning_rate": 0.00015489372534046073,
      "loss": 0.8652,
      "step": 1777
    },
    {
      "epoch": 0.22613672496025436,
      "grad_norm": 1.6998302936553955,
      "learning_rate": 0.00015486827033218788,
      "loss": 0.8477,
      "step": 1778
    },
    {
      "epoch": 0.2262639109697933,
      "grad_norm": 2.349222421646118,
      "learning_rate": 0.00015484281532391497,
      "loss": 0.6654,
      "step": 1779
    },
    {
      "epoch": 0.22639109697933227,
      "grad_norm": 1.7695256471633911,
      "learning_rate": 0.00015481736031564211,
      "loss": 0.6676,
      "step": 1780
    },
    {
      "epoch": 0.22651828298887122,
      "grad_norm": 2.618577480316162,
      "learning_rate": 0.00015479190530736923,
      "loss": 0.7631,
      "step": 1781
    },
    {
      "epoch": 0.22664546899841018,
      "grad_norm": 2.606595993041992,
      "learning_rate": 0.00015476645029909635,
      "loss": 0.6109,
      "step": 1782
    },
    {
      "epoch": 0.22677265500794913,
      "grad_norm": 2.2416484355926514,
      "learning_rate": 0.0001547409952908235,
      "loss": 0.5836,
      "step": 1783
    },
    {
      "epoch": 0.22689984101748809,
      "grad_norm": 1.3832064867019653,
      "learning_rate": 0.0001547155402825506,
      "loss": 0.606,
      "step": 1784
    },
    {
      "epoch": 0.22702702702702704,
      "grad_norm": 1.6151001453399658,
      "learning_rate": 0.00015469008527427774,
      "loss": 0.5449,
      "step": 1785
    },
    {
      "epoch": 0.22715421303656597,
      "grad_norm": 2.4647185802459717,
      "learning_rate": 0.00015466463026600483,
      "loss": 0.603,
      "step": 1786
    },
    {
      "epoch": 0.22728139904610492,
      "grad_norm": 1.8769017457962036,
      "learning_rate": 0.00015463917525773197,
      "loss": 0.7935,
      "step": 1787
    },
    {
      "epoch": 0.22740858505564387,
      "grad_norm": 2.1440834999084473,
      "learning_rate": 0.0001546137202494591,
      "loss": 0.8126,
      "step": 1788
    },
    {
      "epoch": 0.22753577106518283,
      "grad_norm": 2.2964892387390137,
      "learning_rate": 0.0001545882652411862,
      "loss": 0.6097,
      "step": 1789
    },
    {
      "epoch": 0.22766295707472178,
      "grad_norm": 1.9340349435806274,
      "learning_rate": 0.00015456281023291333,
      "loss": 0.4894,
      "step": 1790
    },
    {
      "epoch": 0.22779014308426074,
      "grad_norm": 2.5062692165374756,
      "learning_rate": 0.00015453735522464045,
      "loss": 0.583,
      "step": 1791
    },
    {
      "epoch": 0.2279173290937997,
      "grad_norm": 2.2214694023132324,
      "learning_rate": 0.00015451190021636757,
      "loss": 0.655,
      "step": 1792
    },
    {
      "epoch": 0.22804451510333865,
      "grad_norm": 3.2114758491516113,
      "learning_rate": 0.00015448644520809469,
      "loss": 0.7205,
      "step": 1793
    },
    {
      "epoch": 0.22817170111287757,
      "grad_norm": 2.074272632598877,
      "learning_rate": 0.00015446099019982183,
      "loss": 0.6482,
      "step": 1794
    },
    {
      "epoch": 0.22829888712241653,
      "grad_norm": 2.1671483516693115,
      "learning_rate": 0.00015443553519154895,
      "loss": 0.6162,
      "step": 1795
    },
    {
      "epoch": 0.22842607313195548,
      "grad_norm": 2.1558098793029785,
      "learning_rate": 0.00015441008018327607,
      "loss": 0.7156,
      "step": 1796
    },
    {
      "epoch": 0.22855325914149444,
      "grad_norm": 2.6364340782165527,
      "learning_rate": 0.0001543846251750032,
      "loss": 0.9098,
      "step": 1797
    },
    {
      "epoch": 0.2286804451510334,
      "grad_norm": 2.4714465141296387,
      "learning_rate": 0.0001543591701667303,
      "loss": 0.7874,
      "step": 1798
    },
    {
      "epoch": 0.22880763116057234,
      "grad_norm": 2.7404303550720215,
      "learning_rate": 0.00015433371515845743,
      "loss": 0.9943,
      "step": 1799
    },
    {
      "epoch": 0.2289348171701113,
      "grad_norm": 3.00034499168396,
      "learning_rate": 0.00015430826015018457,
      "loss": 0.8596,
      "step": 1800
    },
    {
      "epoch": 0.22906200317965023,
      "grad_norm": 2.278859853744507,
      "learning_rate": 0.00015428280514191166,
      "loss": 0.6005,
      "step": 1801
    },
    {
      "epoch": 0.22918918918918918,
      "grad_norm": 2.7744529247283936,
      "learning_rate": 0.0001542573501336388,
      "loss": 0.801,
      "step": 1802
    },
    {
      "epoch": 0.22931637519872813,
      "grad_norm": 1.8267110586166382,
      "learning_rate": 0.00015423189512536593,
      "loss": 0.6813,
      "step": 1803
    },
    {
      "epoch": 0.2294435612082671,
      "grad_norm": 2.2005858421325684,
      "learning_rate": 0.00015420644011709305,
      "loss": 0.7452,
      "step": 1804
    },
    {
      "epoch": 0.22957074721780604,
      "grad_norm": 1.9540239572525024,
      "learning_rate": 0.00015418098510882016,
      "loss": 0.6499,
      "step": 1805
    },
    {
      "epoch": 0.229697933227345,
      "grad_norm": 1.3806195259094238,
      "learning_rate": 0.00015415553010054728,
      "loss": 0.5418,
      "step": 1806
    },
    {
      "epoch": 0.22982511923688395,
      "grad_norm": 1.3546712398529053,
      "learning_rate": 0.00015413007509227443,
      "loss": 0.6874,
      "step": 1807
    },
    {
      "epoch": 0.2299523052464229,
      "grad_norm": 1.643060564994812,
      "learning_rate": 0.00015410462008400152,
      "loss": 0.828,
      "step": 1808
    },
    {
      "epoch": 0.23007949125596183,
      "grad_norm": 1.7243950366973877,
      "learning_rate": 0.00015407916507572867,
      "loss": 0.5561,
      "step": 1809
    },
    {
      "epoch": 0.2302066772655008,
      "grad_norm": 1.9698740243911743,
      "learning_rate": 0.00015405371006745576,
      "loss": 0.9159,
      "step": 1810
    },
    {
      "epoch": 0.23033386327503974,
      "grad_norm": 1.3450967073440552,
      "learning_rate": 0.0001540282550591829,
      "loss": 0.6009,
      "step": 1811
    },
    {
      "epoch": 0.2304610492845787,
      "grad_norm": 1.962038516998291,
      "learning_rate": 0.00015400280005091002,
      "loss": 0.9744,
      "step": 1812
    },
    {
      "epoch": 0.23058823529411765,
      "grad_norm": 2.4975380897521973,
      "learning_rate": 0.00015397734504263714,
      "loss": 1.0067,
      "step": 1813
    },
    {
      "epoch": 0.2307154213036566,
      "grad_norm": 2.533907890319824,
      "learning_rate": 0.0001539518900343643,
      "loss": 1.0671,
      "step": 1814
    },
    {
      "epoch": 0.23084260731319556,
      "grad_norm": 1.9532835483551025,
      "learning_rate": 0.00015392643502609138,
      "loss": 0.724,
      "step": 1815
    },
    {
      "epoch": 0.2309697933227345,
      "grad_norm": 2.2476048469543457,
      "learning_rate": 0.00015390098001781853,
      "loss": 0.5705,
      "step": 1816
    },
    {
      "epoch": 0.23109697933227344,
      "grad_norm": 2.279585838317871,
      "learning_rate": 0.00015387552500954562,
      "loss": 0.9545,
      "step": 1817
    },
    {
      "epoch": 0.2312241653418124,
      "grad_norm": 1.8644001483917236,
      "learning_rate": 0.00015385007000127276,
      "loss": 0.6081,
      "step": 1818
    },
    {
      "epoch": 0.23135135135135135,
      "grad_norm": 1.7185323238372803,
      "learning_rate": 0.00015382461499299988,
      "loss": 0.5268,
      "step": 1819
    },
    {
      "epoch": 0.2314785373608903,
      "grad_norm": 2.2054502964019775,
      "learning_rate": 0.000153799159984727,
      "loss": 0.6878,
      "step": 1820
    },
    {
      "epoch": 0.23160572337042926,
      "grad_norm": 1.9999908208847046,
      "learning_rate": 0.00015377370497645412,
      "loss": 0.7973,
      "step": 1821
    },
    {
      "epoch": 0.2317329093799682,
      "grad_norm": 1.643964171409607,
      "learning_rate": 0.00015374824996818124,
      "loss": 0.6197,
      "step": 1822
    },
    {
      "epoch": 0.23186009538950716,
      "grad_norm": 1.623798131942749,
      "learning_rate": 0.00015372279495990838,
      "loss": 0.8102,
      "step": 1823
    },
    {
      "epoch": 0.2319872813990461,
      "grad_norm": 2.09024977684021,
      "learning_rate": 0.00015369733995163547,
      "loss": 1.0264,
      "step": 1824
    },
    {
      "epoch": 0.23211446740858505,
      "grad_norm": 2.2487618923187256,
      "learning_rate": 0.00015367188494336262,
      "loss": 0.9649,
      "step": 1825
    },
    {
      "epoch": 0.232241653418124,
      "grad_norm": 2.008899450302124,
      "learning_rate": 0.00015364642993508974,
      "loss": 0.8068,
      "step": 1826
    },
    {
      "epoch": 0.23236883942766295,
      "grad_norm": 1.8437795639038086,
      "learning_rate": 0.00015362097492681686,
      "loss": 0.837,
      "step": 1827
    },
    {
      "epoch": 0.2324960254372019,
      "grad_norm": 1.35227632522583,
      "learning_rate": 0.00015359551991854398,
      "loss": 0.5658,
      "step": 1828
    },
    {
      "epoch": 0.23262321144674086,
      "grad_norm": 2.150273084640503,
      "learning_rate": 0.0001535700649102711,
      "loss": 0.7854,
      "step": 1829
    },
    {
      "epoch": 0.23275039745627982,
      "grad_norm": 1.837673544883728,
      "learning_rate": 0.00015354460990199821,
      "loss": 0.7026,
      "step": 1830
    },
    {
      "epoch": 0.23287758346581877,
      "grad_norm": 1.811306357383728,
      "learning_rate": 0.00015351915489372536,
      "loss": 0.7267,
      "step": 1831
    },
    {
      "epoch": 0.2330047694753577,
      "grad_norm": 1.8707752227783203,
      "learning_rate": 0.00015349369988545248,
      "loss": 0.8212,
      "step": 1832
    },
    {
      "epoch": 0.23313195548489665,
      "grad_norm": 2.2394747734069824,
      "learning_rate": 0.0001534682448771796,
      "loss": 0.7012,
      "step": 1833
    },
    {
      "epoch": 0.2332591414944356,
      "grad_norm": 1.5331261157989502,
      "learning_rate": 0.00015344278986890672,
      "loss": 0.8827,
      "step": 1834
    },
    {
      "epoch": 0.23338632750397456,
      "grad_norm": 1.7618958950042725,
      "learning_rate": 0.00015341733486063384,
      "loss": 0.6648,
      "step": 1835
    },
    {
      "epoch": 0.23351351351351352,
      "grad_norm": 1.7669562101364136,
      "learning_rate": 0.00015339187985236095,
      "loss": 0.5518,
      "step": 1836
    },
    {
      "epoch": 0.23364069952305247,
      "grad_norm": 1.9335558414459229,
      "learning_rate": 0.00015336642484408807,
      "loss": 0.8234,
      "step": 1837
    },
    {
      "epoch": 0.23376788553259142,
      "grad_norm": 2.058698892593384,
      "learning_rate": 0.00015334096983581522,
      "loss": 0.575,
      "step": 1838
    },
    {
      "epoch": 0.23389507154213038,
      "grad_norm": 1.9411884546279907,
      "learning_rate": 0.0001533155148275423,
      "loss": 0.899,
      "step": 1839
    },
    {
      "epoch": 0.2340222575516693,
      "grad_norm": 1.9979033470153809,
      "learning_rate": 0.00015329005981926946,
      "loss": 0.5922,
      "step": 1840
    },
    {
      "epoch": 0.23414944356120826,
      "grad_norm": 2.356801748275757,
      "learning_rate": 0.00015326460481099657,
      "loss": 0.8807,
      "step": 1841
    },
    {
      "epoch": 0.2342766295707472,
      "grad_norm": 2.179757595062256,
      "learning_rate": 0.0001532391498027237,
      "loss": 0.8345,
      "step": 1842
    },
    {
      "epoch": 0.23440381558028617,
      "grad_norm": 2.5403482913970947,
      "learning_rate": 0.0001532136947944508,
      "loss": 0.6829,
      "step": 1843
    },
    {
      "epoch": 0.23453100158982512,
      "grad_norm": 2.4088728427886963,
      "learning_rate": 0.00015318823978617793,
      "loss": 0.6976,
      "step": 1844
    },
    {
      "epoch": 0.23465818759936408,
      "grad_norm": 1.990148663520813,
      "learning_rate": 0.00015316278477790508,
      "loss": 0.6928,
      "step": 1845
    },
    {
      "epoch": 0.23478537360890303,
      "grad_norm": 1.957055687904358,
      "learning_rate": 0.00015313732976963217,
      "loss": 0.6714,
      "step": 1846
    },
    {
      "epoch": 0.23491255961844199,
      "grad_norm": 1.6362993717193604,
      "learning_rate": 0.00015311187476135931,
      "loss": 0.792,
      "step": 1847
    },
    {
      "epoch": 0.2350397456279809,
      "grad_norm": 2.2871451377868652,
      "learning_rate": 0.0001530864197530864,
      "loss": 0.8924,
      "step": 1848
    },
    {
      "epoch": 0.23516693163751987,
      "grad_norm": 2.2148385047912598,
      "learning_rate": 0.00015306096474481355,
      "loss": 0.8617,
      "step": 1849
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 2.2828569412231445,
      "learning_rate": 0.00015303550973654067,
      "loss": 0.8259,
      "step": 1850
    },
    {
      "epoch": 0.23542130365659777,
      "grad_norm": 2.0446627140045166,
      "learning_rate": 0.0001530100547282678,
      "loss": 0.7272,
      "step": 1851
    },
    {
      "epoch": 0.23554848966613673,
      "grad_norm": 1.5773259401321411,
      "learning_rate": 0.00015298459971999494,
      "loss": 0.522,
      "step": 1852
    },
    {
      "epoch": 0.23567567567567568,
      "grad_norm": 1.9116158485412598,
      "learning_rate": 0.00015295914471172203,
      "loss": 0.726,
      "step": 1853
    },
    {
      "epoch": 0.23580286168521464,
      "grad_norm": 1.7542070150375366,
      "learning_rate": 0.00015293368970344917,
      "loss": 0.6733,
      "step": 1854
    },
    {
      "epoch": 0.23593004769475356,
      "grad_norm": 1.4113049507141113,
      "learning_rate": 0.0001529082346951763,
      "loss": 0.5398,
      "step": 1855
    },
    {
      "epoch": 0.23605723370429252,
      "grad_norm": 1.7724123001098633,
      "learning_rate": 0.0001528827796869034,
      "loss": 0.5261,
      "step": 1856
    },
    {
      "epoch": 0.23618441971383147,
      "grad_norm": 1.9002708196640015,
      "learning_rate": 0.00015285732467863053,
      "loss": 0.6149,
      "step": 1857
    },
    {
      "epoch": 0.23631160572337043,
      "grad_norm": 2.3462696075439453,
      "learning_rate": 0.00015283186967035765,
      "loss": 0.6097,
      "step": 1858
    },
    {
      "epoch": 0.23643879173290938,
      "grad_norm": 2.3564438819885254,
      "learning_rate": 0.00015280641466208477,
      "loss": 0.721,
      "step": 1859
    },
    {
      "epoch": 0.23656597774244834,
      "grad_norm": 1.6923946142196655,
      "learning_rate": 0.00015278095965381189,
      "loss": 0.7267,
      "step": 1860
    },
    {
      "epoch": 0.2366931637519873,
      "grad_norm": 2.9612679481506348,
      "learning_rate": 0.00015275550464553903,
      "loss": 0.9027,
      "step": 1861
    },
    {
      "epoch": 0.23682034976152624,
      "grad_norm": 1.6808232069015503,
      "learning_rate": 0.00015273004963726615,
      "loss": 0.8678,
      "step": 1862
    },
    {
      "epoch": 0.23694753577106517,
      "grad_norm": 2.3506929874420166,
      "learning_rate": 0.00015270459462899327,
      "loss": 0.8878,
      "step": 1863
    },
    {
      "epoch": 0.23707472178060413,
      "grad_norm": 1.8083789348602295,
      "learning_rate": 0.0001526791396207204,
      "loss": 0.6671,
      "step": 1864
    },
    {
      "epoch": 0.23720190779014308,
      "grad_norm": 2.32012939453125,
      "learning_rate": 0.0001526536846124475,
      "loss": 0.7049,
      "step": 1865
    },
    {
      "epoch": 0.23732909379968203,
      "grad_norm": 2.036137104034424,
      "learning_rate": 0.00015262822960417462,
      "loss": 0.6367,
      "step": 1866
    },
    {
      "epoch": 0.237456279809221,
      "grad_norm": 2.8027427196502686,
      "learning_rate": 0.00015260277459590174,
      "loss": 0.7795,
      "step": 1867
    },
    {
      "epoch": 0.23758346581875994,
      "grad_norm": 2.901137351989746,
      "learning_rate": 0.00015257731958762886,
      "loss": 0.8534,
      "step": 1868
    },
    {
      "epoch": 0.2377106518282989,
      "grad_norm": 1.8124785423278809,
      "learning_rate": 0.000152551864579356,
      "loss": 0.7831,
      "step": 1869
    },
    {
      "epoch": 0.23783783783783785,
      "grad_norm": 1.7981071472167969,
      "learning_rate": 0.00015252640957108313,
      "loss": 0.7059,
      "step": 1870
    },
    {
      "epoch": 0.23796502384737678,
      "grad_norm": 1.7989956140518188,
      "learning_rate": 0.00015250095456281025,
      "loss": 0.6518,
      "step": 1871
    },
    {
      "epoch": 0.23809220985691573,
      "grad_norm": 1.896187663078308,
      "learning_rate": 0.00015247549955453736,
      "loss": 0.6928,
      "step": 1872
    },
    {
      "epoch": 0.2382193958664547,
      "grad_norm": 2.232511281967163,
      "learning_rate": 0.00015245004454626448,
      "loss": 0.7908,
      "step": 1873
    },
    {
      "epoch": 0.23834658187599364,
      "grad_norm": 2.243053436279297,
      "learning_rate": 0.0001524245895379916,
      "loss": 0.9526,
      "step": 1874
    },
    {
      "epoch": 0.2384737678855326,
      "grad_norm": 2.243678331375122,
      "learning_rate": 0.00015239913452971872,
      "loss": 0.6558,
      "step": 1875
    },
    {
      "epoch": 0.23860095389507155,
      "grad_norm": 2.0673344135284424,
      "learning_rate": 0.00015237367952144587,
      "loss": 0.6898,
      "step": 1876
    },
    {
      "epoch": 0.2387281399046105,
      "grad_norm": 1.9407657384872437,
      "learning_rate": 0.00015234822451317296,
      "loss": 0.5124,
      "step": 1877
    },
    {
      "epoch": 0.23885532591414943,
      "grad_norm": 1.5716179609298706,
      "learning_rate": 0.0001523227695049001,
      "loss": 0.6137,
      "step": 1878
    },
    {
      "epoch": 0.23898251192368838,
      "grad_norm": 2.053628444671631,
      "learning_rate": 0.0001522973144966272,
      "loss": 0.8101,
      "step": 1879
    },
    {
      "epoch": 0.23910969793322734,
      "grad_norm": 1.8544914722442627,
      "learning_rate": 0.00015227185948835434,
      "loss": 0.6847,
      "step": 1880
    },
    {
      "epoch": 0.2392368839427663,
      "grad_norm": 2.0970699787139893,
      "learning_rate": 0.00015224640448008146,
      "loss": 0.8494,
      "step": 1881
    },
    {
      "epoch": 0.23936406995230525,
      "grad_norm": 1.5206482410430908,
      "learning_rate": 0.00015222094947180858,
      "loss": 0.5907,
      "step": 1882
    },
    {
      "epoch": 0.2394912559618442,
      "grad_norm": 1.630314826965332,
      "learning_rate": 0.00015219549446353572,
      "loss": 0.6214,
      "step": 1883
    },
    {
      "epoch": 0.23961844197138316,
      "grad_norm": 2.4704108238220215,
      "learning_rate": 0.00015217003945526282,
      "loss": 0.7737,
      "step": 1884
    },
    {
      "epoch": 0.2397456279809221,
      "grad_norm": 2.0987236499786377,
      "learning_rate": 0.00015214458444698996,
      "loss": 0.7587,
      "step": 1885
    },
    {
      "epoch": 0.23987281399046104,
      "grad_norm": 1.985436201095581,
      "learning_rate": 0.00015211912943871708,
      "loss": 0.5481,
      "step": 1886
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.6688374280929565,
      "learning_rate": 0.0001520936744304442,
      "loss": 0.5445,
      "step": 1887
    },
    {
      "epoch": 0.24012718600953895,
      "grad_norm": 3.498720407485962,
      "learning_rate": 0.00015206821942217132,
      "loss": 0.7179,
      "step": 1888
    },
    {
      "epoch": 0.2402543720190779,
      "grad_norm": 1.9736506938934326,
      "learning_rate": 0.00015204276441389844,
      "loss": 0.7435,
      "step": 1889
    },
    {
      "epoch": 0.24038155802861685,
      "grad_norm": 1.8237794637680054,
      "learning_rate": 0.00015201730940562558,
      "loss": 0.79,
      "step": 1890
    },
    {
      "epoch": 0.2405087440381558,
      "grad_norm": 1.717350721359253,
      "learning_rate": 0.00015199185439735267,
      "loss": 0.6881,
      "step": 1891
    },
    {
      "epoch": 0.24063593004769476,
      "grad_norm": 2.100855588912964,
      "learning_rate": 0.00015196639938907982,
      "loss": 0.8524,
      "step": 1892
    },
    {
      "epoch": 0.24076311605723372,
      "grad_norm": 1.73454749584198,
      "learning_rate": 0.00015194094438080694,
      "loss": 0.8231,
      "step": 1893
    },
    {
      "epoch": 0.24089030206677264,
      "grad_norm": 2.4164884090423584,
      "learning_rate": 0.00015191548937253406,
      "loss": 1.0148,
      "step": 1894
    },
    {
      "epoch": 0.2410174880763116,
      "grad_norm": 2.1627390384674072,
      "learning_rate": 0.00015189003436426118,
      "loss": 0.8286,
      "step": 1895
    },
    {
      "epoch": 0.24114467408585055,
      "grad_norm": 1.5994068384170532,
      "learning_rate": 0.0001518645793559883,
      "loss": 0.6595,
      "step": 1896
    },
    {
      "epoch": 0.2412718600953895,
      "grad_norm": 1.7155747413635254,
      "learning_rate": 0.00015183912434771541,
      "loss": 0.7036,
      "step": 1897
    },
    {
      "epoch": 0.24139904610492846,
      "grad_norm": 2.3298180103302,
      "learning_rate": 0.00015181366933944253,
      "loss": 0.5682,
      "step": 1898
    },
    {
      "epoch": 0.24152623211446742,
      "grad_norm": 2.496614933013916,
      "learning_rate": 0.00015178821433116968,
      "loss": 0.8141,
      "step": 1899
    },
    {
      "epoch": 0.24165341812400637,
      "grad_norm": 2.0705578327178955,
      "learning_rate": 0.0001517627593228968,
      "loss": 0.8756,
      "step": 1900
    },
    {
      "epoch": 0.24178060413354532,
      "grad_norm": 1.3700579404830933,
      "learning_rate": 0.00015173730431462392,
      "loss": 0.5951,
      "step": 1901
    },
    {
      "epoch": 0.24190779014308425,
      "grad_norm": 1.630896806716919,
      "learning_rate": 0.00015171184930635103,
      "loss": 0.7462,
      "step": 1902
    },
    {
      "epoch": 0.2420349761526232,
      "grad_norm": 2.296074867248535,
      "learning_rate": 0.00015168639429807815,
      "loss": 0.7147,
      "step": 1903
    },
    {
      "epoch": 0.24216216216216216,
      "grad_norm": 1.9701712131500244,
      "learning_rate": 0.00015166093928980527,
      "loss": 0.7132,
      "step": 1904
    },
    {
      "epoch": 0.2422893481717011,
      "grad_norm": 1.503062129020691,
      "learning_rate": 0.0001516354842815324,
      "loss": 0.34,
      "step": 1905
    },
    {
      "epoch": 0.24241653418124007,
      "grad_norm": 2.228668689727783,
      "learning_rate": 0.0001516100292732595,
      "loss": 0.6628,
      "step": 1906
    },
    {
      "epoch": 0.24254372019077902,
      "grad_norm": 1.7922720909118652,
      "learning_rate": 0.00015158457426498666,
      "loss": 0.8144,
      "step": 1907
    },
    {
      "epoch": 0.24267090620031798,
      "grad_norm": 2.6259121894836426,
      "learning_rate": 0.00015155911925671375,
      "loss": 0.6178,
      "step": 1908
    },
    {
      "epoch": 0.2427980922098569,
      "grad_norm": 1.9113472700119019,
      "learning_rate": 0.0001515336642484409,
      "loss": 0.8821,
      "step": 1909
    },
    {
      "epoch": 0.24292527821939586,
      "grad_norm": 3.388796329498291,
      "learning_rate": 0.000151508209240168,
      "loss": 0.7764,
      "step": 1910
    },
    {
      "epoch": 0.2430524642289348,
      "grad_norm": 3.212128162384033,
      "learning_rate": 0.00015148275423189513,
      "loss": 1.2029,
      "step": 1911
    },
    {
      "epoch": 0.24317965023847377,
      "grad_norm": 2.2757763862609863,
      "learning_rate": 0.00015145729922362225,
      "loss": 0.7156,
      "step": 1912
    },
    {
      "epoch": 0.24330683624801272,
      "grad_norm": 2.221210241317749,
      "learning_rate": 0.00015143184421534937,
      "loss": 0.6158,
      "step": 1913
    },
    {
      "epoch": 0.24343402225755167,
      "grad_norm": 2.313495635986328,
      "learning_rate": 0.00015140638920707651,
      "loss": 0.4749,
      "step": 1914
    },
    {
      "epoch": 0.24356120826709063,
      "grad_norm": 2.2877049446105957,
      "learning_rate": 0.0001513809341988036,
      "loss": 0.9074,
      "step": 1915
    },
    {
      "epoch": 0.24368839427662958,
      "grad_norm": 1.6457635164260864,
      "learning_rate": 0.00015135547919053075,
      "loss": 0.8135,
      "step": 1916
    },
    {
      "epoch": 0.2438155802861685,
      "grad_norm": 2.359455108642578,
      "learning_rate": 0.00015133002418225787,
      "loss": 1.0581,
      "step": 1917
    },
    {
      "epoch": 0.24394276629570746,
      "grad_norm": 2.385920524597168,
      "learning_rate": 0.000151304569173985,
      "loss": 0.8002,
      "step": 1918
    },
    {
      "epoch": 0.24406995230524642,
      "grad_norm": 1.769298791885376,
      "learning_rate": 0.00015127911416571213,
      "loss": 0.7486,
      "step": 1919
    },
    {
      "epoch": 0.24419713831478537,
      "grad_norm": 2.269023895263672,
      "learning_rate": 0.00015125365915743923,
      "loss": 0.7874,
      "step": 1920
    },
    {
      "epoch": 0.24432432432432433,
      "grad_norm": 1.4992610216140747,
      "learning_rate": 0.00015122820414916637,
      "loss": 0.8011,
      "step": 1921
    },
    {
      "epoch": 0.24445151033386328,
      "grad_norm": 3.3781814575195312,
      "learning_rate": 0.00015120274914089346,
      "loss": 0.8022,
      "step": 1922
    },
    {
      "epoch": 0.24457869634340224,
      "grad_norm": 2.1602442264556885,
      "learning_rate": 0.0001511772941326206,
      "loss": 0.7916,
      "step": 1923
    },
    {
      "epoch": 0.2447058823529412,
      "grad_norm": 2.4565787315368652,
      "learning_rate": 0.00015115183912434773,
      "loss": 0.8811,
      "step": 1924
    },
    {
      "epoch": 0.24483306836248012,
      "grad_norm": 2.0763473510742188,
      "learning_rate": 0.00015112638411607485,
      "loss": 0.6669,
      "step": 1925
    },
    {
      "epoch": 0.24496025437201907,
      "grad_norm": 1.8396387100219727,
      "learning_rate": 0.00015110092910780197,
      "loss": 0.6587,
      "step": 1926
    },
    {
      "epoch": 0.24508744038155802,
      "grad_norm": 1.8409358263015747,
      "learning_rate": 0.00015107547409952908,
      "loss": 0.6636,
      "step": 1927
    },
    {
      "epoch": 0.24521462639109698,
      "grad_norm": 3.5187466144561768,
      "learning_rate": 0.00015105001909125623,
      "loss": 0.8927,
      "step": 1928
    },
    {
      "epoch": 0.24534181240063593,
      "grad_norm": 2.360584259033203,
      "learning_rate": 0.00015102456408298332,
      "loss": 1.0239,
      "step": 1929
    },
    {
      "epoch": 0.2454689984101749,
      "grad_norm": 1.9474278688430786,
      "learning_rate": 0.00015099910907471047,
      "loss": 0.7495,
      "step": 1930
    },
    {
      "epoch": 0.24559618441971384,
      "grad_norm": 1.9857410192489624,
      "learning_rate": 0.0001509736540664376,
      "loss": 0.6566,
      "step": 1931
    },
    {
      "epoch": 0.24572337042925277,
      "grad_norm": 1.6428775787353516,
      "learning_rate": 0.0001509481990581647,
      "loss": 0.7871,
      "step": 1932
    },
    {
      "epoch": 0.24585055643879172,
      "grad_norm": 1.6976730823516846,
      "learning_rate": 0.00015092274404989182,
      "loss": 0.6684,
      "step": 1933
    },
    {
      "epoch": 0.24597774244833068,
      "grad_norm": 1.7512545585632324,
      "learning_rate": 0.00015089728904161894,
      "loss": 0.4516,
      "step": 1934
    },
    {
      "epoch": 0.24610492845786963,
      "grad_norm": 2.1294398307800293,
      "learning_rate": 0.00015087183403334606,
      "loss": 0.7513,
      "step": 1935
    },
    {
      "epoch": 0.24623211446740859,
      "grad_norm": 1.595379114151001,
      "learning_rate": 0.00015084637902507318,
      "loss": 0.6272,
      "step": 1936
    },
    {
      "epoch": 0.24635930047694754,
      "grad_norm": 1.6730644702911377,
      "learning_rate": 0.0001508209240168003,
      "loss": 0.6795,
      "step": 1937
    },
    {
      "epoch": 0.2464864864864865,
      "grad_norm": 2.362576723098755,
      "learning_rate": 0.00015079546900852744,
      "loss": 0.7217,
      "step": 1938
    },
    {
      "epoch": 0.24661367249602545,
      "grad_norm": 1.772213339805603,
      "learning_rate": 0.00015077001400025456,
      "loss": 0.7226,
      "step": 1939
    },
    {
      "epoch": 0.24674085850556438,
      "grad_norm": 1.9830782413482666,
      "learning_rate": 0.00015074455899198168,
      "loss": 0.7025,
      "step": 1940
    },
    {
      "epoch": 0.24686804451510333,
      "grad_norm": 2.062777042388916,
      "learning_rate": 0.0001507191039837088,
      "loss": 0.6231,
      "step": 1941
    },
    {
      "epoch": 0.24699523052464228,
      "grad_norm": 2.2174549102783203,
      "learning_rate": 0.00015069364897543592,
      "loss": 0.9671,
      "step": 1942
    },
    {
      "epoch": 0.24712241653418124,
      "grad_norm": 1.7767508029937744,
      "learning_rate": 0.00015066819396716304,
      "loss": 0.7264,
      "step": 1943
    },
    {
      "epoch": 0.2472496025437202,
      "grad_norm": 1.9257937669754028,
      "learning_rate": 0.00015064273895889016,
      "loss": 0.6337,
      "step": 1944
    },
    {
      "epoch": 0.24737678855325915,
      "grad_norm": 1.653560996055603,
      "learning_rate": 0.0001506172839506173,
      "loss": 0.7421,
      "step": 1945
    },
    {
      "epoch": 0.2475039745627981,
      "grad_norm": 1.932922601699829,
      "learning_rate": 0.0001505918289423444,
      "loss": 0.7022,
      "step": 1946
    },
    {
      "epoch": 0.24763116057233706,
      "grad_norm": 1.859163522720337,
      "learning_rate": 0.00015056637393407154,
      "loss": 0.7949,
      "step": 1947
    },
    {
      "epoch": 0.24775834658187598,
      "grad_norm": 2.403534412384033,
      "learning_rate": 0.00015054091892579866,
      "loss": 0.6285,
      "step": 1948
    },
    {
      "epoch": 0.24788553259141494,
      "grad_norm": 1.876739263534546,
      "learning_rate": 0.00015051546391752578,
      "loss": 0.5965,
      "step": 1949
    },
    {
      "epoch": 0.2480127186009539,
      "grad_norm": 2.6164865493774414,
      "learning_rate": 0.00015049000890925292,
      "loss": 0.8262,
      "step": 1950
    },
    {
      "epoch": 0.24813990461049285,
      "grad_norm": 1.9605154991149902,
      "learning_rate": 0.00015046455390098002,
      "loss": 0.8677,
      "step": 1951
    },
    {
      "epoch": 0.2482670906200318,
      "grad_norm": 1.7078272104263306,
      "learning_rate": 0.00015043909889270716,
      "loss": 0.7796,
      "step": 1952
    },
    {
      "epoch": 0.24839427662957075,
      "grad_norm": 2.50600528717041,
      "learning_rate": 0.00015041364388443425,
      "loss": 0.8261,
      "step": 1953
    },
    {
      "epoch": 0.2485214626391097,
      "grad_norm": 2.069197177886963,
      "learning_rate": 0.0001503881888761614,
      "loss": 0.5885,
      "step": 1954
    },
    {
      "epoch": 0.24864864864864866,
      "grad_norm": 1.9045583009719849,
      "learning_rate": 0.00015036273386788852,
      "loss": 0.6983,
      "step": 1955
    },
    {
      "epoch": 0.2487758346581876,
      "grad_norm": 1.730440616607666,
      "learning_rate": 0.00015033727885961564,
      "loss": 0.7311,
      "step": 1956
    },
    {
      "epoch": 0.24890302066772654,
      "grad_norm": 2.7035014629364014,
      "learning_rate": 0.00015031182385134276,
      "loss": 0.9611,
      "step": 1957
    },
    {
      "epoch": 0.2490302066772655,
      "grad_norm": 2.2896924018859863,
      "learning_rate": 0.00015028636884306987,
      "loss": 0.8176,
      "step": 1958
    },
    {
      "epoch": 0.24915739268680445,
      "grad_norm": 1.7184879779815674,
      "learning_rate": 0.00015026091383479702,
      "loss": 0.6091,
      "step": 1959
    },
    {
      "epoch": 0.2492845786963434,
      "grad_norm": 1.5537091493606567,
      "learning_rate": 0.0001502354588265241,
      "loss": 0.7693,
      "step": 1960
    },
    {
      "epoch": 0.24941176470588236,
      "grad_norm": 1.9606748819351196,
      "learning_rate": 0.00015021000381825126,
      "loss": 0.775,
      "step": 1961
    },
    {
      "epoch": 0.24953895071542131,
      "grad_norm": 1.755793571472168,
      "learning_rate": 0.00015018454880997838,
      "loss": 0.6388,
      "step": 1962
    },
    {
      "epoch": 0.24966613672496024,
      "grad_norm": 1.7670536041259766,
      "learning_rate": 0.0001501590938017055,
      "loss": 0.7179,
      "step": 1963
    },
    {
      "epoch": 0.2497933227344992,
      "grad_norm": 2.307657241821289,
      "learning_rate": 0.0001501336387934326,
      "loss": 0.7392,
      "step": 1964
    },
    {
      "epoch": 0.24992050874403815,
      "grad_norm": 2.4676778316497803,
      "learning_rate": 0.00015010818378515973,
      "loss": 0.8243,
      "step": 1965
    },
    {
      "epoch": 0.25004769475357713,
      "grad_norm": 2.9105396270751953,
      "learning_rate": 0.00015008272877688685,
      "loss": 0.8556,
      "step": 1966
    },
    {
      "epoch": 0.25017488076311606,
      "grad_norm": 1.8772987127304077,
      "learning_rate": 0.00015005727376861397,
      "loss": 0.5298,
      "step": 1967
    },
    {
      "epoch": 0.250302066772655,
      "grad_norm": 1.6661025285720825,
      "learning_rate": 0.00015003181876034112,
      "loss": 0.6634,
      "step": 1968
    },
    {
      "epoch": 0.25042925278219397,
      "grad_norm": 1.6172082424163818,
      "learning_rate": 0.00015000636375206823,
      "loss": 0.715,
      "step": 1969
    },
    {
      "epoch": 0.2505564387917329,
      "grad_norm": 2.406062126159668,
      "learning_rate": 0.00014998090874379535,
      "loss": 0.9534,
      "step": 1970
    },
    {
      "epoch": 0.2506836248012719,
      "grad_norm": 2.0886802673339844,
      "learning_rate": 0.00014995545373552247,
      "loss": 0.7201,
      "step": 1971
    },
    {
      "epoch": 0.2508108108108108,
      "grad_norm": 2.2289886474609375,
      "learning_rate": 0.0001499299987272496,
      "loss": 0.7547,
      "step": 1972
    },
    {
      "epoch": 0.2509379968203498,
      "grad_norm": 1.8416533470153809,
      "learning_rate": 0.0001499045437189767,
      "loss": 0.7836,
      "step": 1973
    },
    {
      "epoch": 0.2510651828298887,
      "grad_norm": 2.787083625793457,
      "learning_rate": 0.00014987908871070383,
      "loss": 0.6951,
      "step": 1974
    },
    {
      "epoch": 0.25119236883942764,
      "grad_norm": 2.8698301315307617,
      "learning_rate": 0.00014985363370243095,
      "loss": 0.8977,
      "step": 1975
    },
    {
      "epoch": 0.2513195548489666,
      "grad_norm": 2.6612298488616943,
      "learning_rate": 0.0001498281786941581,
      "loss": 0.8904,
      "step": 1976
    },
    {
      "epoch": 0.25144674085850555,
      "grad_norm": 2.262444257736206,
      "learning_rate": 0.0001498027236858852,
      "loss": 0.924,
      "step": 1977
    },
    {
      "epoch": 0.25157392686804453,
      "grad_norm": 1.4087284803390503,
      "learning_rate": 0.00014977726867761233,
      "loss": 0.727,
      "step": 1978
    },
    {
      "epoch": 0.25170111287758345,
      "grad_norm": 1.579716682434082,
      "learning_rate": 0.00014975181366933945,
      "loss": 0.6588,
      "step": 1979
    },
    {
      "epoch": 0.25182829888712244,
      "grad_norm": 1.7584588527679443,
      "learning_rate": 0.00014972635866106657,
      "loss": 0.7332,
      "step": 1980
    },
    {
      "epoch": 0.25195548489666136,
      "grad_norm": 2.305927276611328,
      "learning_rate": 0.0001497009036527937,
      "loss": 0.8184,
      "step": 1981
    },
    {
      "epoch": 0.25208267090620035,
      "grad_norm": 1.6290409564971924,
      "learning_rate": 0.0001496754486445208,
      "loss": 0.5989,
      "step": 1982
    },
    {
      "epoch": 0.25220985691573927,
      "grad_norm": 2.1218082904815674,
      "learning_rate": 0.00014964999363624795,
      "loss": 0.5681,
      "step": 1983
    },
    {
      "epoch": 0.2523370429252782,
      "grad_norm": 1.9968743324279785,
      "learning_rate": 0.00014962453862797504,
      "loss": 0.6951,
      "step": 1984
    },
    {
      "epoch": 0.2524642289348172,
      "grad_norm": 2.4298908710479736,
      "learning_rate": 0.0001495990836197022,
      "loss": 0.5118,
      "step": 1985
    },
    {
      "epoch": 0.2525914149443561,
      "grad_norm": 2.188469648361206,
      "learning_rate": 0.0001495736286114293,
      "loss": 0.5408,
      "step": 1986
    },
    {
      "epoch": 0.2527186009538951,
      "grad_norm": 2.2415409088134766,
      "learning_rate": 0.00014954817360315643,
      "loss": 0.8598,
      "step": 1987
    },
    {
      "epoch": 0.252845786963434,
      "grad_norm": 2.2519235610961914,
      "learning_rate": 0.00014952271859488357,
      "loss": 0.6505,
      "step": 1988
    },
    {
      "epoch": 0.252972972972973,
      "grad_norm": 1.8294316530227661,
      "learning_rate": 0.00014949726358661066,
      "loss": 0.6263,
      "step": 1989
    },
    {
      "epoch": 0.2531001589825119,
      "grad_norm": 1.8808519840240479,
      "learning_rate": 0.0001494718085783378,
      "loss": 0.614,
      "step": 1990
    },
    {
      "epoch": 0.25322734499205085,
      "grad_norm": 1.6975241899490356,
      "learning_rate": 0.0001494463535700649,
      "loss": 0.7848,
      "step": 1991
    },
    {
      "epoch": 0.25335453100158983,
      "grad_norm": 2.0156753063201904,
      "learning_rate": 0.00014942089856179205,
      "loss": 0.5509,
      "step": 1992
    },
    {
      "epoch": 0.25348171701112876,
      "grad_norm": 1.8520573377609253,
      "learning_rate": 0.00014939544355351917,
      "loss": 0.7909,
      "step": 1993
    },
    {
      "epoch": 0.25360890302066774,
      "grad_norm": 1.9335062503814697,
      "learning_rate": 0.00014936998854524628,
      "loss": 0.9425,
      "step": 1994
    },
    {
      "epoch": 0.25373608903020667,
      "grad_norm": 2.5514330863952637,
      "learning_rate": 0.0001493445335369734,
      "loss": 0.8802,
      "step": 1995
    },
    {
      "epoch": 0.25386327503974565,
      "grad_norm": 2.6530661582946777,
      "learning_rate": 0.00014931907852870052,
      "loss": 1.1078,
      "step": 1996
    },
    {
      "epoch": 0.2539904610492846,
      "grad_norm": 2.0938541889190674,
      "learning_rate": 0.00014929362352042767,
      "loss": 0.7535,
      "step": 1997
    },
    {
      "epoch": 0.2541176470588235,
      "grad_norm": 1.6904999017715454,
      "learning_rate": 0.00014926816851215476,
      "loss": 0.6852,
      "step": 1998
    },
    {
      "epoch": 0.2542448330683625,
      "grad_norm": 2.853236198425293,
      "learning_rate": 0.0001492427135038819,
      "loss": 0.8204,
      "step": 1999
    },
    {
      "epoch": 0.2543720190779014,
      "grad_norm": 2.1692564487457275,
      "learning_rate": 0.00014921725849560902,
      "loss": 1.0102,
      "step": 2000
    },
    {
      "epoch": 0.2544992050874404,
      "grad_norm": 2.5210351943969727,
      "learning_rate": 0.00014919180348733614,
      "loss": 1.0034,
      "step": 2001
    },
    {
      "epoch": 0.2546263910969793,
      "grad_norm": 2.5100350379943848,
      "learning_rate": 0.00014916634847906326,
      "loss": 0.6654,
      "step": 2002
    },
    {
      "epoch": 0.2547535771065183,
      "grad_norm": 2.481579542160034,
      "learning_rate": 0.00014914089347079038,
      "loss": 0.8433,
      "step": 2003
    },
    {
      "epoch": 0.25488076311605723,
      "grad_norm": 1.453890085220337,
      "learning_rate": 0.0001491154384625175,
      "loss": 0.6209,
      "step": 2004
    },
    {
      "epoch": 0.2550079491255962,
      "grad_norm": 1.8241302967071533,
      "learning_rate": 0.00014908998345424462,
      "loss": 0.7119,
      "step": 2005
    },
    {
      "epoch": 0.25513513513513514,
      "grad_norm": 2.000199317932129,
      "learning_rate": 0.00014906452844597176,
      "loss": 0.7735,
      "step": 2006
    },
    {
      "epoch": 0.25526232114467406,
      "grad_norm": 2.0439562797546387,
      "learning_rate": 0.00014903907343769888,
      "loss": 1.0824,
      "step": 2007
    },
    {
      "epoch": 0.25538950715421305,
      "grad_norm": 2.1002862453460693,
      "learning_rate": 0.000149013618429426,
      "loss": 0.6343,
      "step": 2008
    },
    {
      "epoch": 0.255516693163752,
      "grad_norm": 1.6213290691375732,
      "learning_rate": 0.00014898816342115312,
      "loss": 0.6896,
      "step": 2009
    },
    {
      "epoch": 0.25564387917329096,
      "grad_norm": 2.508082866668701,
      "learning_rate": 0.00014896270841288024,
      "loss": 0.8898,
      "step": 2010
    },
    {
      "epoch": 0.2557710651828299,
      "grad_norm": 1.931329369544983,
      "learning_rate": 0.00014893725340460736,
      "loss": 0.6913,
      "step": 2011
    },
    {
      "epoch": 0.25589825119236886,
      "grad_norm": 1.8520902395248413,
      "learning_rate": 0.0001489117983963345,
      "loss": 0.7842,
      "step": 2012
    },
    {
      "epoch": 0.2560254372019078,
      "grad_norm": 1.5424164533615112,
      "learning_rate": 0.0001488863433880616,
      "loss": 0.9617,
      "step": 2013
    },
    {
      "epoch": 0.2561526232114467,
      "grad_norm": 1.7595717906951904,
      "learning_rate": 0.00014886088837978874,
      "loss": 0.6699,
      "step": 2014
    },
    {
      "epoch": 0.2562798092209857,
      "grad_norm": 1.1884970664978027,
      "learning_rate": 0.00014883543337151583,
      "loss": 0.5947,
      "step": 2015
    },
    {
      "epoch": 0.2564069952305246,
      "grad_norm": 1.9595832824707031,
      "learning_rate": 0.00014880997836324298,
      "loss": 0.9594,
      "step": 2016
    },
    {
      "epoch": 0.2565341812400636,
      "grad_norm": 1.9570173025131226,
      "learning_rate": 0.0001487845233549701,
      "loss": 0.6745,
      "step": 2017
    },
    {
      "epoch": 0.25666136724960253,
      "grad_norm": 2.209245443344116,
      "learning_rate": 0.00014875906834669722,
      "loss": 0.6213,
      "step": 2018
    },
    {
      "epoch": 0.2567885532591415,
      "grad_norm": 2.3791489601135254,
      "learning_rate": 0.00014873361333842436,
      "loss": 0.739,
      "step": 2019
    },
    {
      "epoch": 0.25691573926868044,
      "grad_norm": 2.012871265411377,
      "learning_rate": 0.00014870815833015145,
      "loss": 0.5908,
      "step": 2020
    },
    {
      "epoch": 0.25704292527821937,
      "grad_norm": 2.0654029846191406,
      "learning_rate": 0.0001486827033218786,
      "loss": 0.7712,
      "step": 2021
    },
    {
      "epoch": 0.25717011128775835,
      "grad_norm": 2.0966217517852783,
      "learning_rate": 0.0001486572483136057,
      "loss": 0.7276,
      "step": 2022
    },
    {
      "epoch": 0.2572972972972973,
      "grad_norm": 2.540212631225586,
      "learning_rate": 0.00014863179330533284,
      "loss": 0.9543,
      "step": 2023
    },
    {
      "epoch": 0.25742448330683626,
      "grad_norm": 1.5110902786254883,
      "learning_rate": 0.00014860633829705995,
      "loss": 0.8414,
      "step": 2024
    },
    {
      "epoch": 0.2575516693163752,
      "grad_norm": 1.8257849216461182,
      "learning_rate": 0.00014858088328878707,
      "loss": 0.5085,
      "step": 2025
    },
    {
      "epoch": 0.25767885532591417,
      "grad_norm": 2.337110757827759,
      "learning_rate": 0.00014855542828051422,
      "loss": 0.756,
      "step": 2026
    },
    {
      "epoch": 0.2578060413354531,
      "grad_norm": 2.582820177078247,
      "learning_rate": 0.0001485299732722413,
      "loss": 0.7814,
      "step": 2027
    },
    {
      "epoch": 0.2579332273449921,
      "grad_norm": 2.40155291557312,
      "learning_rate": 0.00014850451826396846,
      "loss": 0.6993,
      "step": 2028
    },
    {
      "epoch": 0.258060413354531,
      "grad_norm": 2.5764105319976807,
      "learning_rate": 0.00014847906325569555,
      "loss": 0.7987,
      "step": 2029
    },
    {
      "epoch": 0.25818759936406993,
      "grad_norm": 1.867268443107605,
      "learning_rate": 0.0001484536082474227,
      "loss": 0.7792,
      "step": 2030
    },
    {
      "epoch": 0.2583147853736089,
      "grad_norm": 1.8101003170013428,
      "learning_rate": 0.0001484281532391498,
      "loss": 0.6791,
      "step": 2031
    },
    {
      "epoch": 0.25844197138314784,
      "grad_norm": 2.0534725189208984,
      "learning_rate": 0.00014840269823087693,
      "loss": 0.5932,
      "step": 2032
    },
    {
      "epoch": 0.2585691573926868,
      "grad_norm": 1.6792658567428589,
      "learning_rate": 0.00014837724322260405,
      "loss": 0.5861,
      "step": 2033
    },
    {
      "epoch": 0.25869634340222575,
      "grad_norm": 1.4758827686309814,
      "learning_rate": 0.00014835178821433117,
      "loss": 0.7694,
      "step": 2034
    },
    {
      "epoch": 0.25882352941176473,
      "grad_norm": 2.4760847091674805,
      "learning_rate": 0.00014832633320605832,
      "loss": 0.7757,
      "step": 2035
    },
    {
      "epoch": 0.25895071542130366,
      "grad_norm": 1.6459357738494873,
      "learning_rate": 0.0001483008781977854,
      "loss": 0.6601,
      "step": 2036
    },
    {
      "epoch": 0.2590779014308426,
      "grad_norm": 1.2718864679336548,
      "learning_rate": 0.00014827542318951255,
      "loss": 0.3714,
      "step": 2037
    },
    {
      "epoch": 0.25920508744038157,
      "grad_norm": 1.9379756450653076,
      "learning_rate": 0.00014824996818123967,
      "loss": 0.782,
      "step": 2038
    },
    {
      "epoch": 0.2593322734499205,
      "grad_norm": 1.8134493827819824,
      "learning_rate": 0.0001482245131729668,
      "loss": 0.696,
      "step": 2039
    },
    {
      "epoch": 0.2594594594594595,
      "grad_norm": 2.117086887359619,
      "learning_rate": 0.0001481990581646939,
      "loss": 0.8653,
      "step": 2040
    },
    {
      "epoch": 0.2595866454689984,
      "grad_norm": 1.8119237422943115,
      "learning_rate": 0.00014817360315642103,
      "loss": 0.8148,
      "step": 2041
    },
    {
      "epoch": 0.2597138314785374,
      "grad_norm": 1.7881052494049072,
      "learning_rate": 0.00014814814814814815,
      "loss": 0.5417,
      "step": 2042
    },
    {
      "epoch": 0.2598410174880763,
      "grad_norm": 2.387335777282715,
      "learning_rate": 0.0001481226931398753,
      "loss": 0.748,
      "step": 2043
    },
    {
      "epoch": 0.25996820349761524,
      "grad_norm": 1.1521130800247192,
      "learning_rate": 0.00014809723813160238,
      "loss": 0.3554,
      "step": 2044
    },
    {
      "epoch": 0.2600953895071542,
      "grad_norm": 2.627742290496826,
      "learning_rate": 0.00014807178312332953,
      "loss": 0.8832,
      "step": 2045
    },
    {
      "epoch": 0.26022257551669314,
      "grad_norm": 2.3322629928588867,
      "learning_rate": 0.00014804632811505665,
      "loss": 0.6368,
      "step": 2046
    },
    {
      "epoch": 0.2603497615262321,
      "grad_norm": 2.4272398948669434,
      "learning_rate": 0.00014802087310678377,
      "loss": 0.5619,
      "step": 2047
    },
    {
      "epoch": 0.26047694753577105,
      "grad_norm": 2.421915292739868,
      "learning_rate": 0.00014799541809851089,
      "loss": 0.87,
      "step": 2048
    },
    {
      "epoch": 0.26060413354531003,
      "grad_norm": 1.8711955547332764,
      "learning_rate": 0.000147969963090238,
      "loss": 0.8803,
      "step": 2049
    },
    {
      "epoch": 0.26073131955484896,
      "grad_norm": 2.203747034072876,
      "learning_rate": 0.00014794450808196515,
      "loss": 0.845,
      "step": 2050
    },
    {
      "epoch": 0.26085850556438794,
      "grad_norm": 2.1882028579711914,
      "learning_rate": 0.00014791905307369224,
      "loss": 0.6497,
      "step": 2051
    },
    {
      "epoch": 0.26098569157392687,
      "grad_norm": 1.9688527584075928,
      "learning_rate": 0.0001478935980654194,
      "loss": 0.7103,
      "step": 2052
    },
    {
      "epoch": 0.2611128775834658,
      "grad_norm": 2.8936634063720703,
      "learning_rate": 0.00014786814305714648,
      "loss": 1.0409,
      "step": 2053
    },
    {
      "epoch": 0.2612400635930048,
      "grad_norm": 2.1822917461395264,
      "learning_rate": 0.00014784268804887363,
      "loss": 0.6505,
      "step": 2054
    },
    {
      "epoch": 0.2613672496025437,
      "grad_norm": 2.4686410427093506,
      "learning_rate": 0.00014781723304060074,
      "loss": 0.5901,
      "step": 2055
    },
    {
      "epoch": 0.2614944356120827,
      "grad_norm": 2.519477367401123,
      "learning_rate": 0.00014779177803232786,
      "loss": 0.9359,
      "step": 2056
    },
    {
      "epoch": 0.2616216216216216,
      "grad_norm": 1.8210043907165527,
      "learning_rate": 0.000147766323024055,
      "loss": 0.6298,
      "step": 2057
    },
    {
      "epoch": 0.2617488076311606,
      "grad_norm": 2.125081777572632,
      "learning_rate": 0.0001477408680157821,
      "loss": 0.8351,
      "step": 2058
    },
    {
      "epoch": 0.2618759936406995,
      "grad_norm": 1.9362540245056152,
      "learning_rate": 0.00014771541300750925,
      "loss": 0.8879,
      "step": 2059
    },
    {
      "epoch": 0.26200317965023845,
      "grad_norm": 1.468948483467102,
      "learning_rate": 0.00014768995799923634,
      "loss": 0.7956,
      "step": 2060
    },
    {
      "epoch": 0.26213036565977743,
      "grad_norm": 1.3613357543945312,
      "learning_rate": 0.00014766450299096348,
      "loss": 0.5523,
      "step": 2061
    },
    {
      "epoch": 0.26225755166931636,
      "grad_norm": 1.8084654808044434,
      "learning_rate": 0.0001476390479826906,
      "loss": 0.7293,
      "step": 2062
    },
    {
      "epoch": 0.26238473767885534,
      "grad_norm": 1.5616146326065063,
      "learning_rate": 0.00014761359297441772,
      "loss": 0.492,
      "step": 2063
    },
    {
      "epoch": 0.26251192368839427,
      "grad_norm": 1.8582760095596313,
      "learning_rate": 0.00014758813796614487,
      "loss": 0.5537,
      "step": 2064
    },
    {
      "epoch": 0.26263910969793325,
      "grad_norm": 2.380425453186035,
      "learning_rate": 0.00014756268295787196,
      "loss": 0.8487,
      "step": 2065
    },
    {
      "epoch": 0.2627662957074722,
      "grad_norm": 2.3568172454833984,
      "learning_rate": 0.0001475372279495991,
      "loss": 0.7443,
      "step": 2066
    },
    {
      "epoch": 0.2628934817170111,
      "grad_norm": 3.3707501888275146,
      "learning_rate": 0.0001475117729413262,
      "loss": 0.8773,
      "step": 2067
    },
    {
      "epoch": 0.2630206677265501,
      "grad_norm": 2.47465443611145,
      "learning_rate": 0.00014748631793305334,
      "loss": 0.5808,
      "step": 2068
    },
    {
      "epoch": 0.263147853736089,
      "grad_norm": 1.7352030277252197,
      "learning_rate": 0.00014746086292478046,
      "loss": 0.7432,
      "step": 2069
    },
    {
      "epoch": 0.263275039745628,
      "grad_norm": 2.5718796253204346,
      "learning_rate": 0.00014743540791650758,
      "loss": 0.763,
      "step": 2070
    },
    {
      "epoch": 0.2634022257551669,
      "grad_norm": 2.0467922687530518,
      "learning_rate": 0.0001474099529082347,
      "loss": 0.7591,
      "step": 2071
    },
    {
      "epoch": 0.2635294117647059,
      "grad_norm": 1.9915193319320679,
      "learning_rate": 0.00014738449789996182,
      "loss": 0.6632,
      "step": 2072
    },
    {
      "epoch": 0.2636565977742448,
      "grad_norm": 2.6290011405944824,
      "learning_rate": 0.00014735904289168894,
      "loss": 0.8516,
      "step": 2073
    },
    {
      "epoch": 0.2637837837837838,
      "grad_norm": 4.735883712768555,
      "learning_rate": 0.00014733358788341608,
      "loss": 0.735,
      "step": 2074
    },
    {
      "epoch": 0.26391096979332274,
      "grad_norm": 2.502140998840332,
      "learning_rate": 0.0001473081328751432,
      "loss": 0.7178,
      "step": 2075
    },
    {
      "epoch": 0.26403815580286166,
      "grad_norm": 1.7375489473342896,
      "learning_rate": 0.00014728267786687032,
      "loss": 0.6232,
      "step": 2076
    },
    {
      "epoch": 0.26416534181240064,
      "grad_norm": 1.9843237400054932,
      "learning_rate": 0.00014725722285859744,
      "loss": 0.5778,
      "step": 2077
    },
    {
      "epoch": 0.26429252782193957,
      "grad_norm": 1.9679412841796875,
      "learning_rate": 0.00014723176785032456,
      "loss": 0.6944,
      "step": 2078
    },
    {
      "epoch": 0.26441971383147855,
      "grad_norm": 2.0324618816375732,
      "learning_rate": 0.00014720631284205168,
      "loss": 0.7459,
      "step": 2079
    },
    {
      "epoch": 0.2645468998410175,
      "grad_norm": 3.5906333923339844,
      "learning_rate": 0.0001471808578337788,
      "loss": 0.7785,
      "step": 2080
    },
    {
      "epoch": 0.26467408585055646,
      "grad_norm": 2.2436370849609375,
      "learning_rate": 0.00014715540282550594,
      "loss": 0.7751,
      "step": 2081
    },
    {
      "epoch": 0.2648012718600954,
      "grad_norm": 2.2824394702911377,
      "learning_rate": 0.00014712994781723303,
      "loss": 0.7085,
      "step": 2082
    },
    {
      "epoch": 0.2649284578696343,
      "grad_norm": 2.4768919944763184,
      "learning_rate": 0.00014710449280896018,
      "loss": 0.7966,
      "step": 2083
    },
    {
      "epoch": 0.2650556438791733,
      "grad_norm": 2.7018768787384033,
      "learning_rate": 0.0001470790378006873,
      "loss": 0.6907,
      "step": 2084
    },
    {
      "epoch": 0.2651828298887122,
      "grad_norm": 2.839510679244995,
      "learning_rate": 0.00014705358279241441,
      "loss": 0.6138,
      "step": 2085
    },
    {
      "epoch": 0.2653100158982512,
      "grad_norm": 2.065134286880493,
      "learning_rate": 0.00014702812778414153,
      "loss": 0.3953,
      "step": 2086
    },
    {
      "epoch": 0.26543720190779013,
      "grad_norm": 1.8425863981246948,
      "learning_rate": 0.00014700267277586865,
      "loss": 0.7838,
      "step": 2087
    },
    {
      "epoch": 0.2655643879173291,
      "grad_norm": 2.1263623237609863,
      "learning_rate": 0.0001469772177675958,
      "loss": 0.6088,
      "step": 2088
    },
    {
      "epoch": 0.26569157392686804,
      "grad_norm": 1.6767562627792358,
      "learning_rate": 0.0001469517627593229,
      "loss": 0.642,
      "step": 2089
    },
    {
      "epoch": 0.26581875993640697,
      "grad_norm": 2.0959408283233643,
      "learning_rate": 0.00014692630775105004,
      "loss": 0.6404,
      "step": 2090
    },
    {
      "epoch": 0.26594594594594595,
      "grad_norm": 1.7547671794891357,
      "learning_rate": 0.00014690085274277713,
      "loss": 0.6051,
      "step": 2091
    },
    {
      "epoch": 0.2660731319554849,
      "grad_norm": 2.567082166671753,
      "learning_rate": 0.00014687539773450427,
      "loss": 0.6411,
      "step": 2092
    },
    {
      "epoch": 0.26620031796502386,
      "grad_norm": 2.3403706550598145,
      "learning_rate": 0.0001468499427262314,
      "loss": 0.9596,
      "step": 2093
    },
    {
      "epoch": 0.2663275039745628,
      "grad_norm": 2.894221305847168,
      "learning_rate": 0.0001468244877179585,
      "loss": 0.823,
      "step": 2094
    },
    {
      "epoch": 0.26645468998410177,
      "grad_norm": 1.552111029624939,
      "learning_rate": 0.00014679903270968566,
      "loss": 0.8923,
      "step": 2095
    },
    {
      "epoch": 0.2665818759936407,
      "grad_norm": 1.6563730239868164,
      "learning_rate": 0.00014677357770141275,
      "loss": 0.5473,
      "step": 2096
    },
    {
      "epoch": 0.2667090620031797,
      "grad_norm": 2.2473723888397217,
      "learning_rate": 0.0001467481226931399,
      "loss": 0.955,
      "step": 2097
    },
    {
      "epoch": 0.2668362480127186,
      "grad_norm": 1.5014511346817017,
      "learning_rate": 0.000146722667684867,
      "loss": 0.4723,
      "step": 2098
    },
    {
      "epoch": 0.26696343402225753,
      "grad_norm": 2.21641206741333,
      "learning_rate": 0.00014669721267659413,
      "loss": 0.7397,
      "step": 2099
    },
    {
      "epoch": 0.2670906200317965,
      "grad_norm": 1.9223284721374512,
      "learning_rate": 0.00014667175766832125,
      "loss": 0.875,
      "step": 2100
    },
    {
      "epoch": 0.26721780604133544,
      "grad_norm": 1.8875908851623535,
      "learning_rate": 0.00014664630266004837,
      "loss": 0.8175,
      "step": 2101
    },
    {
      "epoch": 0.2673449920508744,
      "grad_norm": 1.4759292602539062,
      "learning_rate": 0.0001466208476517755,
      "loss": 0.6246,
      "step": 2102
    },
    {
      "epoch": 0.26747217806041335,
      "grad_norm": 1.8928018808364868,
      "learning_rate": 0.0001465953926435026,
      "loss": 0.6884,
      "step": 2103
    },
    {
      "epoch": 0.26759936406995233,
      "grad_norm": 1.7429018020629883,
      "learning_rate": 0.00014656993763522975,
      "loss": 0.9542,
      "step": 2104
    },
    {
      "epoch": 0.26772655007949125,
      "grad_norm": 1.8639686107635498,
      "learning_rate": 0.00014654448262695687,
      "loss": 0.9507,
      "step": 2105
    },
    {
      "epoch": 0.2678537360890302,
      "grad_norm": 1.589011549949646,
      "learning_rate": 0.000146519027618684,
      "loss": 0.7154,
      "step": 2106
    },
    {
      "epoch": 0.26798092209856916,
      "grad_norm": 1.9530149698257446,
      "learning_rate": 0.0001464935726104111,
      "loss": 0.7427,
      "step": 2107
    },
    {
      "epoch": 0.2681081081081081,
      "grad_norm": 2.094796895980835,
      "learning_rate": 0.00014646811760213823,
      "loss": 0.8289,
      "step": 2108
    },
    {
      "epoch": 0.26823529411764707,
      "grad_norm": 2.2407143115997314,
      "learning_rate": 0.00014644266259386535,
      "loss": 0.7653,
      "step": 2109
    },
    {
      "epoch": 0.268362480127186,
      "grad_norm": 2.112190008163452,
      "learning_rate": 0.00014641720758559246,
      "loss": 0.7124,
      "step": 2110
    },
    {
      "epoch": 0.268489666136725,
      "grad_norm": 1.5474961996078491,
      "learning_rate": 0.00014639175257731958,
      "loss": 0.7312,
      "step": 2111
    },
    {
      "epoch": 0.2686168521462639,
      "grad_norm": 1.545406699180603,
      "learning_rate": 0.00014636629756904673,
      "loss": 0.7353,
      "step": 2112
    },
    {
      "epoch": 0.2687440381558029,
      "grad_norm": 2.013420343399048,
      "learning_rate": 0.00014634084256077385,
      "loss": 0.6793,
      "step": 2113
    },
    {
      "epoch": 0.2688712241653418,
      "grad_norm": 1.9643776416778564,
      "learning_rate": 0.00014631538755250097,
      "loss": 0.767,
      "step": 2114
    },
    {
      "epoch": 0.26899841017488074,
      "grad_norm": 1.892126202583313,
      "learning_rate": 0.00014628993254422809,
      "loss": 0.6864,
      "step": 2115
    },
    {
      "epoch": 0.2691255961844197,
      "grad_norm": 2.0349268913269043,
      "learning_rate": 0.0001462644775359552,
      "loss": 0.6789,
      "step": 2116
    },
    {
      "epoch": 0.26925278219395865,
      "grad_norm": 2.75118350982666,
      "learning_rate": 0.00014623902252768232,
      "loss": 0.7885,
      "step": 2117
    },
    {
      "epoch": 0.26937996820349763,
      "grad_norm": 2.6873183250427246,
      "learning_rate": 0.00014621356751940944,
      "loss": 0.9852,
      "step": 2118
    },
    {
      "epoch": 0.26950715421303656,
      "grad_norm": 1.8729069232940674,
      "learning_rate": 0.0001461881125111366,
      "loss": 0.8674,
      "step": 2119
    },
    {
      "epoch": 0.26963434022257554,
      "grad_norm": 2.4759521484375,
      "learning_rate": 0.00014616265750286368,
      "loss": 0.8151,
      "step": 2120
    },
    {
      "epoch": 0.26976152623211447,
      "grad_norm": 2.1103146076202393,
      "learning_rate": 0.00014613720249459082,
      "loss": 0.8185,
      "step": 2121
    },
    {
      "epoch": 0.2698887122416534,
      "grad_norm": 1.8306140899658203,
      "learning_rate": 0.00014611174748631792,
      "loss": 0.4526,
      "step": 2122
    },
    {
      "epoch": 0.2700158982511924,
      "grad_norm": 2.194822311401367,
      "learning_rate": 0.00014608629247804506,
      "loss": 0.6979,
      "step": 2123
    },
    {
      "epoch": 0.2701430842607313,
      "grad_norm": 1.7542418241500854,
      "learning_rate": 0.00014606083746977218,
      "loss": 0.7776,
      "step": 2124
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 3.0902113914489746,
      "learning_rate": 0.0001460353824614993,
      "loss": 0.6058,
      "step": 2125
    },
    {
      "epoch": 0.2703974562798092,
      "grad_norm": 1.3929005861282349,
      "learning_rate": 0.00014600992745322645,
      "loss": 0.4813,
      "step": 2126
    },
    {
      "epoch": 0.2705246422893482,
      "grad_norm": 2.8352489471435547,
      "learning_rate": 0.00014598447244495354,
      "loss": 0.8711,
      "step": 2127
    },
    {
      "epoch": 0.2706518282988871,
      "grad_norm": 2.3866002559661865,
      "learning_rate": 0.00014595901743668068,
      "loss": 0.866,
      "step": 2128
    },
    {
      "epoch": 0.27077901430842605,
      "grad_norm": 2.0365278720855713,
      "learning_rate": 0.0001459335624284078,
      "loss": 0.7436,
      "step": 2129
    },
    {
      "epoch": 0.27090620031796503,
      "grad_norm": 1.8849639892578125,
      "learning_rate": 0.00014590810742013492,
      "loss": 0.8963,
      "step": 2130
    },
    {
      "epoch": 0.27103338632750396,
      "grad_norm": 1.7861407995224,
      "learning_rate": 0.00014588265241186204,
      "loss": 0.5394,
      "step": 2131
    },
    {
      "epoch": 0.27116057233704294,
      "grad_norm": 2.428372383117676,
      "learning_rate": 0.00014585719740358916,
      "loss": 0.7554,
      "step": 2132
    },
    {
      "epoch": 0.27128775834658186,
      "grad_norm": 2.180072784423828,
      "learning_rate": 0.0001458317423953163,
      "loss": 0.6979,
      "step": 2133
    },
    {
      "epoch": 0.27141494435612085,
      "grad_norm": 2.2018892765045166,
      "learning_rate": 0.0001458062873870434,
      "loss": 0.6674,
      "step": 2134
    },
    {
      "epoch": 0.2715421303656598,
      "grad_norm": 1.6332237720489502,
      "learning_rate": 0.00014578083237877054,
      "loss": 0.5631,
      "step": 2135
    },
    {
      "epoch": 0.27166931637519875,
      "grad_norm": 2.6698198318481445,
      "learning_rate": 0.00014575537737049766,
      "loss": 0.8519,
      "step": 2136
    },
    {
      "epoch": 0.2717965023847377,
      "grad_norm": 2.0084636211395264,
      "learning_rate": 0.00014572992236222478,
      "loss": 0.8296,
      "step": 2137
    },
    {
      "epoch": 0.2719236883942766,
      "grad_norm": 2.068957567214966,
      "learning_rate": 0.0001457044673539519,
      "loss": 0.8439,
      "step": 2138
    },
    {
      "epoch": 0.2720508744038156,
      "grad_norm": 1.6434651613235474,
      "learning_rate": 0.00014567901234567902,
      "loss": 0.7695,
      "step": 2139
    },
    {
      "epoch": 0.2721780604133545,
      "grad_norm": 1.7635482549667358,
      "learning_rate": 0.00014565355733740614,
      "loss": 0.8127,
      "step": 2140
    },
    {
      "epoch": 0.2723052464228935,
      "grad_norm": 2.2578556537628174,
      "learning_rate": 0.00014562810232913325,
      "loss": 0.8494,
      "step": 2141
    },
    {
      "epoch": 0.2724324324324324,
      "grad_norm": 2.249208688735962,
      "learning_rate": 0.0001456026473208604,
      "loss": 0.7904,
      "step": 2142
    },
    {
      "epoch": 0.2725596184419714,
      "grad_norm": 1.5889211893081665,
      "learning_rate": 0.00014557719231258752,
      "loss": 0.5448,
      "step": 2143
    },
    {
      "epoch": 0.27268680445151033,
      "grad_norm": 1.7054818868637085,
      "learning_rate": 0.00014555173730431464,
      "loss": 0.9448,
      "step": 2144
    },
    {
      "epoch": 0.27281399046104926,
      "grad_norm": 1.740891933441162,
      "learning_rate": 0.00014552628229604176,
      "loss": 0.9676,
      "step": 2145
    },
    {
      "epoch": 0.27294117647058824,
      "grad_norm": 2.7948267459869385,
      "learning_rate": 0.00014550082728776887,
      "loss": 0.9002,
      "step": 2146
    },
    {
      "epoch": 0.27306836248012717,
      "grad_norm": 1.6838490962982178,
      "learning_rate": 0.000145475372279496,
      "loss": 0.7391,
      "step": 2147
    },
    {
      "epoch": 0.27319554848966615,
      "grad_norm": 1.8523740768432617,
      "learning_rate": 0.0001454499172712231,
      "loss": 0.8492,
      "step": 2148
    },
    {
      "epoch": 0.2733227344992051,
      "grad_norm": 2.3175227642059326,
      "learning_rate": 0.00014542446226295023,
      "loss": 0.9911,
      "step": 2149
    },
    {
      "epoch": 0.27344992050874406,
      "grad_norm": 2.2642805576324463,
      "learning_rate": 0.00014539900725467738,
      "loss": 0.8142,
      "step": 2150
    },
    {
      "epoch": 0.273577106518283,
      "grad_norm": 1.6992779970169067,
      "learning_rate": 0.00014537355224640447,
      "loss": 0.8619,
      "step": 2151
    },
    {
      "epoch": 0.2737042925278219,
      "grad_norm": 2.392307996749878,
      "learning_rate": 0.00014534809723813161,
      "loss": 0.5128,
      "step": 2152
    },
    {
      "epoch": 0.2738314785373609,
      "grad_norm": 1.3780484199523926,
      "learning_rate": 0.00014532264222985873,
      "loss": 0.5707,
      "step": 2153
    },
    {
      "epoch": 0.2739586645468998,
      "grad_norm": 1.9892256259918213,
      "learning_rate": 0.00014529718722158585,
      "loss": 0.8264,
      "step": 2154
    },
    {
      "epoch": 0.2740858505564388,
      "grad_norm": 2.50935959815979,
      "learning_rate": 0.00014527173221331297,
      "loss": 0.7406,
      "step": 2155
    },
    {
      "epoch": 0.27421303656597773,
      "grad_norm": 2.399193048477173,
      "learning_rate": 0.0001452462772050401,
      "loss": 0.6409,
      "step": 2156
    },
    {
      "epoch": 0.2743402225755167,
      "grad_norm": 2.358605146408081,
      "learning_rate": 0.00014522082219676723,
      "loss": 0.6361,
      "step": 2157
    },
    {
      "epoch": 0.27446740858505564,
      "grad_norm": 2.5465004444122314,
      "learning_rate": 0.00014519536718849433,
      "loss": 0.5522,
      "step": 2158
    },
    {
      "epoch": 0.2745945945945946,
      "grad_norm": 2.4335038661956787,
      "learning_rate": 0.00014516991218022147,
      "loss": 0.8443,
      "step": 2159
    },
    {
      "epoch": 0.27472178060413355,
      "grad_norm": 2.339735269546509,
      "learning_rate": 0.0001451444571719486,
      "loss": 0.7595,
      "step": 2160
    },
    {
      "epoch": 0.2748489666136725,
      "grad_norm": 2.3534419536590576,
      "learning_rate": 0.0001451190021636757,
      "loss": 0.8667,
      "step": 2161
    },
    {
      "epoch": 0.27497615262321146,
      "grad_norm": 1.8833441734313965,
      "learning_rate": 0.00014509354715540286,
      "loss": 0.691,
      "step": 2162
    },
    {
      "epoch": 0.2751033386327504,
      "grad_norm": 1.9216073751449585,
      "learning_rate": 0.00014506809214712995,
      "loss": 0.6323,
      "step": 2163
    },
    {
      "epoch": 0.27523052464228936,
      "grad_norm": 1.7915276288986206,
      "learning_rate": 0.0001450426371388571,
      "loss": 0.5703,
      "step": 2164
    },
    {
      "epoch": 0.2753577106518283,
      "grad_norm": 3.2222588062286377,
      "learning_rate": 0.00014501718213058418,
      "loss": 1.042,
      "step": 2165
    },
    {
      "epoch": 0.2754848966613673,
      "grad_norm": 2.368001699447632,
      "learning_rate": 0.00014499172712231133,
      "loss": 0.6894,
      "step": 2166
    },
    {
      "epoch": 0.2756120826709062,
      "grad_norm": 3.910597085952759,
      "learning_rate": 0.00014496627211403845,
      "loss": 0.711,
      "step": 2167
    },
    {
      "epoch": 0.2757392686804451,
      "grad_norm": 2.3176143169403076,
      "learning_rate": 0.00014494081710576557,
      "loss": 0.8302,
      "step": 2168
    },
    {
      "epoch": 0.2758664546899841,
      "grad_norm": 1.730799674987793,
      "learning_rate": 0.0001449153620974927,
      "loss": 0.7728,
      "step": 2169
    },
    {
      "epoch": 0.27599364069952304,
      "grad_norm": 2.0490522384643555,
      "learning_rate": 0.0001448899070892198,
      "loss": 0.5624,
      "step": 2170
    },
    {
      "epoch": 0.276120826709062,
      "grad_norm": 2.021894931793213,
      "learning_rate": 0.00014486445208094695,
      "loss": 0.6804,
      "step": 2171
    },
    {
      "epoch": 0.27624801271860094,
      "grad_norm": 2.2811169624328613,
      "learning_rate": 0.00014483899707267404,
      "loss": 0.8157,
      "step": 2172
    },
    {
      "epoch": 0.2763751987281399,
      "grad_norm": 2.8362011909484863,
      "learning_rate": 0.0001448135420644012,
      "loss": 0.8153,
      "step": 2173
    },
    {
      "epoch": 0.27650238473767885,
      "grad_norm": 3.3480327129364014,
      "learning_rate": 0.0001447880870561283,
      "loss": 0.8935,
      "step": 2174
    },
    {
      "epoch": 0.2766295707472178,
      "grad_norm": 2.395500659942627,
      "learning_rate": 0.00014476263204785543,
      "loss": 0.7843,
      "step": 2175
    },
    {
      "epoch": 0.27675675675675676,
      "grad_norm": 2.332986354827881,
      "learning_rate": 0.00014473717703958255,
      "loss": 0.6554,
      "step": 2176
    },
    {
      "epoch": 0.2768839427662957,
      "grad_norm": 3.091787099838257,
      "learning_rate": 0.00014471172203130966,
      "loss": 0.8099,
      "step": 2177
    },
    {
      "epoch": 0.27701112877583467,
      "grad_norm": 2.5165328979492188,
      "learning_rate": 0.00014468626702303678,
      "loss": 0.821,
      "step": 2178
    },
    {
      "epoch": 0.2771383147853736,
      "grad_norm": 1.7123606204986572,
      "learning_rate": 0.0001446608120147639,
      "loss": 0.4454,
      "step": 2179
    },
    {
      "epoch": 0.2772655007949126,
      "grad_norm": 2.52077579498291,
      "learning_rate": 0.00014463535700649102,
      "loss": 0.9086,
      "step": 2180
    },
    {
      "epoch": 0.2773926868044515,
      "grad_norm": 2.185363531112671,
      "learning_rate": 0.00014460990199821817,
      "loss": 0.6569,
      "step": 2181
    },
    {
      "epoch": 0.2775198728139905,
      "grad_norm": 2.0742697715759277,
      "learning_rate": 0.00014458444698994528,
      "loss": 0.7704,
      "step": 2182
    },
    {
      "epoch": 0.2776470588235294,
      "grad_norm": 2.651966094970703,
      "learning_rate": 0.0001445589919816724,
      "loss": 0.9181,
      "step": 2183
    },
    {
      "epoch": 0.27777424483306834,
      "grad_norm": 2.2640671730041504,
      "learning_rate": 0.00014453353697339952,
      "loss": 1.1453,
      "step": 2184
    },
    {
      "epoch": 0.2779014308426073,
      "grad_norm": 2.2019996643066406,
      "learning_rate": 0.00014450808196512664,
      "loss": 0.7096,
      "step": 2185
    },
    {
      "epoch": 0.27802861685214625,
      "grad_norm": 1.679937481880188,
      "learning_rate": 0.00014448262695685376,
      "loss": 0.6857,
      "step": 2186
    },
    {
      "epoch": 0.27815580286168523,
      "grad_norm": 3.2433338165283203,
      "learning_rate": 0.00014445717194858088,
      "loss": 0.8903,
      "step": 2187
    },
    {
      "epoch": 0.27828298887122416,
      "grad_norm": 1.951595664024353,
      "learning_rate": 0.00014443171694030802,
      "loss": 0.8878,
      "step": 2188
    },
    {
      "epoch": 0.27841017488076314,
      "grad_norm": 1.8053253889083862,
      "learning_rate": 0.00014440626193203512,
      "loss": 0.8376,
      "step": 2189
    },
    {
      "epoch": 0.27853736089030207,
      "grad_norm": 2.0497148036956787,
      "learning_rate": 0.00014438080692376226,
      "loss": 0.9574,
      "step": 2190
    },
    {
      "epoch": 0.278664546899841,
      "grad_norm": 1.665345549583435,
      "learning_rate": 0.00014435535191548938,
      "loss": 0.8105,
      "step": 2191
    },
    {
      "epoch": 0.27879173290938,
      "grad_norm": 1.8410090208053589,
      "learning_rate": 0.0001443298969072165,
      "loss": 0.9191,
      "step": 2192
    },
    {
      "epoch": 0.2789189189189189,
      "grad_norm": 3.6750223636627197,
      "learning_rate": 0.00014430444189894365,
      "loss": 0.9277,
      "step": 2193
    },
    {
      "epoch": 0.2790461049284579,
      "grad_norm": 1.463427186012268,
      "learning_rate": 0.00014427898689067074,
      "loss": 0.4985,
      "step": 2194
    },
    {
      "epoch": 0.2791732909379968,
      "grad_norm": 2.5468194484710693,
      "learning_rate": 0.00014425353188239788,
      "loss": 0.9439,
      "step": 2195
    },
    {
      "epoch": 0.2793004769475358,
      "grad_norm": 1.9459391832351685,
      "learning_rate": 0.00014422807687412497,
      "loss": 0.8125,
      "step": 2196
    },
    {
      "epoch": 0.2794276629570747,
      "grad_norm": 2.136061191558838,
      "learning_rate": 0.00014420262186585212,
      "loss": 0.7265,
      "step": 2197
    },
    {
      "epoch": 0.27955484896661364,
      "grad_norm": 2.124793291091919,
      "learning_rate": 0.00014417716685757924,
      "loss": 0.7238,
      "step": 2198
    },
    {
      "epoch": 0.2796820349761526,
      "grad_norm": 1.9269413948059082,
      "learning_rate": 0.00014415171184930636,
      "loss": 0.8155,
      "step": 2199
    },
    {
      "epoch": 0.27980922098569155,
      "grad_norm": 1.7161986827850342,
      "learning_rate": 0.0001441262568410335,
      "loss": 0.583,
      "step": 2200
    },
    {
      "epoch": 0.27993640699523054,
      "grad_norm": 1.7097482681274414,
      "learning_rate": 0.0001441008018327606,
      "loss": 0.5439,
      "step": 2201
    },
    {
      "epoch": 0.28006359300476946,
      "grad_norm": 2.6472136974334717,
      "learning_rate": 0.00014407534682448774,
      "loss": 1.1734,
      "step": 2202
    },
    {
      "epoch": 0.28019077901430844,
      "grad_norm": 1.862548589706421,
      "learning_rate": 0.00014404989181621483,
      "loss": 0.7187,
      "step": 2203
    },
    {
      "epoch": 0.28031796502384737,
      "grad_norm": 2.5460774898529053,
      "learning_rate": 0.00014402443680794198,
      "loss": 1.0658,
      "step": 2204
    },
    {
      "epoch": 0.28044515103338635,
      "grad_norm": 1.6415268182754517,
      "learning_rate": 0.0001439989817996691,
      "loss": 0.4974,
      "step": 2205
    },
    {
      "epoch": 0.2805723370429253,
      "grad_norm": 1.644007682800293,
      "learning_rate": 0.00014397352679139622,
      "loss": 0.7655,
      "step": 2206
    },
    {
      "epoch": 0.2806995230524642,
      "grad_norm": 2.4973721504211426,
      "learning_rate": 0.00014394807178312333,
      "loss": 0.9761,
      "step": 2207
    },
    {
      "epoch": 0.2808267090620032,
      "grad_norm": 1.202089786529541,
      "learning_rate": 0.00014392261677485045,
      "loss": 0.4806,
      "step": 2208
    },
    {
      "epoch": 0.2809538950715421,
      "grad_norm": 2.0458414554595947,
      "learning_rate": 0.00014389716176657757,
      "loss": 0.7347,
      "step": 2209
    },
    {
      "epoch": 0.2810810810810811,
      "grad_norm": 3.1518476009368896,
      "learning_rate": 0.0001438717067583047,
      "loss": 0.9414,
      "step": 2210
    },
    {
      "epoch": 0.28120826709062,
      "grad_norm": 1.991562843322754,
      "learning_rate": 0.00014384625175003184,
      "loss": 0.6245,
      "step": 2211
    },
    {
      "epoch": 0.281335453100159,
      "grad_norm": 2.1656410694122314,
      "learning_rate": 0.00014382079674175896,
      "loss": 0.9116,
      "step": 2212
    },
    {
      "epoch": 0.28146263910969793,
      "grad_norm": 1.7201733589172363,
      "learning_rate": 0.00014379534173348607,
      "loss": 0.729,
      "step": 2213
    },
    {
      "epoch": 0.28158982511923686,
      "grad_norm": 1.8961312770843506,
      "learning_rate": 0.0001437698867252132,
      "loss": 0.754,
      "step": 2214
    },
    {
      "epoch": 0.28171701112877584,
      "grad_norm": 1.7713115215301514,
      "learning_rate": 0.0001437444317169403,
      "loss": 0.6623,
      "step": 2215
    },
    {
      "epoch": 0.28184419713831477,
      "grad_norm": 2.8019678592681885,
      "learning_rate": 0.00014371897670866743,
      "loss": 0.771,
      "step": 2216
    },
    {
      "epoch": 0.28197138314785375,
      "grad_norm": 1.899605393409729,
      "learning_rate": 0.00014369352170039455,
      "loss": 0.5896,
      "step": 2217
    },
    {
      "epoch": 0.2820985691573927,
      "grad_norm": 1.8511067628860474,
      "learning_rate": 0.00014366806669212167,
      "loss": 0.9887,
      "step": 2218
    },
    {
      "epoch": 0.28222575516693166,
      "grad_norm": 2.289098024368286,
      "learning_rate": 0.00014364261168384881,
      "loss": 0.8137,
      "step": 2219
    },
    {
      "epoch": 0.2823529411764706,
      "grad_norm": 2.408464193344116,
      "learning_rate": 0.00014361715667557593,
      "loss": 0.857,
      "step": 2220
    },
    {
      "epoch": 0.28248012718600957,
      "grad_norm": 1.6906594038009644,
      "learning_rate": 0.00014359170166730305,
      "loss": 0.784,
      "step": 2221
    },
    {
      "epoch": 0.2826073131955485,
      "grad_norm": 2.523277997970581,
      "learning_rate": 0.00014356624665903017,
      "loss": 0.8731,
      "step": 2222
    },
    {
      "epoch": 0.2827344992050874,
      "grad_norm": 2.225991725921631,
      "learning_rate": 0.0001435407916507573,
      "loss": 0.7933,
      "step": 2223
    },
    {
      "epoch": 0.2828616852146264,
      "grad_norm": 1.9812109470367432,
      "learning_rate": 0.00014351533664248443,
      "loss": 0.8392,
      "step": 2224
    },
    {
      "epoch": 0.28298887122416533,
      "grad_norm": 2.17790150642395,
      "learning_rate": 0.00014348988163421153,
      "loss": 0.6733,
      "step": 2225
    },
    {
      "epoch": 0.2831160572337043,
      "grad_norm": 2.113454818725586,
      "learning_rate": 0.00014346442662593867,
      "loss": 0.7035,
      "step": 2226
    },
    {
      "epoch": 0.28324324324324324,
      "grad_norm": 2.226181745529175,
      "learning_rate": 0.00014343897161766576,
      "loss": 0.8125,
      "step": 2227
    },
    {
      "epoch": 0.2833704292527822,
      "grad_norm": 1.7800079584121704,
      "learning_rate": 0.0001434135166093929,
      "loss": 0.6412,
      "step": 2228
    },
    {
      "epoch": 0.28349761526232115,
      "grad_norm": 1.597149133682251,
      "learning_rate": 0.00014338806160112003,
      "loss": 0.5651,
      "step": 2229
    },
    {
      "epoch": 0.28362480127186007,
      "grad_norm": 2.2038004398345947,
      "learning_rate": 0.00014336260659284715,
      "loss": 0.6628,
      "step": 2230
    },
    {
      "epoch": 0.28375198728139905,
      "grad_norm": 1.5833290815353394,
      "learning_rate": 0.0001433371515845743,
      "loss": 0.726,
      "step": 2231
    },
    {
      "epoch": 0.283879173290938,
      "grad_norm": 1.7235002517700195,
      "learning_rate": 0.00014331169657630138,
      "loss": 0.6913,
      "step": 2232
    },
    {
      "epoch": 0.28400635930047696,
      "grad_norm": 3.6295721530914307,
      "learning_rate": 0.00014328624156802853,
      "loss": 0.7185,
      "step": 2233
    },
    {
      "epoch": 0.2841335453100159,
      "grad_norm": 1.7791717052459717,
      "learning_rate": 0.00014326078655975562,
      "loss": 0.5249,
      "step": 2234
    },
    {
      "epoch": 0.28426073131955487,
      "grad_norm": 1.930776596069336,
      "learning_rate": 0.00014323533155148277,
      "loss": 0.7404,
      "step": 2235
    },
    {
      "epoch": 0.2843879173290938,
      "grad_norm": 2.6646037101745605,
      "learning_rate": 0.00014320987654320989,
      "loss": 0.8213,
      "step": 2236
    },
    {
      "epoch": 0.2845151033386327,
      "grad_norm": 1.5243500471115112,
      "learning_rate": 0.000143184421534937,
      "loss": 0.701,
      "step": 2237
    },
    {
      "epoch": 0.2846422893481717,
      "grad_norm": 2.295279026031494,
      "learning_rate": 0.00014315896652666412,
      "loss": 0.67,
      "step": 2238
    },
    {
      "epoch": 0.28476947535771063,
      "grad_norm": 1.5763270854949951,
      "learning_rate": 0.00014313351151839124,
      "loss": 0.5708,
      "step": 2239
    },
    {
      "epoch": 0.2848966613672496,
      "grad_norm": 1.6371411085128784,
      "learning_rate": 0.0001431080565101184,
      "loss": 0.562,
      "step": 2240
    },
    {
      "epoch": 0.28502384737678854,
      "grad_norm": 2.602611541748047,
      "learning_rate": 0.00014308260150184548,
      "loss": 0.7933,
      "step": 2241
    },
    {
      "epoch": 0.2851510333863275,
      "grad_norm": 2.0789201259613037,
      "learning_rate": 0.00014305714649357263,
      "loss": 0.6188,
      "step": 2242
    },
    {
      "epoch": 0.28527821939586645,
      "grad_norm": 2.3231406211853027,
      "learning_rate": 0.00014303169148529974,
      "loss": 0.7745,
      "step": 2243
    },
    {
      "epoch": 0.28540540540540543,
      "grad_norm": 2.1786327362060547,
      "learning_rate": 0.00014300623647702686,
      "loss": 0.7473,
      "step": 2244
    },
    {
      "epoch": 0.28553259141494436,
      "grad_norm": 1.9475983381271362,
      "learning_rate": 0.00014298078146875398,
      "loss": 0.5643,
      "step": 2245
    },
    {
      "epoch": 0.2856597774244833,
      "grad_norm": 1.4880629777908325,
      "learning_rate": 0.0001429553264604811,
      "loss": 0.7331,
      "step": 2246
    },
    {
      "epoch": 0.28578696343402227,
      "grad_norm": 1.8067678213119507,
      "learning_rate": 0.00014292987145220822,
      "loss": 0.9246,
      "step": 2247
    },
    {
      "epoch": 0.2859141494435612,
      "grad_norm": 1.6170967817306519,
      "learning_rate": 0.00014290441644393534,
      "loss": 0.6578,
      "step": 2248
    },
    {
      "epoch": 0.2860413354531002,
      "grad_norm": 2.483280658721924,
      "learning_rate": 0.00014287896143566248,
      "loss": 0.7803,
      "step": 2249
    },
    {
      "epoch": 0.2861685214626391,
      "grad_norm": 2.007676124572754,
      "learning_rate": 0.0001428535064273896,
      "loss": 0.6132,
      "step": 2250
    },
    {
      "epoch": 0.2862957074721781,
      "grad_norm": 1.7453160285949707,
      "learning_rate": 0.00014282805141911672,
      "loss": 0.7224,
      "step": 2251
    },
    {
      "epoch": 0.286422893481717,
      "grad_norm": 1.9534199237823486,
      "learning_rate": 0.00014280259641084384,
      "loss": 0.5049,
      "step": 2252
    },
    {
      "epoch": 0.28655007949125594,
      "grad_norm": 2.1244633197784424,
      "learning_rate": 0.00014277714140257096,
      "loss": 0.6713,
      "step": 2253
    },
    {
      "epoch": 0.2866772655007949,
      "grad_norm": 1.6317148208618164,
      "learning_rate": 0.00014275168639429808,
      "loss": 0.5362,
      "step": 2254
    },
    {
      "epoch": 0.28680445151033385,
      "grad_norm": 2.21063232421875,
      "learning_rate": 0.00014272623138602522,
      "loss": 0.7113,
      "step": 2255
    },
    {
      "epoch": 0.28693163751987283,
      "grad_norm": 1.9888982772827148,
      "learning_rate": 0.00014270077637775232,
      "loss": 0.6321,
      "step": 2256
    },
    {
      "epoch": 0.28705882352941176,
      "grad_norm": 1.7701514959335327,
      "learning_rate": 0.00014267532136947946,
      "loss": 0.6334,
      "step": 2257
    },
    {
      "epoch": 0.28718600953895074,
      "grad_norm": 1.8565226793289185,
      "learning_rate": 0.00014264986636120655,
      "loss": 0.717,
      "step": 2258
    },
    {
      "epoch": 0.28731319554848966,
      "grad_norm": 2.3461520671844482,
      "learning_rate": 0.0001426244113529337,
      "loss": 0.6226,
      "step": 2259
    },
    {
      "epoch": 0.2874403815580286,
      "grad_norm": 2.219896078109741,
      "learning_rate": 0.00014259895634466082,
      "loss": 0.6902,
      "step": 2260
    },
    {
      "epoch": 0.2875675675675676,
      "grad_norm": 2.4420852661132812,
      "learning_rate": 0.00014257350133638794,
      "loss": 0.5856,
      "step": 2261
    },
    {
      "epoch": 0.2876947535771065,
      "grad_norm": 2.3645517826080322,
      "learning_rate": 0.00014254804632811508,
      "loss": 0.4629,
      "step": 2262
    },
    {
      "epoch": 0.2878219395866455,
      "grad_norm": 1.9934476613998413,
      "learning_rate": 0.00014252259131984217,
      "loss": 0.7583,
      "step": 2263
    },
    {
      "epoch": 0.2879491255961844,
      "grad_norm": 2.9035162925720215,
      "learning_rate": 0.00014249713631156932,
      "loss": 1.2195,
      "step": 2264
    },
    {
      "epoch": 0.2880763116057234,
      "grad_norm": 2.346010446548462,
      "learning_rate": 0.0001424716813032964,
      "loss": 0.7668,
      "step": 2265
    },
    {
      "epoch": 0.2882034976152623,
      "grad_norm": 2.735603094100952,
      "learning_rate": 0.00014244622629502356,
      "loss": 0.6801,
      "step": 2266
    },
    {
      "epoch": 0.2883306836248013,
      "grad_norm": 2.528294563293457,
      "learning_rate": 0.00014242077128675068,
      "loss": 0.703,
      "step": 2267
    },
    {
      "epoch": 0.2884578696343402,
      "grad_norm": 1.544779658317566,
      "learning_rate": 0.0001423953162784778,
      "loss": 0.778,
      "step": 2268
    },
    {
      "epoch": 0.28858505564387915,
      "grad_norm": 1.673691749572754,
      "learning_rate": 0.00014236986127020494,
      "loss": 0.8505,
      "step": 2269
    },
    {
      "epoch": 0.28871224165341813,
      "grad_norm": 2.4319567680358887,
      "learning_rate": 0.00014234440626193203,
      "loss": 0.7322,
      "step": 2270
    },
    {
      "epoch": 0.28883942766295706,
      "grad_norm": 2.092172861099243,
      "learning_rate": 0.00014231895125365918,
      "loss": 0.686,
      "step": 2271
    },
    {
      "epoch": 0.28896661367249604,
      "grad_norm": 1.837615966796875,
      "learning_rate": 0.00014229349624538627,
      "loss": 0.7431,
      "step": 2272
    },
    {
      "epoch": 0.28909379968203497,
      "grad_norm": 1.788788080215454,
      "learning_rate": 0.00014226804123711342,
      "loss": 0.8816,
      "step": 2273
    },
    {
      "epoch": 0.28922098569157395,
      "grad_norm": 2.32814884185791,
      "learning_rate": 0.00014224258622884053,
      "loss": 0.7499,
      "step": 2274
    },
    {
      "epoch": 0.2893481717011129,
      "grad_norm": 1.886824607849121,
      "learning_rate": 0.00014221713122056765,
      "loss": 0.7786,
      "step": 2275
    },
    {
      "epoch": 0.2894753577106518,
      "grad_norm": 3.3587467670440674,
      "learning_rate": 0.00014219167621229477,
      "loss": 0.8542,
      "step": 2276
    },
    {
      "epoch": 0.2896025437201908,
      "grad_norm": 2.739107131958008,
      "learning_rate": 0.0001421662212040219,
      "loss": 0.8221,
      "step": 2277
    },
    {
      "epoch": 0.2897297297297297,
      "grad_norm": 1.7301626205444336,
      "learning_rate": 0.00014214076619574904,
      "loss": 0.9089,
      "step": 2278
    },
    {
      "epoch": 0.2898569157392687,
      "grad_norm": 1.9436265230178833,
      "learning_rate": 0.00014211531118747613,
      "loss": 0.7441,
      "step": 2279
    },
    {
      "epoch": 0.2899841017488076,
      "grad_norm": 1.8477445840835571,
      "learning_rate": 0.00014208985617920327,
      "loss": 0.4644,
      "step": 2280
    },
    {
      "epoch": 0.2901112877583466,
      "grad_norm": 2.3946409225463867,
      "learning_rate": 0.0001420644011709304,
      "loss": 0.848,
      "step": 2281
    },
    {
      "epoch": 0.29023847376788553,
      "grad_norm": 2.143822431564331,
      "learning_rate": 0.0001420389461626575,
      "loss": 0.5311,
      "step": 2282
    },
    {
      "epoch": 0.29036565977742446,
      "grad_norm": 2.202542781829834,
      "learning_rate": 0.00014201349115438463,
      "loss": 0.6885,
      "step": 2283
    },
    {
      "epoch": 0.29049284578696344,
      "grad_norm": 1.6934188604354858,
      "learning_rate": 0.00014198803614611175,
      "loss": 0.7847,
      "step": 2284
    },
    {
      "epoch": 0.29062003179650236,
      "grad_norm": 1.8648446798324585,
      "learning_rate": 0.00014196258113783887,
      "loss": 0.4992,
      "step": 2285
    },
    {
      "epoch": 0.29074721780604135,
      "grad_norm": 2.3890867233276367,
      "learning_rate": 0.000141937126129566,
      "loss": 0.5965,
      "step": 2286
    },
    {
      "epoch": 0.2908744038155803,
      "grad_norm": 2.213625907897949,
      "learning_rate": 0.0001419116711212931,
      "loss": 0.692,
      "step": 2287
    },
    {
      "epoch": 0.29100158982511926,
      "grad_norm": 1.5225189924240112,
      "learning_rate": 0.00014188621611302025,
      "loss": 0.5745,
      "step": 2288
    },
    {
      "epoch": 0.2911287758346582,
      "grad_norm": 2.4559056758880615,
      "learning_rate": 0.00014186076110474737,
      "loss": 0.7227,
      "step": 2289
    },
    {
      "epoch": 0.29125596184419716,
      "grad_norm": 1.8748703002929688,
      "learning_rate": 0.0001418353060964745,
      "loss": 0.4556,
      "step": 2290
    },
    {
      "epoch": 0.2913831478537361,
      "grad_norm": 2.3641762733459473,
      "learning_rate": 0.0001418098510882016,
      "loss": 0.7501,
      "step": 2291
    },
    {
      "epoch": 0.291510333863275,
      "grad_norm": 1.894599199295044,
      "learning_rate": 0.00014178439607992873,
      "loss": 0.5894,
      "step": 2292
    },
    {
      "epoch": 0.291637519872814,
      "grad_norm": 1.6890435218811035,
      "learning_rate": 0.00014175894107165587,
      "loss": 0.5242,
      "step": 2293
    },
    {
      "epoch": 0.2917647058823529,
      "grad_norm": 2.2574336528778076,
      "learning_rate": 0.00014173348606338296,
      "loss": 0.5112,
      "step": 2294
    },
    {
      "epoch": 0.2918918918918919,
      "grad_norm": 1.540650725364685,
      "learning_rate": 0.0001417080310551101,
      "loss": 0.4504,
      "step": 2295
    },
    {
      "epoch": 0.29201907790143083,
      "grad_norm": 2.282121419906616,
      "learning_rate": 0.0001416825760468372,
      "loss": 0.6895,
      "step": 2296
    },
    {
      "epoch": 0.2921462639109698,
      "grad_norm": 1.931657075881958,
      "learning_rate": 0.00014165712103856435,
      "loss": 0.5669,
      "step": 2297
    },
    {
      "epoch": 0.29227344992050874,
      "grad_norm": 3.0260403156280518,
      "learning_rate": 0.00014163166603029147,
      "loss": 0.8635,
      "step": 2298
    },
    {
      "epoch": 0.29240063593004767,
      "grad_norm": 1.9249546527862549,
      "learning_rate": 0.00014160621102201858,
      "loss": 0.7695,
      "step": 2299
    },
    {
      "epoch": 0.29252782193958665,
      "grad_norm": 2.5385124683380127,
      "learning_rate": 0.00014158075601374573,
      "loss": 0.6714,
      "step": 2300
    },
    {
      "epoch": 0.2926550079491256,
      "grad_norm": 1.7384213209152222,
      "learning_rate": 0.00014155530100547282,
      "loss": 0.6467,
      "step": 2301
    },
    {
      "epoch": 0.29278219395866456,
      "grad_norm": 2.088731288909912,
      "learning_rate": 0.00014152984599719997,
      "loss": 0.7988,
      "step": 2302
    },
    {
      "epoch": 0.2929093799682035,
      "grad_norm": 2.003772497177124,
      "learning_rate": 0.00014150439098892706,
      "loss": 0.5551,
      "step": 2303
    },
    {
      "epoch": 0.29303656597774247,
      "grad_norm": 1.5419763326644897,
      "learning_rate": 0.0001414789359806542,
      "loss": 0.5974,
      "step": 2304
    },
    {
      "epoch": 0.2931637519872814,
      "grad_norm": 1.9495837688446045,
      "learning_rate": 0.00014145348097238132,
      "loss": 0.8391,
      "step": 2305
    },
    {
      "epoch": 0.2932909379968203,
      "grad_norm": 1.891387939453125,
      "learning_rate": 0.00014142802596410844,
      "loss": 0.7716,
      "step": 2306
    },
    {
      "epoch": 0.2934181240063593,
      "grad_norm": 2.3817989826202393,
      "learning_rate": 0.0001414025709558356,
      "loss": 0.774,
      "step": 2307
    },
    {
      "epoch": 0.29354531001589823,
      "grad_norm": 1.9325567483901978,
      "learning_rate": 0.00014137711594756268,
      "loss": 0.7115,
      "step": 2308
    },
    {
      "epoch": 0.2936724960254372,
      "grad_norm": 2.256682872772217,
      "learning_rate": 0.00014135166093928983,
      "loss": 0.5728,
      "step": 2309
    },
    {
      "epoch": 0.29379968203497614,
      "grad_norm": 1.7360135316848755,
      "learning_rate": 0.00014132620593101692,
      "loss": 0.9045,
      "step": 2310
    },
    {
      "epoch": 0.2939268680445151,
      "grad_norm": 1.8496012687683105,
      "learning_rate": 0.00014130075092274406,
      "loss": 0.694,
      "step": 2311
    },
    {
      "epoch": 0.29405405405405405,
      "grad_norm": 1.9740248918533325,
      "learning_rate": 0.00014127529591447118,
      "loss": 0.51,
      "step": 2312
    },
    {
      "epoch": 0.29418124006359303,
      "grad_norm": 1.804123878479004,
      "learning_rate": 0.0001412498409061983,
      "loss": 0.6187,
      "step": 2313
    },
    {
      "epoch": 0.29430842607313196,
      "grad_norm": 3.3787708282470703,
      "learning_rate": 0.00014122438589792542,
      "loss": 0.8495,
      "step": 2314
    },
    {
      "epoch": 0.2944356120826709,
      "grad_norm": 1.7947978973388672,
      "learning_rate": 0.00014119893088965254,
      "loss": 0.8256,
      "step": 2315
    },
    {
      "epoch": 0.29456279809220987,
      "grad_norm": 2.9246826171875,
      "learning_rate": 0.00014117347588137966,
      "loss": 0.7377,
      "step": 2316
    },
    {
      "epoch": 0.2946899841017488,
      "grad_norm": 1.4996429681777954,
      "learning_rate": 0.0001411480208731068,
      "loss": 0.6324,
      "step": 2317
    },
    {
      "epoch": 0.2948171701112878,
      "grad_norm": 1.9484944343566895,
      "learning_rate": 0.00014112256586483392,
      "loss": 0.7922,
      "step": 2318
    },
    {
      "epoch": 0.2949443561208267,
      "grad_norm": 2.014033794403076,
      "learning_rate": 0.00014109711085656104,
      "loss": 0.582,
      "step": 2319
    },
    {
      "epoch": 0.2950715421303657,
      "grad_norm": 2.0131349563598633,
      "learning_rate": 0.00014107165584828816,
      "loss": 0.777,
      "step": 2320
    },
    {
      "epoch": 0.2951987281399046,
      "grad_norm": 3.1312575340270996,
      "learning_rate": 0.00014104620084001528,
      "loss": 1.0076,
      "step": 2321
    },
    {
      "epoch": 0.29532591414944354,
      "grad_norm": 3.284245491027832,
      "learning_rate": 0.0001410207458317424,
      "loss": 0.8245,
      "step": 2322
    },
    {
      "epoch": 0.2954531001589825,
      "grad_norm": 1.6768394708633423,
      "learning_rate": 0.00014099529082346951,
      "loss": 0.4751,
      "step": 2323
    },
    {
      "epoch": 0.29558028616852144,
      "grad_norm": 2.8682267665863037,
      "learning_rate": 0.00014096983581519666,
      "loss": 1.0005,
      "step": 2324
    },
    {
      "epoch": 0.2957074721780604,
      "grad_norm": 2.065028667449951,
      "learning_rate": 0.00014094438080692375,
      "loss": 0.6792,
      "step": 2325
    },
    {
      "epoch": 0.29583465818759935,
      "grad_norm": 2.1000828742980957,
      "learning_rate": 0.0001409189257986509,
      "loss": 0.7961,
      "step": 2326
    },
    {
      "epoch": 0.29596184419713834,
      "grad_norm": 1.9027926921844482,
      "learning_rate": 0.00014089347079037802,
      "loss": 0.8073,
      "step": 2327
    },
    {
      "epoch": 0.29608903020667726,
      "grad_norm": 1.6898343563079834,
      "learning_rate": 0.00014086801578210514,
      "loss": 0.6487,
      "step": 2328
    },
    {
      "epoch": 0.29621621621621624,
      "grad_norm": 2.19868803024292,
      "learning_rate": 0.00014084256077383225,
      "loss": 0.7407,
      "step": 2329
    },
    {
      "epoch": 0.29634340222575517,
      "grad_norm": 1.9934693574905396,
      "learning_rate": 0.00014081710576555937,
      "loss": 0.7516,
      "step": 2330
    },
    {
      "epoch": 0.2964705882352941,
      "grad_norm": 2.4171807765960693,
      "learning_rate": 0.00014079165075728652,
      "loss": 0.9316,
      "step": 2331
    },
    {
      "epoch": 0.2965977742448331,
      "grad_norm": 1.8737177848815918,
      "learning_rate": 0.0001407661957490136,
      "loss": 0.6256,
      "step": 2332
    },
    {
      "epoch": 0.296724960254372,
      "grad_norm": 1.7510063648223877,
      "learning_rate": 0.00014074074074074076,
      "loss": 0.5352,
      "step": 2333
    },
    {
      "epoch": 0.296852146263911,
      "grad_norm": 2.226828098297119,
      "learning_rate": 0.00014071528573246785,
      "loss": 0.861,
      "step": 2334
    },
    {
      "epoch": 0.2969793322734499,
      "grad_norm": 1.3896427154541016,
      "learning_rate": 0.000140689830724195,
      "loss": 0.5206,
      "step": 2335
    },
    {
      "epoch": 0.2971065182829889,
      "grad_norm": 2.345884323120117,
      "learning_rate": 0.0001406643757159221,
      "loss": 1.0183,
      "step": 2336
    },
    {
      "epoch": 0.2972337042925278,
      "grad_norm": 2.329115152359009,
      "learning_rate": 0.00014063892070764923,
      "loss": 0.8742,
      "step": 2337
    },
    {
      "epoch": 0.29736089030206675,
      "grad_norm": 2.0638163089752197,
      "learning_rate": 0.00014061346569937638,
      "loss": 0.637,
      "step": 2338
    },
    {
      "epoch": 0.29748807631160573,
      "grad_norm": 1.4265356063842773,
      "learning_rate": 0.00014058801069110347,
      "loss": 0.5236,
      "step": 2339
    },
    {
      "epoch": 0.29761526232114466,
      "grad_norm": 1.6867408752441406,
      "learning_rate": 0.00014056255568283061,
      "loss": 0.7165,
      "step": 2340
    },
    {
      "epoch": 0.29774244833068364,
      "grad_norm": 2.1239778995513916,
      "learning_rate": 0.00014053710067455773,
      "loss": 0.6279,
      "step": 2341
    },
    {
      "epoch": 0.29786963434022257,
      "grad_norm": 1.8891651630401611,
      "learning_rate": 0.00014051164566628485,
      "loss": 0.5053,
      "step": 2342
    },
    {
      "epoch": 0.29799682034976155,
      "grad_norm": 1.7080541849136353,
      "learning_rate": 0.00014048619065801197,
      "loss": 0.539,
      "step": 2343
    },
    {
      "epoch": 0.2981240063593005,
      "grad_norm": 1.7964069843292236,
      "learning_rate": 0.0001404607356497391,
      "loss": 0.9062,
      "step": 2344
    },
    {
      "epoch": 0.2982511923688394,
      "grad_norm": 2.1334447860717773,
      "learning_rate": 0.0001404352806414662,
      "loss": 0.7554,
      "step": 2345
    },
    {
      "epoch": 0.2983783783783784,
      "grad_norm": 1.8779362440109253,
      "learning_rate": 0.00014040982563319333,
      "loss": 0.7163,
      "step": 2346
    },
    {
      "epoch": 0.2985055643879173,
      "grad_norm": 2.016458511352539,
      "learning_rate": 0.00014038437062492047,
      "loss": 0.8603,
      "step": 2347
    },
    {
      "epoch": 0.2986327503974563,
      "grad_norm": 2.1725246906280518,
      "learning_rate": 0.0001403589156166476,
      "loss": 0.7478,
      "step": 2348
    },
    {
      "epoch": 0.2987599364069952,
      "grad_norm": 2.3399038314819336,
      "learning_rate": 0.0001403334606083747,
      "loss": 0.7964,
      "step": 2349
    },
    {
      "epoch": 0.2988871224165342,
      "grad_norm": 2.1574816703796387,
      "learning_rate": 0.00014030800560010183,
      "loss": 0.9476,
      "step": 2350
    },
    {
      "epoch": 0.2990143084260731,
      "grad_norm": 2.2938499450683594,
      "learning_rate": 0.00014028255059182895,
      "loss": 0.5909,
      "step": 2351
    },
    {
      "epoch": 0.2991414944356121,
      "grad_norm": 3.297147750854492,
      "learning_rate": 0.00014025709558355607,
      "loss": 0.7596,
      "step": 2352
    },
    {
      "epoch": 0.29926868044515104,
      "grad_norm": 2.5905418395996094,
      "learning_rate": 0.00014023164057528319,
      "loss": 0.875,
      "step": 2353
    },
    {
      "epoch": 0.29939586645468996,
      "grad_norm": 2.279958724975586,
      "learning_rate": 0.0001402061855670103,
      "loss": 0.7038,
      "step": 2354
    },
    {
      "epoch": 0.29952305246422894,
      "grad_norm": 1.6739810705184937,
      "learning_rate": 0.00014018073055873745,
      "loss": 0.7812,
      "step": 2355
    },
    {
      "epoch": 0.29965023847376787,
      "grad_norm": 2.379117012023926,
      "learning_rate": 0.00014015527555046457,
      "loss": 0.8485,
      "step": 2356
    },
    {
      "epoch": 0.29977742448330685,
      "grad_norm": 1.7724939584732056,
      "learning_rate": 0.0001401298205421917,
      "loss": 0.6962,
      "step": 2357
    },
    {
      "epoch": 0.2999046104928458,
      "grad_norm": 1.4103158712387085,
      "learning_rate": 0.0001401043655339188,
      "loss": 0.674,
      "step": 2358
    },
    {
      "epoch": 0.30003179650238476,
      "grad_norm": 1.6791919469833374,
      "learning_rate": 0.00014007891052564593,
      "loss": 0.5699,
      "step": 2359
    },
    {
      "epoch": 0.3001589825119237,
      "grad_norm": 2.013350248336792,
      "learning_rate": 0.00014005345551737304,
      "loss": 0.7264,
      "step": 2360
    },
    {
      "epoch": 0.3002861685214626,
      "grad_norm": 1.9392915964126587,
      "learning_rate": 0.00014002800050910016,
      "loss": 0.5242,
      "step": 2361
    },
    {
      "epoch": 0.3004133545310016,
      "grad_norm": 1.8346426486968994,
      "learning_rate": 0.0001400025455008273,
      "loss": 0.6538,
      "step": 2362
    },
    {
      "epoch": 0.3005405405405405,
      "grad_norm": 1.852919101715088,
      "learning_rate": 0.0001399770904925544,
      "loss": 0.8527,
      "step": 2363
    },
    {
      "epoch": 0.3006677265500795,
      "grad_norm": 2.541494131088257,
      "learning_rate": 0.00013995163548428155,
      "loss": 0.848,
      "step": 2364
    },
    {
      "epoch": 0.30079491255961843,
      "grad_norm": 1.9109869003295898,
      "learning_rate": 0.00013992618047600864,
      "loss": 0.9577,
      "step": 2365
    },
    {
      "epoch": 0.3009220985691574,
      "grad_norm": 2.397554636001587,
      "learning_rate": 0.00013990072546773578,
      "loss": 0.7484,
      "step": 2366
    },
    {
      "epoch": 0.30104928457869634,
      "grad_norm": 2.858736991882324,
      "learning_rate": 0.0001398752704594629,
      "loss": 0.8302,
      "step": 2367
    },
    {
      "epoch": 0.30117647058823527,
      "grad_norm": 1.9316327571868896,
      "learning_rate": 0.00013984981545119002,
      "loss": 0.7353,
      "step": 2368
    },
    {
      "epoch": 0.30130365659777425,
      "grad_norm": 2.706725597381592,
      "learning_rate": 0.00013982436044291717,
      "loss": 0.8567,
      "step": 2369
    },
    {
      "epoch": 0.3014308426073132,
      "grad_norm": 1.370147705078125,
      "learning_rate": 0.00013979890543464426,
      "loss": 0.6702,
      "step": 2370
    },
    {
      "epoch": 0.30155802861685216,
      "grad_norm": 1.8671513795852661,
      "learning_rate": 0.0001397734504263714,
      "loss": 0.6066,
      "step": 2371
    },
    {
      "epoch": 0.3016852146263911,
      "grad_norm": 2.1460788249969482,
      "learning_rate": 0.00013974799541809852,
      "loss": 0.823,
      "step": 2372
    },
    {
      "epoch": 0.30181240063593007,
      "grad_norm": 1.530821681022644,
      "learning_rate": 0.00013972254040982564,
      "loss": 0.5523,
      "step": 2373
    },
    {
      "epoch": 0.301939586645469,
      "grad_norm": 1.963772177696228,
      "learning_rate": 0.00013969708540155276,
      "loss": 0.7568,
      "step": 2374
    },
    {
      "epoch": 0.302066772655008,
      "grad_norm": 2.211965799331665,
      "learning_rate": 0.00013967163039327988,
      "loss": 0.6507,
      "step": 2375
    },
    {
      "epoch": 0.3021939586645469,
      "grad_norm": 2.1381869316101074,
      "learning_rate": 0.00013964617538500702,
      "loss": 0.7538,
      "step": 2376
    },
    {
      "epoch": 0.30232114467408583,
      "grad_norm": 1.8122562170028687,
      "learning_rate": 0.00013962072037673412,
      "loss": 0.8446,
      "step": 2377
    },
    {
      "epoch": 0.3024483306836248,
      "grad_norm": 1.9359112977981567,
      "learning_rate": 0.00013959526536846126,
      "loss": 0.6538,
      "step": 2378
    },
    {
      "epoch": 0.30257551669316374,
      "grad_norm": 2.7851598262786865,
      "learning_rate": 0.00013956981036018838,
      "loss": 0.7214,
      "step": 2379
    },
    {
      "epoch": 0.3027027027027027,
      "grad_norm": 1.773335576057434,
      "learning_rate": 0.0001395443553519155,
      "loss": 0.789,
      "step": 2380
    },
    {
      "epoch": 0.30282988871224165,
      "grad_norm": 2.0853655338287354,
      "learning_rate": 0.00013951890034364262,
      "loss": 0.5899,
      "step": 2381
    },
    {
      "epoch": 0.30295707472178063,
      "grad_norm": 2.0242061614990234,
      "learning_rate": 0.00013949344533536974,
      "loss": 0.5741,
      "step": 2382
    },
    {
      "epoch": 0.30308426073131955,
      "grad_norm": 1.734558343887329,
      "learning_rate": 0.00013946799032709686,
      "loss": 0.8204,
      "step": 2383
    },
    {
      "epoch": 0.3032114467408585,
      "grad_norm": 2.434413433074951,
      "learning_rate": 0.00013944253531882397,
      "loss": 0.8017,
      "step": 2384
    },
    {
      "epoch": 0.30333863275039746,
      "grad_norm": 1.8780324459075928,
      "learning_rate": 0.00013941708031055112,
      "loss": 0.643,
      "step": 2385
    },
    {
      "epoch": 0.3034658187599364,
      "grad_norm": 1.435630440711975,
      "learning_rate": 0.00013939162530227824,
      "loss": 0.4614,
      "step": 2386
    },
    {
      "epoch": 0.30359300476947537,
      "grad_norm": 1.572858452796936,
      "learning_rate": 0.00013936617029400536,
      "loss": 0.6316,
      "step": 2387
    },
    {
      "epoch": 0.3037201907790143,
      "grad_norm": 1.4691654443740845,
      "learning_rate": 0.00013934071528573248,
      "loss": 0.6722,
      "step": 2388
    },
    {
      "epoch": 0.3038473767885533,
      "grad_norm": 2.609511375427246,
      "learning_rate": 0.0001393152602774596,
      "loss": 0.6255,
      "step": 2389
    },
    {
      "epoch": 0.3039745627980922,
      "grad_norm": 2.0823867321014404,
      "learning_rate": 0.00013928980526918671,
      "loss": 0.761,
      "step": 2390
    },
    {
      "epoch": 0.30410174880763113,
      "grad_norm": 2.072213888168335,
      "learning_rate": 0.00013926435026091383,
      "loss": 0.7788,
      "step": 2391
    },
    {
      "epoch": 0.3042289348171701,
      "grad_norm": 2.1626691818237305,
      "learning_rate": 0.00013923889525264095,
      "loss": 0.9003,
      "step": 2392
    },
    {
      "epoch": 0.30435612082670904,
      "grad_norm": 1.8063788414001465,
      "learning_rate": 0.0001392134402443681,
      "loss": 0.7564,
      "step": 2393
    },
    {
      "epoch": 0.304483306836248,
      "grad_norm": 2.250150680541992,
      "learning_rate": 0.0001391879852360952,
      "loss": 0.5601,
      "step": 2394
    },
    {
      "epoch": 0.30461049284578695,
      "grad_norm": 2.5040581226348877,
      "learning_rate": 0.00013916253022782234,
      "loss": 0.6073,
      "step": 2395
    },
    {
      "epoch": 0.30473767885532593,
      "grad_norm": 1.8763371706008911,
      "learning_rate": 0.00013913707521954945,
      "loss": 0.7652,
      "step": 2396
    },
    {
      "epoch": 0.30486486486486486,
      "grad_norm": 2.241389036178589,
      "learning_rate": 0.00013911162021127657,
      "loss": 0.742,
      "step": 2397
    },
    {
      "epoch": 0.30499205087440384,
      "grad_norm": 1.8155070543289185,
      "learning_rate": 0.0001390861652030037,
      "loss": 0.5778,
      "step": 2398
    },
    {
      "epoch": 0.30511923688394277,
      "grad_norm": 2.086081027984619,
      "learning_rate": 0.0001390607101947308,
      "loss": 0.8363,
      "step": 2399
    },
    {
      "epoch": 0.3052464228934817,
      "grad_norm": 2.4100446701049805,
      "learning_rate": 0.00013903525518645796,
      "loss": 0.6909,
      "step": 2400
    },
    {
      "epoch": 0.3053736089030207,
      "grad_norm": 2.210536003112793,
      "learning_rate": 0.00013900980017818505,
      "loss": 0.7049,
      "step": 2401
    },
    {
      "epoch": 0.3055007949125596,
      "grad_norm": 2.160369873046875,
      "learning_rate": 0.0001389843451699122,
      "loss": 0.6279,
      "step": 2402
    },
    {
      "epoch": 0.3056279809220986,
      "grad_norm": 3.8423333168029785,
      "learning_rate": 0.0001389588901616393,
      "loss": 0.8146,
      "step": 2403
    },
    {
      "epoch": 0.3057551669316375,
      "grad_norm": 3.0865235328674316,
      "learning_rate": 0.00013893343515336643,
      "loss": 0.6286,
      "step": 2404
    },
    {
      "epoch": 0.3058823529411765,
      "grad_norm": 1.869053602218628,
      "learning_rate": 0.00013890798014509358,
      "loss": 0.6413,
      "step": 2405
    },
    {
      "epoch": 0.3060095389507154,
      "grad_norm": 2.090580701828003,
      "learning_rate": 0.00013888252513682067,
      "loss": 0.6508,
      "step": 2406
    },
    {
      "epoch": 0.30613672496025435,
      "grad_norm": 2.5325517654418945,
      "learning_rate": 0.00013885707012854781,
      "loss": 0.7983,
      "step": 2407
    },
    {
      "epoch": 0.30626391096979333,
      "grad_norm": 2.3399946689605713,
      "learning_rate": 0.0001388316151202749,
      "loss": 0.8698,
      "step": 2408
    },
    {
      "epoch": 0.30639109697933226,
      "grad_norm": 2.0912230014801025,
      "learning_rate": 0.00013880616011200205,
      "loss": 0.9523,
      "step": 2409
    },
    {
      "epoch": 0.30651828298887124,
      "grad_norm": 2.0652167797088623,
      "learning_rate": 0.00013878070510372917,
      "loss": 0.7246,
      "step": 2410
    },
    {
      "epoch": 0.30664546899841016,
      "grad_norm": 2.0816080570220947,
      "learning_rate": 0.0001387552500954563,
      "loss": 0.5752,
      "step": 2411
    },
    {
      "epoch": 0.30677265500794915,
      "grad_norm": 1.787711501121521,
      "learning_rate": 0.0001387297950871834,
      "loss": 0.6886,
      "step": 2412
    },
    {
      "epoch": 0.3068998410174881,
      "grad_norm": 1.7436261177062988,
      "learning_rate": 0.00013870434007891053,
      "loss": 0.8002,
      "step": 2413
    },
    {
      "epoch": 0.307027027027027,
      "grad_norm": 4.357936382293701,
      "learning_rate": 0.00013867888507063767,
      "loss": 0.8523,
      "step": 2414
    },
    {
      "epoch": 0.307154213036566,
      "grad_norm": 2.163149833679199,
      "learning_rate": 0.00013865343006236476,
      "loss": 0.7282,
      "step": 2415
    },
    {
      "epoch": 0.3072813990461049,
      "grad_norm": 1.9318368434906006,
      "learning_rate": 0.0001386279750540919,
      "loss": 0.5737,
      "step": 2416
    },
    {
      "epoch": 0.3074085850556439,
      "grad_norm": 1.7510219812393188,
      "learning_rate": 0.00013860252004581903,
      "loss": 0.8199,
      "step": 2417
    },
    {
      "epoch": 0.3075357710651828,
      "grad_norm": 1.722764015197754,
      "learning_rate": 0.00013857706503754615,
      "loss": 0.6758,
      "step": 2418
    },
    {
      "epoch": 0.3076629570747218,
      "grad_norm": 2.0230116844177246,
      "learning_rate": 0.00013855161002927327,
      "loss": 0.6458,
      "step": 2419
    },
    {
      "epoch": 0.3077901430842607,
      "grad_norm": 1.574857473373413,
      "learning_rate": 0.00013852615502100039,
      "loss": 0.4488,
      "step": 2420
    },
    {
      "epoch": 0.3079173290937997,
      "grad_norm": 1.5877774953842163,
      "learning_rate": 0.0001385007000127275,
      "loss": 0.6027,
      "step": 2421
    },
    {
      "epoch": 0.30804451510333863,
      "grad_norm": 1.9854909181594849,
      "learning_rate": 0.00013847524500445462,
      "loss": 0.8933,
      "step": 2422
    },
    {
      "epoch": 0.30817170111287756,
      "grad_norm": 1.8550975322723389,
      "learning_rate": 0.00013844978999618174,
      "loss": 0.5761,
      "step": 2423
    },
    {
      "epoch": 0.30829888712241654,
      "grad_norm": 1.976882815361023,
      "learning_rate": 0.0001384243349879089,
      "loss": 0.7257,
      "step": 2424
    },
    {
      "epoch": 0.30842607313195547,
      "grad_norm": 2.0255002975463867,
      "learning_rate": 0.000138398879979636,
      "loss": 0.8958,
      "step": 2425
    },
    {
      "epoch": 0.30855325914149445,
      "grad_norm": 2.6022138595581055,
      "learning_rate": 0.00013837342497136312,
      "loss": 0.7616,
      "step": 2426
    },
    {
      "epoch": 0.3086804451510334,
      "grad_norm": 2.2500877380371094,
      "learning_rate": 0.00013834796996309024,
      "loss": 0.685,
      "step": 2427
    },
    {
      "epoch": 0.30880763116057236,
      "grad_norm": 2.5691046714782715,
      "learning_rate": 0.00013832251495481736,
      "loss": 0.6911,
      "step": 2428
    },
    {
      "epoch": 0.3089348171701113,
      "grad_norm": 2.235640048980713,
      "learning_rate": 0.00013829705994654448,
      "loss": 0.6686,
      "step": 2429
    },
    {
      "epoch": 0.3090620031796502,
      "grad_norm": 2.3750405311584473,
      "learning_rate": 0.0001382716049382716,
      "loss": 0.7842,
      "step": 2430
    },
    {
      "epoch": 0.3091891891891892,
      "grad_norm": 2.1188719272613525,
      "learning_rate": 0.00013824614992999875,
      "loss": 0.8255,
      "step": 2431
    },
    {
      "epoch": 0.3093163751987281,
      "grad_norm": 1.8100332021713257,
      "learning_rate": 0.00013822069492172584,
      "loss": 0.3892,
      "step": 2432
    },
    {
      "epoch": 0.3094435612082671,
      "grad_norm": 2.1401219367980957,
      "learning_rate": 0.00013819523991345298,
      "loss": 0.6349,
      "step": 2433
    },
    {
      "epoch": 0.30957074721780603,
      "grad_norm": 2.3361966609954834,
      "learning_rate": 0.0001381697849051801,
      "loss": 0.6727,
      "step": 2434
    },
    {
      "epoch": 0.309697933227345,
      "grad_norm": 1.92726731300354,
      "learning_rate": 0.00013814432989690722,
      "loss": 0.5944,
      "step": 2435
    },
    {
      "epoch": 0.30982511923688394,
      "grad_norm": 1.8096747398376465,
      "learning_rate": 0.00013811887488863437,
      "loss": 0.635,
      "step": 2436
    },
    {
      "epoch": 0.3099523052464229,
      "grad_norm": 1.5959550142288208,
      "learning_rate": 0.00013809341988036146,
      "loss": 0.4935,
      "step": 2437
    },
    {
      "epoch": 0.31007949125596185,
      "grad_norm": 1.9206488132476807,
      "learning_rate": 0.0001380679648720886,
      "loss": 0.6548,
      "step": 2438
    },
    {
      "epoch": 0.3102066772655008,
      "grad_norm": 2.046494483947754,
      "learning_rate": 0.0001380425098638157,
      "loss": 0.5593,
      "step": 2439
    },
    {
      "epoch": 0.31033386327503976,
      "grad_norm": 1.623714804649353,
      "learning_rate": 0.00013801705485554284,
      "loss": 0.9236,
      "step": 2440
    },
    {
      "epoch": 0.3104610492845787,
      "grad_norm": 2.0619263648986816,
      "learning_rate": 0.00013799159984726996,
      "loss": 0.5689,
      "step": 2441
    },
    {
      "epoch": 0.31058823529411766,
      "grad_norm": 1.5852038860321045,
      "learning_rate": 0.00013796614483899708,
      "loss": 0.7505,
      "step": 2442
    },
    {
      "epoch": 0.3107154213036566,
      "grad_norm": 2.6531476974487305,
      "learning_rate": 0.00013794068983072422,
      "loss": 0.8446,
      "step": 2443
    },
    {
      "epoch": 0.3108426073131956,
      "grad_norm": 1.6802449226379395,
      "learning_rate": 0.00013791523482245132,
      "loss": 0.8175,
      "step": 2444
    },
    {
      "epoch": 0.3109697933227345,
      "grad_norm": 1.8230818510055542,
      "learning_rate": 0.00013788977981417846,
      "loss": 0.6944,
      "step": 2445
    },
    {
      "epoch": 0.3110969793322734,
      "grad_norm": 1.419840693473816,
      "learning_rate": 0.00013786432480590555,
      "loss": 0.5368,
      "step": 2446
    },
    {
      "epoch": 0.3112241653418124,
      "grad_norm": 2.423030138015747,
      "learning_rate": 0.0001378388697976327,
      "loss": 0.7462,
      "step": 2447
    },
    {
      "epoch": 0.31135135135135134,
      "grad_norm": 2.2741758823394775,
      "learning_rate": 0.00013781341478935982,
      "loss": 0.7514,
      "step": 2448
    },
    {
      "epoch": 0.3114785373608903,
      "grad_norm": 2.0788004398345947,
      "learning_rate": 0.00013778795978108694,
      "loss": 0.8221,
      "step": 2449
    },
    {
      "epoch": 0.31160572337042924,
      "grad_norm": 1.8876187801361084,
      "learning_rate": 0.00013776250477281406,
      "loss": 0.7514,
      "step": 2450
    },
    {
      "epoch": 0.3117329093799682,
      "grad_norm": 2.068676233291626,
      "learning_rate": 0.00013773704976454117,
      "loss": 0.6614,
      "step": 2451
    },
    {
      "epoch": 0.31186009538950715,
      "grad_norm": 2.356074571609497,
      "learning_rate": 0.0001377115947562683,
      "loss": 0.7131,
      "step": 2452
    },
    {
      "epoch": 0.3119872813990461,
      "grad_norm": 2.3479504585266113,
      "learning_rate": 0.0001376861397479954,
      "loss": 0.6154,
      "step": 2453
    },
    {
      "epoch": 0.31211446740858506,
      "grad_norm": 2.1638131141662598,
      "learning_rate": 0.00013766068473972256,
      "loss": 0.615,
      "step": 2454
    },
    {
      "epoch": 0.312241653418124,
      "grad_norm": 1.8097646236419678,
      "learning_rate": 0.00013763522973144968,
      "loss": 0.8311,
      "step": 2455
    },
    {
      "epoch": 0.31236883942766297,
      "grad_norm": 2.1575541496276855,
      "learning_rate": 0.0001376097747231768,
      "loss": 0.6658,
      "step": 2456
    },
    {
      "epoch": 0.3124960254372019,
      "grad_norm": 2.119266986846924,
      "learning_rate": 0.00013758431971490391,
      "loss": 0.5117,
      "step": 2457
    },
    {
      "epoch": 0.3126232114467409,
      "grad_norm": 2.0550684928894043,
      "learning_rate": 0.00013755886470663103,
      "loss": 0.8328,
      "step": 2458
    },
    {
      "epoch": 0.3127503974562798,
      "grad_norm": 2.3432815074920654,
      "learning_rate": 0.00013753340969835815,
      "loss": 0.8342,
      "step": 2459
    },
    {
      "epoch": 0.3128775834658188,
      "grad_norm": 1.857391357421875,
      "learning_rate": 0.00013750795469008527,
      "loss": 0.6442,
      "step": 2460
    },
    {
      "epoch": 0.3130047694753577,
      "grad_norm": 2.1918444633483887,
      "learning_rate": 0.0001374824996818124,
      "loss": 0.6595,
      "step": 2461
    },
    {
      "epoch": 0.31313195548489664,
      "grad_norm": 2.500293016433716,
      "learning_rate": 0.00013745704467353953,
      "loss": 0.8079,
      "step": 2462
    },
    {
      "epoch": 0.3132591414944356,
      "grad_norm": 1.5195866823196411,
      "learning_rate": 0.00013743158966526665,
      "loss": 0.5305,
      "step": 2463
    },
    {
      "epoch": 0.31338632750397455,
      "grad_norm": 2.2313709259033203,
      "learning_rate": 0.00013740613465699377,
      "loss": 0.8548,
      "step": 2464
    },
    {
      "epoch": 0.31351351351351353,
      "grad_norm": 1.9681787490844727,
      "learning_rate": 0.0001373806796487209,
      "loss": 0.5059,
      "step": 2465
    },
    {
      "epoch": 0.31364069952305246,
      "grad_norm": 2.328655958175659,
      "learning_rate": 0.000137355224640448,
      "loss": 0.6169,
      "step": 2466
    },
    {
      "epoch": 0.31376788553259144,
      "grad_norm": 2.0974955558776855,
      "learning_rate": 0.00013732976963217516,
      "loss": 0.7228,
      "step": 2467
    },
    {
      "epoch": 0.31389507154213037,
      "grad_norm": 1.8202637434005737,
      "learning_rate": 0.00013730431462390225,
      "loss": 0.6804,
      "step": 2468
    },
    {
      "epoch": 0.3140222575516693,
      "grad_norm": 1.6833255290985107,
      "learning_rate": 0.0001372788596156294,
      "loss": 0.5819,
      "step": 2469
    },
    {
      "epoch": 0.3141494435612083,
      "grad_norm": 2.7888002395629883,
      "learning_rate": 0.00013725340460735648,
      "loss": 0.7206,
      "step": 2470
    },
    {
      "epoch": 0.3142766295707472,
      "grad_norm": 1.663896083831787,
      "learning_rate": 0.00013722794959908363,
      "loss": 0.7969,
      "step": 2471
    },
    {
      "epoch": 0.3144038155802862,
      "grad_norm": 1.5977740287780762,
      "learning_rate": 0.00013720249459081075,
      "loss": 0.6839,
      "step": 2472
    },
    {
      "epoch": 0.3145310015898251,
      "grad_norm": 1.9507452249526978,
      "learning_rate": 0.00013717703958253787,
      "loss": 0.652,
      "step": 2473
    },
    {
      "epoch": 0.3146581875993641,
      "grad_norm": 2.0109572410583496,
      "learning_rate": 0.00013715158457426501,
      "loss": 0.7835,
      "step": 2474
    },
    {
      "epoch": 0.314785373608903,
      "grad_norm": 2.6221795082092285,
      "learning_rate": 0.0001371261295659921,
      "loss": 0.8281,
      "step": 2475
    },
    {
      "epoch": 0.31491255961844195,
      "grad_norm": 1.8253093957901,
      "learning_rate": 0.00013710067455771925,
      "loss": 0.4363,
      "step": 2476
    },
    {
      "epoch": 0.3150397456279809,
      "grad_norm": 2.031737804412842,
      "learning_rate": 0.00013707521954944634,
      "loss": 1.1346,
      "step": 2477
    },
    {
      "epoch": 0.31516693163751985,
      "grad_norm": 2.04538893699646,
      "learning_rate": 0.0001370497645411735,
      "loss": 0.889,
      "step": 2478
    },
    {
      "epoch": 0.31529411764705884,
      "grad_norm": 2.284754753112793,
      "learning_rate": 0.0001370243095329006,
      "loss": 0.7511,
      "step": 2479
    },
    {
      "epoch": 0.31542130365659776,
      "grad_norm": 2.017422914505005,
      "learning_rate": 0.00013699885452462773,
      "loss": 0.7766,
      "step": 2480
    },
    {
      "epoch": 0.31554848966613674,
      "grad_norm": 2.5332469940185547,
      "learning_rate": 0.00013697339951635484,
      "loss": 0.6067,
      "step": 2481
    },
    {
      "epoch": 0.31567567567567567,
      "grad_norm": 2.5299832820892334,
      "learning_rate": 0.00013694794450808196,
      "loss": 0.6032,
      "step": 2482
    },
    {
      "epoch": 0.31580286168521465,
      "grad_norm": 1.8281612396240234,
      "learning_rate": 0.0001369224894998091,
      "loss": 0.4974,
      "step": 2483
    },
    {
      "epoch": 0.3159300476947536,
      "grad_norm": 1.6586270332336426,
      "learning_rate": 0.0001368970344915362,
      "loss": 0.6996,
      "step": 2484
    },
    {
      "epoch": 0.3160572337042925,
      "grad_norm": 2.480773687362671,
      "learning_rate": 0.00013687157948326335,
      "loss": 1.0047,
      "step": 2485
    },
    {
      "epoch": 0.3161844197138315,
      "grad_norm": 2.4707961082458496,
      "learning_rate": 0.00013684612447499047,
      "loss": 0.5942,
      "step": 2486
    },
    {
      "epoch": 0.3163116057233704,
      "grad_norm": 1.38508939743042,
      "learning_rate": 0.00013682066946671758,
      "loss": 0.5344,
      "step": 2487
    },
    {
      "epoch": 0.3164387917329094,
      "grad_norm": 2.08478045463562,
      "learning_rate": 0.0001367952144584447,
      "loss": 0.794,
      "step": 2488
    },
    {
      "epoch": 0.3165659777424483,
      "grad_norm": 1.7438867092132568,
      "learning_rate": 0.00013676975945017182,
      "loss": 0.7369,
      "step": 2489
    },
    {
      "epoch": 0.3166931637519873,
      "grad_norm": 2.0692763328552246,
      "learning_rate": 0.00013674430444189894,
      "loss": 0.8807,
      "step": 2490
    },
    {
      "epoch": 0.31682034976152623,
      "grad_norm": 1.8429198265075684,
      "learning_rate": 0.00013671884943362606,
      "loss": 0.519,
      "step": 2491
    },
    {
      "epoch": 0.31694753577106516,
      "grad_norm": 2.182459831237793,
      "learning_rate": 0.0001366933944253532,
      "loss": 0.7842,
      "step": 2492
    },
    {
      "epoch": 0.31707472178060414,
      "grad_norm": 2.2520978450775146,
      "learning_rate": 0.00013666793941708032,
      "loss": 0.6376,
      "step": 2493
    },
    {
      "epoch": 0.31720190779014307,
      "grad_norm": 1.4327980279922485,
      "learning_rate": 0.00013664248440880744,
      "loss": 0.5282,
      "step": 2494
    },
    {
      "epoch": 0.31732909379968205,
      "grad_norm": 1.9061850309371948,
      "learning_rate": 0.00013661702940053456,
      "loss": 0.6154,
      "step": 2495
    },
    {
      "epoch": 0.317456279809221,
      "grad_norm": 1.7741920948028564,
      "learning_rate": 0.00013659157439226168,
      "loss": 0.5838,
      "step": 2496
    },
    {
      "epoch": 0.31758346581875996,
      "grad_norm": 2.3640174865722656,
      "learning_rate": 0.0001365661193839888,
      "loss": 0.8067,
      "step": 2497
    },
    {
      "epoch": 0.3177106518282989,
      "grad_norm": 1.8721522092819214,
      "learning_rate": 0.00013654066437571594,
      "loss": 0.6153,
      "step": 2498
    },
    {
      "epoch": 0.3178378378378378,
      "grad_norm": 2.557145833969116,
      "learning_rate": 0.00013651520936744304,
      "loss": 0.7892,
      "step": 2499
    },
    {
      "epoch": 0.3179650238473768,
      "grad_norm": 2.2124581336975098,
      "learning_rate": 0.00013648975435917018,
      "loss": 0.6318,
      "step": 2500
    },
    {
      "epoch": 0.3180922098569157,
      "grad_norm": 2.3544697761535645,
      "learning_rate": 0.0001364642993508973,
      "loss": 0.608,
      "step": 2501
    },
    {
      "epoch": 0.3182193958664547,
      "grad_norm": 1.7558571100234985,
      "learning_rate": 0.00013643884434262442,
      "loss": 0.7716,
      "step": 2502
    },
    {
      "epoch": 0.31834658187599363,
      "grad_norm": 2.9362685680389404,
      "learning_rate": 0.00013641338933435154,
      "loss": 0.5976,
      "step": 2503
    },
    {
      "epoch": 0.3184737678855326,
      "grad_norm": 2.6641387939453125,
      "learning_rate": 0.00013638793432607866,
      "loss": 0.8964,
      "step": 2504
    },
    {
      "epoch": 0.31860095389507154,
      "grad_norm": 2.3568294048309326,
      "learning_rate": 0.0001363624793178058,
      "loss": 0.7268,
      "step": 2505
    },
    {
      "epoch": 0.3187281399046105,
      "grad_norm": 2.289914846420288,
      "learning_rate": 0.0001363370243095329,
      "loss": 0.6682,
      "step": 2506
    },
    {
      "epoch": 0.31885532591414945,
      "grad_norm": 1.7214537858963013,
      "learning_rate": 0.00013631156930126004,
      "loss": 0.5917,
      "step": 2507
    },
    {
      "epoch": 0.31898251192368837,
      "grad_norm": 2.1281843185424805,
      "learning_rate": 0.00013628611429298713,
      "loss": 0.7627,
      "step": 2508
    },
    {
      "epoch": 0.31910969793322735,
      "grad_norm": 2.9237189292907715,
      "learning_rate": 0.00013626065928471428,
      "loss": 0.8706,
      "step": 2509
    },
    {
      "epoch": 0.3192368839427663,
      "grad_norm": 1.500394344329834,
      "learning_rate": 0.0001362352042764414,
      "loss": 0.5882,
      "step": 2510
    },
    {
      "epoch": 0.31936406995230526,
      "grad_norm": 2.2930991649627686,
      "learning_rate": 0.00013620974926816852,
      "loss": 1.0173,
      "step": 2511
    },
    {
      "epoch": 0.3194912559618442,
      "grad_norm": 2.560574531555176,
      "learning_rate": 0.00013618429425989566,
      "loss": 0.6835,
      "step": 2512
    },
    {
      "epoch": 0.31961844197138317,
      "grad_norm": 2.189066171646118,
      "learning_rate": 0.00013615883925162275,
      "loss": 0.7428,
      "step": 2513
    },
    {
      "epoch": 0.3197456279809221,
      "grad_norm": 2.1523170471191406,
      "learning_rate": 0.0001361333842433499,
      "loss": 0.5047,
      "step": 2514
    },
    {
      "epoch": 0.319872813990461,
      "grad_norm": 2.048584461212158,
      "learning_rate": 0.000136107929235077,
      "loss": 0.5621,
      "step": 2515
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.1975724697113037,
      "learning_rate": 0.00013608247422680414,
      "loss": 0.7535,
      "step": 2516
    },
    {
      "epoch": 0.32012718600953893,
      "grad_norm": 1.7478909492492676,
      "learning_rate": 0.00013605701921853126,
      "loss": 0.5919,
      "step": 2517
    },
    {
      "epoch": 0.3202543720190779,
      "grad_norm": 1.8410154581069946,
      "learning_rate": 0.00013603156421025837,
      "loss": 0.5591,
      "step": 2518
    },
    {
      "epoch": 0.32038155802861684,
      "grad_norm": 1.9831775426864624,
      "learning_rate": 0.0001360061092019855,
      "loss": 0.6652,
      "step": 2519
    },
    {
      "epoch": 0.3205087440381558,
      "grad_norm": 1.4892767667770386,
      "learning_rate": 0.0001359806541937126,
      "loss": 0.499,
      "step": 2520
    },
    {
      "epoch": 0.32063593004769475,
      "grad_norm": 1.7027555704116821,
      "learning_rate": 0.00013595519918543976,
      "loss": 0.5375,
      "step": 2521
    },
    {
      "epoch": 0.3207631160572337,
      "grad_norm": 2.4932680130004883,
      "learning_rate": 0.00013592974417716685,
      "loss": 0.8677,
      "step": 2522
    },
    {
      "epoch": 0.32089030206677266,
      "grad_norm": 1.5071368217468262,
      "learning_rate": 0.000135904289168894,
      "loss": 0.5636,
      "step": 2523
    },
    {
      "epoch": 0.3210174880763116,
      "grad_norm": 1.4756150245666504,
      "learning_rate": 0.0001358788341606211,
      "loss": 0.5374,
      "step": 2524
    },
    {
      "epoch": 0.32114467408585057,
      "grad_norm": 1.569830060005188,
      "learning_rate": 0.00013585337915234823,
      "loss": 0.5546,
      "step": 2525
    },
    {
      "epoch": 0.3212718600953895,
      "grad_norm": 1.9822543859481812,
      "learning_rate": 0.00013582792414407535,
      "loss": 0.7317,
      "step": 2526
    },
    {
      "epoch": 0.3213990461049285,
      "grad_norm": 2.0520424842834473,
      "learning_rate": 0.00013580246913580247,
      "loss": 0.5837,
      "step": 2527
    },
    {
      "epoch": 0.3215262321144674,
      "grad_norm": 1.616567850112915,
      "learning_rate": 0.0001357770141275296,
      "loss": 0.6238,
      "step": 2528
    },
    {
      "epoch": 0.3216534181240064,
      "grad_norm": 1.9590646028518677,
      "learning_rate": 0.00013575155911925673,
      "loss": 0.5856,
      "step": 2529
    },
    {
      "epoch": 0.3217806041335453,
      "grad_norm": 3.6452553272247314,
      "learning_rate": 0.00013572610411098383,
      "loss": 1.1147,
      "step": 2530
    },
    {
      "epoch": 0.32190779014308424,
      "grad_norm": 2.9992833137512207,
      "learning_rate": 0.00013570064910271097,
      "loss": 0.9423,
      "step": 2531
    },
    {
      "epoch": 0.3220349761526232,
      "grad_norm": 1.874637246131897,
      "learning_rate": 0.0001356751940944381,
      "loss": 0.7574,
      "step": 2532
    },
    {
      "epoch": 0.32216216216216215,
      "grad_norm": 2.3256494998931885,
      "learning_rate": 0.0001356497390861652,
      "loss": 0.5864,
      "step": 2533
    },
    {
      "epoch": 0.32228934817170113,
      "grad_norm": 2.3817384243011475,
      "learning_rate": 0.00013562428407789233,
      "loss": 0.588,
      "step": 2534
    },
    {
      "epoch": 0.32241653418124006,
      "grad_norm": 2.295668125152588,
      "learning_rate": 0.00013559882906961945,
      "loss": 0.6416,
      "step": 2535
    },
    {
      "epoch": 0.32254372019077904,
      "grad_norm": 1.8145489692687988,
      "learning_rate": 0.0001355733740613466,
      "loss": 0.6921,
      "step": 2536
    },
    {
      "epoch": 0.32267090620031796,
      "grad_norm": 2.382960081100464,
      "learning_rate": 0.00013554791905307368,
      "loss": 0.872,
      "step": 2537
    },
    {
      "epoch": 0.3227980922098569,
      "grad_norm": 2.898482322692871,
      "learning_rate": 0.00013552246404480083,
      "loss": 0.7292,
      "step": 2538
    },
    {
      "epoch": 0.3229252782193959,
      "grad_norm": 2.8245856761932373,
      "learning_rate": 0.00013549700903652792,
      "loss": 0.7553,
      "step": 2539
    },
    {
      "epoch": 0.3230524642289348,
      "grad_norm": 2.2434241771698,
      "learning_rate": 0.00013547155402825507,
      "loss": 0.7219,
      "step": 2540
    },
    {
      "epoch": 0.3231796502384738,
      "grad_norm": 2.577113628387451,
      "learning_rate": 0.00013544609901998219,
      "loss": 0.8232,
      "step": 2541
    },
    {
      "epoch": 0.3233068362480127,
      "grad_norm": 2.038667678833008,
      "learning_rate": 0.0001354206440117093,
      "loss": 0.898,
      "step": 2542
    },
    {
      "epoch": 0.3234340222575517,
      "grad_norm": 1.9993873834609985,
      "learning_rate": 0.00013539518900343645,
      "loss": 0.8944,
      "step": 2543
    },
    {
      "epoch": 0.3235612082670906,
      "grad_norm": 2.0246825218200684,
      "learning_rate": 0.00013536973399516354,
      "loss": 0.8898,
      "step": 2544
    },
    {
      "epoch": 0.32368839427662954,
      "grad_norm": 2.626816511154175,
      "learning_rate": 0.0001353442789868907,
      "loss": 0.7473,
      "step": 2545
    },
    {
      "epoch": 0.3238155802861685,
      "grad_norm": 2.368152618408203,
      "learning_rate": 0.00013531882397861778,
      "loss": 0.6896,
      "step": 2546
    },
    {
      "epoch": 0.32394276629570745,
      "grad_norm": 2.310948371887207,
      "learning_rate": 0.00013529336897034493,
      "loss": 0.6017,
      "step": 2547
    },
    {
      "epoch": 0.32406995230524643,
      "grad_norm": 1.8575938940048218,
      "learning_rate": 0.00013526791396207204,
      "loss": 0.5611,
      "step": 2548
    },
    {
      "epoch": 0.32419713831478536,
      "grad_norm": 1.6483345031738281,
      "learning_rate": 0.00013524245895379916,
      "loss": 0.512,
      "step": 2549
    },
    {
      "epoch": 0.32432432432432434,
      "grad_norm": 1.8864821195602417,
      "learning_rate": 0.0001352170039455263,
      "loss": 0.7211,
      "step": 2550
    },
    {
      "epoch": 0.32445151033386327,
      "grad_norm": 2.778191089630127,
      "learning_rate": 0.0001351915489372534,
      "loss": 0.9528,
      "step": 2551
    },
    {
      "epoch": 0.32457869634340225,
      "grad_norm": 1.5966365337371826,
      "learning_rate": 0.00013516609392898055,
      "loss": 0.5517,
      "step": 2552
    },
    {
      "epoch": 0.3247058823529412,
      "grad_norm": 1.8557074069976807,
      "learning_rate": 0.00013514063892070767,
      "loss": 0.7927,
      "step": 2553
    },
    {
      "epoch": 0.3248330683624801,
      "grad_norm": 1.6422265768051147,
      "learning_rate": 0.00013511518391243478,
      "loss": 0.5902,
      "step": 2554
    },
    {
      "epoch": 0.3249602543720191,
      "grad_norm": 2.6867988109588623,
      "learning_rate": 0.0001350897289041619,
      "loss": 0.6995,
      "step": 2555
    },
    {
      "epoch": 0.325087440381558,
      "grad_norm": 2.3677139282226562,
      "learning_rate": 0.00013506427389588902,
      "loss": 0.8117,
      "step": 2556
    },
    {
      "epoch": 0.325214626391097,
      "grad_norm": 2.7098402976989746,
      "learning_rate": 0.00013503881888761614,
      "loss": 0.7973,
      "step": 2557
    },
    {
      "epoch": 0.3253418124006359,
      "grad_norm": 2.100276470184326,
      "learning_rate": 0.00013501336387934326,
      "loss": 0.8846,
      "step": 2558
    },
    {
      "epoch": 0.3254689984101749,
      "grad_norm": 2.2066640853881836,
      "learning_rate": 0.00013498790887107038,
      "loss": 0.9285,
      "step": 2559
    },
    {
      "epoch": 0.32559618441971383,
      "grad_norm": 1.859164834022522,
      "learning_rate": 0.00013496245386279752,
      "loss": 0.7612,
      "step": 2560
    },
    {
      "epoch": 0.32572337042925276,
      "grad_norm": 1.7225830554962158,
      "learning_rate": 0.00013493699885452464,
      "loss": 0.5602,
      "step": 2561
    },
    {
      "epoch": 0.32585055643879174,
      "grad_norm": 1.8494479656219482,
      "learning_rate": 0.00013491154384625176,
      "loss": 0.4622,
      "step": 2562
    },
    {
      "epoch": 0.32597774244833067,
      "grad_norm": 1.7305854558944702,
      "learning_rate": 0.00013488608883797888,
      "loss": 0.5902,
      "step": 2563
    },
    {
      "epoch": 0.32610492845786965,
      "grad_norm": 2.3317630290985107,
      "learning_rate": 0.000134860633829706,
      "loss": 0.6121,
      "step": 2564
    },
    {
      "epoch": 0.3262321144674086,
      "grad_norm": 1.7157855033874512,
      "learning_rate": 0.00013483517882143312,
      "loss": 0.8125,
      "step": 2565
    },
    {
      "epoch": 0.32635930047694756,
      "grad_norm": 2.142491579055786,
      "learning_rate": 0.00013480972381316024,
      "loss": 0.6234,
      "step": 2566
    },
    {
      "epoch": 0.3264864864864865,
      "grad_norm": 1.735100507736206,
      "learning_rate": 0.00013478426880488738,
      "loss": 0.7284,
      "step": 2567
    },
    {
      "epoch": 0.32661367249602546,
      "grad_norm": 1.7688193321228027,
      "learning_rate": 0.00013475881379661447,
      "loss": 0.6972,
      "step": 2568
    },
    {
      "epoch": 0.3267408585055644,
      "grad_norm": 1.9608384370803833,
      "learning_rate": 0.00013473335878834162,
      "loss": 0.7174,
      "step": 2569
    },
    {
      "epoch": 0.3268680445151033,
      "grad_norm": 2.290342092514038,
      "learning_rate": 0.00013470790378006874,
      "loss": 0.5375,
      "step": 2570
    },
    {
      "epoch": 0.3269952305246423,
      "grad_norm": 2.2241642475128174,
      "learning_rate": 0.00013468244877179586,
      "loss": 0.7459,
      "step": 2571
    },
    {
      "epoch": 0.3271224165341812,
      "grad_norm": 2.022061586380005,
      "learning_rate": 0.00013465699376352298,
      "loss": 0.738,
      "step": 2572
    },
    {
      "epoch": 0.3272496025437202,
      "grad_norm": 1.6356652975082397,
      "learning_rate": 0.0001346315387552501,
      "loss": 0.5806,
      "step": 2573
    },
    {
      "epoch": 0.32737678855325913,
      "grad_norm": 2.166583299636841,
      "learning_rate": 0.00013460608374697724,
      "loss": 0.7303,
      "step": 2574
    },
    {
      "epoch": 0.3275039745627981,
      "grad_norm": 1.9694715738296509,
      "learning_rate": 0.00013458062873870433,
      "loss": 0.612,
      "step": 2575
    },
    {
      "epoch": 0.32763116057233704,
      "grad_norm": 2.3296632766723633,
      "learning_rate": 0.00013455517373043148,
      "loss": 0.8855,
      "step": 2576
    },
    {
      "epoch": 0.32775834658187597,
      "grad_norm": 2.1162121295928955,
      "learning_rate": 0.00013452971872215857,
      "loss": 0.7246,
      "step": 2577
    },
    {
      "epoch": 0.32788553259141495,
      "grad_norm": 1.6113336086273193,
      "learning_rate": 0.00013450426371388572,
      "loss": 0.4707,
      "step": 2578
    },
    {
      "epoch": 0.3280127186009539,
      "grad_norm": 1.7779536247253418,
      "learning_rate": 0.00013447880870561283,
      "loss": 0.5566,
      "step": 2579
    },
    {
      "epoch": 0.32813990461049286,
      "grad_norm": 1.8820263147354126,
      "learning_rate": 0.00013445335369733995,
      "loss": 0.5231,
      "step": 2580
    },
    {
      "epoch": 0.3282670906200318,
      "grad_norm": 1.991458773612976,
      "learning_rate": 0.0001344278986890671,
      "loss": 0.8872,
      "step": 2581
    },
    {
      "epoch": 0.32839427662957077,
      "grad_norm": 1.7280199527740479,
      "learning_rate": 0.0001344024436807942,
      "loss": 0.8138,
      "step": 2582
    },
    {
      "epoch": 0.3285214626391097,
      "grad_norm": 2.143718719482422,
      "learning_rate": 0.00013437698867252134,
      "loss": 0.6304,
      "step": 2583
    },
    {
      "epoch": 0.3286486486486486,
      "grad_norm": 2.3900179862976074,
      "learning_rate": 0.00013435153366424845,
      "loss": 0.6199,
      "step": 2584
    },
    {
      "epoch": 0.3287758346581876,
      "grad_norm": 1.6313990354537964,
      "learning_rate": 0.00013432607865597557,
      "loss": 0.483,
      "step": 2585
    },
    {
      "epoch": 0.32890302066772653,
      "grad_norm": 2.5386931896209717,
      "learning_rate": 0.0001343006236477027,
      "loss": 0.8668,
      "step": 2586
    },
    {
      "epoch": 0.3290302066772655,
      "grad_norm": 2.009690523147583,
      "learning_rate": 0.0001342751686394298,
      "loss": 0.8044,
      "step": 2587
    },
    {
      "epoch": 0.32915739268680444,
      "grad_norm": 2.505312919616699,
      "learning_rate": 0.00013424971363115693,
      "loss": 0.9719,
      "step": 2588
    },
    {
      "epoch": 0.3292845786963434,
      "grad_norm": 2.601146697998047,
      "learning_rate": 0.00013422425862288405,
      "loss": 0.6341,
      "step": 2589
    },
    {
      "epoch": 0.32941176470588235,
      "grad_norm": 1.8899807929992676,
      "learning_rate": 0.0001341988036146112,
      "loss": 0.8509,
      "step": 2590
    },
    {
      "epoch": 0.32953895071542133,
      "grad_norm": 1.9330933094024658,
      "learning_rate": 0.0001341733486063383,
      "loss": 0.6329,
      "step": 2591
    },
    {
      "epoch": 0.32966613672496026,
      "grad_norm": 2.3204336166381836,
      "learning_rate": 0.00013414789359806543,
      "loss": 0.7877,
      "step": 2592
    },
    {
      "epoch": 0.3297933227344992,
      "grad_norm": 2.7617523670196533,
      "learning_rate": 0.00013412243858979255,
      "loss": 0.7584,
      "step": 2593
    },
    {
      "epoch": 0.32992050874403817,
      "grad_norm": 1.8414627313613892,
      "learning_rate": 0.00013409698358151967,
      "loss": 0.61,
      "step": 2594
    },
    {
      "epoch": 0.3300476947535771,
      "grad_norm": 2.124790668487549,
      "learning_rate": 0.0001340715285732468,
      "loss": 0.6248,
      "step": 2595
    },
    {
      "epoch": 0.3301748807631161,
      "grad_norm": 1.8419102430343628,
      "learning_rate": 0.0001340460735649739,
      "loss": 0.6094,
      "step": 2596
    },
    {
      "epoch": 0.330302066772655,
      "grad_norm": 1.7486584186553955,
      "learning_rate": 0.00013402061855670103,
      "loss": 0.6396,
      "step": 2597
    },
    {
      "epoch": 0.330429252782194,
      "grad_norm": 1.9058253765106201,
      "learning_rate": 0.00013399516354842817,
      "loss": 0.5939,
      "step": 2598
    },
    {
      "epoch": 0.3305564387917329,
      "grad_norm": 2.4084057807922363,
      "learning_rate": 0.0001339697085401553,
      "loss": 0.7774,
      "step": 2599
    },
    {
      "epoch": 0.33068362480127184,
      "grad_norm": 2.60392165184021,
      "learning_rate": 0.0001339442535318824,
      "loss": 0.9036,
      "step": 2600
    },
    {
      "epoch": 0.3308108108108108,
      "grad_norm": 1.7977668046951294,
      "learning_rate": 0.00013391879852360953,
      "loss": 0.7686,
      "step": 2601
    },
    {
      "epoch": 0.33093799682034974,
      "grad_norm": 2.7097058296203613,
      "learning_rate": 0.00013389334351533665,
      "loss": 0.8301,
      "step": 2602
    },
    {
      "epoch": 0.3310651828298887,
      "grad_norm": 2.321012020111084,
      "learning_rate": 0.00013386788850706376,
      "loss": 0.6856,
      "step": 2603
    },
    {
      "epoch": 0.33119236883942765,
      "grad_norm": 2.6920320987701416,
      "learning_rate": 0.00013384243349879088,
      "loss": 0.8625,
      "step": 2604
    },
    {
      "epoch": 0.33131955484896664,
      "grad_norm": 2.91243052482605,
      "learning_rate": 0.00013381697849051803,
      "loss": 0.6084,
      "step": 2605
    },
    {
      "epoch": 0.33144674085850556,
      "grad_norm": 2.7589402198791504,
      "learning_rate": 0.00013379152348224512,
      "loss": 0.7085,
      "step": 2606
    },
    {
      "epoch": 0.3315739268680445,
      "grad_norm": 1.9239002466201782,
      "learning_rate": 0.00013376606847397227,
      "loss": 0.7999,
      "step": 2607
    },
    {
      "epoch": 0.33170111287758347,
      "grad_norm": 2.1896469593048096,
      "learning_rate": 0.00013374061346569939,
      "loss": 0.8467,
      "step": 2608
    },
    {
      "epoch": 0.3318282988871224,
      "grad_norm": 2.4689009189605713,
      "learning_rate": 0.0001337151584574265,
      "loss": 0.7103,
      "step": 2609
    },
    {
      "epoch": 0.3319554848966614,
      "grad_norm": 2.628018856048584,
      "learning_rate": 0.00013368970344915362,
      "loss": 0.6174,
      "step": 2610
    },
    {
      "epoch": 0.3320826709062003,
      "grad_norm": 1.9473720788955688,
      "learning_rate": 0.00013366424844088074,
      "loss": 0.5178,
      "step": 2611
    },
    {
      "epoch": 0.3322098569157393,
      "grad_norm": 2.083543300628662,
      "learning_rate": 0.0001336387934326079,
      "loss": 0.8696,
      "step": 2612
    },
    {
      "epoch": 0.3323370429252782,
      "grad_norm": 1.8416482210159302,
      "learning_rate": 0.00013361333842433498,
      "loss": 0.5642,
      "step": 2613
    },
    {
      "epoch": 0.3324642289348172,
      "grad_norm": 2.686161994934082,
      "learning_rate": 0.00013358788341606213,
      "loss": 1.1189,
      "step": 2614
    },
    {
      "epoch": 0.3325914149443561,
      "grad_norm": 2.17110538482666,
      "learning_rate": 0.00013356242840778924,
      "loss": 0.6468,
      "step": 2615
    },
    {
      "epoch": 0.33271860095389505,
      "grad_norm": 2.0923209190368652,
      "learning_rate": 0.00013353697339951636,
      "loss": 0.6583,
      "step": 2616
    },
    {
      "epoch": 0.33284578696343403,
      "grad_norm": 2.4941699504852295,
      "learning_rate": 0.00013351151839124348,
      "loss": 0.5262,
      "step": 2617
    },
    {
      "epoch": 0.33297297297297296,
      "grad_norm": 1.9283579587936401,
      "learning_rate": 0.0001334860633829706,
      "loss": 0.6765,
      "step": 2618
    },
    {
      "epoch": 0.33310015898251194,
      "grad_norm": 2.5045125484466553,
      "learning_rate": 0.00013346060837469775,
      "loss": 0.9741,
      "step": 2619
    },
    {
      "epoch": 0.33322734499205087,
      "grad_norm": 1.96979558467865,
      "learning_rate": 0.00013343515336642484,
      "loss": 0.6379,
      "step": 2620
    },
    {
      "epoch": 0.33335453100158985,
      "grad_norm": 1.9616297483444214,
      "learning_rate": 0.00013340969835815198,
      "loss": 0.7106,
      "step": 2621
    },
    {
      "epoch": 0.3334817170111288,
      "grad_norm": 2.30155611038208,
      "learning_rate": 0.0001333842433498791,
      "loss": 0.7353,
      "step": 2622
    },
    {
      "epoch": 0.3336089030206677,
      "grad_norm": 1.8971368074417114,
      "learning_rate": 0.00013335878834160622,
      "loss": 0.5678,
      "step": 2623
    },
    {
      "epoch": 0.3337360890302067,
      "grad_norm": 2.029677391052246,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.6383,
      "step": 2624
    },
    {
      "epoch": 0.3338632750397456,
      "grad_norm": 2.0082337856292725,
      "learning_rate": 0.00013330787832506046,
      "loss": 0.5985,
      "step": 2625
    },
    {
      "epoch": 0.3339904610492846,
      "grad_norm": 1.9352389574050903,
      "learning_rate": 0.00013328242331678758,
      "loss": 0.6566,
      "step": 2626
    },
    {
      "epoch": 0.3341176470588235,
      "grad_norm": 2.5212879180908203,
      "learning_rate": 0.0001332569683085147,
      "loss": 0.6238,
      "step": 2627
    },
    {
      "epoch": 0.3342448330683625,
      "grad_norm": 2.334204912185669,
      "learning_rate": 0.00013323151330024184,
      "loss": 0.5994,
      "step": 2628
    },
    {
      "epoch": 0.33437201907790143,
      "grad_norm": 2.4725708961486816,
      "learning_rate": 0.00013320605829196896,
      "loss": 0.7483,
      "step": 2629
    },
    {
      "epoch": 0.33449920508744035,
      "grad_norm": 2.2849531173706055,
      "learning_rate": 0.00013318060328369608,
      "loss": 0.7531,
      "step": 2630
    },
    {
      "epoch": 0.33462639109697934,
      "grad_norm": 2.1735496520996094,
      "learning_rate": 0.0001331551482754232,
      "loss": 0.8242,
      "step": 2631
    },
    {
      "epoch": 0.33475357710651826,
      "grad_norm": 1.800298810005188,
      "learning_rate": 0.00013312969326715032,
      "loss": 0.4766,
      "step": 2632
    },
    {
      "epoch": 0.33488076311605725,
      "grad_norm": 2.165044069290161,
      "learning_rate": 0.00013310423825887744,
      "loss": 0.7887,
      "step": 2633
    },
    {
      "epoch": 0.33500794912559617,
      "grad_norm": 1.7470338344573975,
      "learning_rate": 0.00013307878325060455,
      "loss": 0.5044,
      "step": 2634
    },
    {
      "epoch": 0.33513513513513515,
      "grad_norm": 2.131800413131714,
      "learning_rate": 0.00013305332824233167,
      "loss": 0.7387,
      "step": 2635
    },
    {
      "epoch": 0.3352623211446741,
      "grad_norm": 1.8411439657211304,
      "learning_rate": 0.00013302787323405882,
      "loss": 0.8755,
      "step": 2636
    },
    {
      "epoch": 0.33538950715421306,
      "grad_norm": 2.5181899070739746,
      "learning_rate": 0.00013300241822578594,
      "loss": 0.6671,
      "step": 2637
    },
    {
      "epoch": 0.335516693163752,
      "grad_norm": 1.8221110105514526,
      "learning_rate": 0.00013297696321751306,
      "loss": 0.542,
      "step": 2638
    },
    {
      "epoch": 0.3356438791732909,
      "grad_norm": 2.500049591064453,
      "learning_rate": 0.00013295150820924018,
      "loss": 0.5868,
      "step": 2639
    },
    {
      "epoch": 0.3357710651828299,
      "grad_norm": 2.350487470626831,
      "learning_rate": 0.0001329260532009673,
      "loss": 0.9621,
      "step": 2640
    },
    {
      "epoch": 0.3358982511923688,
      "grad_norm": 1.9083995819091797,
      "learning_rate": 0.0001329005981926944,
      "loss": 0.7484,
      "step": 2641
    },
    {
      "epoch": 0.3360254372019078,
      "grad_norm": 2.4966628551483154,
      "learning_rate": 0.00013287514318442153,
      "loss": 0.8342,
      "step": 2642
    },
    {
      "epoch": 0.33615262321144673,
      "grad_norm": 2.2221646308898926,
      "learning_rate": 0.00013284968817614868,
      "loss": 0.8741,
      "step": 2643
    },
    {
      "epoch": 0.3362798092209857,
      "grad_norm": 2.5449154376983643,
      "learning_rate": 0.00013282423316787577,
      "loss": 0.7798,
      "step": 2644
    },
    {
      "epoch": 0.33640699523052464,
      "grad_norm": 1.8924005031585693,
      "learning_rate": 0.00013279877815960291,
      "loss": 0.6664,
      "step": 2645
    },
    {
      "epoch": 0.33653418124006357,
      "grad_norm": 3.2046446800231934,
      "learning_rate": 0.00013277332315133003,
      "loss": 0.9477,
      "step": 2646
    },
    {
      "epoch": 0.33666136724960255,
      "grad_norm": 1.876221776008606,
      "learning_rate": 0.00013274786814305715,
      "loss": 0.638,
      "step": 2647
    },
    {
      "epoch": 0.3367885532591415,
      "grad_norm": 2.104924440383911,
      "learning_rate": 0.0001327224131347843,
      "loss": 0.6507,
      "step": 2648
    },
    {
      "epoch": 0.33691573926868046,
      "grad_norm": 2.116370677947998,
      "learning_rate": 0.0001326969581265114,
      "loss": 0.3471,
      "step": 2649
    },
    {
      "epoch": 0.3370429252782194,
      "grad_norm": 1.6919549703598022,
      "learning_rate": 0.00013267150311823854,
      "loss": 0.6685,
      "step": 2650
    },
    {
      "epoch": 0.33717011128775837,
      "grad_norm": 2.0851833820343018,
      "learning_rate": 0.00013264604810996563,
      "loss": 0.7379,
      "step": 2651
    },
    {
      "epoch": 0.3372972972972973,
      "grad_norm": 1.8206634521484375,
      "learning_rate": 0.00013262059310169277,
      "loss": 0.7363,
      "step": 2652
    },
    {
      "epoch": 0.3374244833068362,
      "grad_norm": 2.738912582397461,
      "learning_rate": 0.0001325951380934199,
      "loss": 1.3002,
      "step": 2653
    },
    {
      "epoch": 0.3375516693163752,
      "grad_norm": 1.616302490234375,
      "learning_rate": 0.000132569683085147,
      "loss": 0.7877,
      "step": 2654
    },
    {
      "epoch": 0.33767885532591413,
      "grad_norm": 2.039802074432373,
      "learning_rate": 0.00013254422807687413,
      "loss": 0.4946,
      "step": 2655
    },
    {
      "epoch": 0.3378060413354531,
      "grad_norm": 1.7638957500457764,
      "learning_rate": 0.00013251877306860125,
      "loss": 0.8122,
      "step": 2656
    },
    {
      "epoch": 0.33793322734499204,
      "grad_norm": 1.7471181154251099,
      "learning_rate": 0.0001324933180603284,
      "loss": 0.7686,
      "step": 2657
    },
    {
      "epoch": 0.338060413354531,
      "grad_norm": 0.9923273324966431,
      "learning_rate": 0.00013246786305205549,
      "loss": 0.3793,
      "step": 2658
    },
    {
      "epoch": 0.33818759936406995,
      "grad_norm": 1.9875526428222656,
      "learning_rate": 0.00013244240804378263,
      "loss": 0.8983,
      "step": 2659
    },
    {
      "epoch": 0.33831478537360893,
      "grad_norm": 1.752650499343872,
      "learning_rate": 0.00013241695303550975,
      "loss": 0.6155,
      "step": 2660
    },
    {
      "epoch": 0.33844197138314785,
      "grad_norm": 1.9495223760604858,
      "learning_rate": 0.00013239149802723687,
      "loss": 0.6599,
      "step": 2661
    },
    {
      "epoch": 0.3385691573926868,
      "grad_norm": 1.7478810548782349,
      "learning_rate": 0.000132366043018964,
      "loss": 0.7219,
      "step": 2662
    },
    {
      "epoch": 0.33869634340222576,
      "grad_norm": 1.777266025543213,
      "learning_rate": 0.0001323405880106911,
      "loss": 0.5601,
      "step": 2663
    },
    {
      "epoch": 0.3388235294117647,
      "grad_norm": 2.518688678741455,
      "learning_rate": 0.00013231513300241822,
      "loss": 0.721,
      "step": 2664
    },
    {
      "epoch": 0.33895071542130367,
      "grad_norm": 1.5623741149902344,
      "learning_rate": 0.00013228967799414534,
      "loss": 0.5697,
      "step": 2665
    },
    {
      "epoch": 0.3390779014308426,
      "grad_norm": 1.6598948240280151,
      "learning_rate": 0.00013226422298587246,
      "loss": 0.7809,
      "step": 2666
    },
    {
      "epoch": 0.3392050874403816,
      "grad_norm": 1.9880619049072266,
      "learning_rate": 0.0001322387679775996,
      "loss": 0.7026,
      "step": 2667
    },
    {
      "epoch": 0.3393322734499205,
      "grad_norm": 2.1268997192382812,
      "learning_rate": 0.00013221331296932673,
      "loss": 0.6156,
      "step": 2668
    },
    {
      "epoch": 0.33945945945945943,
      "grad_norm": 2.7348272800445557,
      "learning_rate": 0.00013218785796105385,
      "loss": 0.8041,
      "step": 2669
    },
    {
      "epoch": 0.3395866454689984,
      "grad_norm": 2.323200225830078,
      "learning_rate": 0.00013216240295278096,
      "loss": 0.4807,
      "step": 2670
    },
    {
      "epoch": 0.33971383147853734,
      "grad_norm": 2.6674273014068604,
      "learning_rate": 0.00013213694794450808,
      "loss": 0.9594,
      "step": 2671
    },
    {
      "epoch": 0.3398410174880763,
      "grad_norm": 2.297816753387451,
      "learning_rate": 0.0001321114929362352,
      "loss": 0.7809,
      "step": 2672
    },
    {
      "epoch": 0.33996820349761525,
      "grad_norm": 1.9144608974456787,
      "learning_rate": 0.00013208603792796232,
      "loss": 0.4628,
      "step": 2673
    },
    {
      "epoch": 0.34009538950715423,
      "grad_norm": 2.21732497215271,
      "learning_rate": 0.00013206058291968947,
      "loss": 0.7581,
      "step": 2674
    },
    {
      "epoch": 0.34022257551669316,
      "grad_norm": 2.6243743896484375,
      "learning_rate": 0.00013203512791141656,
      "loss": 0.8577,
      "step": 2675
    },
    {
      "epoch": 0.34034976152623214,
      "grad_norm": 1.7802069187164307,
      "learning_rate": 0.0001320096729031437,
      "loss": 0.4948,
      "step": 2676
    },
    {
      "epoch": 0.34047694753577107,
      "grad_norm": 2.0524001121520996,
      "learning_rate": 0.00013198421789487082,
      "loss": 0.673,
      "step": 2677
    },
    {
      "epoch": 0.34060413354531,
      "grad_norm": 2.0448296070098877,
      "learning_rate": 0.00013195876288659794,
      "loss": 0.6372,
      "step": 2678
    },
    {
      "epoch": 0.340731319554849,
      "grad_norm": 1.6091837882995605,
      "learning_rate": 0.0001319333078783251,
      "loss": 0.6734,
      "step": 2679
    },
    {
      "epoch": 0.3408585055643879,
      "grad_norm": 1.6492913961410522,
      "learning_rate": 0.00013190785287005218,
      "loss": 0.7596,
      "step": 2680
    },
    {
      "epoch": 0.3409856915739269,
      "grad_norm": 1.8162808418273926,
      "learning_rate": 0.00013188239786177932,
      "loss": 0.7512,
      "step": 2681
    },
    {
      "epoch": 0.3411128775834658,
      "grad_norm": 2.543881893157959,
      "learning_rate": 0.00013185694285350642,
      "loss": 1.0017,
      "step": 2682
    },
    {
      "epoch": 0.3412400635930048,
      "grad_norm": 2.365792751312256,
      "learning_rate": 0.00013183148784523356,
      "loss": 0.7816,
      "step": 2683
    },
    {
      "epoch": 0.3413672496025437,
      "grad_norm": 2.374405860900879,
      "learning_rate": 0.00013180603283696068,
      "loss": 0.6197,
      "step": 2684
    },
    {
      "epoch": 0.34149443561208265,
      "grad_norm": 1.7387292385101318,
      "learning_rate": 0.0001317805778286878,
      "loss": 0.875,
      "step": 2685
    },
    {
      "epoch": 0.34162162162162163,
      "grad_norm": 2.0623631477355957,
      "learning_rate": 0.00013175512282041495,
      "loss": 0.4574,
      "step": 2686
    },
    {
      "epoch": 0.34174880763116056,
      "grad_norm": 1.9143351316452026,
      "learning_rate": 0.00013172966781214204,
      "loss": 0.8622,
      "step": 2687
    },
    {
      "epoch": 0.34187599364069954,
      "grad_norm": 2.643324375152588,
      "learning_rate": 0.00013170421280386918,
      "loss": 0.9856,
      "step": 2688
    },
    {
      "epoch": 0.34200317965023846,
      "grad_norm": 1.6616326570510864,
      "learning_rate": 0.00013167875779559627,
      "loss": 0.4534,
      "step": 2689
    },
    {
      "epoch": 0.34213036565977745,
      "grad_norm": 1.878601312637329,
      "learning_rate": 0.00013165330278732342,
      "loss": 0.7128,
      "step": 2690
    },
    {
      "epoch": 0.3422575516693164,
      "grad_norm": 2.3224844932556152,
      "learning_rate": 0.00013162784777905054,
      "loss": 0.535,
      "step": 2691
    },
    {
      "epoch": 0.3423847376788553,
      "grad_norm": 1.9456806182861328,
      "learning_rate": 0.00013160239277077766,
      "loss": 0.7183,
      "step": 2692
    },
    {
      "epoch": 0.3425119236883943,
      "grad_norm": 2.4783287048339844,
      "learning_rate": 0.00013157693776250478,
      "loss": 0.7363,
      "step": 2693
    },
    {
      "epoch": 0.3426391096979332,
      "grad_norm": 2.3264918327331543,
      "learning_rate": 0.0001315514827542319,
      "loss": 0.7629,
      "step": 2694
    },
    {
      "epoch": 0.3427662957074722,
      "grad_norm": 2.0230777263641357,
      "learning_rate": 0.00013152602774595901,
      "loss": 0.6962,
      "step": 2695
    },
    {
      "epoch": 0.3428934817170111,
      "grad_norm": 2.4854824542999268,
      "learning_rate": 0.00013150057273768613,
      "loss": 0.669,
      "step": 2696
    },
    {
      "epoch": 0.3430206677265501,
      "grad_norm": 1.8933433294296265,
      "learning_rate": 0.00013147511772941328,
      "loss": 0.6275,
      "step": 2697
    },
    {
      "epoch": 0.343147853736089,
      "grad_norm": 1.6587717533111572,
      "learning_rate": 0.0001314496627211404,
      "loss": 0.6464,
      "step": 2698
    },
    {
      "epoch": 0.343275039745628,
      "grad_norm": 2.60013484954834,
      "learning_rate": 0.00013142420771286752,
      "loss": 0.8969,
      "step": 2699
    },
    {
      "epoch": 0.34340222575516693,
      "grad_norm": 2.9341917037963867,
      "learning_rate": 0.00013139875270459463,
      "loss": 0.6369,
      "step": 2700
    },
    {
      "epoch": 0.34352941176470586,
      "grad_norm": 1.4144649505615234,
      "learning_rate": 0.00013137329769632175,
      "loss": 0.4381,
      "step": 2701
    },
    {
      "epoch": 0.34365659777424484,
      "grad_norm": 2.477513551712036,
      "learning_rate": 0.00013134784268804887,
      "loss": 0.8493,
      "step": 2702
    },
    {
      "epoch": 0.34378378378378377,
      "grad_norm": 2.5407555103302,
      "learning_rate": 0.000131322387679776,
      "loss": 0.8619,
      "step": 2703
    },
    {
      "epoch": 0.34391096979332275,
      "grad_norm": 1.5923150777816772,
      "learning_rate": 0.0001312969326715031,
      "loss": 0.65,
      "step": 2704
    },
    {
      "epoch": 0.3440381558028617,
      "grad_norm": 2.2115254402160645,
      "learning_rate": 0.00013127147766323026,
      "loss": 0.825,
      "step": 2705
    },
    {
      "epoch": 0.34416534181240066,
      "grad_norm": 2.316800832748413,
      "learning_rate": 0.00013124602265495737,
      "loss": 0.5991,
      "step": 2706
    },
    {
      "epoch": 0.3442925278219396,
      "grad_norm": 2.356152057647705,
      "learning_rate": 0.0001312205676466845,
      "loss": 0.557,
      "step": 2707
    },
    {
      "epoch": 0.3444197138314785,
      "grad_norm": 2.7700648307800293,
      "learning_rate": 0.0001311951126384116,
      "loss": 0.6155,
      "step": 2708
    },
    {
      "epoch": 0.3445468998410175,
      "grad_norm": 2.0495336055755615,
      "learning_rate": 0.00013116965763013873,
      "loss": 0.7942,
      "step": 2709
    },
    {
      "epoch": 0.3446740858505564,
      "grad_norm": 1.8538435697555542,
      "learning_rate": 0.00013114420262186588,
      "loss": 0.6447,
      "step": 2710
    },
    {
      "epoch": 0.3448012718600954,
      "grad_norm": 2.083526611328125,
      "learning_rate": 0.00013111874761359297,
      "loss": 0.6457,
      "step": 2711
    },
    {
      "epoch": 0.34492845786963433,
      "grad_norm": 2.2420036792755127,
      "learning_rate": 0.00013109329260532011,
      "loss": 0.9724,
      "step": 2712
    },
    {
      "epoch": 0.3450556438791733,
      "grad_norm": 2.445075511932373,
      "learning_rate": 0.0001310678375970472,
      "loss": 0.5895,
      "step": 2713
    },
    {
      "epoch": 0.34518282988871224,
      "grad_norm": 1.6107676029205322,
      "learning_rate": 0.00013104238258877435,
      "loss": 0.4559,
      "step": 2714
    },
    {
      "epoch": 0.34531001589825117,
      "grad_norm": 1.6209715604782104,
      "learning_rate": 0.00013101692758050147,
      "loss": 0.6491,
      "step": 2715
    },
    {
      "epoch": 0.34543720190779015,
      "grad_norm": 1.7839744091033936,
      "learning_rate": 0.0001309914725722286,
      "loss": 0.5216,
      "step": 2716
    },
    {
      "epoch": 0.3455643879173291,
      "grad_norm": 2.6330761909484863,
      "learning_rate": 0.00013096601756395573,
      "loss": 0.7554,
      "step": 2717
    },
    {
      "epoch": 0.34569157392686806,
      "grad_norm": 1.7404955625534058,
      "learning_rate": 0.00013094056255568283,
      "loss": 0.7508,
      "step": 2718
    },
    {
      "epoch": 0.345818759936407,
      "grad_norm": 2.2859671115875244,
      "learning_rate": 0.00013091510754740997,
      "loss": 0.3889,
      "step": 2719
    },
    {
      "epoch": 0.34594594594594597,
      "grad_norm": 1.9407978057861328,
      "learning_rate": 0.00013088965253913706,
      "loss": 0.4519,
      "step": 2720
    },
    {
      "epoch": 0.3460731319554849,
      "grad_norm": 1.7915230989456177,
      "learning_rate": 0.0001308641975308642,
      "loss": 0.4628,
      "step": 2721
    },
    {
      "epoch": 0.3462003179650239,
      "grad_norm": 3.3656327724456787,
      "learning_rate": 0.00013083874252259133,
      "loss": 1.0751,
      "step": 2722
    },
    {
      "epoch": 0.3463275039745628,
      "grad_norm": 2.0484495162963867,
      "learning_rate": 0.00013081328751431845,
      "loss": 0.7012,
      "step": 2723
    },
    {
      "epoch": 0.3464546899841017,
      "grad_norm": 1.9488950967788696,
      "learning_rate": 0.00013078783250604557,
      "loss": 0.5116,
      "step": 2724
    },
    {
      "epoch": 0.3465818759936407,
      "grad_norm": 2.7055273056030273,
      "learning_rate": 0.00013076237749777268,
      "loss": 0.8809,
      "step": 2725
    },
    {
      "epoch": 0.34670906200317964,
      "grad_norm": 2.7390873432159424,
      "learning_rate": 0.00013073692248949983,
      "loss": 0.7532,
      "step": 2726
    },
    {
      "epoch": 0.3468362480127186,
      "grad_norm": 2.7911226749420166,
      "learning_rate": 0.00013071146748122692,
      "loss": 0.9181,
      "step": 2727
    },
    {
      "epoch": 0.34696343402225754,
      "grad_norm": 2.4650959968566895,
      "learning_rate": 0.00013068601247295407,
      "loss": 0.6701,
      "step": 2728
    },
    {
      "epoch": 0.3470906200317965,
      "grad_norm": 1.6217262744903564,
      "learning_rate": 0.0001306605574646812,
      "loss": 0.597,
      "step": 2729
    },
    {
      "epoch": 0.34721780604133545,
      "grad_norm": 1.9546505212783813,
      "learning_rate": 0.0001306351024564083,
      "loss": 0.7528,
      "step": 2730
    },
    {
      "epoch": 0.3473449920508744,
      "grad_norm": 2.341449737548828,
      "learning_rate": 0.00013060964744813542,
      "loss": 0.7342,
      "step": 2731
    },
    {
      "epoch": 0.34747217806041336,
      "grad_norm": 2.9776008129119873,
      "learning_rate": 0.00013058419243986254,
      "loss": 0.7144,
      "step": 2732
    },
    {
      "epoch": 0.3475993640699523,
      "grad_norm": 2.43282151222229,
      "learning_rate": 0.00013055873743158966,
      "loss": 0.7688,
      "step": 2733
    },
    {
      "epoch": 0.34772655007949127,
      "grad_norm": 2.938899040222168,
      "learning_rate": 0.00013053328242331678,
      "loss": 0.6475,
      "step": 2734
    },
    {
      "epoch": 0.3478537360890302,
      "grad_norm": 3.066478967666626,
      "learning_rate": 0.00013050782741504393,
      "loss": 0.7802,
      "step": 2735
    },
    {
      "epoch": 0.3479809220985692,
      "grad_norm": 1.698704481124878,
      "learning_rate": 0.00013048237240677105,
      "loss": 0.5811,
      "step": 2736
    },
    {
      "epoch": 0.3481081081081081,
      "grad_norm": 2.1031534671783447,
      "learning_rate": 0.00013045691739849816,
      "loss": 0.8128,
      "step": 2737
    },
    {
      "epoch": 0.34823529411764703,
      "grad_norm": 1.8374041318893433,
      "learning_rate": 0.00013043146239022528,
      "loss": 0.6349,
      "step": 2738
    },
    {
      "epoch": 0.348362480127186,
      "grad_norm": 1.8584449291229248,
      "learning_rate": 0.0001304060073819524,
      "loss": 0.6543,
      "step": 2739
    },
    {
      "epoch": 0.34848966613672494,
      "grad_norm": 1.5527141094207764,
      "learning_rate": 0.00013038055237367952,
      "loss": 0.6635,
      "step": 2740
    },
    {
      "epoch": 0.3486168521462639,
      "grad_norm": 1.6047943830490112,
      "learning_rate": 0.00013035509736540667,
      "loss": 0.7924,
      "step": 2741
    },
    {
      "epoch": 0.34874403815580285,
      "grad_norm": 2.287014961242676,
      "learning_rate": 0.00013032964235713376,
      "loss": 0.7845,
      "step": 2742
    },
    {
      "epoch": 0.34887122416534183,
      "grad_norm": 2.0773298740386963,
      "learning_rate": 0.0001303041873488609,
      "loss": 0.6938,
      "step": 2743
    },
    {
      "epoch": 0.34899841017488076,
      "grad_norm": 2.065035343170166,
      "learning_rate": 0.00013027873234058802,
      "loss": 0.6159,
      "step": 2744
    },
    {
      "epoch": 0.34912559618441974,
      "grad_norm": 1.504050374031067,
      "learning_rate": 0.00013025327733231514,
      "loss": 0.6206,
      "step": 2745
    },
    {
      "epoch": 0.34925278219395867,
      "grad_norm": 2.156773805618286,
      "learning_rate": 0.00013022782232404226,
      "loss": 0.5918,
      "step": 2746
    },
    {
      "epoch": 0.3493799682034976,
      "grad_norm": 1.8963723182678223,
      "learning_rate": 0.00013020236731576938,
      "loss": 0.6471,
      "step": 2747
    },
    {
      "epoch": 0.3495071542130366,
      "grad_norm": 2.253537893295288,
      "learning_rate": 0.00013017691230749652,
      "loss": 0.6889,
      "step": 2748
    },
    {
      "epoch": 0.3496343402225755,
      "grad_norm": 1.6666306257247925,
      "learning_rate": 0.00013015145729922362,
      "loss": 0.4383,
      "step": 2749
    },
    {
      "epoch": 0.3497615262321145,
      "grad_norm": 1.588847279548645,
      "learning_rate": 0.00013012600229095076,
      "loss": 0.4964,
      "step": 2750
    },
    {
      "epoch": 0.3498887122416534,
      "grad_norm": 1.991524577140808,
      "learning_rate": 0.00013010054728267785,
      "loss": 0.8622,
      "step": 2751
    },
    {
      "epoch": 0.3500158982511924,
      "grad_norm": 2.8700363636016846,
      "learning_rate": 0.000130075092274405,
      "loss": 0.7628,
      "step": 2752
    },
    {
      "epoch": 0.3501430842607313,
      "grad_norm": 2.7950847148895264,
      "learning_rate": 0.00013004963726613212,
      "loss": 0.8528,
      "step": 2753
    },
    {
      "epoch": 0.35027027027027025,
      "grad_norm": 2.4416086673736572,
      "learning_rate": 0.00013002418225785924,
      "loss": 0.6783,
      "step": 2754
    },
    {
      "epoch": 0.3503974562798092,
      "grad_norm": 1.9102082252502441,
      "learning_rate": 0.00012999872724958638,
      "loss": 0.6311,
      "step": 2755
    },
    {
      "epoch": 0.35052464228934815,
      "grad_norm": 2.2082302570343018,
      "learning_rate": 0.00012997327224131347,
      "loss": 0.8415,
      "step": 2756
    },
    {
      "epoch": 0.35065182829888714,
      "grad_norm": 3.6800906658172607,
      "learning_rate": 0.00012994781723304062,
      "loss": 0.8891,
      "step": 2757
    },
    {
      "epoch": 0.35077901430842606,
      "grad_norm": 2.9944941997528076,
      "learning_rate": 0.0001299223622247677,
      "loss": 0.7186,
      "step": 2758
    },
    {
      "epoch": 0.35090620031796504,
      "grad_norm": 1.9510481357574463,
      "learning_rate": 0.00012989690721649486,
      "loss": 0.6177,
      "step": 2759
    },
    {
      "epoch": 0.35103338632750397,
      "grad_norm": 3.133021593093872,
      "learning_rate": 0.00012987145220822198,
      "loss": 0.7153,
      "step": 2760
    },
    {
      "epoch": 0.3511605723370429,
      "grad_norm": 1.786927580833435,
      "learning_rate": 0.0001298459971999491,
      "loss": 0.8115,
      "step": 2761
    },
    {
      "epoch": 0.3512877583465819,
      "grad_norm": 2.353337526321411,
      "learning_rate": 0.00012982054219167621,
      "loss": 0.6971,
      "step": 2762
    },
    {
      "epoch": 0.3514149443561208,
      "grad_norm": 2.301009178161621,
      "learning_rate": 0.00012979508718340333,
      "loss": 0.8669,
      "step": 2763
    },
    {
      "epoch": 0.3515421303656598,
      "grad_norm": 1.9174085855484009,
      "learning_rate": 0.00012976963217513048,
      "loss": 0.6828,
      "step": 2764
    },
    {
      "epoch": 0.3516693163751987,
      "grad_norm": 2.0018324851989746,
      "learning_rate": 0.00012974417716685757,
      "loss": 0.7185,
      "step": 2765
    },
    {
      "epoch": 0.3517965023847377,
      "grad_norm": 2.216182231903076,
      "learning_rate": 0.00012971872215858472,
      "loss": 0.7696,
      "step": 2766
    },
    {
      "epoch": 0.3519236883942766,
      "grad_norm": 2.2803194522857666,
      "learning_rate": 0.00012969326715031183,
      "loss": 0.8917,
      "step": 2767
    },
    {
      "epoch": 0.3520508744038156,
      "grad_norm": 2.1465582847595215,
      "learning_rate": 0.00012966781214203895,
      "loss": 1.0714,
      "step": 2768
    },
    {
      "epoch": 0.35217806041335453,
      "grad_norm": 2.2561755180358887,
      "learning_rate": 0.00012964235713376607,
      "loss": 0.8223,
      "step": 2769
    },
    {
      "epoch": 0.35230524642289346,
      "grad_norm": 1.7946637868881226,
      "learning_rate": 0.0001296169021254932,
      "loss": 0.7307,
      "step": 2770
    },
    {
      "epoch": 0.35243243243243244,
      "grad_norm": 1.6545345783233643,
      "learning_rate": 0.0001295914471172203,
      "loss": 0.6084,
      "step": 2771
    },
    {
      "epoch": 0.35255961844197137,
      "grad_norm": 2.389047861099243,
      "learning_rate": 0.00012956599210894746,
      "loss": 0.6934,
      "step": 2772
    },
    {
      "epoch": 0.35268680445151035,
      "grad_norm": 1.5983972549438477,
      "learning_rate": 0.00012954053710067457,
      "loss": 0.5229,
      "step": 2773
    },
    {
      "epoch": 0.3528139904610493,
      "grad_norm": 1.5740201473236084,
      "learning_rate": 0.0001295150820924017,
      "loss": 0.6937,
      "step": 2774
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 1.8667757511138916,
      "learning_rate": 0.0001294896270841288,
      "loss": 0.7188,
      "step": 2775
    },
    {
      "epoch": 0.3530683624801272,
      "grad_norm": 2.2586944103240967,
      "learning_rate": 0.00012946417207585593,
      "loss": 0.7012,
      "step": 2776
    },
    {
      "epoch": 0.3531955484896661,
      "grad_norm": 2.0285449028015137,
      "learning_rate": 0.00012943871706758305,
      "loss": 0.7096,
      "step": 2777
    },
    {
      "epoch": 0.3533227344992051,
      "grad_norm": 2.107501983642578,
      "learning_rate": 0.00012941326205931017,
      "loss": 0.746,
      "step": 2778
    },
    {
      "epoch": 0.353449920508744,
      "grad_norm": 2.0962955951690674,
      "learning_rate": 0.00012938780705103731,
      "loss": 0.8751,
      "step": 2779
    },
    {
      "epoch": 0.353577106518283,
      "grad_norm": 2.3502392768859863,
      "learning_rate": 0.0001293623520427644,
      "loss": 0.547,
      "step": 2780
    },
    {
      "epoch": 0.35370429252782193,
      "grad_norm": 1.9503364562988281,
      "learning_rate": 0.00012933689703449155,
      "loss": 0.5639,
      "step": 2781
    },
    {
      "epoch": 0.3538314785373609,
      "grad_norm": 2.033900737762451,
      "learning_rate": 0.00012931144202621864,
      "loss": 0.6354,
      "step": 2782
    },
    {
      "epoch": 0.35395866454689984,
      "grad_norm": 2.264752149581909,
      "learning_rate": 0.0001292859870179458,
      "loss": 0.6718,
      "step": 2783
    },
    {
      "epoch": 0.3540858505564388,
      "grad_norm": 1.9918795824050903,
      "learning_rate": 0.0001292605320096729,
      "loss": 0.7872,
      "step": 2784
    },
    {
      "epoch": 0.35421303656597775,
      "grad_norm": 2.064821243286133,
      "learning_rate": 0.00012923507700140003,
      "loss": 0.5185,
      "step": 2785
    },
    {
      "epoch": 0.3543402225755167,
      "grad_norm": 2.1166703701019287,
      "learning_rate": 0.00012920962199312717,
      "loss": 0.8048,
      "step": 2786
    },
    {
      "epoch": 0.35446740858505565,
      "grad_norm": 1.5417951345443726,
      "learning_rate": 0.00012918416698485426,
      "loss": 0.6665,
      "step": 2787
    },
    {
      "epoch": 0.3545945945945946,
      "grad_norm": 2.2639400959014893,
      "learning_rate": 0.0001291587119765814,
      "loss": 0.5288,
      "step": 2788
    },
    {
      "epoch": 0.35472178060413356,
      "grad_norm": 2.3275692462921143,
      "learning_rate": 0.0001291332569683085,
      "loss": 0.6272,
      "step": 2789
    },
    {
      "epoch": 0.3548489666136725,
      "grad_norm": 1.7150903940200806,
      "learning_rate": 0.00012910780196003565,
      "loss": 0.7557,
      "step": 2790
    },
    {
      "epoch": 0.35497615262321147,
      "grad_norm": 2.2961387634277344,
      "learning_rate": 0.00012908234695176277,
      "loss": 0.8764,
      "step": 2791
    },
    {
      "epoch": 0.3551033386327504,
      "grad_norm": 1.9771263599395752,
      "learning_rate": 0.00012905689194348988,
      "loss": 0.5855,
      "step": 2792
    },
    {
      "epoch": 0.3552305246422893,
      "grad_norm": 1.342096209526062,
      "learning_rate": 0.00012903143693521703,
      "loss": 0.4435,
      "step": 2793
    },
    {
      "epoch": 0.3553577106518283,
      "grad_norm": 2.2587103843688965,
      "learning_rate": 0.00012900598192694412,
      "loss": 0.664,
      "step": 2794
    },
    {
      "epoch": 0.35548489666136723,
      "grad_norm": 2.5031867027282715,
      "learning_rate": 0.00012898052691867127,
      "loss": 0.7313,
      "step": 2795
    },
    {
      "epoch": 0.3556120826709062,
      "grad_norm": 2.2354073524475098,
      "learning_rate": 0.00012895507191039839,
      "loss": 0.7595,
      "step": 2796
    },
    {
      "epoch": 0.35573926868044514,
      "grad_norm": 2.2084155082702637,
      "learning_rate": 0.0001289296169021255,
      "loss": 0.5016,
      "step": 2797
    },
    {
      "epoch": 0.3558664546899841,
      "grad_norm": 1.9617637395858765,
      "learning_rate": 0.00012890416189385262,
      "loss": 0.9512,
      "step": 2798
    },
    {
      "epoch": 0.35599364069952305,
      "grad_norm": 2.61140775680542,
      "learning_rate": 0.00012887870688557974,
      "loss": 1.0543,
      "step": 2799
    },
    {
      "epoch": 0.356120826709062,
      "grad_norm": 1.7082709074020386,
      "learning_rate": 0.00012885325187730686,
      "loss": 0.5875,
      "step": 2800
    },
    {
      "epoch": 0.35624801271860096,
      "grad_norm": 1.911022663116455,
      "learning_rate": 0.00012882779686903398,
      "loss": 0.6002,
      "step": 2801
    },
    {
      "epoch": 0.3563751987281399,
      "grad_norm": 2.2385029792785645,
      "learning_rate": 0.0001288023418607611,
      "loss": 0.5741,
      "step": 2802
    },
    {
      "epoch": 0.35650238473767887,
      "grad_norm": 1.595901370048523,
      "learning_rate": 0.00012877688685248824,
      "loss": 0.4791,
      "step": 2803
    },
    {
      "epoch": 0.3566295707472178,
      "grad_norm": 2.0352773666381836,
      "learning_rate": 0.00012875143184421536,
      "loss": 0.7061,
      "step": 2804
    },
    {
      "epoch": 0.3567567567567568,
      "grad_norm": 2.5501315593719482,
      "learning_rate": 0.00012872597683594248,
      "loss": 0.4794,
      "step": 2805
    },
    {
      "epoch": 0.3568839427662957,
      "grad_norm": 2.0899152755737305,
      "learning_rate": 0.0001287005218276696,
      "loss": 0.65,
      "step": 2806
    },
    {
      "epoch": 0.3570111287758347,
      "grad_norm": 1.4307887554168701,
      "learning_rate": 0.00012867506681939672,
      "loss": 0.5657,
      "step": 2807
    },
    {
      "epoch": 0.3571383147853736,
      "grad_norm": 2.484118700027466,
      "learning_rate": 0.00012864961181112384,
      "loss": 0.7662,
      "step": 2808
    },
    {
      "epoch": 0.35726550079491254,
      "grad_norm": 2.2228856086730957,
      "learning_rate": 0.00012862415680285096,
      "loss": 0.6726,
      "step": 2809
    },
    {
      "epoch": 0.3573926868044515,
      "grad_norm": 2.2996912002563477,
      "learning_rate": 0.0001285987017945781,
      "loss": 0.4297,
      "step": 2810
    },
    {
      "epoch": 0.35751987281399045,
      "grad_norm": 2.212040424346924,
      "learning_rate": 0.0001285732467863052,
      "loss": 0.7958,
      "step": 2811
    },
    {
      "epoch": 0.35764705882352943,
      "grad_norm": 1.575020432472229,
      "learning_rate": 0.00012854779177803234,
      "loss": 0.6185,
      "step": 2812
    },
    {
      "epoch": 0.35777424483306836,
      "grad_norm": 2.9172706604003906,
      "learning_rate": 0.00012852233676975946,
      "loss": 0.919,
      "step": 2813
    },
    {
      "epoch": 0.35790143084260734,
      "grad_norm": 1.681836724281311,
      "learning_rate": 0.00012849688176148658,
      "loss": 0.5127,
      "step": 2814
    },
    {
      "epoch": 0.35802861685214626,
      "grad_norm": 2.2971115112304688,
      "learning_rate": 0.0001284714267532137,
      "loss": 0.71,
      "step": 2815
    },
    {
      "epoch": 0.3581558028616852,
      "grad_norm": 1.7418855428695679,
      "learning_rate": 0.00012844597174494082,
      "loss": 0.7829,
      "step": 2816
    },
    {
      "epoch": 0.3582829888712242,
      "grad_norm": 1.9789072275161743,
      "learning_rate": 0.00012842051673666796,
      "loss": 0.8109,
      "step": 2817
    },
    {
      "epoch": 0.3584101748807631,
      "grad_norm": 2.175570487976074,
      "learning_rate": 0.00012839506172839505,
      "loss": 0.7365,
      "step": 2818
    },
    {
      "epoch": 0.3585373608903021,
      "grad_norm": 4.294043064117432,
      "learning_rate": 0.0001283696067201222,
      "loss": 0.6451,
      "step": 2819
    },
    {
      "epoch": 0.358664546899841,
      "grad_norm": 2.2021310329437256,
      "learning_rate": 0.0001283441517118493,
      "loss": 0.8045,
      "step": 2820
    },
    {
      "epoch": 0.35879173290938,
      "grad_norm": 1.7414370775222778,
      "learning_rate": 0.00012831869670357644,
      "loss": 0.5815,
      "step": 2821
    },
    {
      "epoch": 0.3589189189189189,
      "grad_norm": 2.017911195755005,
      "learning_rate": 0.00012829324169530355,
      "loss": 0.7084,
      "step": 2822
    },
    {
      "epoch": 0.35904610492845784,
      "grad_norm": 2.5338141918182373,
      "learning_rate": 0.00012826778668703067,
      "loss": 0.8125,
      "step": 2823
    },
    {
      "epoch": 0.3591732909379968,
      "grad_norm": 2.3927221298217773,
      "learning_rate": 0.00012824233167875782,
      "loss": 0.5752,
      "step": 2824
    },
    {
      "epoch": 0.35930047694753575,
      "grad_norm": 1.8505470752716064,
      "learning_rate": 0.0001282168766704849,
      "loss": 0.5353,
      "step": 2825
    },
    {
      "epoch": 0.35942766295707473,
      "grad_norm": 2.233034133911133,
      "learning_rate": 0.00012819142166221206,
      "loss": 0.6767,
      "step": 2826
    },
    {
      "epoch": 0.35955484896661366,
      "grad_norm": 1.7532858848571777,
      "learning_rate": 0.00012816596665393918,
      "loss": 0.6088,
      "step": 2827
    },
    {
      "epoch": 0.35968203497615264,
      "grad_norm": 2.4690067768096924,
      "learning_rate": 0.0001281405116456663,
      "loss": 0.787,
      "step": 2828
    },
    {
      "epoch": 0.35980922098569157,
      "grad_norm": 1.8065674304962158,
      "learning_rate": 0.0001281150566373934,
      "loss": 0.5009,
      "step": 2829
    },
    {
      "epoch": 0.35993640699523055,
      "grad_norm": 1.7340511083602905,
      "learning_rate": 0.00012808960162912053,
      "loss": 0.6149,
      "step": 2830
    },
    {
      "epoch": 0.3600635930047695,
      "grad_norm": 2.9144413471221924,
      "learning_rate": 0.00012806414662084765,
      "loss": 0.632,
      "step": 2831
    },
    {
      "epoch": 0.3601907790143084,
      "grad_norm": 2.2846574783325195,
      "learning_rate": 0.00012803869161257477,
      "loss": 0.8221,
      "step": 2832
    },
    {
      "epoch": 0.3603179650238474,
      "grad_norm": 2.30972957611084,
      "learning_rate": 0.00012801323660430192,
      "loss": 0.6886,
      "step": 2833
    },
    {
      "epoch": 0.3604451510333863,
      "grad_norm": 1.859649896621704,
      "learning_rate": 0.00012798778159602903,
      "loss": 0.6231,
      "step": 2834
    },
    {
      "epoch": 0.3605723370429253,
      "grad_norm": 1.9898439645767212,
      "learning_rate": 0.00012796232658775615,
      "loss": 0.6244,
      "step": 2835
    },
    {
      "epoch": 0.3606995230524642,
      "grad_norm": 2.3142247200012207,
      "learning_rate": 0.00012793687157948327,
      "loss": 0.7553,
      "step": 2836
    },
    {
      "epoch": 0.3608267090620032,
      "grad_norm": 2.0431313514709473,
      "learning_rate": 0.0001279114165712104,
      "loss": 0.5646,
      "step": 2837
    },
    {
      "epoch": 0.36095389507154213,
      "grad_norm": 1.8202766180038452,
      "learning_rate": 0.0001278859615629375,
      "loss": 0.5746,
      "step": 2838
    },
    {
      "epoch": 0.36108108108108106,
      "grad_norm": 2.761627674102783,
      "learning_rate": 0.00012786050655466463,
      "loss": 0.7575,
      "step": 2839
    },
    {
      "epoch": 0.36120826709062004,
      "grad_norm": 3.0639889240264893,
      "learning_rate": 0.00012783505154639175,
      "loss": 0.7983,
      "step": 2840
    },
    {
      "epoch": 0.36133545310015897,
      "grad_norm": 1.6466797590255737,
      "learning_rate": 0.0001278095965381189,
      "loss": 0.6733,
      "step": 2841
    },
    {
      "epoch": 0.36146263910969795,
      "grad_norm": 2.4644603729248047,
      "learning_rate": 0.000127784141529846,
      "loss": 0.7497,
      "step": 2842
    },
    {
      "epoch": 0.3615898251192369,
      "grad_norm": 1.8416643142700195,
      "learning_rate": 0.00012775868652157313,
      "loss": 0.6725,
      "step": 2843
    },
    {
      "epoch": 0.36171701112877586,
      "grad_norm": 2.167584180831909,
      "learning_rate": 0.00012773323151330025,
      "loss": 0.7055,
      "step": 2844
    },
    {
      "epoch": 0.3618441971383148,
      "grad_norm": 2.2750625610351562,
      "learning_rate": 0.00012770777650502737,
      "loss": 0.8481,
      "step": 2845
    },
    {
      "epoch": 0.3619713831478537,
      "grad_norm": 2.1896297931671143,
      "learning_rate": 0.00012768232149675449,
      "loss": 0.8881,
      "step": 2846
    },
    {
      "epoch": 0.3620985691573927,
      "grad_norm": 1.4989776611328125,
      "learning_rate": 0.0001276568664884816,
      "loss": 0.4698,
      "step": 2847
    },
    {
      "epoch": 0.3622257551669316,
      "grad_norm": 2.0249311923980713,
      "learning_rate": 0.00012763141148020875,
      "loss": 0.5329,
      "step": 2848
    },
    {
      "epoch": 0.3623529411764706,
      "grad_norm": 2.0258708000183105,
      "learning_rate": 0.00012760595647193584,
      "loss": 0.8517,
      "step": 2849
    },
    {
      "epoch": 0.3624801271860095,
      "grad_norm": 2.17230224609375,
      "learning_rate": 0.000127580501463663,
      "loss": 0.6937,
      "step": 2850
    },
    {
      "epoch": 0.3626073131955485,
      "grad_norm": 1.6248949766159058,
      "learning_rate": 0.0001275550464553901,
      "loss": 0.664,
      "step": 2851
    },
    {
      "epoch": 0.36273449920508744,
      "grad_norm": 1.795914649963379,
      "learning_rate": 0.00012752959144711723,
      "loss": 0.771,
      "step": 2852
    },
    {
      "epoch": 0.3628616852146264,
      "grad_norm": 1.9744511842727661,
      "learning_rate": 0.00012750413643884434,
      "loss": 0.6145,
      "step": 2853
    },
    {
      "epoch": 0.36298887122416534,
      "grad_norm": 1.5046415328979492,
      "learning_rate": 0.00012747868143057146,
      "loss": 0.5519,
      "step": 2854
    },
    {
      "epoch": 0.36311605723370427,
      "grad_norm": 2.0920844078063965,
      "learning_rate": 0.0001274532264222986,
      "loss": 0.9282,
      "step": 2855
    },
    {
      "epoch": 0.36324324324324325,
      "grad_norm": 2.5913357734680176,
      "learning_rate": 0.0001274277714140257,
      "loss": 1.1988,
      "step": 2856
    },
    {
      "epoch": 0.3633704292527822,
      "grad_norm": 2.2484612464904785,
      "learning_rate": 0.00012740231640575285,
      "loss": 0.7444,
      "step": 2857
    },
    {
      "epoch": 0.36349761526232116,
      "grad_norm": 2.3438968658447266,
      "learning_rate": 0.00012737686139747997,
      "loss": 0.6321,
      "step": 2858
    },
    {
      "epoch": 0.3636248012718601,
      "grad_norm": 2.291386365890503,
      "learning_rate": 0.00012735140638920708,
      "loss": 0.7893,
      "step": 2859
    },
    {
      "epoch": 0.36375198728139907,
      "grad_norm": 1.8060070276260376,
      "learning_rate": 0.0001273259513809342,
      "loss": 0.5079,
      "step": 2860
    },
    {
      "epoch": 0.363879173290938,
      "grad_norm": 2.221975803375244,
      "learning_rate": 0.00012730049637266132,
      "loss": 0.8724,
      "step": 2861
    },
    {
      "epoch": 0.3640063593004769,
      "grad_norm": 1.7014933824539185,
      "learning_rate": 0.00012727504136438847,
      "loss": 0.6041,
      "step": 2862
    },
    {
      "epoch": 0.3641335453100159,
      "grad_norm": 1.8495069742202759,
      "learning_rate": 0.00012724958635611556,
      "loss": 0.807,
      "step": 2863
    },
    {
      "epoch": 0.36426073131955483,
      "grad_norm": 1.7726045846939087,
      "learning_rate": 0.0001272241313478427,
      "loss": 0.6467,
      "step": 2864
    },
    {
      "epoch": 0.3643879173290938,
      "grad_norm": 1.973764419555664,
      "learning_rate": 0.00012719867633956982,
      "loss": 0.6763,
      "step": 2865
    },
    {
      "epoch": 0.36451510333863274,
      "grad_norm": 2.0817296504974365,
      "learning_rate": 0.00012717322133129694,
      "loss": 0.7056,
      "step": 2866
    },
    {
      "epoch": 0.3646422893481717,
      "grad_norm": 2.0127811431884766,
      "learning_rate": 0.00012714776632302406,
      "loss": 0.6368,
      "step": 2867
    },
    {
      "epoch": 0.36476947535771065,
      "grad_norm": 2.043198823928833,
      "learning_rate": 0.00012712231131475118,
      "loss": 0.8555,
      "step": 2868
    },
    {
      "epoch": 0.3648966613672496,
      "grad_norm": 1.4404839277267456,
      "learning_rate": 0.0001270968563064783,
      "loss": 0.3248,
      "step": 2869
    },
    {
      "epoch": 0.36502384737678856,
      "grad_norm": 2.0823585987091064,
      "learning_rate": 0.00012707140129820542,
      "loss": 0.7112,
      "step": 2870
    },
    {
      "epoch": 0.3651510333863275,
      "grad_norm": 1.9540551900863647,
      "learning_rate": 0.00012704594628993256,
      "loss": 0.6569,
      "step": 2871
    },
    {
      "epoch": 0.36527821939586647,
      "grad_norm": 2.6551384925842285,
      "learning_rate": 0.00012702049128165968,
      "loss": 0.7228,
      "step": 2872
    },
    {
      "epoch": 0.3654054054054054,
      "grad_norm": 2.481837272644043,
      "learning_rate": 0.0001269950362733868,
      "loss": 0.5996,
      "step": 2873
    },
    {
      "epoch": 0.3655325914149444,
      "grad_norm": 2.2340340614318848,
      "learning_rate": 0.00012696958126511392,
      "loss": 0.7556,
      "step": 2874
    },
    {
      "epoch": 0.3656597774244833,
      "grad_norm": 2.049729108810425,
      "learning_rate": 0.00012694412625684104,
      "loss": 0.6907,
      "step": 2875
    },
    {
      "epoch": 0.3657869634340223,
      "grad_norm": 2.4369289875030518,
      "learning_rate": 0.00012691867124856816,
      "loss": 0.7984,
      "step": 2876
    },
    {
      "epoch": 0.3659141494435612,
      "grad_norm": 2.4531803131103516,
      "learning_rate": 0.00012689321624029528,
      "loss": 0.9076,
      "step": 2877
    },
    {
      "epoch": 0.36604133545310014,
      "grad_norm": 2.1138393878936768,
      "learning_rate": 0.0001268677612320224,
      "loss": 0.6864,
      "step": 2878
    },
    {
      "epoch": 0.3661685214626391,
      "grad_norm": 1.7447490692138672,
      "learning_rate": 0.00012684230622374954,
      "loss": 0.7277,
      "step": 2879
    },
    {
      "epoch": 0.36629570747217804,
      "grad_norm": 1.5840997695922852,
      "learning_rate": 0.00012681685121547666,
      "loss": 0.504,
      "step": 2880
    },
    {
      "epoch": 0.366422893481717,
      "grad_norm": 2.7898571491241455,
      "learning_rate": 0.00012679139620720378,
      "loss": 0.8164,
      "step": 2881
    },
    {
      "epoch": 0.36655007949125595,
      "grad_norm": 1.6807407140731812,
      "learning_rate": 0.0001267659411989309,
      "loss": 0.6941,
      "step": 2882
    },
    {
      "epoch": 0.36667726550079494,
      "grad_norm": 3.299992322921753,
      "learning_rate": 0.00012674048619065801,
      "loss": 0.6482,
      "step": 2883
    },
    {
      "epoch": 0.36680445151033386,
      "grad_norm": 2.488548994064331,
      "learning_rate": 0.00012671503118238513,
      "loss": 0.7669,
      "step": 2884
    },
    {
      "epoch": 0.3669316375198728,
      "grad_norm": 2.1256351470947266,
      "learning_rate": 0.00012668957617411225,
      "loss": 0.6329,
      "step": 2885
    },
    {
      "epoch": 0.36705882352941177,
      "grad_norm": 2.82419753074646,
      "learning_rate": 0.0001266641211658394,
      "loss": 0.6884,
      "step": 2886
    },
    {
      "epoch": 0.3671860095389507,
      "grad_norm": 1.3310141563415527,
      "learning_rate": 0.0001266386661575665,
      "loss": 0.6172,
      "step": 2887
    },
    {
      "epoch": 0.3673131955484897,
      "grad_norm": 1.7994037866592407,
      "learning_rate": 0.00012661321114929364,
      "loss": 0.5683,
      "step": 2888
    },
    {
      "epoch": 0.3674403815580286,
      "grad_norm": 2.1854312419891357,
      "learning_rate": 0.00012658775614102075,
      "loss": 0.7529,
      "step": 2889
    },
    {
      "epoch": 0.3675675675675676,
      "grad_norm": 1.8747066259384155,
      "learning_rate": 0.00012656230113274787,
      "loss": 0.7543,
      "step": 2890
    },
    {
      "epoch": 0.3676947535771065,
      "grad_norm": 2.5212812423706055,
      "learning_rate": 0.00012653684612447502,
      "loss": 0.6044,
      "step": 2891
    },
    {
      "epoch": 0.3678219395866455,
      "grad_norm": 3.2109880447387695,
      "learning_rate": 0.0001265113911162021,
      "loss": 0.7425,
      "step": 2892
    },
    {
      "epoch": 0.3679491255961844,
      "grad_norm": 1.8241264820098877,
      "learning_rate": 0.00012648593610792926,
      "loss": 0.6709,
      "step": 2893
    },
    {
      "epoch": 0.36807631160572335,
      "grad_norm": 1.8558911085128784,
      "learning_rate": 0.00012646048109965635,
      "loss": 0.7151,
      "step": 2894
    },
    {
      "epoch": 0.36820349761526233,
      "grad_norm": 2.3645145893096924,
      "learning_rate": 0.0001264350260913835,
      "loss": 0.5453,
      "step": 2895
    },
    {
      "epoch": 0.36833068362480126,
      "grad_norm": 1.7656691074371338,
      "learning_rate": 0.0001264095710831106,
      "loss": 0.5843,
      "step": 2896
    },
    {
      "epoch": 0.36845786963434024,
      "grad_norm": 2.257939577102661,
      "learning_rate": 0.00012638411607483773,
      "loss": 0.6474,
      "step": 2897
    },
    {
      "epoch": 0.36858505564387917,
      "grad_norm": 2.262481451034546,
      "learning_rate": 0.00012635866106656485,
      "loss": 0.7608,
      "step": 2898
    },
    {
      "epoch": 0.36871224165341815,
      "grad_norm": 1.985422968864441,
      "learning_rate": 0.00012633320605829197,
      "loss": 0.9011,
      "step": 2899
    },
    {
      "epoch": 0.3688394276629571,
      "grad_norm": 2.2632038593292236,
      "learning_rate": 0.00012630775105001911,
      "loss": 0.5271,
      "step": 2900
    },
    {
      "epoch": 0.368966613672496,
      "grad_norm": 1.9672436714172363,
      "learning_rate": 0.0001262822960417462,
      "loss": 0.5594,
      "step": 2901
    },
    {
      "epoch": 0.369093799682035,
      "grad_norm": 2.3208742141723633,
      "learning_rate": 0.00012625684103347335,
      "loss": 0.6104,
      "step": 2902
    },
    {
      "epoch": 0.3692209856915739,
      "grad_norm": 1.9241079092025757,
      "learning_rate": 0.00012623138602520047,
      "loss": 0.712,
      "step": 2903
    },
    {
      "epoch": 0.3693481717011129,
      "grad_norm": 1.8389030694961548,
      "learning_rate": 0.0001262059310169276,
      "loss": 0.5681,
      "step": 2904
    },
    {
      "epoch": 0.3694753577106518,
      "grad_norm": 2.1144206523895264,
      "learning_rate": 0.0001261804760086547,
      "loss": 0.5967,
      "step": 2905
    },
    {
      "epoch": 0.3696025437201908,
      "grad_norm": 1.8524470329284668,
      "learning_rate": 0.00012615502100038183,
      "loss": 0.6158,
      "step": 2906
    },
    {
      "epoch": 0.36972972972972973,
      "grad_norm": 2.4334566593170166,
      "learning_rate": 0.00012612956599210895,
      "loss": 0.6618,
      "step": 2907
    },
    {
      "epoch": 0.36985691573926865,
      "grad_norm": 1.8137246370315552,
      "learning_rate": 0.00012610411098383606,
      "loss": 0.7857,
      "step": 2908
    },
    {
      "epoch": 0.36998410174880764,
      "grad_norm": 1.6682807207107544,
      "learning_rate": 0.0001260786559755632,
      "loss": 0.5843,
      "step": 2909
    },
    {
      "epoch": 0.37011128775834656,
      "grad_norm": 2.350597381591797,
      "learning_rate": 0.00012605320096729033,
      "loss": 0.8103,
      "step": 2910
    },
    {
      "epoch": 0.37023847376788555,
      "grad_norm": 2.625847816467285,
      "learning_rate": 0.00012602774595901745,
      "loss": 0.741,
      "step": 2911
    },
    {
      "epoch": 0.37036565977742447,
      "grad_norm": 1.5381633043289185,
      "learning_rate": 0.00012600229095074457,
      "loss": 0.4493,
      "step": 2912
    },
    {
      "epoch": 0.37049284578696345,
      "grad_norm": 1.8985347747802734,
      "learning_rate": 0.00012597683594247169,
      "loss": 0.7292,
      "step": 2913
    },
    {
      "epoch": 0.3706200317965024,
      "grad_norm": 2.580979347229004,
      "learning_rate": 0.0001259513809341988,
      "loss": 0.8833,
      "step": 2914
    },
    {
      "epoch": 0.37074721780604136,
      "grad_norm": 2.263719081878662,
      "learning_rate": 0.00012592592592592592,
      "loss": 1.0982,
      "step": 2915
    },
    {
      "epoch": 0.3708744038155803,
      "grad_norm": 1.887232780456543,
      "learning_rate": 0.00012590047091765304,
      "loss": 0.5202,
      "step": 2916
    },
    {
      "epoch": 0.3710015898251192,
      "grad_norm": 2.1062729358673096,
      "learning_rate": 0.0001258750159093802,
      "loss": 0.659,
      "step": 2917
    },
    {
      "epoch": 0.3711287758346582,
      "grad_norm": 1.9848898649215698,
      "learning_rate": 0.00012584956090110728,
      "loss": 0.6458,
      "step": 2918
    },
    {
      "epoch": 0.3712559618441971,
      "grad_norm": 2.317188262939453,
      "learning_rate": 0.00012582410589283442,
      "loss": 0.7703,
      "step": 2919
    },
    {
      "epoch": 0.3713831478537361,
      "grad_norm": 1.98828125,
      "learning_rate": 0.00012579865088456154,
      "loss": 0.713,
      "step": 2920
    },
    {
      "epoch": 0.37151033386327503,
      "grad_norm": 1.8743562698364258,
      "learning_rate": 0.00012577319587628866,
      "loss": 0.7393,
      "step": 2921
    },
    {
      "epoch": 0.371637519872814,
      "grad_norm": 2.6755189895629883,
      "learning_rate": 0.0001257477408680158,
      "loss": 0.8317,
      "step": 2922
    },
    {
      "epoch": 0.37176470588235294,
      "grad_norm": 1.7332502603530884,
      "learning_rate": 0.0001257222858597429,
      "loss": 0.5576,
      "step": 2923
    },
    {
      "epoch": 0.37189189189189187,
      "grad_norm": 1.7223702669143677,
      "learning_rate": 0.00012569683085147005,
      "loss": 0.6109,
      "step": 2924
    },
    {
      "epoch": 0.37201907790143085,
      "grad_norm": 1.9975552558898926,
      "learning_rate": 0.00012567137584319714,
      "loss": 0.8527,
      "step": 2925
    },
    {
      "epoch": 0.3721462639109698,
      "grad_norm": 1.5885118246078491,
      "learning_rate": 0.00012564592083492428,
      "loss": 0.7346,
      "step": 2926
    },
    {
      "epoch": 0.37227344992050876,
      "grad_norm": 2.1126441955566406,
      "learning_rate": 0.0001256204658266514,
      "loss": 0.7325,
      "step": 2927
    },
    {
      "epoch": 0.3724006359300477,
      "grad_norm": 2.3062617778778076,
      "learning_rate": 0.00012559501081837852,
      "loss": 0.7571,
      "step": 2928
    },
    {
      "epoch": 0.37252782193958667,
      "grad_norm": 1.3235520124435425,
      "learning_rate": 0.00012556955581010567,
      "loss": 0.4417,
      "step": 2929
    },
    {
      "epoch": 0.3726550079491256,
      "grad_norm": 2.1128358840942383,
      "learning_rate": 0.00012554410080183276,
      "loss": 0.5695,
      "step": 2930
    },
    {
      "epoch": 0.3727821939586645,
      "grad_norm": 1.7443835735321045,
      "learning_rate": 0.0001255186457935599,
      "loss": 0.6781,
      "step": 2931
    },
    {
      "epoch": 0.3729093799682035,
      "grad_norm": 1.7901982069015503,
      "learning_rate": 0.000125493190785287,
      "loss": 0.9644,
      "step": 2932
    },
    {
      "epoch": 0.37303656597774243,
      "grad_norm": 2.33426833152771,
      "learning_rate": 0.00012546773577701414,
      "loss": 0.4971,
      "step": 2933
    },
    {
      "epoch": 0.3731637519872814,
      "grad_norm": 2.0724375247955322,
      "learning_rate": 0.00012544228076874126,
      "loss": 0.8305,
      "step": 2934
    },
    {
      "epoch": 0.37329093799682034,
      "grad_norm": 1.7627745866775513,
      "learning_rate": 0.00012541682576046838,
      "loss": 0.6055,
      "step": 2935
    },
    {
      "epoch": 0.3734181240063593,
      "grad_norm": 2.648210287094116,
      "learning_rate": 0.0001253913707521955,
      "loss": 0.8528,
      "step": 2936
    },
    {
      "epoch": 0.37354531001589825,
      "grad_norm": 1.8474675416946411,
      "learning_rate": 0.00012536591574392262,
      "loss": 0.7117,
      "step": 2937
    },
    {
      "epoch": 0.37367249602543723,
      "grad_norm": 2.347209930419922,
      "learning_rate": 0.00012534046073564974,
      "loss": 0.6446,
      "step": 2938
    },
    {
      "epoch": 0.37379968203497616,
      "grad_norm": 1.4855437278747559,
      "learning_rate": 0.00012531500572737685,
      "loss": 0.5355,
      "step": 2939
    },
    {
      "epoch": 0.3739268680445151,
      "grad_norm": 2.612685203552246,
      "learning_rate": 0.000125289550719104,
      "loss": 0.9618,
      "step": 2940
    },
    {
      "epoch": 0.37405405405405406,
      "grad_norm": 2.3183751106262207,
      "learning_rate": 0.00012526409571083112,
      "loss": 0.6619,
      "step": 2941
    },
    {
      "epoch": 0.374181240063593,
      "grad_norm": 2.314793586730957,
      "learning_rate": 0.00012523864070255824,
      "loss": 0.5292,
      "step": 2942
    },
    {
      "epoch": 0.374308426073132,
      "grad_norm": 2.305510997772217,
      "learning_rate": 0.00012521318569428536,
      "loss": 0.9554,
      "step": 2943
    },
    {
      "epoch": 0.3744356120826709,
      "grad_norm": 1.593490481376648,
      "learning_rate": 0.00012518773068601247,
      "loss": 0.642,
      "step": 2944
    },
    {
      "epoch": 0.3745627980922099,
      "grad_norm": 1.784620761871338,
      "learning_rate": 0.0001251622756777396,
      "loss": 0.5746,
      "step": 2945
    },
    {
      "epoch": 0.3746899841017488,
      "grad_norm": 1.6706907749176025,
      "learning_rate": 0.0001251368206694667,
      "loss": 0.5374,
      "step": 2946
    },
    {
      "epoch": 0.37481717011128773,
      "grad_norm": 1.75792396068573,
      "learning_rate": 0.00012511136566119383,
      "loss": 0.5689,
      "step": 2947
    },
    {
      "epoch": 0.3749443561208267,
      "grad_norm": 2.0729000568389893,
      "learning_rate": 0.00012508591065292098,
      "loss": 0.646,
      "step": 2948
    },
    {
      "epoch": 0.37507154213036564,
      "grad_norm": 2.4366440773010254,
      "learning_rate": 0.0001250604556446481,
      "loss": 0.7133,
      "step": 2949
    },
    {
      "epoch": 0.3751987281399046,
      "grad_norm": 2.528978109359741,
      "learning_rate": 0.00012503500063637521,
      "loss": 0.9408,
      "step": 2950
    },
    {
      "epoch": 0.37532591414944355,
      "grad_norm": 2.8992629051208496,
      "learning_rate": 0.00012500954562810233,
      "loss": 0.9987,
      "step": 2951
    },
    {
      "epoch": 0.37545310015898253,
      "grad_norm": 2.1770477294921875,
      "learning_rate": 0.00012498409061982945,
      "loss": 0.9067,
      "step": 2952
    },
    {
      "epoch": 0.37558028616852146,
      "grad_norm": 2.052948236465454,
      "learning_rate": 0.0001249586356115566,
      "loss": 0.809,
      "step": 2953
    },
    {
      "epoch": 0.3757074721780604,
      "grad_norm": 1.73981773853302,
      "learning_rate": 0.0001249331806032837,
      "loss": 0.5497,
      "step": 2954
    },
    {
      "epoch": 0.37583465818759937,
      "grad_norm": 2.9677016735076904,
      "learning_rate": 0.00012490772559501084,
      "loss": 0.7631,
      "step": 2955
    },
    {
      "epoch": 0.3759618441971383,
      "grad_norm": 1.813653826713562,
      "learning_rate": 0.00012488227058673793,
      "loss": 0.5307,
      "step": 2956
    },
    {
      "epoch": 0.3760890302066773,
      "grad_norm": 2.7752275466918945,
      "learning_rate": 0.00012485681557846507,
      "loss": 0.7914,
      "step": 2957
    },
    {
      "epoch": 0.3762162162162162,
      "grad_norm": 2.002913236618042,
      "learning_rate": 0.0001248313605701922,
      "loss": 0.6091,
      "step": 2958
    },
    {
      "epoch": 0.3763434022257552,
      "grad_norm": 1.619179368019104,
      "learning_rate": 0.0001248059055619193,
      "loss": 0.639,
      "step": 2959
    },
    {
      "epoch": 0.3764705882352941,
      "grad_norm": 1.8903980255126953,
      "learning_rate": 0.00012478045055364646,
      "loss": 0.6013,
      "step": 2960
    },
    {
      "epoch": 0.3765977742448331,
      "grad_norm": 1.8790662288665771,
      "learning_rate": 0.00012475499554537355,
      "loss": 0.6399,
      "step": 2961
    },
    {
      "epoch": 0.376724960254372,
      "grad_norm": 1.9813936948776245,
      "learning_rate": 0.0001247295405371007,
      "loss": 0.7025,
      "step": 2962
    },
    {
      "epoch": 0.37685214626391095,
      "grad_norm": 2.0489518642425537,
      "learning_rate": 0.00012470408552882779,
      "loss": 0.5866,
      "step": 2963
    },
    {
      "epoch": 0.37697933227344993,
      "grad_norm": 1.890166163444519,
      "learning_rate": 0.00012467863052055493,
      "loss": 0.5659,
      "step": 2964
    },
    {
      "epoch": 0.37710651828298886,
      "grad_norm": 2.1221561431884766,
      "learning_rate": 0.00012465317551228205,
      "loss": 0.7091,
      "step": 2965
    },
    {
      "epoch": 0.37723370429252784,
      "grad_norm": 1.826733946800232,
      "learning_rate": 0.00012462772050400917,
      "loss": 0.5404,
      "step": 2966
    },
    {
      "epoch": 0.37736089030206676,
      "grad_norm": 2.633324146270752,
      "learning_rate": 0.0001246022654957363,
      "loss": 0.756,
      "step": 2967
    },
    {
      "epoch": 0.37748807631160575,
      "grad_norm": 1.7318997383117676,
      "learning_rate": 0.0001245768104874634,
      "loss": 1.0175,
      "step": 2968
    },
    {
      "epoch": 0.3776152623211447,
      "grad_norm": 2.274007558822632,
      "learning_rate": 0.00012455135547919055,
      "loss": 0.7085,
      "step": 2969
    },
    {
      "epoch": 0.3777424483306836,
      "grad_norm": 1.933408260345459,
      "learning_rate": 0.00012452590047091764,
      "loss": 0.8014,
      "step": 2970
    },
    {
      "epoch": 0.3778696343402226,
      "grad_norm": 1.6350458860397339,
      "learning_rate": 0.0001245004454626448,
      "loss": 0.7532,
      "step": 2971
    },
    {
      "epoch": 0.3779968203497615,
      "grad_norm": 2.323477268218994,
      "learning_rate": 0.0001244749904543719,
      "loss": 0.6606,
      "step": 2972
    },
    {
      "epoch": 0.3781240063593005,
      "grad_norm": 1.8908659219741821,
      "learning_rate": 0.00012444953544609903,
      "loss": 0.6962,
      "step": 2973
    },
    {
      "epoch": 0.3782511923688394,
      "grad_norm": 2.2240347862243652,
      "learning_rate": 0.00012442408043782615,
      "loss": 0.6561,
      "step": 2974
    },
    {
      "epoch": 0.3783783783783784,
      "grad_norm": 2.3716471195220947,
      "learning_rate": 0.00012439862542955326,
      "loss": 0.4912,
      "step": 2975
    },
    {
      "epoch": 0.3785055643879173,
      "grad_norm": 2.405625820159912,
      "learning_rate": 0.00012437317042128038,
      "loss": 0.7873,
      "step": 2976
    },
    {
      "epoch": 0.37863275039745625,
      "grad_norm": 2.235250234603882,
      "learning_rate": 0.0001243477154130075,
      "loss": 0.6908,
      "step": 2977
    },
    {
      "epoch": 0.37875993640699523,
      "grad_norm": 1.8301113843917847,
      "learning_rate": 0.00012432226040473465,
      "loss": 0.4547,
      "step": 2978
    },
    {
      "epoch": 0.37888712241653416,
      "grad_norm": 1.7720279693603516,
      "learning_rate": 0.00012429680539646177,
      "loss": 0.4856,
      "step": 2979
    },
    {
      "epoch": 0.37901430842607314,
      "grad_norm": 1.8810005187988281,
      "learning_rate": 0.00012427135038818888,
      "loss": 0.5965,
      "step": 2980
    },
    {
      "epoch": 0.37914149443561207,
      "grad_norm": 1.8329720497131348,
      "learning_rate": 0.000124245895379916,
      "loss": 0.5862,
      "step": 2981
    },
    {
      "epoch": 0.37926868044515105,
      "grad_norm": 2.117180347442627,
      "learning_rate": 0.00012422044037164312,
      "loss": 0.8628,
      "step": 2982
    },
    {
      "epoch": 0.37939586645469,
      "grad_norm": 2.000269651412964,
      "learning_rate": 0.00012419498536337024,
      "loss": 0.9196,
      "step": 2983
    },
    {
      "epoch": 0.37952305246422896,
      "grad_norm": 1.957532525062561,
      "learning_rate": 0.0001241695303550974,
      "loss": 0.5713,
      "step": 2984
    },
    {
      "epoch": 0.3796502384737679,
      "grad_norm": 2.4780166149139404,
      "learning_rate": 0.00012414407534682448,
      "loss": 0.7296,
      "step": 2985
    },
    {
      "epoch": 0.3797774244833068,
      "grad_norm": 1.865394115447998,
      "learning_rate": 0.00012411862033855162,
      "loss": 0.6198,
      "step": 2986
    },
    {
      "epoch": 0.3799046104928458,
      "grad_norm": 1.8563331365585327,
      "learning_rate": 0.00012409316533027874,
      "loss": 0.5307,
      "step": 2987
    },
    {
      "epoch": 0.3800317965023847,
      "grad_norm": 2.032147169113159,
      "learning_rate": 0.00012406771032200586,
      "loss": 0.5564,
      "step": 2988
    },
    {
      "epoch": 0.3801589825119237,
      "grad_norm": 2.7176342010498047,
      "learning_rate": 0.00012404225531373298,
      "loss": 0.6329,
      "step": 2989
    },
    {
      "epoch": 0.38028616852146263,
      "grad_norm": 2.335442066192627,
      "learning_rate": 0.0001240168003054601,
      "loss": 0.7993,
      "step": 2990
    },
    {
      "epoch": 0.3804133545310016,
      "grad_norm": 2.2254185676574707,
      "learning_rate": 0.00012399134529718725,
      "loss": 0.7421,
      "step": 2991
    },
    {
      "epoch": 0.38054054054054054,
      "grad_norm": 2.7062835693359375,
      "learning_rate": 0.00012396589028891434,
      "loss": 0.7189,
      "step": 2992
    },
    {
      "epoch": 0.38066772655007947,
      "grad_norm": 2.3726894855499268,
      "learning_rate": 0.00012394043528064148,
      "loss": 0.865,
      "step": 2993
    },
    {
      "epoch": 0.38079491255961845,
      "grad_norm": 1.6033012866973877,
      "learning_rate": 0.00012391498027236857,
      "loss": 0.5789,
      "step": 2994
    },
    {
      "epoch": 0.3809220985691574,
      "grad_norm": 2.860997200012207,
      "learning_rate": 0.00012388952526409572,
      "loss": 0.5693,
      "step": 2995
    },
    {
      "epoch": 0.38104928457869636,
      "grad_norm": 2.050396203994751,
      "learning_rate": 0.00012386407025582284,
      "loss": 0.7628,
      "step": 2996
    },
    {
      "epoch": 0.3811764705882353,
      "grad_norm": 2.0161118507385254,
      "learning_rate": 0.00012383861524754996,
      "loss": 0.6317,
      "step": 2997
    },
    {
      "epoch": 0.38130365659777427,
      "grad_norm": 2.0475518703460693,
      "learning_rate": 0.0001238131602392771,
      "loss": 0.8178,
      "step": 2998
    },
    {
      "epoch": 0.3814308426073132,
      "grad_norm": 2.3075544834136963,
      "learning_rate": 0.0001237877052310042,
      "loss": 0.75,
      "step": 2999
    },
    {
      "epoch": 0.3815580286168522,
      "grad_norm": 2.2060086727142334,
      "learning_rate": 0.00012376225022273134,
      "loss": 0.7045,
      "step": 3000
    },
    {
      "epoch": 0.3816852146263911,
      "grad_norm": 2.010751485824585,
      "learning_rate": 0.00012373679521445843,
      "loss": 0.8385,
      "step": 3001
    },
    {
      "epoch": 0.38181240063593,
      "grad_norm": 1.9512909650802612,
      "learning_rate": 0.00012371134020618558,
      "loss": 0.9296,
      "step": 3002
    },
    {
      "epoch": 0.381939586645469,
      "grad_norm": 1.9078588485717773,
      "learning_rate": 0.0001236858851979127,
      "loss": 0.66,
      "step": 3003
    },
    {
      "epoch": 0.38206677265500794,
      "grad_norm": 1.8994238376617432,
      "learning_rate": 0.00012366043018963982,
      "loss": 0.7247,
      "step": 3004
    },
    {
      "epoch": 0.3821939586645469,
      "grad_norm": 1.617434024810791,
      "learning_rate": 0.00012363497518136693,
      "loss": 0.7648,
      "step": 3005
    },
    {
      "epoch": 0.38232114467408584,
      "grad_norm": 1.606786847114563,
      "learning_rate": 0.00012360952017309405,
      "loss": 0.7033,
      "step": 3006
    },
    {
      "epoch": 0.3824483306836248,
      "grad_norm": 2.357114315032959,
      "learning_rate": 0.0001235840651648212,
      "loss": 0.793,
      "step": 3007
    },
    {
      "epoch": 0.38257551669316375,
      "grad_norm": 1.9313809871673584,
      "learning_rate": 0.00012355861015654832,
      "loss": 0.489,
      "step": 3008
    },
    {
      "epoch": 0.3827027027027027,
      "grad_norm": 1.9487110376358032,
      "learning_rate": 0.00012353315514827544,
      "loss": 0.8347,
      "step": 3009
    },
    {
      "epoch": 0.38282988871224166,
      "grad_norm": 2.5522005558013916,
      "learning_rate": 0.00012350770014000256,
      "loss": 0.7982,
      "step": 3010
    },
    {
      "epoch": 0.3829570747217806,
      "grad_norm": 2.099682092666626,
      "learning_rate": 0.00012348224513172967,
      "loss": 0.7977,
      "step": 3011
    },
    {
      "epoch": 0.38308426073131957,
      "grad_norm": 2.859241008758545,
      "learning_rate": 0.0001234567901234568,
      "loss": 0.873,
      "step": 3012
    },
    {
      "epoch": 0.3832114467408585,
      "grad_norm": 1.7587486505508423,
      "learning_rate": 0.0001234313351151839,
      "loss": 0.6489,
      "step": 3013
    },
    {
      "epoch": 0.3833386327503975,
      "grad_norm": 1.5990469455718994,
      "learning_rate": 0.00012340588010691103,
      "loss": 0.6323,
      "step": 3014
    },
    {
      "epoch": 0.3834658187599364,
      "grad_norm": 2.7718276977539062,
      "learning_rate": 0.00012338042509863818,
      "loss": 0.7946,
      "step": 3015
    },
    {
      "epoch": 0.38359300476947533,
      "grad_norm": 2.245856523513794,
      "learning_rate": 0.0001233549700903653,
      "loss": 0.5706,
      "step": 3016
    },
    {
      "epoch": 0.3837201907790143,
      "grad_norm": 2.1855292320251465,
      "learning_rate": 0.00012332951508209241,
      "loss": 0.6783,
      "step": 3017
    },
    {
      "epoch": 0.38384737678855324,
      "grad_norm": 1.9586083889007568,
      "learning_rate": 0.00012330406007381953,
      "loss": 0.8922,
      "step": 3018
    },
    {
      "epoch": 0.3839745627980922,
      "grad_norm": 1.6646913290023804,
      "learning_rate": 0.00012327860506554665,
      "loss": 0.5335,
      "step": 3019
    },
    {
      "epoch": 0.38410174880763115,
      "grad_norm": 1.8939263820648193,
      "learning_rate": 0.00012325315005727377,
      "loss": 0.6776,
      "step": 3020
    },
    {
      "epoch": 0.38422893481717013,
      "grad_norm": 2.2281696796417236,
      "learning_rate": 0.0001232276950490009,
      "loss": 0.6178,
      "step": 3021
    },
    {
      "epoch": 0.38435612082670906,
      "grad_norm": 1.8405109643936157,
      "learning_rate": 0.00012320224004072803,
      "loss": 0.7893,
      "step": 3022
    },
    {
      "epoch": 0.38448330683624804,
      "grad_norm": 2.37990140914917,
      "learning_rate": 0.00012317678503245513,
      "loss": 0.9461,
      "step": 3023
    },
    {
      "epoch": 0.38461049284578697,
      "grad_norm": 1.493129849433899,
      "learning_rate": 0.00012315133002418227,
      "loss": 0.6181,
      "step": 3024
    },
    {
      "epoch": 0.3847376788553259,
      "grad_norm": 1.7415080070495605,
      "learning_rate": 0.00012312587501590936,
      "loss": 0.6591,
      "step": 3025
    },
    {
      "epoch": 0.3848648648648649,
      "grad_norm": 2.6573452949523926,
      "learning_rate": 0.0001231004200076365,
      "loss": 0.7188,
      "step": 3026
    },
    {
      "epoch": 0.3849920508744038,
      "grad_norm": 1.9167852401733398,
      "learning_rate": 0.00012307496499936363,
      "loss": 0.6052,
      "step": 3027
    },
    {
      "epoch": 0.3851192368839428,
      "grad_norm": 2.5984368324279785,
      "learning_rate": 0.00012304950999109075,
      "loss": 0.8537,
      "step": 3028
    },
    {
      "epoch": 0.3852464228934817,
      "grad_norm": 1.8077062368392944,
      "learning_rate": 0.0001230240549828179,
      "loss": 0.6037,
      "step": 3029
    },
    {
      "epoch": 0.3853736089030207,
      "grad_norm": 1.5317811965942383,
      "learning_rate": 0.00012299859997454498,
      "loss": 0.3896,
      "step": 3030
    },
    {
      "epoch": 0.3855007949125596,
      "grad_norm": 2.422591209411621,
      "learning_rate": 0.00012297314496627213,
      "loss": 0.6106,
      "step": 3031
    },
    {
      "epoch": 0.38562798092209855,
      "grad_norm": 2.2381043434143066,
      "learning_rate": 0.00012294768995799922,
      "loss": 0.6094,
      "step": 3032
    },
    {
      "epoch": 0.3857551669316375,
      "grad_norm": 1.698812484741211,
      "learning_rate": 0.00012292223494972637,
      "loss": 0.5725,
      "step": 3033
    },
    {
      "epoch": 0.38588235294117645,
      "grad_norm": 2.210679769515991,
      "learning_rate": 0.0001228967799414535,
      "loss": 0.7059,
      "step": 3034
    },
    {
      "epoch": 0.38600953895071544,
      "grad_norm": 2.127302408218384,
      "learning_rate": 0.0001228713249331806,
      "loss": 0.8129,
      "step": 3035
    },
    {
      "epoch": 0.38613672496025436,
      "grad_norm": 2.215503454208374,
      "learning_rate": 0.00012284586992490775,
      "loss": 0.8639,
      "step": 3036
    },
    {
      "epoch": 0.38626391096979334,
      "grad_norm": 1.8192344903945923,
      "learning_rate": 0.00012282041491663484,
      "loss": 0.62,
      "step": 3037
    },
    {
      "epoch": 0.38639109697933227,
      "grad_norm": 1.873236894607544,
      "learning_rate": 0.000122794959908362,
      "loss": 0.6633,
      "step": 3038
    },
    {
      "epoch": 0.3865182829888712,
      "grad_norm": 2.362053871154785,
      "learning_rate": 0.0001227695049000891,
      "loss": 0.6137,
      "step": 3039
    },
    {
      "epoch": 0.3866454689984102,
      "grad_norm": 2.0867161750793457,
      "learning_rate": 0.00012274404989181623,
      "loss": 0.7124,
      "step": 3040
    },
    {
      "epoch": 0.3867726550079491,
      "grad_norm": 2.9594180583953857,
      "learning_rate": 0.00012271859488354334,
      "loss": 0.9945,
      "step": 3041
    },
    {
      "epoch": 0.3868998410174881,
      "grad_norm": 3.130873441696167,
      "learning_rate": 0.00012269313987527046,
      "loss": 0.9711,
      "step": 3042
    },
    {
      "epoch": 0.387027027027027,
      "grad_norm": 2.143007755279541,
      "learning_rate": 0.00012266768486699758,
      "loss": 0.7969,
      "step": 3043
    },
    {
      "epoch": 0.387154213036566,
      "grad_norm": 1.6583184003829956,
      "learning_rate": 0.0001226422298587247,
      "loss": 0.4871,
      "step": 3044
    },
    {
      "epoch": 0.3872813990461049,
      "grad_norm": 2.0703890323638916,
      "learning_rate": 0.00012261677485045185,
      "loss": 0.609,
      "step": 3045
    },
    {
      "epoch": 0.3874085850556439,
      "grad_norm": 2.3963143825531006,
      "learning_rate": 0.00012259131984217897,
      "loss": 0.5476,
      "step": 3046
    },
    {
      "epoch": 0.38753577106518283,
      "grad_norm": 1.6972352266311646,
      "learning_rate": 0.00012256586483390608,
      "loss": 0.6272,
      "step": 3047
    },
    {
      "epoch": 0.38766295707472176,
      "grad_norm": 2.4922821521759033,
      "learning_rate": 0.0001225404098256332,
      "loss": 0.5865,
      "step": 3048
    },
    {
      "epoch": 0.38779014308426074,
      "grad_norm": 2.328000783920288,
      "learning_rate": 0.00012251495481736032,
      "loss": 0.5438,
      "step": 3049
    },
    {
      "epoch": 0.38791732909379967,
      "grad_norm": 2.078770399093628,
      "learning_rate": 0.00012248949980908744,
      "loss": 0.6719,
      "step": 3050
    },
    {
      "epoch": 0.38804451510333865,
      "grad_norm": 2.587472438812256,
      "learning_rate": 0.00012246404480081456,
      "loss": 0.9391,
      "step": 3051
    },
    {
      "epoch": 0.3881717011128776,
      "grad_norm": 1.8213646411895752,
      "learning_rate": 0.00012243858979254168,
      "loss": 0.7282,
      "step": 3052
    },
    {
      "epoch": 0.38829888712241656,
      "grad_norm": 2.02266526222229,
      "learning_rate": 0.00012241313478426882,
      "loss": 0.6107,
      "step": 3053
    },
    {
      "epoch": 0.3884260731319555,
      "grad_norm": 2.6264419555664062,
      "learning_rate": 0.00012238767977599592,
      "loss": 0.7806,
      "step": 3054
    },
    {
      "epoch": 0.3885532591414944,
      "grad_norm": 1.9376229047775269,
      "learning_rate": 0.00012236222476772306,
      "loss": 0.5455,
      "step": 3055
    },
    {
      "epoch": 0.3886804451510334,
      "grad_norm": 1.8896074295043945,
      "learning_rate": 0.00012233676975945018,
      "loss": 0.6602,
      "step": 3056
    },
    {
      "epoch": 0.3888076311605723,
      "grad_norm": 1.6715160608291626,
      "learning_rate": 0.0001223113147511773,
      "loss": 0.6012,
      "step": 3057
    },
    {
      "epoch": 0.3889348171701113,
      "grad_norm": 2.52545428276062,
      "learning_rate": 0.00012228585974290442,
      "loss": 0.922,
      "step": 3058
    },
    {
      "epoch": 0.38906200317965023,
      "grad_norm": 2.617672920227051,
      "learning_rate": 0.00012226040473463154,
      "loss": 0.7892,
      "step": 3059
    },
    {
      "epoch": 0.3891891891891892,
      "grad_norm": 2.1896793842315674,
      "learning_rate": 0.00012223494972635868,
      "loss": 0.7835,
      "step": 3060
    },
    {
      "epoch": 0.38931637519872814,
      "grad_norm": 1.8053072690963745,
      "learning_rate": 0.00012220949471808577,
      "loss": 0.602,
      "step": 3061
    },
    {
      "epoch": 0.38944356120826706,
      "grad_norm": 2.3028056621551514,
      "learning_rate": 0.00012218403970981292,
      "loss": 0.5633,
      "step": 3062
    },
    {
      "epoch": 0.38957074721780605,
      "grad_norm": 1.9358458518981934,
      "learning_rate": 0.00012215858470154,
      "loss": 0.7583,
      "step": 3063
    },
    {
      "epoch": 0.389697933227345,
      "grad_norm": 2.7291030883789062,
      "learning_rate": 0.00012213312969326716,
      "loss": 0.6274,
      "step": 3064
    },
    {
      "epoch": 0.38982511923688395,
      "grad_norm": 1.2925540208816528,
      "learning_rate": 0.00012210767468499428,
      "loss": 0.4842,
      "step": 3065
    },
    {
      "epoch": 0.3899523052464229,
      "grad_norm": 2.195732831954956,
      "learning_rate": 0.0001220822196767214,
      "loss": 0.748,
      "step": 3066
    },
    {
      "epoch": 0.39007949125596186,
      "grad_norm": 1.505068302154541,
      "learning_rate": 0.00012205676466844853,
      "loss": 0.5826,
      "step": 3067
    },
    {
      "epoch": 0.3902066772655008,
      "grad_norm": 2.3640449047088623,
      "learning_rate": 0.00012203130966017565,
      "loss": 0.5903,
      "step": 3068
    },
    {
      "epoch": 0.39033386327503977,
      "grad_norm": 1.7263131141662598,
      "learning_rate": 0.00012200585465190278,
      "loss": 0.5277,
      "step": 3069
    },
    {
      "epoch": 0.3904610492845787,
      "grad_norm": 1.8830183744430542,
      "learning_rate": 0.00012198039964362988,
      "loss": 0.7028,
      "step": 3070
    },
    {
      "epoch": 0.3905882352941176,
      "grad_norm": 1.9258875846862793,
      "learning_rate": 0.00012195494463535702,
      "loss": 0.6093,
      "step": 3071
    },
    {
      "epoch": 0.3907154213036566,
      "grad_norm": 1.4550352096557617,
      "learning_rate": 0.00012192948962708412,
      "loss": 0.7172,
      "step": 3072
    },
    {
      "epoch": 0.39084260731319553,
      "grad_norm": 2.007927894592285,
      "learning_rate": 0.00012190403461881125,
      "loss": 0.6274,
      "step": 3073
    },
    {
      "epoch": 0.3909697933227345,
      "grad_norm": 2.0613584518432617,
      "learning_rate": 0.00012187857961053839,
      "loss": 0.7927,
      "step": 3074
    },
    {
      "epoch": 0.39109697933227344,
      "grad_norm": 1.805351734161377,
      "learning_rate": 0.0001218531246022655,
      "loss": 0.6665,
      "step": 3075
    },
    {
      "epoch": 0.3912241653418124,
      "grad_norm": 2.071373462677002,
      "learning_rate": 0.00012182766959399264,
      "loss": 0.6818,
      "step": 3076
    },
    {
      "epoch": 0.39135135135135135,
      "grad_norm": 1.934676170349121,
      "learning_rate": 0.00012180221458571974,
      "loss": 0.5325,
      "step": 3077
    },
    {
      "epoch": 0.3914785373608903,
      "grad_norm": 1.9041340351104736,
      "learning_rate": 0.00012177675957744687,
      "loss": 0.4458,
      "step": 3078
    },
    {
      "epoch": 0.39160572337042926,
      "grad_norm": 2.0051076412200928,
      "learning_rate": 0.00012175130456917398,
      "loss": 0.6238,
      "step": 3079
    },
    {
      "epoch": 0.3917329093799682,
      "grad_norm": 2.199103355407715,
      "learning_rate": 0.00012172584956090111,
      "loss": 0.7454,
      "step": 3080
    },
    {
      "epoch": 0.39186009538950717,
      "grad_norm": 3.035108804702759,
      "learning_rate": 0.00012170039455262823,
      "loss": 0.8659,
      "step": 3081
    },
    {
      "epoch": 0.3919872813990461,
      "grad_norm": 1.6173745393753052,
      "learning_rate": 0.00012167493954435536,
      "loss": 1.1414,
      "step": 3082
    },
    {
      "epoch": 0.3921144674085851,
      "grad_norm": 2.167400360107422,
      "learning_rate": 0.00012164948453608247,
      "loss": 0.8834,
      "step": 3083
    },
    {
      "epoch": 0.392241653418124,
      "grad_norm": 2.0598995685577393,
      "learning_rate": 0.0001216240295278096,
      "loss": 0.7557,
      "step": 3084
    },
    {
      "epoch": 0.39236883942766293,
      "grad_norm": 1.8897358179092407,
      "learning_rate": 0.00012159857451953673,
      "loss": 0.6861,
      "step": 3085
    },
    {
      "epoch": 0.3924960254372019,
      "grad_norm": 2.068498134613037,
      "learning_rate": 0.00012157311951126385,
      "loss": 0.7309,
      "step": 3086
    },
    {
      "epoch": 0.39262321144674084,
      "grad_norm": 1.7025388479232788,
      "learning_rate": 0.00012154766450299098,
      "loss": 0.5749,
      "step": 3087
    },
    {
      "epoch": 0.3927503974562798,
      "grad_norm": 1.41879403591156,
      "learning_rate": 0.00012152220949471809,
      "loss": 0.6635,
      "step": 3088
    },
    {
      "epoch": 0.39287758346581875,
      "grad_norm": 2.5626609325408936,
      "learning_rate": 0.00012149675448644522,
      "loss": 0.5455,
      "step": 3089
    },
    {
      "epoch": 0.39300476947535773,
      "grad_norm": 2.6479005813598633,
      "learning_rate": 0.00012147129947817233,
      "loss": 0.649,
      "step": 3090
    },
    {
      "epoch": 0.39313195548489666,
      "grad_norm": 1.8458211421966553,
      "learning_rate": 0.00012144584446989946,
      "loss": 0.5366,
      "step": 3091
    },
    {
      "epoch": 0.39325914149443564,
      "grad_norm": 2.173717975616455,
      "learning_rate": 0.00012142038946162658,
      "loss": 0.6518,
      "step": 3092
    },
    {
      "epoch": 0.39338632750397456,
      "grad_norm": 2.6471750736236572,
      "learning_rate": 0.00012139493445335371,
      "loss": 0.9217,
      "step": 3093
    },
    {
      "epoch": 0.3935135135135135,
      "grad_norm": 2.0770246982574463,
      "learning_rate": 0.00012136947944508084,
      "loss": 0.8417,
      "step": 3094
    },
    {
      "epoch": 0.3936406995230525,
      "grad_norm": 2.106576442718506,
      "learning_rate": 0.00012134402443680795,
      "loss": 0.6355,
      "step": 3095
    },
    {
      "epoch": 0.3937678855325914,
      "grad_norm": 2.7724525928497314,
      "learning_rate": 0.00012131856942853508,
      "loss": 0.5614,
      "step": 3096
    },
    {
      "epoch": 0.3938950715421304,
      "grad_norm": 2.3069071769714355,
      "learning_rate": 0.00012129311442026218,
      "loss": 0.6165,
      "step": 3097
    },
    {
      "epoch": 0.3940222575516693,
      "grad_norm": 1.781111478805542,
      "learning_rate": 0.00012126765941198932,
      "loss": 0.5049,
      "step": 3098
    },
    {
      "epoch": 0.3941494435612083,
      "grad_norm": 1.6255443096160889,
      "learning_rate": 0.00012124220440371644,
      "loss": 0.5026,
      "step": 3099
    },
    {
      "epoch": 0.3942766295707472,
      "grad_norm": 2.5425925254821777,
      "learning_rate": 0.00012121674939544357,
      "loss": 0.7044,
      "step": 3100
    },
    {
      "epoch": 0.39440381558028614,
      "grad_norm": 2.6685712337493896,
      "learning_rate": 0.00012119129438717067,
      "loss": 0.9064,
      "step": 3101
    },
    {
      "epoch": 0.3945310015898251,
      "grad_norm": 2.200486898422241,
      "learning_rate": 0.0001211658393788978,
      "loss": 0.5116,
      "step": 3102
    },
    {
      "epoch": 0.39465818759936405,
      "grad_norm": 2.151482582092285,
      "learning_rate": 0.00012114038437062491,
      "loss": 0.7519,
      "step": 3103
    },
    {
      "epoch": 0.39478537360890303,
      "grad_norm": 4.180220127105713,
      "learning_rate": 0.00012111492936235204,
      "loss": 0.7399,
      "step": 3104
    },
    {
      "epoch": 0.39491255961844196,
      "grad_norm": 2.5592195987701416,
      "learning_rate": 0.00012108947435407917,
      "loss": 1.0457,
      "step": 3105
    },
    {
      "epoch": 0.39503974562798094,
      "grad_norm": 1.832789421081543,
      "learning_rate": 0.0001210640193458063,
      "loss": 0.5645,
      "step": 3106
    },
    {
      "epoch": 0.39516693163751987,
      "grad_norm": 2.4184694290161133,
      "learning_rate": 0.00012103856433753343,
      "loss": 0.7351,
      "step": 3107
    },
    {
      "epoch": 0.3952941176470588,
      "grad_norm": 1.66824471950531,
      "learning_rate": 0.00012101310932926053,
      "loss": 0.5769,
      "step": 3108
    },
    {
      "epoch": 0.3954213036565978,
      "grad_norm": 2.1284799575805664,
      "learning_rate": 0.00012098765432098766,
      "loss": 0.5993,
      "step": 3109
    },
    {
      "epoch": 0.3955484896661367,
      "grad_norm": 2.3396806716918945,
      "learning_rate": 0.00012096219931271477,
      "loss": 1.0006,
      "step": 3110
    },
    {
      "epoch": 0.3956756756756757,
      "grad_norm": 2.325428009033203,
      "learning_rate": 0.0001209367443044419,
      "loss": 0.7698,
      "step": 3111
    },
    {
      "epoch": 0.3958028616852146,
      "grad_norm": 2.576641321182251,
      "learning_rate": 0.00012091128929616902,
      "loss": 0.782,
      "step": 3112
    },
    {
      "epoch": 0.3959300476947536,
      "grad_norm": 2.3622565269470215,
      "learning_rate": 0.00012088583428789615,
      "loss": 0.5378,
      "step": 3113
    },
    {
      "epoch": 0.3960572337042925,
      "grad_norm": 2.040846109390259,
      "learning_rate": 0.00012086037927962328,
      "loss": 0.7322,
      "step": 3114
    },
    {
      "epoch": 0.3961844197138315,
      "grad_norm": 2.5753307342529297,
      "learning_rate": 0.00012083492427135039,
      "loss": 0.7778,
      "step": 3115
    },
    {
      "epoch": 0.39631160572337043,
      "grad_norm": 1.9655423164367676,
      "learning_rate": 0.00012080946926307752,
      "loss": 0.7164,
      "step": 3116
    },
    {
      "epoch": 0.39643879173290936,
      "grad_norm": 1.8441368341445923,
      "learning_rate": 0.00012078401425480464,
      "loss": 0.7771,
      "step": 3117
    },
    {
      "epoch": 0.39656597774244834,
      "grad_norm": 1.955615758895874,
      "learning_rate": 0.00012075855924653177,
      "loss": 0.6215,
      "step": 3118
    },
    {
      "epoch": 0.39669316375198727,
      "grad_norm": 2.2362887859344482,
      "learning_rate": 0.00012073310423825888,
      "loss": 0.7603,
      "step": 3119
    },
    {
      "epoch": 0.39682034976152625,
      "grad_norm": 3.2391200065612793,
      "learning_rate": 0.00012070764922998601,
      "loss": 0.7615,
      "step": 3120
    },
    {
      "epoch": 0.3969475357710652,
      "grad_norm": 1.6805657148361206,
      "learning_rate": 0.00012068219422171312,
      "loss": 0.696,
      "step": 3121
    },
    {
      "epoch": 0.39707472178060416,
      "grad_norm": 1.4914952516555786,
      "learning_rate": 0.00012065673921344025,
      "loss": 0.5026,
      "step": 3122
    },
    {
      "epoch": 0.3972019077901431,
      "grad_norm": 1.5214273929595947,
      "learning_rate": 0.00012063128420516738,
      "loss": 0.5769,
      "step": 3123
    },
    {
      "epoch": 0.397329093799682,
      "grad_norm": 3.115032911300659,
      "learning_rate": 0.0001206058291968945,
      "loss": 0.6693,
      "step": 3124
    },
    {
      "epoch": 0.397456279809221,
      "grad_norm": 1.6825027465820312,
      "learning_rate": 0.00012058037418862163,
      "loss": 0.656,
      "step": 3125
    },
    {
      "epoch": 0.3975834658187599,
      "grad_norm": 1.8958275318145752,
      "learning_rate": 0.00012055491918034874,
      "loss": 0.6225,
      "step": 3126
    },
    {
      "epoch": 0.3977106518282989,
      "grad_norm": 1.687050700187683,
      "learning_rate": 0.00012052946417207587,
      "loss": 0.7696,
      "step": 3127
    },
    {
      "epoch": 0.3978378378378378,
      "grad_norm": 2.095954418182373,
      "learning_rate": 0.00012050400916380297,
      "loss": 0.6119,
      "step": 3128
    },
    {
      "epoch": 0.3979650238473768,
      "grad_norm": 2.090179920196533,
      "learning_rate": 0.0001204785541555301,
      "loss": 0.6402,
      "step": 3129
    },
    {
      "epoch": 0.39809220985691574,
      "grad_norm": 2.6548430919647217,
      "learning_rate": 0.00012045309914725722,
      "loss": 0.6563,
      "step": 3130
    },
    {
      "epoch": 0.3982193958664547,
      "grad_norm": 2.344546318054199,
      "learning_rate": 0.00012042764413898436,
      "loss": 0.5457,
      "step": 3131
    },
    {
      "epoch": 0.39834658187599364,
      "grad_norm": 1.4757277965545654,
      "learning_rate": 0.00012040218913071146,
      "loss": 0.5566,
      "step": 3132
    },
    {
      "epoch": 0.39847376788553257,
      "grad_norm": 2.8137717247009277,
      "learning_rate": 0.0001203767341224386,
      "loss": 0.9253,
      "step": 3133
    },
    {
      "epoch": 0.39860095389507155,
      "grad_norm": 1.926669716835022,
      "learning_rate": 0.00012035127911416573,
      "loss": 0.5361,
      "step": 3134
    },
    {
      "epoch": 0.3987281399046105,
      "grad_norm": 2.1090645790100098,
      "learning_rate": 0.00012032582410589283,
      "loss": 0.5005,
      "step": 3135
    },
    {
      "epoch": 0.39885532591414946,
      "grad_norm": 2.7077136039733887,
      "learning_rate": 0.00012030036909761996,
      "loss": 0.4908,
      "step": 3136
    },
    {
      "epoch": 0.3989825119236884,
      "grad_norm": 2.0073580741882324,
      "learning_rate": 0.00012027491408934708,
      "loss": 0.5878,
      "step": 3137
    },
    {
      "epoch": 0.39910969793322737,
      "grad_norm": 2.165686845779419,
      "learning_rate": 0.00012024945908107421,
      "loss": 0.6555,
      "step": 3138
    },
    {
      "epoch": 0.3992368839427663,
      "grad_norm": 2.1581532955169678,
      "learning_rate": 0.00012022400407280132,
      "loss": 0.6385,
      "step": 3139
    },
    {
      "epoch": 0.3993640699523052,
      "grad_norm": 1.630535364151001,
      "learning_rate": 0.00012019854906452845,
      "loss": 0.4393,
      "step": 3140
    },
    {
      "epoch": 0.3994912559618442,
      "grad_norm": 1.59300696849823,
      "learning_rate": 0.00012017309405625556,
      "loss": 0.6257,
      "step": 3141
    },
    {
      "epoch": 0.39961844197138313,
      "grad_norm": 2.3663675785064697,
      "learning_rate": 0.00012014763904798269,
      "loss": 0.8462,
      "step": 3142
    },
    {
      "epoch": 0.3997456279809221,
      "grad_norm": 1.9222902059555054,
      "learning_rate": 0.00012012218403970982,
      "loss": 0.7794,
      "step": 3143
    },
    {
      "epoch": 0.39987281399046104,
      "grad_norm": 2.018540620803833,
      "learning_rate": 0.00012009672903143694,
      "loss": 0.8217,
      "step": 3144
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.7296559810638428,
      "learning_rate": 0.00012007127402316407,
      "loss": 0.4321,
      "step": 3145
    },
    {
      "epoch": 0.40012718600953895,
      "grad_norm": 2.412597417831421,
      "learning_rate": 0.00012004581901489118,
      "loss": 0.7967,
      "step": 3146
    },
    {
      "epoch": 0.4002543720190779,
      "grad_norm": 1.719178318977356,
      "learning_rate": 0.00012002036400661831,
      "loss": 0.7054,
      "step": 3147
    },
    {
      "epoch": 0.40038155802861686,
      "grad_norm": 1.7551686763763428,
      "learning_rate": 0.00011999490899834543,
      "loss": 0.6463,
      "step": 3148
    },
    {
      "epoch": 0.4005087440381558,
      "grad_norm": 1.5352833271026611,
      "learning_rate": 0.00011996945399007256,
      "loss": 0.5323,
      "step": 3149
    },
    {
      "epoch": 0.40063593004769477,
      "grad_norm": 2.2199113368988037,
      "learning_rate": 0.00011994399898179967,
      "loss": 0.5991,
      "step": 3150
    },
    {
      "epoch": 0.4007631160572337,
      "grad_norm": 3.1362860202789307,
      "learning_rate": 0.0001199185439735268,
      "loss": 0.6233,
      "step": 3151
    },
    {
      "epoch": 0.4008903020667727,
      "grad_norm": 3.347078800201416,
      "learning_rate": 0.00011989308896525393,
      "loss": 0.4129,
      "step": 3152
    },
    {
      "epoch": 0.4010174880763116,
      "grad_norm": 2.463270664215088,
      "learning_rate": 0.00011986763395698104,
      "loss": 0.6275,
      "step": 3153
    },
    {
      "epoch": 0.4011446740858506,
      "grad_norm": 2.178664207458496,
      "learning_rate": 0.00011984217894870817,
      "loss": 0.6389,
      "step": 3154
    },
    {
      "epoch": 0.4012718600953895,
      "grad_norm": 2.0458827018737793,
      "learning_rate": 0.00011981672394043529,
      "loss": 0.7789,
      "step": 3155
    },
    {
      "epoch": 0.40139904610492844,
      "grad_norm": 2.7034385204315186,
      "learning_rate": 0.00011979126893216242,
      "loss": 0.6223,
      "step": 3156
    },
    {
      "epoch": 0.4015262321144674,
      "grad_norm": 2.422344446182251,
      "learning_rate": 0.00011976581392388953,
      "loss": 0.7004,
      "step": 3157
    },
    {
      "epoch": 0.40165341812400635,
      "grad_norm": 1.9025629758834839,
      "learning_rate": 0.00011974035891561666,
      "loss": 0.7382,
      "step": 3158
    },
    {
      "epoch": 0.4017806041335453,
      "grad_norm": 1.7050225734710693,
      "learning_rate": 0.00011971490390734376,
      "loss": 0.2927,
      "step": 3159
    },
    {
      "epoch": 0.40190779014308425,
      "grad_norm": 3.0645668506622314,
      "learning_rate": 0.0001196894488990709,
      "loss": 0.8837,
      "step": 3160
    },
    {
      "epoch": 0.40203497615262324,
      "grad_norm": 2.1990437507629395,
      "learning_rate": 0.00011966399389079801,
      "loss": 0.6824,
      "step": 3161
    },
    {
      "epoch": 0.40216216216216216,
      "grad_norm": 1.569983959197998,
      "learning_rate": 0.00011963853888252515,
      "loss": 0.407,
      "step": 3162
    },
    {
      "epoch": 0.4022893481717011,
      "grad_norm": 2.158590078353882,
      "learning_rate": 0.00011961308387425228,
      "loss": 0.538,
      "step": 3163
    },
    {
      "epoch": 0.40241653418124007,
      "grad_norm": 2.1001415252685547,
      "learning_rate": 0.00011958762886597938,
      "loss": 0.8163,
      "step": 3164
    },
    {
      "epoch": 0.402543720190779,
      "grad_norm": 2.0596110820770264,
      "learning_rate": 0.00011956217385770652,
      "loss": 0.6022,
      "step": 3165
    },
    {
      "epoch": 0.402670906200318,
      "grad_norm": 2.1068003177642822,
      "learning_rate": 0.00011953671884943362,
      "loss": 0.8096,
      "step": 3166
    },
    {
      "epoch": 0.4027980922098569,
      "grad_norm": 2.134507417678833,
      "learning_rate": 0.00011951126384116075,
      "loss": 0.6529,
      "step": 3167
    },
    {
      "epoch": 0.4029252782193959,
      "grad_norm": 1.9430667161941528,
      "learning_rate": 0.00011948580883288787,
      "loss": 0.8664,
      "step": 3168
    },
    {
      "epoch": 0.4030524642289348,
      "grad_norm": 1.5769200325012207,
      "learning_rate": 0.000119460353824615,
      "loss": 0.5671,
      "step": 3169
    },
    {
      "epoch": 0.40317965023847374,
      "grad_norm": 2.0898597240448,
      "learning_rate": 0.00011943489881634211,
      "loss": 0.7107,
      "step": 3170
    },
    {
      "epoch": 0.4033068362480127,
      "grad_norm": 1.4808508157730103,
      "learning_rate": 0.00011940944380806924,
      "loss": 0.5472,
      "step": 3171
    },
    {
      "epoch": 0.40343402225755165,
      "grad_norm": 1.8998268842697144,
      "learning_rate": 0.00011938398879979637,
      "loss": 0.6686,
      "step": 3172
    },
    {
      "epoch": 0.40356120826709063,
      "grad_norm": 3.0735480785369873,
      "learning_rate": 0.00011935853379152348,
      "loss": 0.4604,
      "step": 3173
    },
    {
      "epoch": 0.40368839427662956,
      "grad_norm": 2.4893691539764404,
      "learning_rate": 0.00011933307878325061,
      "loss": 0.8216,
      "step": 3174
    },
    {
      "epoch": 0.40381558028616854,
      "grad_norm": 3.038414716720581,
      "learning_rate": 0.00011930762377497773,
      "loss": 0.8618,
      "step": 3175
    },
    {
      "epoch": 0.40394276629570747,
      "grad_norm": 2.2854154109954834,
      "learning_rate": 0.00011928216876670486,
      "loss": 0.8001,
      "step": 3176
    },
    {
      "epoch": 0.40406995230524645,
      "grad_norm": 2.2543447017669678,
      "learning_rate": 0.00011925671375843197,
      "loss": 0.5738,
      "step": 3177
    },
    {
      "epoch": 0.4041971383147854,
      "grad_norm": 2.376880407333374,
      "learning_rate": 0.0001192312587501591,
      "loss": 0.6507,
      "step": 3178
    },
    {
      "epoch": 0.4043243243243243,
      "grad_norm": 2.317178726196289,
      "learning_rate": 0.00011920580374188622,
      "loss": 0.6069,
      "step": 3179
    },
    {
      "epoch": 0.4044515103338633,
      "grad_norm": 1.9191615581512451,
      "learning_rate": 0.00011918034873361335,
      "loss": 0.6014,
      "step": 3180
    },
    {
      "epoch": 0.4045786963434022,
      "grad_norm": 3.8777670860290527,
      "learning_rate": 0.00011915489372534048,
      "loss": 0.5943,
      "step": 3181
    },
    {
      "epoch": 0.4047058823529412,
      "grad_norm": 1.6761938333511353,
      "learning_rate": 0.00011912943871706759,
      "loss": 0.5348,
      "step": 3182
    },
    {
      "epoch": 0.4048330683624801,
      "grad_norm": 2.208902597427368,
      "learning_rate": 0.00011910398370879472,
      "loss": 0.6711,
      "step": 3183
    },
    {
      "epoch": 0.4049602543720191,
      "grad_norm": 1.9946473836898804,
      "learning_rate": 0.00011907852870052183,
      "loss": 0.4956,
      "step": 3184
    },
    {
      "epoch": 0.40508744038155803,
      "grad_norm": 3.049126386642456,
      "learning_rate": 0.00011905307369224896,
      "loss": 0.6262,
      "step": 3185
    },
    {
      "epoch": 0.40521462639109695,
      "grad_norm": 5.629910469055176,
      "learning_rate": 0.00011902761868397608,
      "loss": 0.636,
      "step": 3186
    },
    {
      "epoch": 0.40534181240063594,
      "grad_norm": 2.0112059116363525,
      "learning_rate": 0.00011900216367570321,
      "loss": 0.5826,
      "step": 3187
    },
    {
      "epoch": 0.40546899841017486,
      "grad_norm": 2.5756521224975586,
      "learning_rate": 0.00011897670866743031,
      "loss": 0.9454,
      "step": 3188
    },
    {
      "epoch": 0.40559618441971385,
      "grad_norm": 1.9610531330108643,
      "learning_rate": 0.00011895125365915745,
      "loss": 1.0157,
      "step": 3189
    },
    {
      "epoch": 0.40572337042925277,
      "grad_norm": 1.9810277223587036,
      "learning_rate": 0.00011892579865088455,
      "loss": 0.737,
      "step": 3190
    },
    {
      "epoch": 0.40585055643879175,
      "grad_norm": 2.0268657207489014,
      "learning_rate": 0.00011890034364261168,
      "loss": 0.7195,
      "step": 3191
    },
    {
      "epoch": 0.4059777424483307,
      "grad_norm": 1.555337905883789,
      "learning_rate": 0.00011887488863433882,
      "loss": 0.5799,
      "step": 3192
    },
    {
      "epoch": 0.4061049284578696,
      "grad_norm": 2.8499221801757812,
      "learning_rate": 0.00011884943362606594,
      "loss": 0.926,
      "step": 3193
    },
    {
      "epoch": 0.4062321144674086,
      "grad_norm": 1.9592320919036865,
      "learning_rate": 0.00011882397861779307,
      "loss": 0.5396,
      "step": 3194
    },
    {
      "epoch": 0.4063593004769475,
      "grad_norm": 1.8811777830123901,
      "learning_rate": 0.00011879852360952017,
      "loss": 0.6303,
      "step": 3195
    },
    {
      "epoch": 0.4064864864864865,
      "grad_norm": 2.5137369632720947,
      "learning_rate": 0.0001187730686012473,
      "loss": 0.8115,
      "step": 3196
    },
    {
      "epoch": 0.4066136724960254,
      "grad_norm": 1.8771071434020996,
      "learning_rate": 0.00011874761359297441,
      "loss": 0.6117,
      "step": 3197
    },
    {
      "epoch": 0.4067408585055644,
      "grad_norm": 2.1947379112243652,
      "learning_rate": 0.00011872215858470154,
      "loss": 0.9236,
      "step": 3198
    },
    {
      "epoch": 0.40686804451510333,
      "grad_norm": 2.165147304534912,
      "learning_rate": 0.00011869670357642866,
      "loss": 0.7892,
      "step": 3199
    },
    {
      "epoch": 0.4069952305246423,
      "grad_norm": 1.7898095846176147,
      "learning_rate": 0.0001186712485681558,
      "loss": 0.8741,
      "step": 3200
    },
    {
      "epoch": 0.40712241653418124,
      "grad_norm": 1.3511815071105957,
      "learning_rate": 0.00011864579355988293,
      "loss": 0.5265,
      "step": 3201
    },
    {
      "epoch": 0.40724960254372017,
      "grad_norm": 2.267164945602417,
      "learning_rate": 0.00011862033855161003,
      "loss": 0.7803,
      "step": 3202
    },
    {
      "epoch": 0.40737678855325915,
      "grad_norm": 1.926284670829773,
      "learning_rate": 0.00011859488354333716,
      "loss": 0.6327,
      "step": 3203
    },
    {
      "epoch": 0.4075039745627981,
      "grad_norm": 2.9993155002593994,
      "learning_rate": 0.00011856942853506427,
      "loss": 0.722,
      "step": 3204
    },
    {
      "epoch": 0.40763116057233706,
      "grad_norm": 1.8770962953567505,
      "learning_rate": 0.0001185439735267914,
      "loss": 0.6604,
      "step": 3205
    },
    {
      "epoch": 0.407758346581876,
      "grad_norm": 2.055150270462036,
      "learning_rate": 0.00011851851851851852,
      "loss": 0.6429,
      "step": 3206
    },
    {
      "epoch": 0.40788553259141497,
      "grad_norm": 2.3892340660095215,
      "learning_rate": 0.00011849306351024565,
      "loss": 0.698,
      "step": 3207
    },
    {
      "epoch": 0.4080127186009539,
      "grad_norm": 2.392595052719116,
      "learning_rate": 0.00011846760850197276,
      "loss": 0.6491,
      "step": 3208
    },
    {
      "epoch": 0.4081399046104928,
      "grad_norm": 2.218418598175049,
      "learning_rate": 0.00011844215349369989,
      "loss": 1.0804,
      "step": 3209
    },
    {
      "epoch": 0.4082670906200318,
      "grad_norm": 2.5062758922576904,
      "learning_rate": 0.00011841669848542702,
      "loss": 0.87,
      "step": 3210
    },
    {
      "epoch": 0.40839427662957073,
      "grad_norm": 6.891639232635498,
      "learning_rate": 0.00011839124347715414,
      "loss": 0.5584,
      "step": 3211
    },
    {
      "epoch": 0.4085214626391097,
      "grad_norm": 2.244673013687134,
      "learning_rate": 0.00011836578846888127,
      "loss": 0.7645,
      "step": 3212
    },
    {
      "epoch": 0.40864864864864864,
      "grad_norm": 2.2613887786865234,
      "learning_rate": 0.00011834033346060838,
      "loss": 0.6488,
      "step": 3213
    },
    {
      "epoch": 0.4087758346581876,
      "grad_norm": 2.2181899547576904,
      "learning_rate": 0.00011831487845233551,
      "loss": 0.5228,
      "step": 3214
    },
    {
      "epoch": 0.40890302066772655,
      "grad_norm": 2.3039164543151855,
      "learning_rate": 0.00011828942344406262,
      "loss": 0.5635,
      "step": 3215
    },
    {
      "epoch": 0.4090302066772655,
      "grad_norm": 2.41255784034729,
      "learning_rate": 0.00011826396843578975,
      "loss": 0.7161,
      "step": 3216
    },
    {
      "epoch": 0.40915739268680446,
      "grad_norm": 2.602569580078125,
      "learning_rate": 0.00011823851342751687,
      "loss": 0.6944,
      "step": 3217
    },
    {
      "epoch": 0.4092845786963434,
      "grad_norm": 2.208022356033325,
      "learning_rate": 0.000118213058419244,
      "loss": 0.7005,
      "step": 3218
    },
    {
      "epoch": 0.40941176470588236,
      "grad_norm": 2.355471611022949,
      "learning_rate": 0.0001181876034109711,
      "loss": 0.7043,
      "step": 3219
    },
    {
      "epoch": 0.4095389507154213,
      "grad_norm": 2.2360422611236572,
      "learning_rate": 0.00011816214840269824,
      "loss": 0.7833,
      "step": 3220
    },
    {
      "epoch": 0.4096661367249603,
      "grad_norm": 1.9948025941848755,
      "learning_rate": 0.00011813669339442537,
      "loss": 0.8044,
      "step": 3221
    },
    {
      "epoch": 0.4097933227344992,
      "grad_norm": 1.6476401090621948,
      "learning_rate": 0.00011811123838615247,
      "loss": 0.662,
      "step": 3222
    },
    {
      "epoch": 0.4099205087440382,
      "grad_norm": 1.433721661567688,
      "learning_rate": 0.0001180857833778796,
      "loss": 0.4994,
      "step": 3223
    },
    {
      "epoch": 0.4100476947535771,
      "grad_norm": 2.267613410949707,
      "learning_rate": 0.00011806032836960672,
      "loss": 0.563,
      "step": 3224
    },
    {
      "epoch": 0.41017488076311603,
      "grad_norm": 2.8089301586151123,
      "learning_rate": 0.00011803487336133386,
      "loss": 0.6165,
      "step": 3225
    },
    {
      "epoch": 0.410302066772655,
      "grad_norm": 1.5917562246322632,
      "learning_rate": 0.00011800941835306096,
      "loss": 0.5939,
      "step": 3226
    },
    {
      "epoch": 0.41042925278219394,
      "grad_norm": 2.0035605430603027,
      "learning_rate": 0.0001179839633447881,
      "loss": 0.6652,
      "step": 3227
    },
    {
      "epoch": 0.4105564387917329,
      "grad_norm": 2.2830142974853516,
      "learning_rate": 0.0001179585083365152,
      "loss": 0.9696,
      "step": 3228
    },
    {
      "epoch": 0.41068362480127185,
      "grad_norm": 3.7529408931732178,
      "learning_rate": 0.00011793305332824233,
      "loss": 0.8334,
      "step": 3229
    },
    {
      "epoch": 0.41081081081081083,
      "grad_norm": 2.0795202255249023,
      "learning_rate": 0.00011790759831996946,
      "loss": 0.7171,
      "step": 3230
    },
    {
      "epoch": 0.41093799682034976,
      "grad_norm": 3.309431314468384,
      "learning_rate": 0.00011788214331169658,
      "loss": 0.6132,
      "step": 3231
    },
    {
      "epoch": 0.4110651828298887,
      "grad_norm": 1.9320851564407349,
      "learning_rate": 0.00011785668830342372,
      "loss": 0.7196,
      "step": 3232
    },
    {
      "epoch": 0.41119236883942767,
      "grad_norm": 3.000624656677246,
      "learning_rate": 0.00011783123329515082,
      "loss": 0.8209,
      "step": 3233
    },
    {
      "epoch": 0.4113195548489666,
      "grad_norm": 1.6814552545547485,
      "learning_rate": 0.00011780577828687795,
      "loss": 0.6808,
      "step": 3234
    },
    {
      "epoch": 0.4114467408585056,
      "grad_norm": 1.810019612312317,
      "learning_rate": 0.00011778032327860506,
      "loss": 0.6902,
      "step": 3235
    },
    {
      "epoch": 0.4115739268680445,
      "grad_norm": 2.0163609981536865,
      "learning_rate": 0.00011775486827033219,
      "loss": 0.782,
      "step": 3236
    },
    {
      "epoch": 0.4117011128775835,
      "grad_norm": 1.4598921537399292,
      "learning_rate": 0.00011772941326205931,
      "loss": 0.6628,
      "step": 3237
    },
    {
      "epoch": 0.4118282988871224,
      "grad_norm": 1.9357601404190063,
      "learning_rate": 0.00011770395825378644,
      "loss": 0.5986,
      "step": 3238
    },
    {
      "epoch": 0.4119554848966614,
      "grad_norm": 2.1034998893737793,
      "learning_rate": 0.00011767850324551355,
      "loss": 0.7342,
      "step": 3239
    },
    {
      "epoch": 0.4120826709062003,
      "grad_norm": 2.5608181953430176,
      "learning_rate": 0.00011765304823724068,
      "loss": 0.7117,
      "step": 3240
    },
    {
      "epoch": 0.41220985691573925,
      "grad_norm": 2.229051351547241,
      "learning_rate": 0.00011762759322896781,
      "loss": 0.759,
      "step": 3241
    },
    {
      "epoch": 0.41233704292527823,
      "grad_norm": 2.1322145462036133,
      "learning_rate": 0.00011760213822069493,
      "loss": 0.5538,
      "step": 3242
    },
    {
      "epoch": 0.41246422893481716,
      "grad_norm": 2.972869873046875,
      "learning_rate": 0.00011757668321242206,
      "loss": 0.8865,
      "step": 3243
    },
    {
      "epoch": 0.41259141494435614,
      "grad_norm": 2.064173698425293,
      "learning_rate": 0.00011755122820414917,
      "loss": 0.7693,
      "step": 3244
    },
    {
      "epoch": 0.41271860095389507,
      "grad_norm": 2.953721523284912,
      "learning_rate": 0.0001175257731958763,
      "loss": 0.7431,
      "step": 3245
    },
    {
      "epoch": 0.41284578696343405,
      "grad_norm": 2.9297595024108887,
      "learning_rate": 0.0001175003181876034,
      "loss": 0.9094,
      "step": 3246
    },
    {
      "epoch": 0.412972972972973,
      "grad_norm": 1.200460433959961,
      "learning_rate": 0.00011747486317933054,
      "loss": 0.2401,
      "step": 3247
    },
    {
      "epoch": 0.4131001589825119,
      "grad_norm": 2.39021897315979,
      "learning_rate": 0.00011744940817105766,
      "loss": 0.5091,
      "step": 3248
    },
    {
      "epoch": 0.4132273449920509,
      "grad_norm": 1.8745698928833008,
      "learning_rate": 0.00011742395316278479,
      "loss": 0.4811,
      "step": 3249
    },
    {
      "epoch": 0.4133545310015898,
      "grad_norm": 2.0725603103637695,
      "learning_rate": 0.00011739849815451192,
      "loss": 0.6358,
      "step": 3250
    },
    {
      "epoch": 0.4134817170111288,
      "grad_norm": 1.8421189785003662,
      "learning_rate": 0.00011737304314623903,
      "loss": 0.687,
      "step": 3251
    },
    {
      "epoch": 0.4136089030206677,
      "grad_norm": 1.7499245405197144,
      "learning_rate": 0.00011734758813796616,
      "loss": 0.9935,
      "step": 3252
    },
    {
      "epoch": 0.4137360890302067,
      "grad_norm": 2.1567206382751465,
      "learning_rate": 0.00011732213312969326,
      "loss": 0.675,
      "step": 3253
    },
    {
      "epoch": 0.4138632750397456,
      "grad_norm": 2.1137073040008545,
      "learning_rate": 0.0001172966781214204,
      "loss": 0.6118,
      "step": 3254
    },
    {
      "epoch": 0.41399046104928455,
      "grad_norm": 1.9751569032669067,
      "learning_rate": 0.00011727122311314751,
      "loss": 0.5905,
      "step": 3255
    },
    {
      "epoch": 0.41411764705882353,
      "grad_norm": 1.949123501777649,
      "learning_rate": 0.00011724576810487465,
      "loss": 0.6256,
      "step": 3256
    },
    {
      "epoch": 0.41424483306836246,
      "grad_norm": 2.81388258934021,
      "learning_rate": 0.00011722031309660175,
      "loss": 0.6765,
      "step": 3257
    },
    {
      "epoch": 0.41437201907790144,
      "grad_norm": 1.933489203453064,
      "learning_rate": 0.00011719485808832888,
      "loss": 0.8033,
      "step": 3258
    },
    {
      "epoch": 0.41449920508744037,
      "grad_norm": 1.9901105165481567,
      "learning_rate": 0.00011716940308005602,
      "loss": 0.6923,
      "step": 3259
    },
    {
      "epoch": 0.41462639109697935,
      "grad_norm": 2.2443816661834717,
      "learning_rate": 0.00011714394807178312,
      "loss": 0.6439,
      "step": 3260
    },
    {
      "epoch": 0.4147535771065183,
      "grad_norm": 1.724233627319336,
      "learning_rate": 0.00011711849306351025,
      "loss": 0.655,
      "step": 3261
    },
    {
      "epoch": 0.41488076311605726,
      "grad_norm": 1.91912043094635,
      "learning_rate": 0.00011709303805523737,
      "loss": 0.8215,
      "step": 3262
    },
    {
      "epoch": 0.4150079491255962,
      "grad_norm": 2.0287210941314697,
      "learning_rate": 0.0001170675830469645,
      "loss": 0.7899,
      "step": 3263
    },
    {
      "epoch": 0.4151351351351351,
      "grad_norm": 1.5976520776748657,
      "learning_rate": 0.00011704212803869161,
      "loss": 0.6326,
      "step": 3264
    },
    {
      "epoch": 0.4152623211446741,
      "grad_norm": 2.59128737449646,
      "learning_rate": 0.00011701667303041874,
      "loss": 0.7382,
      "step": 3265
    },
    {
      "epoch": 0.415389507154213,
      "grad_norm": 1.4331119060516357,
      "learning_rate": 0.00011699121802214586,
      "loss": 0.5279,
      "step": 3266
    },
    {
      "epoch": 0.415516693163752,
      "grad_norm": 2.5077381134033203,
      "learning_rate": 0.00011696576301387299,
      "loss": 0.8623,
      "step": 3267
    },
    {
      "epoch": 0.41564387917329093,
      "grad_norm": 1.6313436031341553,
      "learning_rate": 0.0001169403080056001,
      "loss": 0.6917,
      "step": 3268
    },
    {
      "epoch": 0.4157710651828299,
      "grad_norm": 2.0650601387023926,
      "learning_rate": 0.00011691485299732723,
      "loss": 0.7138,
      "step": 3269
    },
    {
      "epoch": 0.41589825119236884,
      "grad_norm": 1.8939770460128784,
      "learning_rate": 0.00011688939798905436,
      "loss": 0.6623,
      "step": 3270
    },
    {
      "epoch": 0.41602543720190777,
      "grad_norm": 2.305612802505493,
      "learning_rate": 0.00011686394298078147,
      "loss": 0.7135,
      "step": 3271
    },
    {
      "epoch": 0.41615262321144675,
      "grad_norm": 2.9398975372314453,
      "learning_rate": 0.0001168384879725086,
      "loss": 1.0018,
      "step": 3272
    },
    {
      "epoch": 0.4162798092209857,
      "grad_norm": 1.8621082305908203,
      "learning_rate": 0.00011681303296423572,
      "loss": 0.8033,
      "step": 3273
    },
    {
      "epoch": 0.41640699523052466,
      "grad_norm": 1.8702298402786255,
      "learning_rate": 0.00011678757795596285,
      "loss": 0.8565,
      "step": 3274
    },
    {
      "epoch": 0.4165341812400636,
      "grad_norm": 2.429628849029541,
      "learning_rate": 0.00011676212294768996,
      "loss": 0.6477,
      "step": 3275
    },
    {
      "epoch": 0.41666136724960257,
      "grad_norm": 2.045219659805298,
      "learning_rate": 0.00011673666793941709,
      "loss": 0.6971,
      "step": 3276
    },
    {
      "epoch": 0.4167885532591415,
      "grad_norm": 2.8503615856170654,
      "learning_rate": 0.0001167112129311442,
      "loss": 0.773,
      "step": 3277
    },
    {
      "epoch": 0.4169157392686804,
      "grad_norm": 3.523071527481079,
      "learning_rate": 0.00011668575792287133,
      "loss": 0.5473,
      "step": 3278
    },
    {
      "epoch": 0.4170429252782194,
      "grad_norm": 2.8313004970550537,
      "learning_rate": 0.00011666030291459846,
      "loss": 0.7925,
      "step": 3279
    },
    {
      "epoch": 0.4171701112877583,
      "grad_norm": 1.8160456418991089,
      "learning_rate": 0.00011663484790632558,
      "loss": 0.7715,
      "step": 3280
    },
    {
      "epoch": 0.4172972972972973,
      "grad_norm": 3.0832936763763428,
      "learning_rate": 0.00011660939289805271,
      "loss": 0.6097,
      "step": 3281
    },
    {
      "epoch": 0.41742448330683624,
      "grad_norm": 2.19941782951355,
      "learning_rate": 0.00011658393788977981,
      "loss": 1.0799,
      "step": 3282
    },
    {
      "epoch": 0.4175516693163752,
      "grad_norm": 2.1999218463897705,
      "learning_rate": 0.00011655848288150695,
      "loss": 0.6083,
      "step": 3283
    },
    {
      "epoch": 0.41767885532591414,
      "grad_norm": 2.72477126121521,
      "learning_rate": 0.00011653302787323405,
      "loss": 0.781,
      "step": 3284
    },
    {
      "epoch": 0.4178060413354531,
      "grad_norm": 1.6652191877365112,
      "learning_rate": 0.00011650757286496118,
      "loss": 0.5194,
      "step": 3285
    },
    {
      "epoch": 0.41793322734499205,
      "grad_norm": 1.7576550245285034,
      "learning_rate": 0.0001164821178566883,
      "loss": 0.6438,
      "step": 3286
    },
    {
      "epoch": 0.418060413354531,
      "grad_norm": 1.2370994091033936,
      "learning_rate": 0.00011645666284841544,
      "loss": 0.4587,
      "step": 3287
    },
    {
      "epoch": 0.41818759936406996,
      "grad_norm": 2.0685157775878906,
      "learning_rate": 0.00011643120784014257,
      "loss": 0.8622,
      "step": 3288
    },
    {
      "epoch": 0.4183147853736089,
      "grad_norm": 2.1859023571014404,
      "learning_rate": 0.00011640575283186967,
      "loss": 0.9277,
      "step": 3289
    },
    {
      "epoch": 0.41844197138314787,
      "grad_norm": 1.7253344058990479,
      "learning_rate": 0.0001163802978235968,
      "loss": 0.8399,
      "step": 3290
    },
    {
      "epoch": 0.4185691573926868,
      "grad_norm": 2.0898759365081787,
      "learning_rate": 0.00011635484281532391,
      "loss": 0.789,
      "step": 3291
    },
    {
      "epoch": 0.4186963434022258,
      "grad_norm": 1.9090335369110107,
      "learning_rate": 0.00011632938780705104,
      "loss": 0.4982,
      "step": 3292
    },
    {
      "epoch": 0.4188235294117647,
      "grad_norm": 1.9696965217590332,
      "learning_rate": 0.00011630393279877816,
      "loss": 0.9885,
      "step": 3293
    },
    {
      "epoch": 0.41895071542130363,
      "grad_norm": 2.0540823936462402,
      "learning_rate": 0.0001162784777905053,
      "loss": 0.5724,
      "step": 3294
    },
    {
      "epoch": 0.4190779014308426,
      "grad_norm": 1.9716615676879883,
      "learning_rate": 0.0001162530227822324,
      "loss": 0.8028,
      "step": 3295
    },
    {
      "epoch": 0.41920508744038154,
      "grad_norm": 1.4665461778640747,
      "learning_rate": 0.00011622756777395953,
      "loss": 0.5316,
      "step": 3296
    },
    {
      "epoch": 0.4193322734499205,
      "grad_norm": 2.2551193237304688,
      "learning_rate": 0.00011620211276568665,
      "loss": 0.8136,
      "step": 3297
    },
    {
      "epoch": 0.41945945945945945,
      "grad_norm": 2.315147876739502,
      "learning_rate": 0.00011617665775741378,
      "loss": 0.8302,
      "step": 3298
    },
    {
      "epoch": 0.41958664546899843,
      "grad_norm": 2.157982349395752,
      "learning_rate": 0.00011615120274914091,
      "loss": 0.5936,
      "step": 3299
    },
    {
      "epoch": 0.41971383147853736,
      "grad_norm": 1.3833081722259521,
      "learning_rate": 0.00011612574774086802,
      "loss": 0.4612,
      "step": 3300
    },
    {
      "epoch": 0.4198410174880763,
      "grad_norm": 2.570343494415283,
      "learning_rate": 0.00011610029273259515,
      "loss": 0.7508,
      "step": 3301
    },
    {
      "epoch": 0.41996820349761527,
      "grad_norm": 1.6436036825180054,
      "learning_rate": 0.00011607483772432226,
      "loss": 0.9313,
      "step": 3302
    },
    {
      "epoch": 0.4200953895071542,
      "grad_norm": 2.108375072479248,
      "learning_rate": 0.00011604938271604939,
      "loss": 0.5587,
      "step": 3303
    },
    {
      "epoch": 0.4202225755166932,
      "grad_norm": 2.508948802947998,
      "learning_rate": 0.00011602392770777651,
      "loss": 0.6753,
      "step": 3304
    },
    {
      "epoch": 0.4203497615262321,
      "grad_norm": 2.669994831085205,
      "learning_rate": 0.00011599847269950364,
      "loss": 0.6477,
      "step": 3305
    },
    {
      "epoch": 0.4204769475357711,
      "grad_norm": 2.5035157203674316,
      "learning_rate": 0.00011597301769123075,
      "loss": 0.7563,
      "step": 3306
    },
    {
      "epoch": 0.42060413354531,
      "grad_norm": 2.0423569679260254,
      "learning_rate": 0.00011594756268295788,
      "loss": 0.818,
      "step": 3307
    },
    {
      "epoch": 0.420731319554849,
      "grad_norm": 2.3370654582977295,
      "learning_rate": 0.00011592210767468501,
      "loss": 0.665,
      "step": 3308
    },
    {
      "epoch": 0.4208585055643879,
      "grad_norm": 1.6687184572219849,
      "learning_rate": 0.00011589665266641212,
      "loss": 0.5976,
      "step": 3309
    },
    {
      "epoch": 0.42098569157392685,
      "grad_norm": 1.7474956512451172,
      "learning_rate": 0.00011587119765813925,
      "loss": 0.5005,
      "step": 3310
    },
    {
      "epoch": 0.42111287758346583,
      "grad_norm": 2.4829883575439453,
      "learning_rate": 0.00011584574264986637,
      "loss": 0.7738,
      "step": 3311
    },
    {
      "epoch": 0.42124006359300475,
      "grad_norm": 1.9253435134887695,
      "learning_rate": 0.0001158202876415935,
      "loss": 0.6188,
      "step": 3312
    },
    {
      "epoch": 0.42136724960254374,
      "grad_norm": 2.096332550048828,
      "learning_rate": 0.0001157948326333206,
      "loss": 1.0094,
      "step": 3313
    },
    {
      "epoch": 0.42149443561208266,
      "grad_norm": 1.6146107912063599,
      "learning_rate": 0.00011576937762504774,
      "loss": 0.3737,
      "step": 3314
    },
    {
      "epoch": 0.42162162162162165,
      "grad_norm": 2.3617055416107178,
      "learning_rate": 0.00011574392261677484,
      "loss": 0.4839,
      "step": 3315
    },
    {
      "epoch": 0.42174880763116057,
      "grad_norm": 1.9210000038146973,
      "learning_rate": 0.00011571846760850197,
      "loss": 0.5996,
      "step": 3316
    },
    {
      "epoch": 0.4218759936406995,
      "grad_norm": 2.265671491622925,
      "learning_rate": 0.0001156930126002291,
      "loss": 0.6345,
      "step": 3317
    },
    {
      "epoch": 0.4220031796502385,
      "grad_norm": 1.6864984035491943,
      "learning_rate": 0.00011566755759195623,
      "loss": 0.7263,
      "step": 3318
    },
    {
      "epoch": 0.4221303656597774,
      "grad_norm": 2.020756483078003,
      "learning_rate": 0.00011564210258368336,
      "loss": 0.5651,
      "step": 3319
    },
    {
      "epoch": 0.4222575516693164,
      "grad_norm": 1.6193217039108276,
      "learning_rate": 0.00011561664757541046,
      "loss": 0.5536,
      "step": 3320
    },
    {
      "epoch": 0.4223847376788553,
      "grad_norm": 2.4789061546325684,
      "learning_rate": 0.0001155911925671376,
      "loss": 0.6009,
      "step": 3321
    },
    {
      "epoch": 0.4225119236883943,
      "grad_norm": 2.7451348304748535,
      "learning_rate": 0.0001155657375588647,
      "loss": 0.663,
      "step": 3322
    },
    {
      "epoch": 0.4226391096979332,
      "grad_norm": 2.1592187881469727,
      "learning_rate": 0.00011554028255059183,
      "loss": 0.7703,
      "step": 3323
    },
    {
      "epoch": 0.42276629570747215,
      "grad_norm": 1.638642430305481,
      "learning_rate": 0.00011551482754231895,
      "loss": 0.4059,
      "step": 3324
    },
    {
      "epoch": 0.42289348171701113,
      "grad_norm": 1.9453773498535156,
      "learning_rate": 0.00011548937253404608,
      "loss": 0.8634,
      "step": 3325
    },
    {
      "epoch": 0.42302066772655006,
      "grad_norm": 2.8032803535461426,
      "learning_rate": 0.00011546391752577319,
      "loss": 0.8501,
      "step": 3326
    },
    {
      "epoch": 0.42314785373608904,
      "grad_norm": 3.8893425464630127,
      "learning_rate": 0.00011543846251750032,
      "loss": 0.9774,
      "step": 3327
    },
    {
      "epoch": 0.42327503974562797,
      "grad_norm": 2.3360934257507324,
      "learning_rate": 0.00011541300750922745,
      "loss": 0.8417,
      "step": 3328
    },
    {
      "epoch": 0.42340222575516695,
      "grad_norm": 1.9285496473312378,
      "learning_rate": 0.00011538755250095457,
      "loss": 0.5582,
      "step": 3329
    },
    {
      "epoch": 0.4235294117647059,
      "grad_norm": 1.9049144983291626,
      "learning_rate": 0.0001153620974926817,
      "loss": 0.6937,
      "step": 3330
    },
    {
      "epoch": 0.42365659777424486,
      "grad_norm": 2.7670371532440186,
      "learning_rate": 0.00011533664248440881,
      "loss": 0.7361,
      "step": 3331
    },
    {
      "epoch": 0.4237837837837838,
      "grad_norm": 1.916550636291504,
      "learning_rate": 0.00011531118747613594,
      "loss": 0.5833,
      "step": 3332
    },
    {
      "epoch": 0.4239109697933227,
      "grad_norm": 2.117373466491699,
      "learning_rate": 0.00011528573246786305,
      "loss": 0.7823,
      "step": 3333
    },
    {
      "epoch": 0.4240381558028617,
      "grad_norm": 2.541316032409668,
      "learning_rate": 0.00011526027745959018,
      "loss": 0.5654,
      "step": 3334
    },
    {
      "epoch": 0.4241653418124006,
      "grad_norm": 2.4373819828033447,
      "learning_rate": 0.0001152348224513173,
      "loss": 0.8579,
      "step": 3335
    },
    {
      "epoch": 0.4242925278219396,
      "grad_norm": 2.8466155529022217,
      "learning_rate": 0.00011520936744304443,
      "loss": 0.8067,
      "step": 3336
    },
    {
      "epoch": 0.42441971383147853,
      "grad_norm": 1.8073277473449707,
      "learning_rate": 0.00011518391243477156,
      "loss": 0.6501,
      "step": 3337
    },
    {
      "epoch": 0.4245468998410175,
      "grad_norm": 2.5385031700134277,
      "learning_rate": 0.00011515845742649867,
      "loss": 0.6946,
      "step": 3338
    },
    {
      "epoch": 0.42467408585055644,
      "grad_norm": 2.6794772148132324,
      "learning_rate": 0.0001151330024182258,
      "loss": 1.0212,
      "step": 3339
    },
    {
      "epoch": 0.42480127186009536,
      "grad_norm": 2.0430166721343994,
      "learning_rate": 0.0001151075474099529,
      "loss": 0.6107,
      "step": 3340
    },
    {
      "epoch": 0.42492845786963435,
      "grad_norm": 1.5543076992034912,
      "learning_rate": 0.00011508209240168004,
      "loss": 0.729,
      "step": 3341
    },
    {
      "epoch": 0.4250556438791733,
      "grad_norm": 1.80260169506073,
      "learning_rate": 0.00011505663739340716,
      "loss": 0.6183,
      "step": 3342
    },
    {
      "epoch": 0.42518282988871225,
      "grad_norm": 1.9929970502853394,
      "learning_rate": 0.00011503118238513429,
      "loss": 0.7411,
      "step": 3343
    },
    {
      "epoch": 0.4253100158982512,
      "grad_norm": 2.212294101715088,
      "learning_rate": 0.0001150057273768614,
      "loss": 0.551,
      "step": 3344
    },
    {
      "epoch": 0.42543720190779016,
      "grad_norm": 2.0615432262420654,
      "learning_rate": 0.00011498027236858853,
      "loss": 0.6194,
      "step": 3345
    },
    {
      "epoch": 0.4255643879173291,
      "grad_norm": 1.674743890762329,
      "learning_rate": 0.00011495481736031566,
      "loss": 0.4973,
      "step": 3346
    },
    {
      "epoch": 0.42569157392686807,
      "grad_norm": 2.2463722229003906,
      "learning_rate": 0.00011492936235204276,
      "loss": 0.595,
      "step": 3347
    },
    {
      "epoch": 0.425818759936407,
      "grad_norm": 1.771070957183838,
      "learning_rate": 0.0001149039073437699,
      "loss": 0.8256,
      "step": 3348
    },
    {
      "epoch": 0.4259459459459459,
      "grad_norm": 2.6777966022491455,
      "learning_rate": 0.00011487845233549701,
      "loss": 0.8023,
      "step": 3349
    },
    {
      "epoch": 0.4260731319554849,
      "grad_norm": 2.019334077835083,
      "learning_rate": 0.00011485299732722415,
      "loss": 0.7215,
      "step": 3350
    },
    {
      "epoch": 0.42620031796502383,
      "grad_norm": 2.571587324142456,
      "learning_rate": 0.00011482754231895125,
      "loss": 0.7882,
      "step": 3351
    },
    {
      "epoch": 0.4263275039745628,
      "grad_norm": 1.6669498682022095,
      "learning_rate": 0.00011480208731067838,
      "loss": 0.6727,
      "step": 3352
    },
    {
      "epoch": 0.42645468998410174,
      "grad_norm": 1.47750985622406,
      "learning_rate": 0.00011477663230240549,
      "loss": 0.5401,
      "step": 3353
    },
    {
      "epoch": 0.4265818759936407,
      "grad_norm": 1.775447964668274,
      "learning_rate": 0.00011475117729413262,
      "loss": 0.6726,
      "step": 3354
    },
    {
      "epoch": 0.42670906200317965,
      "grad_norm": 2.3540661334991455,
      "learning_rate": 0.00011472572228585974,
      "loss": 0.7383,
      "step": 3355
    },
    {
      "epoch": 0.4268362480127186,
      "grad_norm": 2.7082467079162598,
      "learning_rate": 0.00011470026727758687,
      "loss": 0.7015,
      "step": 3356
    },
    {
      "epoch": 0.42696343402225756,
      "grad_norm": 1.7969563007354736,
      "learning_rate": 0.000114674812269314,
      "loss": 0.5361,
      "step": 3357
    },
    {
      "epoch": 0.4270906200317965,
      "grad_norm": 1.9438273906707764,
      "learning_rate": 0.00011464935726104111,
      "loss": 0.877,
      "step": 3358
    },
    {
      "epoch": 0.42721780604133547,
      "grad_norm": 2.056330442428589,
      "learning_rate": 0.00011462390225276824,
      "loss": 0.6923,
      "step": 3359
    },
    {
      "epoch": 0.4273449920508744,
      "grad_norm": 2.3408777713775635,
      "learning_rate": 0.00011459844724449536,
      "loss": 0.6406,
      "step": 3360
    },
    {
      "epoch": 0.4274721780604134,
      "grad_norm": 2.4014365673065186,
      "learning_rate": 0.0001145729922362225,
      "loss": 0.8046,
      "step": 3361
    },
    {
      "epoch": 0.4275993640699523,
      "grad_norm": 2.2291414737701416,
      "learning_rate": 0.0001145475372279496,
      "loss": 0.6251,
      "step": 3362
    },
    {
      "epoch": 0.42772655007949123,
      "grad_norm": 2.6507604122161865,
      "learning_rate": 0.00011452208221967673,
      "loss": 0.6716,
      "step": 3363
    },
    {
      "epoch": 0.4278537360890302,
      "grad_norm": 2.2576003074645996,
      "learning_rate": 0.00011449662721140384,
      "loss": 0.4245,
      "step": 3364
    },
    {
      "epoch": 0.42798092209856914,
      "grad_norm": 2.525625705718994,
      "learning_rate": 0.00011447117220313097,
      "loss": 0.8861,
      "step": 3365
    },
    {
      "epoch": 0.4281081081081081,
      "grad_norm": 2.3890624046325684,
      "learning_rate": 0.0001144457171948581,
      "loss": 0.8287,
      "step": 3366
    },
    {
      "epoch": 0.42823529411764705,
      "grad_norm": 1.9380971193313599,
      "learning_rate": 0.00011442026218658522,
      "loss": 0.7436,
      "step": 3367
    },
    {
      "epoch": 0.42836248012718603,
      "grad_norm": 1.3349604606628418,
      "learning_rate": 0.00011439480717831235,
      "loss": 0.5076,
      "step": 3368
    },
    {
      "epoch": 0.42848966613672496,
      "grad_norm": 2.495190143585205,
      "learning_rate": 0.00011436935217003946,
      "loss": 0.5366,
      "step": 3369
    },
    {
      "epoch": 0.42861685214626394,
      "grad_norm": 1.9031500816345215,
      "learning_rate": 0.00011434389716176659,
      "loss": 0.5845,
      "step": 3370
    },
    {
      "epoch": 0.42874403815580286,
      "grad_norm": 1.795367956161499,
      "learning_rate": 0.0001143184421534937,
      "loss": 0.6197,
      "step": 3371
    },
    {
      "epoch": 0.4288712241653418,
      "grad_norm": 2.006507396697998,
      "learning_rate": 0.00011429298714522083,
      "loss": 0.8248,
      "step": 3372
    },
    {
      "epoch": 0.4289984101748808,
      "grad_norm": 2.2872474193573,
      "learning_rate": 0.00011426753213694795,
      "loss": 0.7681,
      "step": 3373
    },
    {
      "epoch": 0.4291255961844197,
      "grad_norm": 1.9785088300704956,
      "learning_rate": 0.00011424207712867508,
      "loss": 0.6463,
      "step": 3374
    },
    {
      "epoch": 0.4292527821939587,
      "grad_norm": 2.00521183013916,
      "learning_rate": 0.00011421662212040218,
      "loss": 0.606,
      "step": 3375
    },
    {
      "epoch": 0.4293799682034976,
      "grad_norm": 2.3176095485687256,
      "learning_rate": 0.00011419116711212932,
      "loss": 0.4399,
      "step": 3376
    },
    {
      "epoch": 0.4295071542130366,
      "grad_norm": 1.9315205812454224,
      "learning_rate": 0.00011416571210385645,
      "loss": 0.637,
      "step": 3377
    },
    {
      "epoch": 0.4296343402225755,
      "grad_norm": 2.1262757778167725,
      "learning_rate": 0.00011414025709558355,
      "loss": 0.9138,
      "step": 3378
    },
    {
      "epoch": 0.42976152623211444,
      "grad_norm": 1.5602679252624512,
      "learning_rate": 0.00011411480208731069,
      "loss": 0.5217,
      "step": 3379
    },
    {
      "epoch": 0.4298887122416534,
      "grad_norm": 2.1608660221099854,
      "learning_rate": 0.0001140893470790378,
      "loss": 0.8525,
      "step": 3380
    },
    {
      "epoch": 0.43001589825119235,
      "grad_norm": 2.49700927734375,
      "learning_rate": 0.00011406389207076494,
      "loss": 0.8803,
      "step": 3381
    },
    {
      "epoch": 0.43014308426073133,
      "grad_norm": 2.2729039192199707,
      "learning_rate": 0.00011403843706249204,
      "loss": 0.5515,
      "step": 3382
    },
    {
      "epoch": 0.43027027027027026,
      "grad_norm": 2.7891757488250732,
      "learning_rate": 0.00011401298205421917,
      "loss": 0.6367,
      "step": 3383
    },
    {
      "epoch": 0.43039745627980924,
      "grad_norm": 2.0519440174102783,
      "learning_rate": 0.00011398752704594628,
      "loss": 0.75,
      "step": 3384
    },
    {
      "epoch": 0.43052464228934817,
      "grad_norm": 1.9399892091751099,
      "learning_rate": 0.00011396207203767341,
      "loss": 0.7446,
      "step": 3385
    },
    {
      "epoch": 0.4306518282988871,
      "grad_norm": 2.100978374481201,
      "learning_rate": 0.00011393661702940054,
      "loss": 0.5914,
      "step": 3386
    },
    {
      "epoch": 0.4307790143084261,
      "grad_norm": 2.4797046184539795,
      "learning_rate": 0.00011391116202112766,
      "loss": 0.7108,
      "step": 3387
    },
    {
      "epoch": 0.430906200317965,
      "grad_norm": 2.6668612957000732,
      "learning_rate": 0.0001138857070128548,
      "loss": 0.8204,
      "step": 3388
    },
    {
      "epoch": 0.431033386327504,
      "grad_norm": 1.7253732681274414,
      "learning_rate": 0.0001138602520045819,
      "loss": 0.6676,
      "step": 3389
    },
    {
      "epoch": 0.4311605723370429,
      "grad_norm": 2.172804832458496,
      "learning_rate": 0.00011383479699630903,
      "loss": 0.7692,
      "step": 3390
    },
    {
      "epoch": 0.4312877583465819,
      "grad_norm": 1.758162498474121,
      "learning_rate": 0.00011380934198803615,
      "loss": 0.6041,
      "step": 3391
    },
    {
      "epoch": 0.4314149443561208,
      "grad_norm": 2.174175500869751,
      "learning_rate": 0.00011378388697976328,
      "loss": 0.7565,
      "step": 3392
    },
    {
      "epoch": 0.4315421303656598,
      "grad_norm": 1.9630918502807617,
      "learning_rate": 0.00011375843197149039,
      "loss": 0.7003,
      "step": 3393
    },
    {
      "epoch": 0.43166931637519873,
      "grad_norm": 1.7362563610076904,
      "learning_rate": 0.00011373297696321752,
      "loss": 0.472,
      "step": 3394
    },
    {
      "epoch": 0.43179650238473766,
      "grad_norm": 1.7660077810287476,
      "learning_rate": 0.00011370752195494465,
      "loss": 0.6428,
      "step": 3395
    },
    {
      "epoch": 0.43192368839427664,
      "grad_norm": 2.416661262512207,
      "learning_rate": 0.00011368206694667176,
      "loss": 1.0176,
      "step": 3396
    },
    {
      "epoch": 0.43205087440381557,
      "grad_norm": 1.7404096126556396,
      "learning_rate": 0.00011365661193839889,
      "loss": 0.5431,
      "step": 3397
    },
    {
      "epoch": 0.43217806041335455,
      "grad_norm": 2.8440933227539062,
      "learning_rate": 0.00011363115693012601,
      "loss": 1.0428,
      "step": 3398
    },
    {
      "epoch": 0.4323052464228935,
      "grad_norm": 1.6406755447387695,
      "learning_rate": 0.00011360570192185314,
      "loss": 0.6824,
      "step": 3399
    },
    {
      "epoch": 0.43243243243243246,
      "grad_norm": 2.5167930126190186,
      "learning_rate": 0.00011358024691358025,
      "loss": 0.8946,
      "step": 3400
    },
    {
      "epoch": 0.4325596184419714,
      "grad_norm": 2.1438868045806885,
      "learning_rate": 0.00011355479190530738,
      "loss": 0.8132,
      "step": 3401
    },
    {
      "epoch": 0.4326868044515103,
      "grad_norm": 1.9818072319030762,
      "learning_rate": 0.00011352933689703448,
      "loss": 0.6045,
      "step": 3402
    },
    {
      "epoch": 0.4328139904610493,
      "grad_norm": 1.7686861753463745,
      "learning_rate": 0.00011350388188876162,
      "loss": 0.6388,
      "step": 3403
    },
    {
      "epoch": 0.4329411764705882,
      "grad_norm": 2.814687490463257,
      "learning_rate": 0.00011347842688048873,
      "loss": 0.7343,
      "step": 3404
    },
    {
      "epoch": 0.4330683624801272,
      "grad_norm": 2.07194185256958,
      "learning_rate": 0.00011345297187221587,
      "loss": 0.6324,
      "step": 3405
    },
    {
      "epoch": 0.4331955484896661,
      "grad_norm": 2.236794948577881,
      "learning_rate": 0.000113427516863943,
      "loss": 0.6487,
      "step": 3406
    },
    {
      "epoch": 0.4333227344992051,
      "grad_norm": 2.0670652389526367,
      "learning_rate": 0.0001134020618556701,
      "loss": 0.5863,
      "step": 3407
    },
    {
      "epoch": 0.43344992050874404,
      "grad_norm": 2.3001961708068848,
      "learning_rate": 0.00011337660684739724,
      "loss": 1.04,
      "step": 3408
    },
    {
      "epoch": 0.43357710651828296,
      "grad_norm": 2.434992790222168,
      "learning_rate": 0.00011335115183912434,
      "loss": 0.6849,
      "step": 3409
    },
    {
      "epoch": 0.43370429252782194,
      "grad_norm": 2.5382649898529053,
      "learning_rate": 0.00011332569683085147,
      "loss": 0.9513,
      "step": 3410
    },
    {
      "epoch": 0.43383147853736087,
      "grad_norm": 2.540154218673706,
      "learning_rate": 0.00011330024182257859,
      "loss": 0.8369,
      "step": 3411
    },
    {
      "epoch": 0.43395866454689985,
      "grad_norm": 1.58306884765625,
      "learning_rate": 0.00011327478681430573,
      "loss": 0.6188,
      "step": 3412
    },
    {
      "epoch": 0.4340858505564388,
      "grad_norm": 2.5197031497955322,
      "learning_rate": 0.00011324933180603283,
      "loss": 0.5577,
      "step": 3413
    },
    {
      "epoch": 0.43421303656597776,
      "grad_norm": 2.8637564182281494,
      "learning_rate": 0.00011322387679775996,
      "loss": 0.8871,
      "step": 3414
    },
    {
      "epoch": 0.4343402225755167,
      "grad_norm": 2.4126806259155273,
      "learning_rate": 0.0001131984217894871,
      "loss": 0.5208,
      "step": 3415
    },
    {
      "epoch": 0.43446740858505567,
      "grad_norm": 1.9460142850875854,
      "learning_rate": 0.0001131729667812142,
      "loss": 1.0445,
      "step": 3416
    },
    {
      "epoch": 0.4345945945945946,
      "grad_norm": 1.9613373279571533,
      "learning_rate": 0.00011314751177294133,
      "loss": 0.7582,
      "step": 3417
    },
    {
      "epoch": 0.4347217806041335,
      "grad_norm": 2.0106117725372314,
      "learning_rate": 0.00011312205676466845,
      "loss": 0.6404,
      "step": 3418
    },
    {
      "epoch": 0.4348489666136725,
      "grad_norm": 1.8790901899337769,
      "learning_rate": 0.00011309660175639558,
      "loss": 0.7388,
      "step": 3419
    },
    {
      "epoch": 0.43497615262321143,
      "grad_norm": 2.0954062938690186,
      "learning_rate": 0.00011307114674812269,
      "loss": 0.8446,
      "step": 3420
    },
    {
      "epoch": 0.4351033386327504,
      "grad_norm": 2.4273860454559326,
      "learning_rate": 0.00011304569173984982,
      "loss": 0.937,
      "step": 3421
    },
    {
      "epoch": 0.43523052464228934,
      "grad_norm": 1.2818865776062012,
      "learning_rate": 0.00011302023673157694,
      "loss": 0.5798,
      "step": 3422
    },
    {
      "epoch": 0.4353577106518283,
      "grad_norm": 1.8447836637496948,
      "learning_rate": 0.00011299478172330407,
      "loss": 0.4565,
      "step": 3423
    },
    {
      "epoch": 0.43548489666136725,
      "grad_norm": 1.8133304119110107,
      "learning_rate": 0.0001129693267150312,
      "loss": 0.4841,
      "step": 3424
    },
    {
      "epoch": 0.4356120826709062,
      "grad_norm": 1.9438763856887817,
      "learning_rate": 0.00011294387170675831,
      "loss": 0.6797,
      "step": 3425
    },
    {
      "epoch": 0.43573926868044516,
      "grad_norm": 2.6533894538879395,
      "learning_rate": 0.00011291841669848544,
      "loss": 0.6006,
      "step": 3426
    },
    {
      "epoch": 0.4358664546899841,
      "grad_norm": 1.923101544380188,
      "learning_rate": 0.00011289296169021255,
      "loss": 0.488,
      "step": 3427
    },
    {
      "epoch": 0.43599364069952307,
      "grad_norm": 2.470763921737671,
      "learning_rate": 0.00011286750668193968,
      "loss": 0.8876,
      "step": 3428
    },
    {
      "epoch": 0.436120826709062,
      "grad_norm": 3.007856607437134,
      "learning_rate": 0.0001128420516736668,
      "loss": 0.6393,
      "step": 3429
    },
    {
      "epoch": 0.436248012718601,
      "grad_norm": 1.870757818222046,
      "learning_rate": 0.00011281659666539393,
      "loss": 0.7177,
      "step": 3430
    },
    {
      "epoch": 0.4363751987281399,
      "grad_norm": 2.5306849479675293,
      "learning_rate": 0.00011279114165712104,
      "loss": 0.7192,
      "step": 3431
    },
    {
      "epoch": 0.43650238473767883,
      "grad_norm": 2.7161262035369873,
      "learning_rate": 0.00011276568664884817,
      "loss": 0.6903,
      "step": 3432
    },
    {
      "epoch": 0.4366295707472178,
      "grad_norm": 2.4690966606140137,
      "learning_rate": 0.00011274023164057527,
      "loss": 0.6404,
      "step": 3433
    },
    {
      "epoch": 0.43675675675675674,
      "grad_norm": 2.5690934658050537,
      "learning_rate": 0.0001127147766323024,
      "loss": 0.8099,
      "step": 3434
    },
    {
      "epoch": 0.4368839427662957,
      "grad_norm": 1.9002916812896729,
      "learning_rate": 0.00011268932162402954,
      "loss": 0.6217,
      "step": 3435
    },
    {
      "epoch": 0.43701112877583465,
      "grad_norm": 2.294320583343506,
      "learning_rate": 0.00011266386661575666,
      "loss": 0.6224,
      "step": 3436
    },
    {
      "epoch": 0.4371383147853736,
      "grad_norm": 2.3175528049468994,
      "learning_rate": 0.00011263841160748379,
      "loss": 0.8064,
      "step": 3437
    },
    {
      "epoch": 0.43726550079491255,
      "grad_norm": 2.384397029876709,
      "learning_rate": 0.0001126129565992109,
      "loss": 0.5363,
      "step": 3438
    },
    {
      "epoch": 0.43739268680445154,
      "grad_norm": 1.6753908395767212,
      "learning_rate": 0.00011258750159093803,
      "loss": 0.6437,
      "step": 3439
    },
    {
      "epoch": 0.43751987281399046,
      "grad_norm": 1.7694700956344604,
      "learning_rate": 0.00011256204658266513,
      "loss": 0.4234,
      "step": 3440
    },
    {
      "epoch": 0.4376470588235294,
      "grad_norm": 1.7369444370269775,
      "learning_rate": 0.00011253659157439226,
      "loss": 0.5765,
      "step": 3441
    },
    {
      "epoch": 0.43777424483306837,
      "grad_norm": 1.5612376928329468,
      "learning_rate": 0.00011251113656611938,
      "loss": 0.3858,
      "step": 3442
    },
    {
      "epoch": 0.4379014308426073,
      "grad_norm": 2.4966847896575928,
      "learning_rate": 0.00011248568155784651,
      "loss": 0.6923,
      "step": 3443
    },
    {
      "epoch": 0.4380286168521463,
      "grad_norm": 2.0956473350524902,
      "learning_rate": 0.00011246022654957365,
      "loss": 0.7879,
      "step": 3444
    },
    {
      "epoch": 0.4381558028616852,
      "grad_norm": 2.5715889930725098,
      "learning_rate": 0.00011243477154130075,
      "loss": 0.7584,
      "step": 3445
    },
    {
      "epoch": 0.4382829888712242,
      "grad_norm": 2.634080648422241,
      "learning_rate": 0.00011240931653302788,
      "loss": 0.622,
      "step": 3446
    },
    {
      "epoch": 0.4384101748807631,
      "grad_norm": 1.6436939239501953,
      "learning_rate": 0.00011238386152475499,
      "loss": 0.4409,
      "step": 3447
    },
    {
      "epoch": 0.43853736089030204,
      "grad_norm": 2.972093105316162,
      "learning_rate": 0.00011235840651648212,
      "loss": 0.8094,
      "step": 3448
    },
    {
      "epoch": 0.438664546899841,
      "grad_norm": 2.7239534854888916,
      "learning_rate": 0.00011233295150820924,
      "loss": 0.6425,
      "step": 3449
    },
    {
      "epoch": 0.43879173290937995,
      "grad_norm": 2.503178119659424,
      "learning_rate": 0.00011230749649993637,
      "loss": 0.6897,
      "step": 3450
    },
    {
      "epoch": 0.43891891891891893,
      "grad_norm": 1.9372833967208862,
      "learning_rate": 0.00011228204149166348,
      "loss": 0.6883,
      "step": 3451
    },
    {
      "epoch": 0.43904610492845786,
      "grad_norm": 1.8986424207687378,
      "learning_rate": 0.00011225658648339061,
      "loss": 0.6251,
      "step": 3452
    },
    {
      "epoch": 0.43917329093799684,
      "grad_norm": 1.8093489408493042,
      "learning_rate": 0.00011223113147511774,
      "loss": 0.6596,
      "step": 3453
    },
    {
      "epoch": 0.43930047694753577,
      "grad_norm": 1.5862691402435303,
      "learning_rate": 0.00011220567646684486,
      "loss": 0.5234,
      "step": 3454
    },
    {
      "epoch": 0.43942766295707475,
      "grad_norm": 2.167039155960083,
      "learning_rate": 0.000112180221458572,
      "loss": 0.4489,
      "step": 3455
    },
    {
      "epoch": 0.4395548489666137,
      "grad_norm": 3.1143717765808105,
      "learning_rate": 0.0001121547664502991,
      "loss": 0.6752,
      "step": 3456
    },
    {
      "epoch": 0.4396820349761526,
      "grad_norm": 2.149559497833252,
      "learning_rate": 0.00011212931144202623,
      "loss": 0.8806,
      "step": 3457
    },
    {
      "epoch": 0.4398092209856916,
      "grad_norm": 2.664686441421509,
      "learning_rate": 0.00011210385643375334,
      "loss": 0.7581,
      "step": 3458
    },
    {
      "epoch": 0.4399364069952305,
      "grad_norm": 2.4211807250976562,
      "learning_rate": 0.00011207840142548047,
      "loss": 0.5885,
      "step": 3459
    },
    {
      "epoch": 0.4400635930047695,
      "grad_norm": 1.9268691539764404,
      "learning_rate": 0.00011205294641720759,
      "loss": 0.592,
      "step": 3460
    },
    {
      "epoch": 0.4401907790143084,
      "grad_norm": 2.409348726272583,
      "learning_rate": 0.00011202749140893472,
      "loss": 0.9441,
      "step": 3461
    },
    {
      "epoch": 0.4403179650238474,
      "grad_norm": 1.5582458972930908,
      "learning_rate": 0.00011200203640066183,
      "loss": 0.6821,
      "step": 3462
    },
    {
      "epoch": 0.44044515103338633,
      "grad_norm": 1.6591495275497437,
      "learning_rate": 0.00011197658139238896,
      "loss": 0.6583,
      "step": 3463
    },
    {
      "epoch": 0.44057233704292526,
      "grad_norm": 2.1028761863708496,
      "learning_rate": 0.00011195112638411609,
      "loss": 0.9539,
      "step": 3464
    },
    {
      "epoch": 0.44069952305246424,
      "grad_norm": 1.880691409111023,
      "learning_rate": 0.0001119256713758432,
      "loss": 0.5439,
      "step": 3465
    },
    {
      "epoch": 0.44082670906200316,
      "grad_norm": 2.2044546604156494,
      "learning_rate": 0.00011190021636757033,
      "loss": 0.6875,
      "step": 3466
    },
    {
      "epoch": 0.44095389507154215,
      "grad_norm": 1.9972773790359497,
      "learning_rate": 0.00011187476135929745,
      "loss": 0.5069,
      "step": 3467
    },
    {
      "epoch": 0.4410810810810811,
      "grad_norm": 2.414515256881714,
      "learning_rate": 0.00011184930635102458,
      "loss": 0.5469,
      "step": 3468
    },
    {
      "epoch": 0.44120826709062005,
      "grad_norm": 2.118495464324951,
      "learning_rate": 0.00011182385134275168,
      "loss": 0.748,
      "step": 3469
    },
    {
      "epoch": 0.441335453100159,
      "grad_norm": 2.5278313159942627,
      "learning_rate": 0.00011179839633447882,
      "loss": 0.6365,
      "step": 3470
    },
    {
      "epoch": 0.4414626391096979,
      "grad_norm": 3.1278560161590576,
      "learning_rate": 0.00011177294132620592,
      "loss": 0.7391,
      "step": 3471
    },
    {
      "epoch": 0.4415898251192369,
      "grad_norm": 2.740715742111206,
      "learning_rate": 0.00011174748631793305,
      "loss": 0.5918,
      "step": 3472
    },
    {
      "epoch": 0.4417170111287758,
      "grad_norm": 1.6722676753997803,
      "learning_rate": 0.00011172203130966019,
      "loss": 0.7441,
      "step": 3473
    },
    {
      "epoch": 0.4418441971383148,
      "grad_norm": 1.392735481262207,
      "learning_rate": 0.0001116965763013873,
      "loss": 0.5646,
      "step": 3474
    },
    {
      "epoch": 0.4419713831478537,
      "grad_norm": 1.6122703552246094,
      "learning_rate": 0.00011167112129311444,
      "loss": 0.5612,
      "step": 3475
    },
    {
      "epoch": 0.4420985691573927,
      "grad_norm": 2.566969156265259,
      "learning_rate": 0.00011164566628484154,
      "loss": 0.7193,
      "step": 3476
    },
    {
      "epoch": 0.44222575516693163,
      "grad_norm": 2.0582828521728516,
      "learning_rate": 0.00011162021127656867,
      "loss": 0.7275,
      "step": 3477
    },
    {
      "epoch": 0.4423529411764706,
      "grad_norm": 1.7465702295303345,
      "learning_rate": 0.00011159475626829579,
      "loss": 0.6361,
      "step": 3478
    },
    {
      "epoch": 0.44248012718600954,
      "grad_norm": 2.3404343128204346,
      "learning_rate": 0.00011156930126002292,
      "loss": 0.536,
      "step": 3479
    },
    {
      "epoch": 0.44260731319554847,
      "grad_norm": 1.271968960762024,
      "learning_rate": 0.00011154384625175003,
      "loss": 0.3428,
      "step": 3480
    },
    {
      "epoch": 0.44273449920508745,
      "grad_norm": 2.3145766258239746,
      "learning_rate": 0.00011151839124347716,
      "loss": 0.9864,
      "step": 3481
    },
    {
      "epoch": 0.4428616852146264,
      "grad_norm": 2.203444719314575,
      "learning_rate": 0.0001114929362352043,
      "loss": 0.7565,
      "step": 3482
    },
    {
      "epoch": 0.44298887122416536,
      "grad_norm": 2.0381112098693848,
      "learning_rate": 0.0001114674812269314,
      "loss": 0.6968,
      "step": 3483
    },
    {
      "epoch": 0.4431160572337043,
      "grad_norm": 1.7705186605453491,
      "learning_rate": 0.00011144202621865853,
      "loss": 0.8476,
      "step": 3484
    },
    {
      "epoch": 0.44324324324324327,
      "grad_norm": 2.5551183223724365,
      "learning_rate": 0.00011141657121038565,
      "loss": 0.5052,
      "step": 3485
    },
    {
      "epoch": 0.4433704292527822,
      "grad_norm": 1.4649831056594849,
      "learning_rate": 0.00011139111620211278,
      "loss": 0.5137,
      "step": 3486
    },
    {
      "epoch": 0.4434976152623211,
      "grad_norm": 1.6246734857559204,
      "learning_rate": 0.00011136566119383989,
      "loss": 0.6756,
      "step": 3487
    },
    {
      "epoch": 0.4436248012718601,
      "grad_norm": 1.934891700744629,
      "learning_rate": 0.00011134020618556702,
      "loss": 0.8929,
      "step": 3488
    },
    {
      "epoch": 0.44375198728139903,
      "grad_norm": 2.0489399433135986,
      "learning_rate": 0.00011131475117729413,
      "loss": 0.7498,
      "step": 3489
    },
    {
      "epoch": 0.443879173290938,
      "grad_norm": 1.9392695426940918,
      "learning_rate": 0.00011128929616902126,
      "loss": 0.8275,
      "step": 3490
    },
    {
      "epoch": 0.44400635930047694,
      "grad_norm": 2.4748220443725586,
      "learning_rate": 0.00011126384116074838,
      "loss": 0.6995,
      "step": 3491
    },
    {
      "epoch": 0.4441335453100159,
      "grad_norm": 1.4239888191223145,
      "learning_rate": 0.00011123838615247551,
      "loss": 0.5704,
      "step": 3492
    },
    {
      "epoch": 0.44426073131955485,
      "grad_norm": 1.5437774658203125,
      "learning_rate": 0.00011121293114420264,
      "loss": 0.6094,
      "step": 3493
    },
    {
      "epoch": 0.4443879173290938,
      "grad_norm": 2.504096508026123,
      "learning_rate": 0.00011118747613592975,
      "loss": 0.6054,
      "step": 3494
    },
    {
      "epoch": 0.44451510333863276,
      "grad_norm": 2.58577823638916,
      "learning_rate": 0.00011116202112765688,
      "loss": 0.8555,
      "step": 3495
    },
    {
      "epoch": 0.4446422893481717,
      "grad_norm": 1.6634267568588257,
      "learning_rate": 0.00011113656611938398,
      "loss": 0.6859,
      "step": 3496
    },
    {
      "epoch": 0.44476947535771066,
      "grad_norm": 2.6673483848571777,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.4468,
      "step": 3497
    },
    {
      "epoch": 0.4448966613672496,
      "grad_norm": 3.015507459640503,
      "learning_rate": 0.00011108565610283824,
      "loss": 0.6592,
      "step": 3498
    },
    {
      "epoch": 0.4450238473767886,
      "grad_norm": 2.402095079421997,
      "learning_rate": 0.00011106020109456537,
      "loss": 0.7423,
      "step": 3499
    },
    {
      "epoch": 0.4451510333863275,
      "grad_norm": 1.9685187339782715,
      "learning_rate": 0.00011103474608629247,
      "loss": 0.7381,
      "step": 3500
    },
    {
      "epoch": 0.4452782193958665,
      "grad_norm": 2.4760162830352783,
      "learning_rate": 0.0001110092910780196,
      "loss": 0.4645,
      "step": 3501
    },
    {
      "epoch": 0.4454054054054054,
      "grad_norm": 2.0357213020324707,
      "learning_rate": 0.00011098383606974674,
      "loss": 0.5659,
      "step": 3502
    },
    {
      "epoch": 0.44553259141494433,
      "grad_norm": 2.284693479537964,
      "learning_rate": 0.00011095838106147384,
      "loss": 0.6077,
      "step": 3503
    },
    {
      "epoch": 0.4456597774244833,
      "grad_norm": 2.7422096729278564,
      "learning_rate": 0.00011093292605320097,
      "loss": 0.5323,
      "step": 3504
    },
    {
      "epoch": 0.44578696343402224,
      "grad_norm": 3.529465913772583,
      "learning_rate": 0.0001109074710449281,
      "loss": 0.7336,
      "step": 3505
    },
    {
      "epoch": 0.4459141494435612,
      "grad_norm": 2.0182642936706543,
      "learning_rate": 0.00011088201603665523,
      "loss": 0.5788,
      "step": 3506
    },
    {
      "epoch": 0.44604133545310015,
      "grad_norm": 2.2824385166168213,
      "learning_rate": 0.00011085656102838233,
      "loss": 0.5192,
      "step": 3507
    },
    {
      "epoch": 0.44616852146263913,
      "grad_norm": 1.8670626878738403,
      "learning_rate": 0.00011083110602010946,
      "loss": 0.5533,
      "step": 3508
    },
    {
      "epoch": 0.44629570747217806,
      "grad_norm": 1.8167697191238403,
      "learning_rate": 0.00011080565101183658,
      "loss": 0.5131,
      "step": 3509
    },
    {
      "epoch": 0.446422893481717,
      "grad_norm": 2.808439254760742,
      "learning_rate": 0.00011078019600356371,
      "loss": 0.6722,
      "step": 3510
    },
    {
      "epoch": 0.44655007949125597,
      "grad_norm": 2.7994794845581055,
      "learning_rate": 0.00011075474099529082,
      "loss": 0.5157,
      "step": 3511
    },
    {
      "epoch": 0.4466772655007949,
      "grad_norm": 2.2786061763763428,
      "learning_rate": 0.00011072928598701795,
      "loss": 0.5369,
      "step": 3512
    },
    {
      "epoch": 0.4468044515103339,
      "grad_norm": 2.04598331451416,
      "learning_rate": 0.00011070383097874508,
      "loss": 0.5742,
      "step": 3513
    },
    {
      "epoch": 0.4469316375198728,
      "grad_norm": 1.6926190853118896,
      "learning_rate": 0.00011067837597047219,
      "loss": 0.5582,
      "step": 3514
    },
    {
      "epoch": 0.4470588235294118,
      "grad_norm": 1.8983924388885498,
      "learning_rate": 0.00011065292096219932,
      "loss": 0.5142,
      "step": 3515
    },
    {
      "epoch": 0.4471860095389507,
      "grad_norm": 2.207993984222412,
      "learning_rate": 0.00011062746595392644,
      "loss": 0.5834,
      "step": 3516
    },
    {
      "epoch": 0.44731319554848964,
      "grad_norm": 1.8395110368728638,
      "learning_rate": 0.00011060201094565357,
      "loss": 0.4225,
      "step": 3517
    },
    {
      "epoch": 0.4474403815580286,
      "grad_norm": 1.8822740316390991,
      "learning_rate": 0.00011057655593738068,
      "loss": 0.4653,
      "step": 3518
    },
    {
      "epoch": 0.44756756756756755,
      "grad_norm": 1.9835069179534912,
      "learning_rate": 0.00011055110092910781,
      "loss": 0.6682,
      "step": 3519
    },
    {
      "epoch": 0.44769475357710653,
      "grad_norm": 2.186706066131592,
      "learning_rate": 0.00011052564592083492,
      "loss": 0.7019,
      "step": 3520
    },
    {
      "epoch": 0.44782193958664546,
      "grad_norm": 1.7188270092010498,
      "learning_rate": 0.00011050019091256205,
      "loss": 0.7884,
      "step": 3521
    },
    {
      "epoch": 0.44794912559618444,
      "grad_norm": 2.3014094829559326,
      "learning_rate": 0.00011047473590428918,
      "loss": 0.5243,
      "step": 3522
    },
    {
      "epoch": 0.44807631160572337,
      "grad_norm": 2.105747938156128,
      "learning_rate": 0.0001104492808960163,
      "loss": 0.6375,
      "step": 3523
    },
    {
      "epoch": 0.44820349761526235,
      "grad_norm": 2.643979549407959,
      "learning_rate": 0.00011042382588774343,
      "loss": 0.669,
      "step": 3524
    },
    {
      "epoch": 0.4483306836248013,
      "grad_norm": 2.245518445968628,
      "learning_rate": 0.00011039837087947054,
      "loss": 0.611,
      "step": 3525
    },
    {
      "epoch": 0.4484578696343402,
      "grad_norm": 2.8546242713928223,
      "learning_rate": 0.00011037291587119767,
      "loss": 0.6034,
      "step": 3526
    },
    {
      "epoch": 0.4485850556438792,
      "grad_norm": 3.0216972827911377,
      "learning_rate": 0.00011034746086292477,
      "loss": 1.0206,
      "step": 3527
    },
    {
      "epoch": 0.4487122416534181,
      "grad_norm": 1.9864155054092407,
      "learning_rate": 0.0001103220058546519,
      "loss": 0.5512,
      "step": 3528
    },
    {
      "epoch": 0.4488394276629571,
      "grad_norm": 1.7120985984802246,
      "learning_rate": 0.00011029655084637902,
      "loss": 0.4032,
      "step": 3529
    },
    {
      "epoch": 0.448966613672496,
      "grad_norm": 2.5093777179718018,
      "learning_rate": 0.00011027109583810616,
      "loss": 0.6889,
      "step": 3530
    },
    {
      "epoch": 0.449093799682035,
      "grad_norm": 1.84027099609375,
      "learning_rate": 0.00011024564082983329,
      "loss": 0.7913,
      "step": 3531
    },
    {
      "epoch": 0.4492209856915739,
      "grad_norm": 2.3501405715942383,
      "learning_rate": 0.0001102201858215604,
      "loss": 0.7246,
      "step": 3532
    },
    {
      "epoch": 0.44934817170111285,
      "grad_norm": 2.66894793510437,
      "learning_rate": 0.00011019473081328753,
      "loss": 0.8046,
      "step": 3533
    },
    {
      "epoch": 0.44947535771065183,
      "grad_norm": 1.9772803783416748,
      "learning_rate": 0.00011016927580501463,
      "loss": 0.5604,
      "step": 3534
    },
    {
      "epoch": 0.44960254372019076,
      "grad_norm": 1.8430262804031372,
      "learning_rate": 0.00011014382079674176,
      "loss": 0.6222,
      "step": 3535
    },
    {
      "epoch": 0.44972972972972974,
      "grad_norm": 2.9117133617401123,
      "learning_rate": 0.00011011836578846888,
      "loss": 1.1533,
      "step": 3536
    },
    {
      "epoch": 0.44985691573926867,
      "grad_norm": 2.497772216796875,
      "learning_rate": 0.00011009291078019602,
      "loss": 0.5971,
      "step": 3537
    },
    {
      "epoch": 0.44998410174880765,
      "grad_norm": 2.423448324203491,
      "learning_rate": 0.00011006745577192312,
      "loss": 0.7426,
      "step": 3538
    },
    {
      "epoch": 0.4501112877583466,
      "grad_norm": 1.7548623085021973,
      "learning_rate": 0.00011004200076365025,
      "loss": 0.5938,
      "step": 3539
    },
    {
      "epoch": 0.4502384737678855,
      "grad_norm": 1.7164685726165771,
      "learning_rate": 0.00011001654575537737,
      "loss": 0.5849,
      "step": 3540
    },
    {
      "epoch": 0.4503656597774245,
      "grad_norm": 2.2504963874816895,
      "learning_rate": 0.0001099910907471045,
      "loss": 0.7619,
      "step": 3541
    },
    {
      "epoch": 0.4504928457869634,
      "grad_norm": 2.01466703414917,
      "learning_rate": 0.00010996563573883164,
      "loss": 0.6619,
      "step": 3542
    },
    {
      "epoch": 0.4506200317965024,
      "grad_norm": 2.2327325344085693,
      "learning_rate": 0.00010994018073055874,
      "loss": 0.7487,
      "step": 3543
    },
    {
      "epoch": 0.4507472178060413,
      "grad_norm": 2.1762771606445312,
      "learning_rate": 0.00010991472572228587,
      "loss": 0.4598,
      "step": 3544
    },
    {
      "epoch": 0.4508744038155803,
      "grad_norm": 2.075138807296753,
      "learning_rate": 0.00010988927071401298,
      "loss": 0.6803,
      "step": 3545
    },
    {
      "epoch": 0.45100158982511923,
      "grad_norm": 2.395826816558838,
      "learning_rate": 0.00010986381570574011,
      "loss": 0.6935,
      "step": 3546
    },
    {
      "epoch": 0.4511287758346582,
      "grad_norm": 2.225424289703369,
      "learning_rate": 0.00010983836069746723,
      "loss": 0.8153,
      "step": 3547
    },
    {
      "epoch": 0.45125596184419714,
      "grad_norm": 2.9451282024383545,
      "learning_rate": 0.00010981290568919436,
      "loss": 0.9604,
      "step": 3548
    },
    {
      "epoch": 0.45138314785373607,
      "grad_norm": 2.0024266242980957,
      "learning_rate": 0.00010978745068092147,
      "loss": 0.7593,
      "step": 3549
    },
    {
      "epoch": 0.45151033386327505,
      "grad_norm": 2.1514744758605957,
      "learning_rate": 0.0001097619956726486,
      "loss": 0.8835,
      "step": 3550
    },
    {
      "epoch": 0.451637519872814,
      "grad_norm": 1.7890350818634033,
      "learning_rate": 0.00010973654066437573,
      "loss": 0.4615,
      "step": 3551
    },
    {
      "epoch": 0.45176470588235296,
      "grad_norm": 1.824824571609497,
      "learning_rate": 0.00010971108565610284,
      "loss": 0.6892,
      "step": 3552
    },
    {
      "epoch": 0.4518918918918919,
      "grad_norm": 1.9441417455673218,
      "learning_rate": 0.00010968563064782997,
      "loss": 0.7666,
      "step": 3553
    },
    {
      "epoch": 0.45201907790143087,
      "grad_norm": 1.554930329322815,
      "learning_rate": 0.00010966017563955709,
      "loss": 0.587,
      "step": 3554
    },
    {
      "epoch": 0.4521462639109698,
      "grad_norm": 1.8748835325241089,
      "learning_rate": 0.00010963472063128422,
      "loss": 0.5535,
      "step": 3555
    },
    {
      "epoch": 0.4522734499205087,
      "grad_norm": 1.607176661491394,
      "learning_rate": 0.00010960926562301133,
      "loss": 0.5089,
      "step": 3556
    },
    {
      "epoch": 0.4524006359300477,
      "grad_norm": 2.272078275680542,
      "learning_rate": 0.00010958381061473846,
      "loss": 0.6815,
      "step": 3557
    },
    {
      "epoch": 0.4525278219395866,
      "grad_norm": 2.053830623626709,
      "learning_rate": 0.00010955835560646556,
      "loss": 0.7072,
      "step": 3558
    },
    {
      "epoch": 0.4526550079491256,
      "grad_norm": 1.7622709274291992,
      "learning_rate": 0.0001095329005981927,
      "loss": 0.4422,
      "step": 3559
    },
    {
      "epoch": 0.45278219395866454,
      "grad_norm": 1.9286478757858276,
      "learning_rate": 0.00010950744558991983,
      "loss": 0.5419,
      "step": 3560
    },
    {
      "epoch": 0.4529093799682035,
      "grad_norm": 1.6647464036941528,
      "learning_rate": 0.00010948199058164695,
      "loss": 0.6435,
      "step": 3561
    },
    {
      "epoch": 0.45303656597774244,
      "grad_norm": 2.335632562637329,
      "learning_rate": 0.00010945653557337408,
      "loss": 0.723,
      "step": 3562
    },
    {
      "epoch": 0.45316375198728137,
      "grad_norm": 2.068599224090576,
      "learning_rate": 0.00010943108056510118,
      "loss": 0.6422,
      "step": 3563
    },
    {
      "epoch": 0.45329093799682035,
      "grad_norm": 2.028745174407959,
      "learning_rate": 0.00010940562555682832,
      "loss": 0.644,
      "step": 3564
    },
    {
      "epoch": 0.4534181240063593,
      "grad_norm": 1.7175304889678955,
      "learning_rate": 0.00010938017054855542,
      "loss": 0.6247,
      "step": 3565
    },
    {
      "epoch": 0.45354531001589826,
      "grad_norm": 2.477581262588501,
      "learning_rate": 0.00010935471554028255,
      "loss": 0.7756,
      "step": 3566
    },
    {
      "epoch": 0.4536724960254372,
      "grad_norm": 1.6779932975769043,
      "learning_rate": 0.00010932926053200967,
      "loss": 0.5409,
      "step": 3567
    },
    {
      "epoch": 0.45379968203497617,
      "grad_norm": 2.283219337463379,
      "learning_rate": 0.0001093038055237368,
      "loss": 0.7455,
      "step": 3568
    },
    {
      "epoch": 0.4539268680445151,
      "grad_norm": 3.2007174491882324,
      "learning_rate": 0.00010927835051546391,
      "loss": 0.7308,
      "step": 3569
    },
    {
      "epoch": 0.4540540540540541,
      "grad_norm": 1.8750386238098145,
      "learning_rate": 0.00010925289550719104,
      "loss": 0.8846,
      "step": 3570
    },
    {
      "epoch": 0.454181240063593,
      "grad_norm": 2.1928462982177734,
      "learning_rate": 0.00010922744049891817,
      "loss": 0.6666,
      "step": 3571
    },
    {
      "epoch": 0.45430842607313193,
      "grad_norm": 2.0605242252349854,
      "learning_rate": 0.00010920198549064529,
      "loss": 0.5686,
      "step": 3572
    },
    {
      "epoch": 0.4544356120826709,
      "grad_norm": 2.109978199005127,
      "learning_rate": 0.00010917653048237243,
      "loss": 0.5952,
      "step": 3573
    },
    {
      "epoch": 0.45456279809220984,
      "grad_norm": 1.8988829851150513,
      "learning_rate": 0.00010915107547409953,
      "loss": 0.6309,
      "step": 3574
    },
    {
      "epoch": 0.4546899841017488,
      "grad_norm": 1.534977912902832,
      "learning_rate": 0.00010912562046582666,
      "loss": 0.4915,
      "step": 3575
    },
    {
      "epoch": 0.45481717011128775,
      "grad_norm": 2.7999141216278076,
      "learning_rate": 0.00010910016545755377,
      "loss": 0.9704,
      "step": 3576
    },
    {
      "epoch": 0.45494435612082673,
      "grad_norm": 2.1415109634399414,
      "learning_rate": 0.0001090747104492809,
      "loss": 0.4432,
      "step": 3577
    },
    {
      "epoch": 0.45507154213036566,
      "grad_norm": 2.3089683055877686,
      "learning_rate": 0.00010904925544100802,
      "loss": 0.5451,
      "step": 3578
    },
    {
      "epoch": 0.4551987281399046,
      "grad_norm": 2.016303539276123,
      "learning_rate": 0.00010902380043273515,
      "loss": 0.8194,
      "step": 3579
    },
    {
      "epoch": 0.45532591414944357,
      "grad_norm": 2.2990384101867676,
      "learning_rate": 0.00010899834542446228,
      "loss": 0.5105,
      "step": 3580
    },
    {
      "epoch": 0.4554531001589825,
      "grad_norm": 2.071195363998413,
      "learning_rate": 0.00010897289041618939,
      "loss": 0.555,
      "step": 3581
    },
    {
      "epoch": 0.4555802861685215,
      "grad_norm": 2.222337007522583,
      "learning_rate": 0.00010894743540791652,
      "loss": 0.6079,
      "step": 3582
    },
    {
      "epoch": 0.4557074721780604,
      "grad_norm": 2.375624418258667,
      "learning_rate": 0.00010892198039964363,
      "loss": 0.628,
      "step": 3583
    },
    {
      "epoch": 0.4558346581875994,
      "grad_norm": 2.4731407165527344,
      "learning_rate": 0.00010889652539137076,
      "loss": 0.8852,
      "step": 3584
    },
    {
      "epoch": 0.4559618441971383,
      "grad_norm": 2.3237457275390625,
      "learning_rate": 0.00010887107038309788,
      "loss": 0.6448,
      "step": 3585
    },
    {
      "epoch": 0.4560890302066773,
      "grad_norm": 2.320962905883789,
      "learning_rate": 0.00010884561537482501,
      "loss": 0.6944,
      "step": 3586
    },
    {
      "epoch": 0.4562162162162162,
      "grad_norm": 1.942986249923706,
      "learning_rate": 0.00010882016036655211,
      "loss": 0.6745,
      "step": 3587
    },
    {
      "epoch": 0.45634340222575515,
      "grad_norm": 2.383472442626953,
      "learning_rate": 0.00010879470535827925,
      "loss": 0.6565,
      "step": 3588
    },
    {
      "epoch": 0.45647058823529413,
      "grad_norm": 1.558690071105957,
      "learning_rate": 0.00010876925035000638,
      "loss": 0.3578,
      "step": 3589
    },
    {
      "epoch": 0.45659777424483305,
      "grad_norm": 1.6118563413619995,
      "learning_rate": 0.00010874379534173348,
      "loss": 0.6486,
      "step": 3590
    },
    {
      "epoch": 0.45672496025437204,
      "grad_norm": 1.8672937154769897,
      "learning_rate": 0.00010871834033346062,
      "loss": 0.5052,
      "step": 3591
    },
    {
      "epoch": 0.45685214626391096,
      "grad_norm": 2.2956924438476562,
      "learning_rate": 0.00010869288532518774,
      "loss": 0.7638,
      "step": 3592
    },
    {
      "epoch": 0.45697933227344995,
      "grad_norm": 2.4109432697296143,
      "learning_rate": 0.00010866743031691487,
      "loss": 0.7148,
      "step": 3593
    },
    {
      "epoch": 0.45710651828298887,
      "grad_norm": 3.02366042137146,
      "learning_rate": 0.00010864197530864197,
      "loss": 0.7405,
      "step": 3594
    },
    {
      "epoch": 0.4572337042925278,
      "grad_norm": 1.778524398803711,
      "learning_rate": 0.0001086165203003691,
      "loss": 0.569,
      "step": 3595
    },
    {
      "epoch": 0.4573608903020668,
      "grad_norm": 2.8508353233337402,
      "learning_rate": 0.00010859106529209621,
      "loss": 0.7584,
      "step": 3596
    },
    {
      "epoch": 0.4574880763116057,
      "grad_norm": 1.5905568599700928,
      "learning_rate": 0.00010856561028382334,
      "loss": 0.574,
      "step": 3597
    },
    {
      "epoch": 0.4576152623211447,
      "grad_norm": 2.05242919921875,
      "learning_rate": 0.00010854015527555046,
      "loss": 0.428,
      "step": 3598
    },
    {
      "epoch": 0.4577424483306836,
      "grad_norm": 2.279475688934326,
      "learning_rate": 0.0001085147002672776,
      "loss": 0.5595,
      "step": 3599
    },
    {
      "epoch": 0.4578696343402226,
      "grad_norm": 1.7869282960891724,
      "learning_rate": 0.00010848924525900473,
      "loss": 0.7353,
      "step": 3600
    },
    {
      "epoch": 0.4579968203497615,
      "grad_norm": 2.5313072204589844,
      "learning_rate": 0.00010846379025073183,
      "loss": 0.659,
      "step": 3601
    },
    {
      "epoch": 0.45812400635930045,
      "grad_norm": 2.4434814453125,
      "learning_rate": 0.00010843833524245896,
      "loss": 0.8443,
      "step": 3602
    },
    {
      "epoch": 0.45825119236883943,
      "grad_norm": 2.433814525604248,
      "learning_rate": 0.00010841288023418608,
      "loss": 0.5336,
      "step": 3603
    },
    {
      "epoch": 0.45837837837837836,
      "grad_norm": 1.7966771125793457,
      "learning_rate": 0.00010838742522591321,
      "loss": 0.5994,
      "step": 3604
    },
    {
      "epoch": 0.45850556438791734,
      "grad_norm": 1.743552327156067,
      "learning_rate": 0.00010836197021764032,
      "loss": 0.5976,
      "step": 3605
    },
    {
      "epoch": 0.45863275039745627,
      "grad_norm": 2.0603034496307373,
      "learning_rate": 0.00010833651520936745,
      "loss": 0.6107,
      "step": 3606
    },
    {
      "epoch": 0.45875993640699525,
      "grad_norm": 1.6879173517227173,
      "learning_rate": 0.00010831106020109456,
      "loss": 0.6484,
      "step": 3607
    },
    {
      "epoch": 0.4588871224165342,
      "grad_norm": 2.3212451934814453,
      "learning_rate": 0.00010828560519282169,
      "loss": 0.8187,
      "step": 3608
    },
    {
      "epoch": 0.45901430842607316,
      "grad_norm": 2.6824169158935547,
      "learning_rate": 0.00010826015018454882,
      "loss": 0.7231,
      "step": 3609
    },
    {
      "epoch": 0.4591414944356121,
      "grad_norm": 2.4687657356262207,
      "learning_rate": 0.00010823469517627594,
      "loss": 0.564,
      "step": 3610
    },
    {
      "epoch": 0.459268680445151,
      "grad_norm": 3.2816333770751953,
      "learning_rate": 0.00010820924016800307,
      "loss": 0.5907,
      "step": 3611
    },
    {
      "epoch": 0.45939586645469,
      "grad_norm": 2.326997995376587,
      "learning_rate": 0.00010818378515973018,
      "loss": 0.7861,
      "step": 3612
    },
    {
      "epoch": 0.4595230524642289,
      "grad_norm": 2.2982170581817627,
      "learning_rate": 0.00010815833015145731,
      "loss": 0.8509,
      "step": 3613
    },
    {
      "epoch": 0.4596502384737679,
      "grad_norm": 1.6438157558441162,
      "learning_rate": 0.00010813287514318442,
      "loss": 0.591,
      "step": 3614
    },
    {
      "epoch": 0.45977742448330683,
      "grad_norm": 2.128145217895508,
      "learning_rate": 0.00010810742013491155,
      "loss": 0.7294,
      "step": 3615
    },
    {
      "epoch": 0.4599046104928458,
      "grad_norm": 1.7134394645690918,
      "learning_rate": 0.00010808196512663867,
      "loss": 0.516,
      "step": 3616
    },
    {
      "epoch": 0.46003179650238474,
      "grad_norm": 1.9845870733261108,
      "learning_rate": 0.0001080565101183658,
      "loss": 0.568,
      "step": 3617
    },
    {
      "epoch": 0.46015898251192366,
      "grad_norm": 2.2166450023651123,
      "learning_rate": 0.00010803105511009293,
      "loss": 0.5325,
      "step": 3618
    },
    {
      "epoch": 0.46028616852146265,
      "grad_norm": 1.806366205215454,
      "learning_rate": 0.00010800560010182004,
      "loss": 0.5534,
      "step": 3619
    },
    {
      "epoch": 0.4604133545310016,
      "grad_norm": 2.1559934616088867,
      "learning_rate": 0.00010798014509354717,
      "loss": 0.7837,
      "step": 3620
    },
    {
      "epoch": 0.46054054054054056,
      "grad_norm": 1.8562208414077759,
      "learning_rate": 0.00010795469008527427,
      "loss": 0.5401,
      "step": 3621
    },
    {
      "epoch": 0.4606677265500795,
      "grad_norm": 2.299323320388794,
      "learning_rate": 0.0001079292350770014,
      "loss": 0.5818,
      "step": 3622
    },
    {
      "epoch": 0.46079491255961846,
      "grad_norm": 2.359227180480957,
      "learning_rate": 0.00010790378006872852,
      "loss": 0.724,
      "step": 3623
    },
    {
      "epoch": 0.4609220985691574,
      "grad_norm": 2.3013670444488525,
      "learning_rate": 0.00010787832506045566,
      "loss": 0.6849,
      "step": 3624
    },
    {
      "epoch": 0.4610492845786963,
      "grad_norm": 2.3555331230163574,
      "learning_rate": 0.00010785287005218276,
      "loss": 0.5802,
      "step": 3625
    },
    {
      "epoch": 0.4611764705882353,
      "grad_norm": 2.3849146366119385,
      "learning_rate": 0.0001078274150439099,
      "loss": 0.6594,
      "step": 3626
    },
    {
      "epoch": 0.4613036565977742,
      "grad_norm": 2.3358442783355713,
      "learning_rate": 0.000107801960035637,
      "loss": 0.4769,
      "step": 3627
    },
    {
      "epoch": 0.4614308426073132,
      "grad_norm": 2.467813491821289,
      "learning_rate": 0.00010777650502736413,
      "loss": 0.7311,
      "step": 3628
    },
    {
      "epoch": 0.46155802861685213,
      "grad_norm": 2.461214780807495,
      "learning_rate": 0.00010775105001909126,
      "loss": 0.5653,
      "step": 3629
    },
    {
      "epoch": 0.4616852146263911,
      "grad_norm": 2.2427968978881836,
      "learning_rate": 0.00010772559501081838,
      "loss": 0.9704,
      "step": 3630
    },
    {
      "epoch": 0.46181240063593004,
      "grad_norm": 1.867202639579773,
      "learning_rate": 0.00010770014000254552,
      "loss": 0.7007,
      "step": 3631
    },
    {
      "epoch": 0.461939586645469,
      "grad_norm": 2.9405035972595215,
      "learning_rate": 0.00010767468499427262,
      "loss": 0.8385,
      "step": 3632
    },
    {
      "epoch": 0.46206677265500795,
      "grad_norm": 2.263439178466797,
      "learning_rate": 0.00010764922998599975,
      "loss": 0.6442,
      "step": 3633
    },
    {
      "epoch": 0.4621939586645469,
      "grad_norm": 1.4954692125320435,
      "learning_rate": 0.00010762377497772687,
      "loss": 0.5486,
      "step": 3634
    },
    {
      "epoch": 0.46232114467408586,
      "grad_norm": 1.4669767618179321,
      "learning_rate": 0.000107598319969454,
      "loss": 0.5113,
      "step": 3635
    },
    {
      "epoch": 0.4624483306836248,
      "grad_norm": 1.486098289489746,
      "learning_rate": 0.00010757286496118111,
      "loss": 0.6827,
      "step": 3636
    },
    {
      "epoch": 0.46257551669316377,
      "grad_norm": 2.034224033355713,
      "learning_rate": 0.00010754740995290824,
      "loss": 0.7308,
      "step": 3637
    },
    {
      "epoch": 0.4627027027027027,
      "grad_norm": 2.059591054916382,
      "learning_rate": 0.00010752195494463537,
      "loss": 0.5611,
      "step": 3638
    },
    {
      "epoch": 0.4628298887122417,
      "grad_norm": 1.3081247806549072,
      "learning_rate": 0.00010749649993636248,
      "loss": 0.5856,
      "step": 3639
    },
    {
      "epoch": 0.4629570747217806,
      "grad_norm": 3.004207134246826,
      "learning_rate": 0.00010747104492808961,
      "loss": 0.9582,
      "step": 3640
    },
    {
      "epoch": 0.46308426073131953,
      "grad_norm": 3.852668285369873,
      "learning_rate": 0.00010744558991981673,
      "loss": 0.7911,
      "step": 3641
    },
    {
      "epoch": 0.4632114467408585,
      "grad_norm": 2.200101137161255,
      "learning_rate": 0.00010742013491154386,
      "loss": 0.6618,
      "step": 3642
    },
    {
      "epoch": 0.46333863275039744,
      "grad_norm": 1.7234110832214355,
      "learning_rate": 0.00010739467990327097,
      "loss": 0.6629,
      "step": 3643
    },
    {
      "epoch": 0.4634658187599364,
      "grad_norm": 1.7612433433532715,
      "learning_rate": 0.0001073692248949981,
      "loss": 0.4719,
      "step": 3644
    },
    {
      "epoch": 0.46359300476947535,
      "grad_norm": 1.912500262260437,
      "learning_rate": 0.0001073437698867252,
      "loss": 0.4422,
      "step": 3645
    },
    {
      "epoch": 0.46372019077901433,
      "grad_norm": 2.385432481765747,
      "learning_rate": 0.00010731831487845234,
      "loss": 0.5752,
      "step": 3646
    },
    {
      "epoch": 0.46384737678855326,
      "grad_norm": 1.7623577117919922,
      "learning_rate": 0.00010729285987017947,
      "loss": 0.6872,
      "step": 3647
    },
    {
      "epoch": 0.4639745627980922,
      "grad_norm": 2.4731247425079346,
      "learning_rate": 0.00010726740486190659,
      "loss": 0.4401,
      "step": 3648
    },
    {
      "epoch": 0.46410174880763116,
      "grad_norm": 1.6754951477050781,
      "learning_rate": 0.00010724194985363372,
      "loss": 0.6376,
      "step": 3649
    },
    {
      "epoch": 0.4642289348171701,
      "grad_norm": 2.4426963329315186,
      "learning_rate": 0.00010721649484536083,
      "loss": 0.547,
      "step": 3650
    },
    {
      "epoch": 0.4643561208267091,
      "grad_norm": 1.962848424911499,
      "learning_rate": 0.00010719103983708796,
      "loss": 0.4957,
      "step": 3651
    },
    {
      "epoch": 0.464483306836248,
      "grad_norm": 2.1675496101379395,
      "learning_rate": 0.00010716558482881506,
      "loss": 0.6952,
      "step": 3652
    },
    {
      "epoch": 0.464610492845787,
      "grad_norm": 2.937930107116699,
      "learning_rate": 0.0001071401298205422,
      "loss": 0.7184,
      "step": 3653
    },
    {
      "epoch": 0.4647376788553259,
      "grad_norm": 2.4309885501861572,
      "learning_rate": 0.00010711467481226931,
      "loss": 0.4972,
      "step": 3654
    },
    {
      "epoch": 0.4648648648648649,
      "grad_norm": 2.696855068206787,
      "learning_rate": 0.00010708921980399645,
      "loss": 0.7965,
      "step": 3655
    },
    {
      "epoch": 0.4649920508744038,
      "grad_norm": 2.046571731567383,
      "learning_rate": 0.00010706376479572355,
      "loss": 0.6696,
      "step": 3656
    },
    {
      "epoch": 0.46511923688394274,
      "grad_norm": 1.794150948524475,
      "learning_rate": 0.00010703830978745068,
      "loss": 0.5614,
      "step": 3657
    },
    {
      "epoch": 0.4652464228934817,
      "grad_norm": 1.3207874298095703,
      "learning_rate": 0.00010701285477917782,
      "loss": 0.3882,
      "step": 3658
    },
    {
      "epoch": 0.46537360890302065,
      "grad_norm": 3.4020516872406006,
      "learning_rate": 0.00010698739977090492,
      "loss": 0.7097,
      "step": 3659
    },
    {
      "epoch": 0.46550079491255963,
      "grad_norm": 2.909778594970703,
      "learning_rate": 0.00010696194476263205,
      "loss": 0.6103,
      "step": 3660
    },
    {
      "epoch": 0.46562798092209856,
      "grad_norm": 2.3851430416107178,
      "learning_rate": 0.00010693648975435917,
      "loss": 0.592,
      "step": 3661
    },
    {
      "epoch": 0.46575516693163754,
      "grad_norm": 2.1833086013793945,
      "learning_rate": 0.0001069110347460863,
      "loss": 0.5516,
      "step": 3662
    },
    {
      "epoch": 0.46588235294117647,
      "grad_norm": 2.4970006942749023,
      "learning_rate": 0.00010688557973781341,
      "loss": 0.9613,
      "step": 3663
    },
    {
      "epoch": 0.4660095389507154,
      "grad_norm": 3.010225772857666,
      "learning_rate": 0.00010686012472954054,
      "loss": 0.9056,
      "step": 3664
    },
    {
      "epoch": 0.4661367249602544,
      "grad_norm": 2.038524627685547,
      "learning_rate": 0.00010683466972126766,
      "loss": 0.7085,
      "step": 3665
    },
    {
      "epoch": 0.4662639109697933,
      "grad_norm": 1.615440011024475,
      "learning_rate": 0.0001068092147129948,
      "loss": 0.2712,
      "step": 3666
    },
    {
      "epoch": 0.4663910969793323,
      "grad_norm": 2.458534002304077,
      "learning_rate": 0.00010678375970472193,
      "loss": 0.4942,
      "step": 3667
    },
    {
      "epoch": 0.4665182829888712,
      "grad_norm": 1.5794787406921387,
      "learning_rate": 0.00010675830469644903,
      "loss": 0.486,
      "step": 3668
    },
    {
      "epoch": 0.4666454689984102,
      "grad_norm": 1.8383859395980835,
      "learning_rate": 0.00010673284968817616,
      "loss": 0.6893,
      "step": 3669
    },
    {
      "epoch": 0.4667726550079491,
      "grad_norm": 1.706144094467163,
      "learning_rate": 0.00010670739467990327,
      "loss": 0.6548,
      "step": 3670
    },
    {
      "epoch": 0.46689984101748805,
      "grad_norm": 3.773002862930298,
      "learning_rate": 0.0001066819396716304,
      "loss": 0.9103,
      "step": 3671
    },
    {
      "epoch": 0.46702702702702703,
      "grad_norm": 1.3903660774230957,
      "learning_rate": 0.00010665648466335752,
      "loss": 0.4438,
      "step": 3672
    },
    {
      "epoch": 0.46715421303656596,
      "grad_norm": 2.556493043899536,
      "learning_rate": 0.00010663102965508465,
      "loss": 0.5635,
      "step": 3673
    },
    {
      "epoch": 0.46728139904610494,
      "grad_norm": 2.076084852218628,
      "learning_rate": 0.00010660557464681176,
      "loss": 0.4815,
      "step": 3674
    },
    {
      "epoch": 0.46740858505564387,
      "grad_norm": 2.696366548538208,
      "learning_rate": 0.00010658011963853889,
      "loss": 0.4422,
      "step": 3675
    },
    {
      "epoch": 0.46753577106518285,
      "grad_norm": 2.446331024169922,
      "learning_rate": 0.000106554664630266,
      "loss": 0.6691,
      "step": 3676
    },
    {
      "epoch": 0.4676629570747218,
      "grad_norm": 2.2910754680633545,
      "learning_rate": 0.00010652920962199313,
      "loss": 0.9005,
      "step": 3677
    },
    {
      "epoch": 0.46779014308426076,
      "grad_norm": 2.180553436279297,
      "learning_rate": 0.00010650375461372026,
      "loss": 0.475,
      "step": 3678
    },
    {
      "epoch": 0.4679173290937997,
      "grad_norm": 2.3370537757873535,
      "learning_rate": 0.00010647829960544738,
      "loss": 0.5618,
      "step": 3679
    },
    {
      "epoch": 0.4680445151033386,
      "grad_norm": 2.4380733966827393,
      "learning_rate": 0.00010645284459717451,
      "loss": 0.7288,
      "step": 3680
    },
    {
      "epoch": 0.4681717011128776,
      "grad_norm": 1.5217342376708984,
      "learning_rate": 0.00010642738958890162,
      "loss": 0.694,
      "step": 3681
    },
    {
      "epoch": 0.4682988871224165,
      "grad_norm": 1.8109182119369507,
      "learning_rate": 0.00010640193458062875,
      "loss": 0.6509,
      "step": 3682
    },
    {
      "epoch": 0.4684260731319555,
      "grad_norm": 2.214010000228882,
      "learning_rate": 0.00010637647957235585,
      "loss": 0.8224,
      "step": 3683
    },
    {
      "epoch": 0.4685532591414944,
      "grad_norm": 2.183354616165161,
      "learning_rate": 0.00010635102456408298,
      "loss": 0.5734,
      "step": 3684
    },
    {
      "epoch": 0.4686804451510334,
      "grad_norm": 2.290799379348755,
      "learning_rate": 0.0001063255695558101,
      "loss": 0.6423,
      "step": 3685
    },
    {
      "epoch": 0.46880763116057234,
      "grad_norm": 2.2512996196746826,
      "learning_rate": 0.00010630011454753724,
      "loss": 0.5466,
      "step": 3686
    },
    {
      "epoch": 0.46893481717011126,
      "grad_norm": 2.1978273391723633,
      "learning_rate": 0.00010627465953926437,
      "loss": 0.3996,
      "step": 3687
    },
    {
      "epoch": 0.46906200317965024,
      "grad_norm": 2.237760066986084,
      "learning_rate": 0.00010624920453099147,
      "loss": 0.5474,
      "step": 3688
    },
    {
      "epoch": 0.46918918918918917,
      "grad_norm": 2.1632962226867676,
      "learning_rate": 0.0001062237495227186,
      "loss": 0.613,
      "step": 3689
    },
    {
      "epoch": 0.46931637519872815,
      "grad_norm": 1.7619686126708984,
      "learning_rate": 0.00010619829451444571,
      "loss": 0.6887,
      "step": 3690
    },
    {
      "epoch": 0.4694435612082671,
      "grad_norm": 2.2509267330169678,
      "learning_rate": 0.00010617283950617284,
      "loss": 0.7666,
      "step": 3691
    },
    {
      "epoch": 0.46957074721780606,
      "grad_norm": 2.077223300933838,
      "learning_rate": 0.00010614738449789996,
      "loss": 0.5154,
      "step": 3692
    },
    {
      "epoch": 0.469697933227345,
      "grad_norm": 2.422973871231079,
      "learning_rate": 0.0001061219294896271,
      "loss": 1.0823,
      "step": 3693
    },
    {
      "epoch": 0.46982511923688397,
      "grad_norm": 1.8243921995162964,
      "learning_rate": 0.0001060964744813542,
      "loss": 0.6852,
      "step": 3694
    },
    {
      "epoch": 0.4699523052464229,
      "grad_norm": 1.275580644607544,
      "learning_rate": 0.00010607101947308133,
      "loss": 0.4152,
      "step": 3695
    },
    {
      "epoch": 0.4700794912559618,
      "grad_norm": 1.7618396282196045,
      "learning_rate": 0.00010604556446480846,
      "loss": 0.6926,
      "step": 3696
    },
    {
      "epoch": 0.4702066772655008,
      "grad_norm": 2.9871816635131836,
      "learning_rate": 0.00010602010945653558,
      "loss": 0.5701,
      "step": 3697
    },
    {
      "epoch": 0.47033386327503973,
      "grad_norm": 2.2427003383636475,
      "learning_rate": 0.00010599465444826271,
      "loss": 0.7403,
      "step": 3698
    },
    {
      "epoch": 0.4704610492845787,
      "grad_norm": 1.516911506652832,
      "learning_rate": 0.00010596919943998982,
      "loss": 0.5991,
      "step": 3699
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 2.6679086685180664,
      "learning_rate": 0.00010594374443171695,
      "loss": 0.5964,
      "step": 3700
    },
    {
      "epoch": 0.4707154213036566,
      "grad_norm": 2.646216869354248,
      "learning_rate": 0.00010591828942344406,
      "loss": 0.9793,
      "step": 3701
    },
    {
      "epoch": 0.47084260731319555,
      "grad_norm": 3.3479504585266113,
      "learning_rate": 0.00010589283441517119,
      "loss": 0.8028,
      "step": 3702
    },
    {
      "epoch": 0.4709697933227345,
      "grad_norm": 2.3937673568725586,
      "learning_rate": 0.00010586737940689831,
      "loss": 0.7659,
      "step": 3703
    },
    {
      "epoch": 0.47109697933227346,
      "grad_norm": 1.8752312660217285,
      "learning_rate": 0.00010584192439862544,
      "loss": 0.7687,
      "step": 3704
    },
    {
      "epoch": 0.4712241653418124,
      "grad_norm": 2.1091818809509277,
      "learning_rate": 0.00010581646939035255,
      "loss": 0.6993,
      "step": 3705
    },
    {
      "epoch": 0.47135135135135137,
      "grad_norm": 1.8681527376174927,
      "learning_rate": 0.00010579101438207968,
      "loss": 0.646,
      "step": 3706
    },
    {
      "epoch": 0.4714785373608903,
      "grad_norm": 1.6597498655319214,
      "learning_rate": 0.00010576555937380681,
      "loss": 0.5271,
      "step": 3707
    },
    {
      "epoch": 0.4716057233704293,
      "grad_norm": 1.726274847984314,
      "learning_rate": 0.00010574010436553392,
      "loss": 0.6207,
      "step": 3708
    },
    {
      "epoch": 0.4717329093799682,
      "grad_norm": 2.196629762649536,
      "learning_rate": 0.00010571464935726105,
      "loss": 0.4742,
      "step": 3709
    },
    {
      "epoch": 0.47186009538950713,
      "grad_norm": 2.077230930328369,
      "learning_rate": 0.00010568919434898817,
      "loss": 0.8127,
      "step": 3710
    },
    {
      "epoch": 0.4719872813990461,
      "grad_norm": 2.2475898265838623,
      "learning_rate": 0.0001056637393407153,
      "loss": 0.7719,
      "step": 3711
    },
    {
      "epoch": 0.47211446740858504,
      "grad_norm": 1.7953979969024658,
      "learning_rate": 0.0001056382843324424,
      "loss": 0.701,
      "step": 3712
    },
    {
      "epoch": 0.472241653418124,
      "grad_norm": 1.8278148174285889,
      "learning_rate": 0.00010561282932416954,
      "loss": 0.8051,
      "step": 3713
    },
    {
      "epoch": 0.47236883942766295,
      "grad_norm": 2.937532901763916,
      "learning_rate": 0.00010558737431589664,
      "loss": 0.9653,
      "step": 3714
    },
    {
      "epoch": 0.4724960254372019,
      "grad_norm": 1.8305789232254028,
      "learning_rate": 0.00010556191930762377,
      "loss": 0.4492,
      "step": 3715
    },
    {
      "epoch": 0.47262321144674085,
      "grad_norm": 2.5603086948394775,
      "learning_rate": 0.0001055364642993509,
      "loss": 0.5465,
      "step": 3716
    },
    {
      "epoch": 0.47275039745627984,
      "grad_norm": 2.868774890899658,
      "learning_rate": 0.00010551100929107803,
      "loss": 0.5396,
      "step": 3717
    },
    {
      "epoch": 0.47287758346581876,
      "grad_norm": 1.7678982019424438,
      "learning_rate": 0.00010548555428280516,
      "loss": 0.5398,
      "step": 3718
    },
    {
      "epoch": 0.4730047694753577,
      "grad_norm": 2.211111068725586,
      "learning_rate": 0.00010546009927453226,
      "loss": 0.5014,
      "step": 3719
    },
    {
      "epoch": 0.47313195548489667,
      "grad_norm": 2.8713173866271973,
      "learning_rate": 0.0001054346442662594,
      "loss": 0.7057,
      "step": 3720
    },
    {
      "epoch": 0.4732591414944356,
      "grad_norm": 2.708098888397217,
      "learning_rate": 0.00010540918925798651,
      "loss": 0.6224,
      "step": 3721
    },
    {
      "epoch": 0.4733863275039746,
      "grad_norm": 2.5986790657043457,
      "learning_rate": 0.00010538373424971365,
      "loss": 0.5563,
      "step": 3722
    },
    {
      "epoch": 0.4735135135135135,
      "grad_norm": 1.868638038635254,
      "learning_rate": 0.00010535827924144075,
      "loss": 0.6845,
      "step": 3723
    },
    {
      "epoch": 0.4736406995230525,
      "grad_norm": 2.168604850769043,
      "learning_rate": 0.00010533282423316788,
      "loss": 0.7218,
      "step": 3724
    },
    {
      "epoch": 0.4737678855325914,
      "grad_norm": 2.740036725997925,
      "learning_rate": 0.00010530736922489502,
      "loss": 0.8859,
      "step": 3725
    },
    {
      "epoch": 0.47389507154213034,
      "grad_norm": 2.0255963802337646,
      "learning_rate": 0.00010528191421662212,
      "loss": 0.5175,
      "step": 3726
    },
    {
      "epoch": 0.4740222575516693,
      "grad_norm": 2.2277634143829346,
      "learning_rate": 0.00010525645920834925,
      "loss": 0.6094,
      "step": 3727
    },
    {
      "epoch": 0.47414944356120825,
      "grad_norm": 2.551658868789673,
      "learning_rate": 0.00010523100420007637,
      "loss": 0.7903,
      "step": 3728
    },
    {
      "epoch": 0.47427662957074723,
      "grad_norm": 2.893611431121826,
      "learning_rate": 0.0001052055491918035,
      "loss": 0.9847,
      "step": 3729
    },
    {
      "epoch": 0.47440381558028616,
      "grad_norm": 1.4129009246826172,
      "learning_rate": 0.00010518009418353061,
      "loss": 0.5872,
      "step": 3730
    },
    {
      "epoch": 0.47453100158982514,
      "grad_norm": 2.4148213863372803,
      "learning_rate": 0.00010515463917525774,
      "loss": 0.6565,
      "step": 3731
    },
    {
      "epoch": 0.47465818759936407,
      "grad_norm": 2.138979434967041,
      "learning_rate": 0.00010512918416698485,
      "loss": 0.56,
      "step": 3732
    },
    {
      "epoch": 0.474785373608903,
      "grad_norm": 2.378633975982666,
      "learning_rate": 0.00010510372915871198,
      "loss": 0.7541,
      "step": 3733
    },
    {
      "epoch": 0.474912559618442,
      "grad_norm": 2.1228394508361816,
      "learning_rate": 0.0001050782741504391,
      "loss": 0.5767,
      "step": 3734
    },
    {
      "epoch": 0.4750397456279809,
      "grad_norm": 2.6536850929260254,
      "learning_rate": 0.00010505281914216623,
      "loss": 0.6496,
      "step": 3735
    },
    {
      "epoch": 0.4751669316375199,
      "grad_norm": 2.056431770324707,
      "learning_rate": 0.00010502736413389336,
      "loss": 0.4421,
      "step": 3736
    },
    {
      "epoch": 0.4752941176470588,
      "grad_norm": 3.0158870220184326,
      "learning_rate": 0.00010500190912562047,
      "loss": 0.6514,
      "step": 3737
    },
    {
      "epoch": 0.4754213036565978,
      "grad_norm": 1.9218512773513794,
      "learning_rate": 0.0001049764541173476,
      "loss": 0.6217,
      "step": 3738
    },
    {
      "epoch": 0.4755484896661367,
      "grad_norm": 1.4733892679214478,
      "learning_rate": 0.0001049509991090747,
      "loss": 0.5068,
      "step": 3739
    },
    {
      "epoch": 0.4756756756756757,
      "grad_norm": 1.9799410104751587,
      "learning_rate": 0.00010492554410080184,
      "loss": 0.5867,
      "step": 3740
    },
    {
      "epoch": 0.47580286168521463,
      "grad_norm": 2.0363376140594482,
      "learning_rate": 0.00010490008909252896,
      "loss": 0.6202,
      "step": 3741
    },
    {
      "epoch": 0.47593004769475356,
      "grad_norm": 2.513735055923462,
      "learning_rate": 0.00010487463408425609,
      "loss": 0.499,
      "step": 3742
    },
    {
      "epoch": 0.47605723370429254,
      "grad_norm": 1.5990700721740723,
      "learning_rate": 0.0001048491790759832,
      "loss": 0.3164,
      "step": 3743
    },
    {
      "epoch": 0.47618441971383146,
      "grad_norm": 2.1670711040496826,
      "learning_rate": 0.00010482372406771033,
      "loss": 0.7396,
      "step": 3744
    },
    {
      "epoch": 0.47631160572337045,
      "grad_norm": 1.8678538799285889,
      "learning_rate": 0.00010479826905943746,
      "loss": 0.5795,
      "step": 3745
    },
    {
      "epoch": 0.4764387917329094,
      "grad_norm": 2.5884850025177,
      "learning_rate": 0.00010477281405116456,
      "loss": 0.8609,
      "step": 3746
    },
    {
      "epoch": 0.47656597774244835,
      "grad_norm": 2.7848005294799805,
      "learning_rate": 0.0001047473590428917,
      "loss": 0.9465,
      "step": 3747
    },
    {
      "epoch": 0.4766931637519873,
      "grad_norm": 2.4381532669067383,
      "learning_rate": 0.00010472190403461881,
      "loss": 0.6156,
      "step": 3748
    },
    {
      "epoch": 0.4768203497615262,
      "grad_norm": 1.6847089529037476,
      "learning_rate": 0.00010469644902634595,
      "loss": 0.4979,
      "step": 3749
    },
    {
      "epoch": 0.4769475357710652,
      "grad_norm": 2.1484646797180176,
      "learning_rate": 0.00010467099401807305,
      "loss": 0.8035,
      "step": 3750
    },
    {
      "epoch": 0.4770747217806041,
      "grad_norm": 2.376736640930176,
      "learning_rate": 0.00010464553900980018,
      "loss": 0.4696,
      "step": 3751
    },
    {
      "epoch": 0.4772019077901431,
      "grad_norm": 1.7432441711425781,
      "learning_rate": 0.0001046200840015273,
      "loss": 0.6575,
      "step": 3752
    },
    {
      "epoch": 0.477329093799682,
      "grad_norm": 2.9810497760772705,
      "learning_rate": 0.00010459462899325444,
      "loss": 1.1421,
      "step": 3753
    },
    {
      "epoch": 0.477456279809221,
      "grad_norm": 2.036968231201172,
      "learning_rate": 0.00010456917398498157,
      "loss": 0.6479,
      "step": 3754
    },
    {
      "epoch": 0.47758346581875993,
      "grad_norm": 2.381887435913086,
      "learning_rate": 0.00010454371897670867,
      "loss": 0.6502,
      "step": 3755
    },
    {
      "epoch": 0.47771065182829886,
      "grad_norm": 2.518852710723877,
      "learning_rate": 0.0001045182639684358,
      "loss": 1.0798,
      "step": 3756
    },
    {
      "epoch": 0.47783783783783784,
      "grad_norm": 2.4470133781433105,
      "learning_rate": 0.00010449280896016291,
      "loss": 0.666,
      "step": 3757
    },
    {
      "epoch": 0.47796502384737677,
      "grad_norm": 2.3098034858703613,
      "learning_rate": 0.00010446735395189004,
      "loss": 0.503,
      "step": 3758
    },
    {
      "epoch": 0.47809220985691575,
      "grad_norm": 2.9612863063812256,
      "learning_rate": 0.00010444189894361716,
      "loss": 0.8916,
      "step": 3759
    },
    {
      "epoch": 0.4782193958664547,
      "grad_norm": 1.736385703086853,
      "learning_rate": 0.0001044164439353443,
      "loss": 0.7122,
      "step": 3760
    },
    {
      "epoch": 0.47834658187599366,
      "grad_norm": 1.8808380365371704,
      "learning_rate": 0.0001043909889270714,
      "loss": 0.5992,
      "step": 3761
    },
    {
      "epoch": 0.4784737678855326,
      "grad_norm": 2.264488697052002,
      "learning_rate": 0.00010436553391879853,
      "loss": 0.9112,
      "step": 3762
    },
    {
      "epoch": 0.47860095389507157,
      "grad_norm": 1.5559189319610596,
      "learning_rate": 0.00010434007891052564,
      "loss": 0.6293,
      "step": 3763
    },
    {
      "epoch": 0.4787281399046105,
      "grad_norm": 2.0883047580718994,
      "learning_rate": 0.00010431462390225277,
      "loss": 0.6367,
      "step": 3764
    },
    {
      "epoch": 0.4788553259141494,
      "grad_norm": 1.7076159715652466,
      "learning_rate": 0.0001042891688939799,
      "loss": 0.5558,
      "step": 3765
    },
    {
      "epoch": 0.4789825119236884,
      "grad_norm": 2.079770803451538,
      "learning_rate": 0.00010426371388570702,
      "loss": 0.5791,
      "step": 3766
    },
    {
      "epoch": 0.47910969793322733,
      "grad_norm": 1.78713858127594,
      "learning_rate": 0.00010423825887743415,
      "loss": 0.5886,
      "step": 3767
    },
    {
      "epoch": 0.4792368839427663,
      "grad_norm": 1.5925008058547974,
      "learning_rate": 0.00010421280386916126,
      "loss": 0.5956,
      "step": 3768
    },
    {
      "epoch": 0.47936406995230524,
      "grad_norm": 1.955613374710083,
      "learning_rate": 0.00010418734886088839,
      "loss": 0.5493,
      "step": 3769
    },
    {
      "epoch": 0.4794912559618442,
      "grad_norm": 1.8334473371505737,
      "learning_rate": 0.0001041618938526155,
      "loss": 0.5371,
      "step": 3770
    },
    {
      "epoch": 0.47961844197138315,
      "grad_norm": 1.5386810302734375,
      "learning_rate": 0.00010413643884434263,
      "loss": 0.7706,
      "step": 3771
    },
    {
      "epoch": 0.4797456279809221,
      "grad_norm": 2.852360486984253,
      "learning_rate": 0.00010411098383606975,
      "loss": 0.4945,
      "step": 3772
    },
    {
      "epoch": 0.47987281399046106,
      "grad_norm": 2.323587417602539,
      "learning_rate": 0.00010408552882779688,
      "loss": 0.6313,
      "step": 3773
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.269512414932251,
      "learning_rate": 0.00010406007381952401,
      "loss": 0.4893,
      "step": 3774
    },
    {
      "epoch": 0.48012718600953896,
      "grad_norm": 1.6866484880447388,
      "learning_rate": 0.00010403461881125112,
      "loss": 0.4841,
      "step": 3775
    },
    {
      "epoch": 0.4802543720190779,
      "grad_norm": 1.7979862689971924,
      "learning_rate": 0.00010400916380297825,
      "loss": 0.6304,
      "step": 3776
    },
    {
      "epoch": 0.4803815580286169,
      "grad_norm": 1.9881943464279175,
      "learning_rate": 0.00010398370879470535,
      "loss": 0.5139,
      "step": 3777
    },
    {
      "epoch": 0.4805087440381558,
      "grad_norm": 2.2379605770111084,
      "learning_rate": 0.00010395825378643249,
      "loss": 0.5852,
      "step": 3778
    },
    {
      "epoch": 0.4806359300476947,
      "grad_norm": 1.8528136014938354,
      "learning_rate": 0.0001039327987781596,
      "loss": 0.5603,
      "step": 3779
    },
    {
      "epoch": 0.4807631160572337,
      "grad_norm": 1.8702752590179443,
      "learning_rate": 0.00010390734376988674,
      "loss": 0.5544,
      "step": 3780
    },
    {
      "epoch": 0.48089030206677263,
      "grad_norm": 1.787660002708435,
      "learning_rate": 0.00010388188876161384,
      "loss": 0.6205,
      "step": 3781
    },
    {
      "epoch": 0.4810174880763116,
      "grad_norm": 2.2901694774627686,
      "learning_rate": 0.00010385643375334097,
      "loss": 0.4865,
      "step": 3782
    },
    {
      "epoch": 0.48114467408585054,
      "grad_norm": 1.9814214706420898,
      "learning_rate": 0.0001038309787450681,
      "loss": 0.6176,
      "step": 3783
    },
    {
      "epoch": 0.4812718600953895,
      "grad_norm": 2.243253231048584,
      "learning_rate": 0.00010380552373679522,
      "loss": 0.7482,
      "step": 3784
    },
    {
      "epoch": 0.48139904610492845,
      "grad_norm": 2.2311971187591553,
      "learning_rate": 0.00010378006872852236,
      "loss": 0.8041,
      "step": 3785
    },
    {
      "epoch": 0.48152623211446743,
      "grad_norm": 2.1352224349975586,
      "learning_rate": 0.00010375461372024946,
      "loss": 0.6179,
      "step": 3786
    },
    {
      "epoch": 0.48165341812400636,
      "grad_norm": 1.856050968170166,
      "learning_rate": 0.0001037291587119766,
      "loss": 0.533,
      "step": 3787
    },
    {
      "epoch": 0.4817806041335453,
      "grad_norm": 2.9274802207946777,
      "learning_rate": 0.0001037037037037037,
      "loss": 0.7355,
      "step": 3788
    },
    {
      "epoch": 0.48190779014308427,
      "grad_norm": 2.784369945526123,
      "learning_rate": 0.00010367824869543083,
      "loss": 0.6533,
      "step": 3789
    },
    {
      "epoch": 0.4820349761526232,
      "grad_norm": 2.2116026878356934,
      "learning_rate": 0.00010365279368715795,
      "loss": 0.6827,
      "step": 3790
    },
    {
      "epoch": 0.4821621621621622,
      "grad_norm": 2.0303843021392822,
      "learning_rate": 0.00010362733867888508,
      "loss": 0.6227,
      "step": 3791
    },
    {
      "epoch": 0.4822893481717011,
      "grad_norm": 2.8882436752319336,
      "learning_rate": 0.00010360188367061219,
      "loss": 0.6598,
      "step": 3792
    },
    {
      "epoch": 0.4824165341812401,
      "grad_norm": 2.2129769325256348,
      "learning_rate": 0.00010357642866233932,
      "loss": 0.6972,
      "step": 3793
    },
    {
      "epoch": 0.482543720190779,
      "grad_norm": 2.8555848598480225,
      "learning_rate": 0.00010355097365406645,
      "loss": 0.5983,
      "step": 3794
    },
    {
      "epoch": 0.48267090620031794,
      "grad_norm": 2.3266522884368896,
      "learning_rate": 0.00010352551864579356,
      "loss": 0.4654,
      "step": 3795
    },
    {
      "epoch": 0.4827980922098569,
      "grad_norm": 1.8138248920440674,
      "learning_rate": 0.00010350006363752069,
      "loss": 0.4623,
      "step": 3796
    },
    {
      "epoch": 0.48292527821939585,
      "grad_norm": 5.294563293457031,
      "learning_rate": 0.00010347460862924781,
      "loss": 0.9218,
      "step": 3797
    },
    {
      "epoch": 0.48305246422893483,
      "grad_norm": 2.0808420181274414,
      "learning_rate": 0.00010344915362097494,
      "loss": 0.7116,
      "step": 3798
    },
    {
      "epoch": 0.48317965023847376,
      "grad_norm": 2.847088575363159,
      "learning_rate": 0.00010342369861270205,
      "loss": 0.5228,
      "step": 3799
    },
    {
      "epoch": 0.48330683624801274,
      "grad_norm": 3.495713710784912,
      "learning_rate": 0.00010339824360442918,
      "loss": 0.9088,
      "step": 3800
    },
    {
      "epoch": 0.48343402225755167,
      "grad_norm": 3.770920753479004,
      "learning_rate": 0.00010337278859615628,
      "loss": 0.8087,
      "step": 3801
    },
    {
      "epoch": 0.48356120826709065,
      "grad_norm": 1.79207444190979,
      "learning_rate": 0.00010334733358788342,
      "loss": 0.5321,
      "step": 3802
    },
    {
      "epoch": 0.4836883942766296,
      "grad_norm": 1.7382922172546387,
      "learning_rate": 0.00010332187857961055,
      "loss": 0.7592,
      "step": 3803
    },
    {
      "epoch": 0.4838155802861685,
      "grad_norm": 2.044590473175049,
      "learning_rate": 0.00010329642357133767,
      "loss": 0.4967,
      "step": 3804
    },
    {
      "epoch": 0.4839427662957075,
      "grad_norm": 2.083214521408081,
      "learning_rate": 0.0001032709685630648,
      "loss": 0.7154,
      "step": 3805
    },
    {
      "epoch": 0.4840699523052464,
      "grad_norm": 2.253969669342041,
      "learning_rate": 0.0001032455135547919,
      "loss": 0.52,
      "step": 3806
    },
    {
      "epoch": 0.4841971383147854,
      "grad_norm": 2.5316381454467773,
      "learning_rate": 0.00010322005854651904,
      "loss": 0.5404,
      "step": 3807
    },
    {
      "epoch": 0.4843243243243243,
      "grad_norm": 1.9159600734710693,
      "learning_rate": 0.00010319460353824614,
      "loss": 0.4648,
      "step": 3808
    },
    {
      "epoch": 0.4844515103338633,
      "grad_norm": 1.7212470769882202,
      "learning_rate": 0.00010316914852997327,
      "loss": 0.6883,
      "step": 3809
    },
    {
      "epoch": 0.4845786963434022,
      "grad_norm": 1.9269497394561768,
      "learning_rate": 0.00010314369352170039,
      "loss": 0.5712,
      "step": 3810
    },
    {
      "epoch": 0.48470588235294115,
      "grad_norm": 1.7332532405853271,
      "learning_rate": 0.00010311823851342753,
      "loss": 0.5343,
      "step": 3811
    },
    {
      "epoch": 0.48483306836248014,
      "grad_norm": 2.080155849456787,
      "learning_rate": 0.00010309278350515463,
      "loss": 0.6756,
      "step": 3812
    },
    {
      "epoch": 0.48496025437201906,
      "grad_norm": 1.9508305788040161,
      "learning_rate": 0.00010306732849688176,
      "loss": 0.7028,
      "step": 3813
    },
    {
      "epoch": 0.48508744038155804,
      "grad_norm": 1.2678741216659546,
      "learning_rate": 0.0001030418734886089,
      "loss": 0.5077,
      "step": 3814
    },
    {
      "epoch": 0.48521462639109697,
      "grad_norm": 2.5331356525421143,
      "learning_rate": 0.00010301641848033601,
      "loss": 0.8792,
      "step": 3815
    },
    {
      "epoch": 0.48534181240063595,
      "grad_norm": 3.1268060207366943,
      "learning_rate": 0.00010299096347206315,
      "loss": 0.9362,
      "step": 3816
    },
    {
      "epoch": 0.4854689984101749,
      "grad_norm": 2.117169141769409,
      "learning_rate": 0.00010296550846379025,
      "loss": 0.681,
      "step": 3817
    },
    {
      "epoch": 0.4855961844197138,
      "grad_norm": 2.3966455459594727,
      "learning_rate": 0.00010294005345551738,
      "loss": 0.4979,
      "step": 3818
    },
    {
      "epoch": 0.4857233704292528,
      "grad_norm": 1.8672186136245728,
      "learning_rate": 0.00010291459844724449,
      "loss": 0.5091,
      "step": 3819
    },
    {
      "epoch": 0.4858505564387917,
      "grad_norm": 2.1712868213653564,
      "learning_rate": 0.00010288914343897162,
      "loss": 0.6675,
      "step": 3820
    },
    {
      "epoch": 0.4859777424483307,
      "grad_norm": 2.063433885574341,
      "learning_rate": 0.00010286368843069874,
      "loss": 0.757,
      "step": 3821
    },
    {
      "epoch": 0.4861049284578696,
      "grad_norm": 1.771238923072815,
      "learning_rate": 0.00010283823342242587,
      "loss": 0.5923,
      "step": 3822
    },
    {
      "epoch": 0.4862321144674086,
      "grad_norm": 1.7390453815460205,
      "learning_rate": 0.000102812778414153,
      "loss": 0.4846,
      "step": 3823
    },
    {
      "epoch": 0.48635930047694753,
      "grad_norm": 2.646074056625366,
      "learning_rate": 0.00010278732340588011,
      "loss": 0.5239,
      "step": 3824
    },
    {
      "epoch": 0.4864864864864865,
      "grad_norm": 1.5963239669799805,
      "learning_rate": 0.00010276186839760724,
      "loss": 0.4139,
      "step": 3825
    },
    {
      "epoch": 0.48661367249602544,
      "grad_norm": 3.0136654376983643,
      "learning_rate": 0.00010273641338933435,
      "loss": 0.7106,
      "step": 3826
    },
    {
      "epoch": 0.48674085850556437,
      "grad_norm": 1.7452160120010376,
      "learning_rate": 0.00010271095838106148,
      "loss": 0.6069,
      "step": 3827
    },
    {
      "epoch": 0.48686804451510335,
      "grad_norm": 2.6453146934509277,
      "learning_rate": 0.0001026855033727886,
      "loss": 0.7685,
      "step": 3828
    },
    {
      "epoch": 0.4869952305246423,
      "grad_norm": 1.9398585557937622,
      "learning_rate": 0.00010266004836451573,
      "loss": 0.6133,
      "step": 3829
    },
    {
      "epoch": 0.48712241653418126,
      "grad_norm": 2.1773781776428223,
      "learning_rate": 0.00010263459335624284,
      "loss": 0.4963,
      "step": 3830
    },
    {
      "epoch": 0.4872496025437202,
      "grad_norm": 1.9669586420059204,
      "learning_rate": 0.00010260913834796997,
      "loss": 0.492,
      "step": 3831
    },
    {
      "epoch": 0.48737678855325917,
      "grad_norm": 2.7427632808685303,
      "learning_rate": 0.0001025836833396971,
      "loss": 0.3844,
      "step": 3832
    },
    {
      "epoch": 0.4875039745627981,
      "grad_norm": 2.292754888534546,
      "learning_rate": 0.0001025582283314242,
      "loss": 0.7504,
      "step": 3833
    },
    {
      "epoch": 0.487631160572337,
      "grad_norm": 1.7527018785476685,
      "learning_rate": 0.00010253277332315134,
      "loss": 0.5582,
      "step": 3834
    },
    {
      "epoch": 0.487758346581876,
      "grad_norm": 1.6175583600997925,
      "learning_rate": 0.00010250731831487846,
      "loss": 0.6953,
      "step": 3835
    },
    {
      "epoch": 0.4878855325914149,
      "grad_norm": 1.7888011932373047,
      "learning_rate": 0.00010248186330660559,
      "loss": 0.5689,
      "step": 3836
    },
    {
      "epoch": 0.4880127186009539,
      "grad_norm": 2.5667693614959717,
      "learning_rate": 0.0001024564082983327,
      "loss": 0.7755,
      "step": 3837
    },
    {
      "epoch": 0.48813990461049284,
      "grad_norm": 1.6817752122879028,
      "learning_rate": 0.00010243095329005983,
      "loss": 0.6704,
      "step": 3838
    },
    {
      "epoch": 0.4882670906200318,
      "grad_norm": 1.8530033826828003,
      "learning_rate": 0.00010240549828178693,
      "loss": 0.5789,
      "step": 3839
    },
    {
      "epoch": 0.48839427662957074,
      "grad_norm": 1.950394868850708,
      "learning_rate": 0.00010238004327351406,
      "loss": 0.6998,
      "step": 3840
    },
    {
      "epoch": 0.48852146263910967,
      "grad_norm": 2.174466133117676,
      "learning_rate": 0.00010235458826524118,
      "loss": 0.4001,
      "step": 3841
    },
    {
      "epoch": 0.48864864864864865,
      "grad_norm": 2.846806049346924,
      "learning_rate": 0.00010232913325696831,
      "loss": 0.685,
      "step": 3842
    },
    {
      "epoch": 0.4887758346581876,
      "grad_norm": 2.195164442062378,
      "learning_rate": 0.00010230367824869545,
      "loss": 0.5994,
      "step": 3843
    },
    {
      "epoch": 0.48890302066772656,
      "grad_norm": 2.3645026683807373,
      "learning_rate": 0.00010227822324042255,
      "loss": 0.8199,
      "step": 3844
    },
    {
      "epoch": 0.4890302066772655,
      "grad_norm": 2.122523546218872,
      "learning_rate": 0.00010225276823214968,
      "loss": 0.4938,
      "step": 3845
    },
    {
      "epoch": 0.48915739268680447,
      "grad_norm": 2.216167449951172,
      "learning_rate": 0.0001022273132238768,
      "loss": 0.8207,
      "step": 3846
    },
    {
      "epoch": 0.4892845786963434,
      "grad_norm": 2.340883255004883,
      "learning_rate": 0.00010220185821560394,
      "loss": 0.6488,
      "step": 3847
    },
    {
      "epoch": 0.4894117647058824,
      "grad_norm": 2.442251443862915,
      "learning_rate": 0.00010217640320733104,
      "loss": 0.6366,
      "step": 3848
    },
    {
      "epoch": 0.4895389507154213,
      "grad_norm": 2.276754379272461,
      "learning_rate": 0.00010215094819905817,
      "loss": 0.7003,
      "step": 3849
    },
    {
      "epoch": 0.48966613672496023,
      "grad_norm": 2.1523196697235107,
      "learning_rate": 0.00010212549319078528,
      "loss": 0.7007,
      "step": 3850
    },
    {
      "epoch": 0.4897933227344992,
      "grad_norm": 2.0127015113830566,
      "learning_rate": 0.00010210003818251241,
      "loss": 0.5968,
      "step": 3851
    },
    {
      "epoch": 0.48992050874403814,
      "grad_norm": 2.5243046283721924,
      "learning_rate": 0.00010207458317423954,
      "loss": 0.7823,
      "step": 3852
    },
    {
      "epoch": 0.4900476947535771,
      "grad_norm": 2.8506319522857666,
      "learning_rate": 0.00010204912816596666,
      "loss": 0.7812,
      "step": 3853
    },
    {
      "epoch": 0.49017488076311605,
      "grad_norm": 1.4832651615142822,
      "learning_rate": 0.0001020236731576938,
      "loss": 0.5197,
      "step": 3854
    },
    {
      "epoch": 0.49030206677265503,
      "grad_norm": 4.081019401550293,
      "learning_rate": 0.0001019982181494209,
      "loss": 0.4726,
      "step": 3855
    },
    {
      "epoch": 0.49042925278219396,
      "grad_norm": 2.5191848278045654,
      "learning_rate": 0.00010197276314114803,
      "loss": 0.5777,
      "step": 3856
    },
    {
      "epoch": 0.4905564387917329,
      "grad_norm": 2.212111234664917,
      "learning_rate": 0.00010194730813287514,
      "loss": 0.6059,
      "step": 3857
    },
    {
      "epoch": 0.49068362480127187,
      "grad_norm": 2.0287296772003174,
      "learning_rate": 0.00010192185312460227,
      "loss": 0.648,
      "step": 3858
    },
    {
      "epoch": 0.4908108108108108,
      "grad_norm": 2.0916669368743896,
      "learning_rate": 0.00010189639811632939,
      "loss": 0.6209,
      "step": 3859
    },
    {
      "epoch": 0.4909379968203498,
      "grad_norm": 1.7963505983352661,
      "learning_rate": 0.00010187094310805652,
      "loss": 0.4889,
      "step": 3860
    },
    {
      "epoch": 0.4910651828298887,
      "grad_norm": 2.313655376434326,
      "learning_rate": 0.00010184548809978365,
      "loss": 0.7641,
      "step": 3861
    },
    {
      "epoch": 0.4911923688394277,
      "grad_norm": 2.12284779548645,
      "learning_rate": 0.00010182003309151076,
      "loss": 0.6174,
      "step": 3862
    },
    {
      "epoch": 0.4913195548489666,
      "grad_norm": 2.5342323780059814,
      "learning_rate": 0.00010179457808323789,
      "loss": 0.6307,
      "step": 3863
    },
    {
      "epoch": 0.49144674085850554,
      "grad_norm": 1.9809807538986206,
      "learning_rate": 0.000101769123074965,
      "loss": 0.6992,
      "step": 3864
    },
    {
      "epoch": 0.4915739268680445,
      "grad_norm": 2.209319591522217,
      "learning_rate": 0.00010174366806669213,
      "loss": 0.823,
      "step": 3865
    },
    {
      "epoch": 0.49170111287758345,
      "grad_norm": 1.5957216024398804,
      "learning_rate": 0.00010171821305841925,
      "loss": 0.5102,
      "step": 3866
    },
    {
      "epoch": 0.49182829888712243,
      "grad_norm": 2.2545039653778076,
      "learning_rate": 0.00010169275805014638,
      "loss": 0.8647,
      "step": 3867
    },
    {
      "epoch": 0.49195548489666135,
      "grad_norm": 2.6602437496185303,
      "learning_rate": 0.00010166730304187348,
      "loss": 0.7408,
      "step": 3868
    },
    {
      "epoch": 0.49208267090620034,
      "grad_norm": 2.337186574935913,
      "learning_rate": 0.00010164184803360062,
      "loss": 0.6831,
      "step": 3869
    },
    {
      "epoch": 0.49220985691573926,
      "grad_norm": 1.8592555522918701,
      "learning_rate": 0.00010161639302532772,
      "loss": 0.411,
      "step": 3870
    },
    {
      "epoch": 0.49233704292527825,
      "grad_norm": 5.030737400054932,
      "learning_rate": 0.00010159093801705485,
      "loss": 0.551,
      "step": 3871
    },
    {
      "epoch": 0.49246422893481717,
      "grad_norm": 2.576127290725708,
      "learning_rate": 0.00010156548300878199,
      "loss": 0.6182,
      "step": 3872
    },
    {
      "epoch": 0.4925914149443561,
      "grad_norm": 2.4619133472442627,
      "learning_rate": 0.0001015400280005091,
      "loss": 0.8781,
      "step": 3873
    },
    {
      "epoch": 0.4927186009538951,
      "grad_norm": 2.5704617500305176,
      "learning_rate": 0.00010151457299223624,
      "loss": 0.8218,
      "step": 3874
    },
    {
      "epoch": 0.492845786963434,
      "grad_norm": 2.306148052215576,
      "learning_rate": 0.00010148911798396334,
      "loss": 0.7004,
      "step": 3875
    },
    {
      "epoch": 0.492972972972973,
      "grad_norm": 2.637618064880371,
      "learning_rate": 0.00010146366297569047,
      "loss": 0.7948,
      "step": 3876
    },
    {
      "epoch": 0.4931001589825119,
      "grad_norm": 2.3864822387695312,
      "learning_rate": 0.00010143820796741759,
      "loss": 0.4678,
      "step": 3877
    },
    {
      "epoch": 0.4932273449920509,
      "grad_norm": 1.932227611541748,
      "learning_rate": 0.00010141275295914473,
      "loss": 0.7298,
      "step": 3878
    },
    {
      "epoch": 0.4933545310015898,
      "grad_norm": 1.9255398511886597,
      "learning_rate": 0.00010138729795087183,
      "loss": 0.5053,
      "step": 3879
    },
    {
      "epoch": 0.49348171701112875,
      "grad_norm": 1.5273724794387817,
      "learning_rate": 0.00010136184294259896,
      "loss": 0.6109,
      "step": 3880
    },
    {
      "epoch": 0.49360890302066773,
      "grad_norm": 1.85038423538208,
      "learning_rate": 0.0001013363879343261,
      "loss": 0.6426,
      "step": 3881
    },
    {
      "epoch": 0.49373608903020666,
      "grad_norm": 2.300468683242798,
      "learning_rate": 0.0001013109329260532,
      "loss": 0.8483,
      "step": 3882
    },
    {
      "epoch": 0.49386327503974564,
      "grad_norm": 1.7585713863372803,
      "learning_rate": 0.00010128547791778033,
      "loss": 0.3741,
      "step": 3883
    },
    {
      "epoch": 0.49399046104928457,
      "grad_norm": 2.252868413925171,
      "learning_rate": 0.00010126002290950745,
      "loss": 0.4688,
      "step": 3884
    },
    {
      "epoch": 0.49411764705882355,
      "grad_norm": 2.429145574569702,
      "learning_rate": 0.00010123456790123458,
      "loss": 0.6191,
      "step": 3885
    },
    {
      "epoch": 0.4942448330683625,
      "grad_norm": 1.7138830423355103,
      "learning_rate": 0.00010120911289296169,
      "loss": 0.6142,
      "step": 3886
    },
    {
      "epoch": 0.4943720190779014,
      "grad_norm": 2.2298784255981445,
      "learning_rate": 0.00010118365788468882,
      "loss": 0.5453,
      "step": 3887
    },
    {
      "epoch": 0.4944992050874404,
      "grad_norm": 1.6879453659057617,
      "learning_rate": 0.00010115820287641593,
      "loss": 0.6576,
      "step": 3888
    },
    {
      "epoch": 0.4946263910969793,
      "grad_norm": 1.7406479120254517,
      "learning_rate": 0.00010113274786814306,
      "loss": 0.4555,
      "step": 3889
    },
    {
      "epoch": 0.4947535771065183,
      "grad_norm": 2.489812135696411,
      "learning_rate": 0.00010110729285987019,
      "loss": 0.353,
      "step": 3890
    },
    {
      "epoch": 0.4948807631160572,
      "grad_norm": 2.5902631282806396,
      "learning_rate": 0.00010108183785159731,
      "loss": 0.631,
      "step": 3891
    },
    {
      "epoch": 0.4950079491255962,
      "grad_norm": 2.200967311859131,
      "learning_rate": 0.00010105638284332444,
      "loss": 0.7314,
      "step": 3892
    },
    {
      "epoch": 0.49513513513513513,
      "grad_norm": 2.8912198543548584,
      "learning_rate": 0.00010103092783505155,
      "loss": 0.8697,
      "step": 3893
    },
    {
      "epoch": 0.4952623211446741,
      "grad_norm": 2.1438612937927246,
      "learning_rate": 0.00010100547282677868,
      "loss": 0.7395,
      "step": 3894
    },
    {
      "epoch": 0.49538950715421304,
      "grad_norm": 2.055708408355713,
      "learning_rate": 0.00010098001781850578,
      "loss": 0.5631,
      "step": 3895
    },
    {
      "epoch": 0.49551669316375196,
      "grad_norm": 1.7922507524490356,
      "learning_rate": 0.00010095456281023292,
      "loss": 0.6781,
      "step": 3896
    },
    {
      "epoch": 0.49564387917329095,
      "grad_norm": 3.2234342098236084,
      "learning_rate": 0.00010092910780196004,
      "loss": 0.5861,
      "step": 3897
    },
    {
      "epoch": 0.4957710651828299,
      "grad_norm": 2.1757984161376953,
      "learning_rate": 0.00010090365279368717,
      "loss": 0.5073,
      "step": 3898
    },
    {
      "epoch": 0.49589825119236886,
      "grad_norm": 3.157332420349121,
      "learning_rate": 0.00010087819778541427,
      "loss": 0.5702,
      "step": 3899
    },
    {
      "epoch": 0.4960254372019078,
      "grad_norm": 2.2724239826202393,
      "learning_rate": 0.0001008527427771414,
      "loss": 0.4185,
      "step": 3900
    },
    {
      "epoch": 0.49615262321144676,
      "grad_norm": 3.6351678371429443,
      "learning_rate": 0.00010082728776886854,
      "loss": 0.7848,
      "step": 3901
    },
    {
      "epoch": 0.4962798092209857,
      "grad_norm": 2.285489797592163,
      "learning_rate": 0.00010080183276059564,
      "loss": 0.4453,
      "step": 3902
    },
    {
      "epoch": 0.4964069952305246,
      "grad_norm": 2.2066948413848877,
      "learning_rate": 0.00010077637775232277,
      "loss": 0.56,
      "step": 3903
    },
    {
      "epoch": 0.4965341812400636,
      "grad_norm": 2.4082865715026855,
      "learning_rate": 0.0001007509227440499,
      "loss": 0.7249,
      "step": 3904
    },
    {
      "epoch": 0.4966613672496025,
      "grad_norm": 1.7203476428985596,
      "learning_rate": 0.00010072546773577703,
      "loss": 0.452,
      "step": 3905
    },
    {
      "epoch": 0.4967885532591415,
      "grad_norm": 1.5315196514129639,
      "learning_rate": 0.00010070001272750413,
      "loss": 0.62,
      "step": 3906
    },
    {
      "epoch": 0.49691573926868043,
      "grad_norm": 2.0493550300598145,
      "learning_rate": 0.00010067455771923126,
      "loss": 0.4564,
      "step": 3907
    },
    {
      "epoch": 0.4970429252782194,
      "grad_norm": 2.4418625831604004,
      "learning_rate": 0.00010064910271095838,
      "loss": 0.5715,
      "step": 3908
    },
    {
      "epoch": 0.49717011128775834,
      "grad_norm": 2.1803090572357178,
      "learning_rate": 0.00010062364770268551,
      "loss": 0.5961,
      "step": 3909
    },
    {
      "epoch": 0.4972972972972973,
      "grad_norm": 2.3528249263763428,
      "learning_rate": 0.00010059819269441265,
      "loss": 0.4936,
      "step": 3910
    },
    {
      "epoch": 0.49742448330683625,
      "grad_norm": 1.9482377767562866,
      "learning_rate": 0.00010057273768613975,
      "loss": 0.5541,
      "step": 3911
    },
    {
      "epoch": 0.4975516693163752,
      "grad_norm": 1.1150012016296387,
      "learning_rate": 0.00010054728267786688,
      "loss": 0.4975,
      "step": 3912
    },
    {
      "epoch": 0.49767885532591416,
      "grad_norm": 2.9336180686950684,
      "learning_rate": 0.00010052182766959399,
      "loss": 0.8659,
      "step": 3913
    },
    {
      "epoch": 0.4978060413354531,
      "grad_norm": 1.3965400457382202,
      "learning_rate": 0.00010049637266132112,
      "loss": 0.6188,
      "step": 3914
    },
    {
      "epoch": 0.49793322734499207,
      "grad_norm": 2.853013038635254,
      "learning_rate": 0.00010047091765304824,
      "loss": 0.7341,
      "step": 3915
    },
    {
      "epoch": 0.498060413354531,
      "grad_norm": 2.3533811569213867,
      "learning_rate": 0.00010044546264477537,
      "loss": 0.4904,
      "step": 3916
    },
    {
      "epoch": 0.49818759936407,
      "grad_norm": 2.732455253601074,
      "learning_rate": 0.00010042000763650248,
      "loss": 0.6273,
      "step": 3917
    },
    {
      "epoch": 0.4983147853736089,
      "grad_norm": 2.004615306854248,
      "learning_rate": 0.00010039455262822961,
      "loss": 0.4873,
      "step": 3918
    },
    {
      "epoch": 0.49844197138314783,
      "grad_norm": 2.1653878688812256,
      "learning_rate": 0.00010036909761995674,
      "loss": 0.8357,
      "step": 3919
    },
    {
      "epoch": 0.4985691573926868,
      "grad_norm": 2.093634843826294,
      "learning_rate": 0.00010034364261168385,
      "loss": 0.8595,
      "step": 3920
    },
    {
      "epoch": 0.49869634340222574,
      "grad_norm": 2.123403787612915,
      "learning_rate": 0.00010031818760341098,
      "loss": 0.6089,
      "step": 3921
    },
    {
      "epoch": 0.4988235294117647,
      "grad_norm": 1.828134298324585,
      "learning_rate": 0.0001002927325951381,
      "loss": 0.5388,
      "step": 3922
    },
    {
      "epoch": 0.49895071542130365,
      "grad_norm": 1.8947399854660034,
      "learning_rate": 0.00010026727758686523,
      "loss": 0.3951,
      "step": 3923
    },
    {
      "epoch": 0.49907790143084263,
      "grad_norm": 2.6105833053588867,
      "learning_rate": 0.00010024182257859234,
      "loss": 0.5058,
      "step": 3924
    },
    {
      "epoch": 0.49920508744038156,
      "grad_norm": 2.936400890350342,
      "learning_rate": 0.00010021636757031947,
      "loss": 0.9033,
      "step": 3925
    },
    {
      "epoch": 0.4993322734499205,
      "grad_norm": 1.867972731590271,
      "learning_rate": 0.00010019091256204657,
      "loss": 0.7211,
      "step": 3926
    },
    {
      "epoch": 0.49945945945945946,
      "grad_norm": 2.060450792312622,
      "learning_rate": 0.0001001654575537737,
      "loss": 0.6042,
      "step": 3927
    },
    {
      "epoch": 0.4995866454689984,
      "grad_norm": 2.2352144718170166,
      "learning_rate": 0.00010014000254550082,
      "loss": 0.6593,
      "step": 3928
    },
    {
      "epoch": 0.4997138314785374,
      "grad_norm": 2.405007839202881,
      "learning_rate": 0.00010011454753722796,
      "loss": 0.7991,
      "step": 3929
    },
    {
      "epoch": 0.4998410174880763,
      "grad_norm": 1.4545783996582031,
      "learning_rate": 0.00010008909252895509,
      "loss": 0.6184,
      "step": 3930
    },
    {
      "epoch": 0.4999682034976153,
      "grad_norm": 3.1996593475341797,
      "learning_rate": 0.0001000636375206822,
      "loss": 0.6495,
      "step": 3931
    },
    {
      "epoch": 0.5000953895071543,
      "grad_norm": 1.9340574741363525,
      "learning_rate": 0.00010003818251240933,
      "loss": 0.9698,
      "step": 3932
    },
    {
      "epoch": 0.5002225755166931,
      "grad_norm": 2.1424143314361572,
      "learning_rate": 0.00010001272750413645,
      "loss": 0.6744,
      "step": 3933
    },
    {
      "epoch": 0.5003497615262321,
      "grad_norm": 1.9874874353408813,
      "learning_rate": 9.998727249586356e-05,
      "loss": 0.5532,
      "step": 3934
    },
    {
      "epoch": 0.5004769475357711,
      "grad_norm": 2.036360263824463,
      "learning_rate": 9.99618174875907e-05,
      "loss": 0.7082,
      "step": 3935
    },
    {
      "epoch": 0.50060413354531,
      "grad_norm": 1.8696269989013672,
      "learning_rate": 9.993636247931782e-05,
      "loss": 0.9021,
      "step": 3936
    },
    {
      "epoch": 0.500731319554849,
      "grad_norm": 1.7497724294662476,
      "learning_rate": 9.991090747104493e-05,
      "loss": 0.5978,
      "step": 3937
    },
    {
      "epoch": 0.5008585055643879,
      "grad_norm": 2.060251235961914,
      "learning_rate": 9.988545246277205e-05,
      "loss": 0.5872,
      "step": 3938
    },
    {
      "epoch": 0.5009856915739269,
      "grad_norm": 1.7640070915222168,
      "learning_rate": 9.985999745449917e-05,
      "loss": 0.7033,
      "step": 3939
    },
    {
      "epoch": 0.5011128775834658,
      "grad_norm": 1.7837259769439697,
      "learning_rate": 9.98345424462263e-05,
      "loss": 0.6501,
      "step": 3940
    },
    {
      "epoch": 0.5012400635930048,
      "grad_norm": 1.694066047668457,
      "learning_rate": 9.980908743795342e-05,
      "loss": 0.6213,
      "step": 3941
    },
    {
      "epoch": 0.5013672496025438,
      "grad_norm": 1.935469150543213,
      "learning_rate": 9.978363242968054e-05,
      "loss": 0.8147,
      "step": 3942
    },
    {
      "epoch": 0.5014944356120826,
      "grad_norm": 2.3578410148620605,
      "learning_rate": 9.975817742140766e-05,
      "loss": 0.7844,
      "step": 3943
    },
    {
      "epoch": 0.5016216216216216,
      "grad_norm": 1.99942946434021,
      "learning_rate": 9.973272241313479e-05,
      "loss": 0.8236,
      "step": 3944
    },
    {
      "epoch": 0.5017488076311606,
      "grad_norm": 1.8696081638336182,
      "learning_rate": 9.970726740486191e-05,
      "loss": 0.7879,
      "step": 3945
    },
    {
      "epoch": 0.5018759936406996,
      "grad_norm": 1.691031575202942,
      "learning_rate": 9.968181239658903e-05,
      "loss": 0.6114,
      "step": 3946
    },
    {
      "epoch": 0.5020031796502384,
      "grad_norm": 1.6490118503570557,
      "learning_rate": 9.965635738831616e-05,
      "loss": 0.5136,
      "step": 3947
    },
    {
      "epoch": 0.5021303656597774,
      "grad_norm": 1.7043507099151611,
      "learning_rate": 9.963090238004328e-05,
      "loss": 0.6636,
      "step": 3948
    },
    {
      "epoch": 0.5022575516693164,
      "grad_norm": 1.966400384902954,
      "learning_rate": 9.96054473717704e-05,
      "loss": 0.4854,
      "step": 3949
    },
    {
      "epoch": 0.5023847376788553,
      "grad_norm": 1.8841185569763184,
      "learning_rate": 9.957999236349752e-05,
      "loss": 0.8273,
      "step": 3950
    },
    {
      "epoch": 0.5025119236883943,
      "grad_norm": 2.0711023807525635,
      "learning_rate": 9.955453735522464e-05,
      "loss": 0.6083,
      "step": 3951
    },
    {
      "epoch": 0.5026391096979332,
      "grad_norm": 2.286661148071289,
      "learning_rate": 9.952908234695177e-05,
      "loss": 0.6483,
      "step": 3952
    },
    {
      "epoch": 0.5027662957074722,
      "grad_norm": 2.4487171173095703,
      "learning_rate": 9.95036273386789e-05,
      "loss": 0.4765,
      "step": 3953
    },
    {
      "epoch": 0.5028934817170111,
      "grad_norm": 2.196204423904419,
      "learning_rate": 9.947817233040602e-05,
      "loss": 0.8113,
      "step": 3954
    },
    {
      "epoch": 0.5030206677265501,
      "grad_norm": 2.6039092540740967,
      "learning_rate": 9.945271732213314e-05,
      "loss": 0.6311,
      "step": 3955
    },
    {
      "epoch": 0.5031478537360891,
      "grad_norm": 1.88522469997406,
      "learning_rate": 9.942726231386026e-05,
      "loss": 0.8773,
      "step": 3956
    },
    {
      "epoch": 0.5032750397456279,
      "grad_norm": 2.6393558979034424,
      "learning_rate": 9.940180730558738e-05,
      "loss": 0.598,
      "step": 3957
    },
    {
      "epoch": 0.5034022257551669,
      "grad_norm": 2.6958682537078857,
      "learning_rate": 9.93763522973145e-05,
      "loss": 0.7105,
      "step": 3958
    },
    {
      "epoch": 0.5035294117647059,
      "grad_norm": 2.868464469909668,
      "learning_rate": 9.935089728904163e-05,
      "loss": 0.6464,
      "step": 3959
    },
    {
      "epoch": 0.5036565977742449,
      "grad_norm": 2.731562614440918,
      "learning_rate": 9.932544228076875e-05,
      "loss": 0.664,
      "step": 3960
    },
    {
      "epoch": 0.5037837837837837,
      "grad_norm": 1.910070776939392,
      "learning_rate": 9.929998727249586e-05,
      "loss": 0.4404,
      "step": 3961
    },
    {
      "epoch": 0.5039109697933227,
      "grad_norm": 2.8790454864501953,
      "learning_rate": 9.927453226422298e-05,
      "loss": 0.5246,
      "step": 3962
    },
    {
      "epoch": 0.5040381558028617,
      "grad_norm": 2.344787120819092,
      "learning_rate": 9.924907725595012e-05,
      "loss": 0.7239,
      "step": 3963
    },
    {
      "epoch": 0.5041653418124007,
      "grad_norm": 2.636631488800049,
      "learning_rate": 9.922362224767723e-05,
      "loss": 0.8088,
      "step": 3964
    },
    {
      "epoch": 0.5042925278219396,
      "grad_norm": 1.9952117204666138,
      "learning_rate": 9.919816723940437e-05,
      "loss": 0.628,
      "step": 3965
    },
    {
      "epoch": 0.5044197138314785,
      "grad_norm": 2.1156411170959473,
      "learning_rate": 9.917271223113149e-05,
      "loss": 0.5753,
      "step": 3966
    },
    {
      "epoch": 0.5045468998410175,
      "grad_norm": 2.0892271995544434,
      "learning_rate": 9.91472572228586e-05,
      "loss": 0.9262,
      "step": 3967
    },
    {
      "epoch": 0.5046740858505564,
      "grad_norm": 1.8571879863739014,
      "learning_rate": 9.912180221458572e-05,
      "loss": 0.7789,
      "step": 3968
    },
    {
      "epoch": 0.5048012718600954,
      "grad_norm": 1.5273360013961792,
      "learning_rate": 9.909634720631284e-05,
      "loss": 0.6059,
      "step": 3969
    },
    {
      "epoch": 0.5049284578696344,
      "grad_norm": 2.4455995559692383,
      "learning_rate": 9.907089219803996e-05,
      "loss": 0.5288,
      "step": 3970
    },
    {
      "epoch": 0.5050556438791733,
      "grad_norm": 2.5070533752441406,
      "learning_rate": 9.904543718976709e-05,
      "loss": 0.4876,
      "step": 3971
    },
    {
      "epoch": 0.5051828298887122,
      "grad_norm": 2.2102177143096924,
      "learning_rate": 9.901998218149421e-05,
      "loss": 0.903,
      "step": 3972
    },
    {
      "epoch": 0.5053100158982512,
      "grad_norm": 3.1952402591705322,
      "learning_rate": 9.899452717322134e-05,
      "loss": 0.7854,
      "step": 3973
    },
    {
      "epoch": 0.5054372019077902,
      "grad_norm": 2.325748920440674,
      "learning_rate": 9.896907216494846e-05,
      "loss": 0.6258,
      "step": 3974
    },
    {
      "epoch": 0.505564387917329,
      "grad_norm": 1.7878482341766357,
      "learning_rate": 9.894361715667558e-05,
      "loss": 0.5707,
      "step": 3975
    },
    {
      "epoch": 0.505691573926868,
      "grad_norm": 2.5294125080108643,
      "learning_rate": 9.89181621484027e-05,
      "loss": 0.5378,
      "step": 3976
    },
    {
      "epoch": 0.505818759936407,
      "grad_norm": 1.8073874711990356,
      "learning_rate": 9.889270714012982e-05,
      "loss": 0.6305,
      "step": 3977
    },
    {
      "epoch": 0.505945945945946,
      "grad_norm": 2.033000946044922,
      "learning_rate": 9.886725213185695e-05,
      "loss": 0.6581,
      "step": 3978
    },
    {
      "epoch": 0.5060731319554849,
      "grad_norm": 1.986346960067749,
      "learning_rate": 9.884179712358407e-05,
      "loss": 0.5892,
      "step": 3979
    },
    {
      "epoch": 0.5062003179650238,
      "grad_norm": 2.5114121437072754,
      "learning_rate": 9.881634211531119e-05,
      "loss": 0.7697,
      "step": 3980
    },
    {
      "epoch": 0.5063275039745628,
      "grad_norm": 1.7393505573272705,
      "learning_rate": 9.879088710703831e-05,
      "loss": 0.5237,
      "step": 3981
    },
    {
      "epoch": 0.5064546899841017,
      "grad_norm": 2.069080114364624,
      "learning_rate": 9.876543209876543e-05,
      "loss": 0.6479,
      "step": 3982
    },
    {
      "epoch": 0.5065818759936407,
      "grad_norm": 1.50110924243927,
      "learning_rate": 9.873997709049256e-05,
      "loss": 0.5253,
      "step": 3983
    },
    {
      "epoch": 0.5067090620031797,
      "grad_norm": 1.896155595779419,
      "learning_rate": 9.871452208221969e-05,
      "loss": 0.4121,
      "step": 3984
    },
    {
      "epoch": 0.5068362480127186,
      "grad_norm": 3.3073904514312744,
      "learning_rate": 9.868906707394681e-05,
      "loss": 0.9205,
      "step": 3985
    },
    {
      "epoch": 0.5069634340222575,
      "grad_norm": 2.4685511589050293,
      "learning_rate": 9.866361206567393e-05,
      "loss": 0.5663,
      "step": 3986
    },
    {
      "epoch": 0.5070906200317965,
      "grad_norm": 1.8871026039123535,
      "learning_rate": 9.863815705740105e-05,
      "loss": 0.6729,
      "step": 3987
    },
    {
      "epoch": 0.5072178060413355,
      "grad_norm": 1.8997355699539185,
      "learning_rate": 9.861270204912817e-05,
      "loss": 0.5343,
      "step": 3988
    },
    {
      "epoch": 0.5073449920508744,
      "grad_norm": 1.847590684890747,
      "learning_rate": 9.858724704085528e-05,
      "loss": 0.4944,
      "step": 3989
    },
    {
      "epoch": 0.5074721780604133,
      "grad_norm": 2.2915289402008057,
      "learning_rate": 9.856179203258242e-05,
      "loss": 0.6522,
      "step": 3990
    },
    {
      "epoch": 0.5075993640699523,
      "grad_norm": 2.381324291229248,
      "learning_rate": 9.853633702430954e-05,
      "loss": 0.4513,
      "step": 3991
    },
    {
      "epoch": 0.5077265500794913,
      "grad_norm": 2.9607481956481934,
      "learning_rate": 9.851088201603667e-05,
      "loss": 0.6142,
      "step": 3992
    },
    {
      "epoch": 0.5078537360890302,
      "grad_norm": 2.837205410003662,
      "learning_rate": 9.848542700776379e-05,
      "loss": 0.8986,
      "step": 3993
    },
    {
      "epoch": 0.5079809220985692,
      "grad_norm": 1.9525617361068726,
      "learning_rate": 9.84599719994909e-05,
      "loss": 0.4951,
      "step": 3994
    },
    {
      "epoch": 0.5081081081081081,
      "grad_norm": 1.5714013576507568,
      "learning_rate": 9.843451699121802e-05,
      "loss": 0.496,
      "step": 3995
    },
    {
      "epoch": 0.508235294117647,
      "grad_norm": 2.1252310276031494,
      "learning_rate": 9.840906198294516e-05,
      "loss": 0.8559,
      "step": 3996
    },
    {
      "epoch": 0.508362480127186,
      "grad_norm": 2.166435480117798,
      "learning_rate": 9.838360697467228e-05,
      "loss": 0.6685,
      "step": 3997
    },
    {
      "epoch": 0.508489666136725,
      "grad_norm": 2.249899387359619,
      "learning_rate": 9.83581519663994e-05,
      "loss": 0.8436,
      "step": 3998
    },
    {
      "epoch": 0.508616852146264,
      "grad_norm": 1.8977417945861816,
      "learning_rate": 9.833269695812651e-05,
      "loss": 0.4229,
      "step": 3999
    },
    {
      "epoch": 0.5087440381558028,
      "grad_norm": 2.215851306915283,
      "learning_rate": 9.830724194985363e-05,
      "loss": 0.3038,
      "step": 4000
    },
    {
      "epoch": 0.5088712241653418,
      "grad_norm": 1.6780874729156494,
      "learning_rate": 9.828178694158075e-05,
      "loss": 0.4342,
      "step": 4001
    },
    {
      "epoch": 0.5089984101748808,
      "grad_norm": 1.2306956052780151,
      "learning_rate": 9.825633193330788e-05,
      "loss": 0.5043,
      "step": 4002
    },
    {
      "epoch": 0.5091255961844197,
      "grad_norm": 2.639713764190674,
      "learning_rate": 9.823087692503501e-05,
      "loss": 0.8496,
      "step": 4003
    },
    {
      "epoch": 0.5092527821939586,
      "grad_norm": 2.670281410217285,
      "learning_rate": 9.820542191676213e-05,
      "loss": 0.5383,
      "step": 4004
    },
    {
      "epoch": 0.5093799682034976,
      "grad_norm": 1.8610697984695435,
      "learning_rate": 9.817996690848925e-05,
      "loss": 0.6637,
      "step": 4005
    },
    {
      "epoch": 0.5095071542130366,
      "grad_norm": 1.7953460216522217,
      "learning_rate": 9.815451190021637e-05,
      "loss": 0.579,
      "step": 4006
    },
    {
      "epoch": 0.5096343402225755,
      "grad_norm": 1.8431607484817505,
      "learning_rate": 9.812905689194349e-05,
      "loss": 0.5327,
      "step": 4007
    },
    {
      "epoch": 0.5097615262321145,
      "grad_norm": 2.1120870113372803,
      "learning_rate": 9.810360188367061e-05,
      "loss": 0.5259,
      "step": 4008
    },
    {
      "epoch": 0.5098887122416534,
      "grad_norm": 1.7520374059677124,
      "learning_rate": 9.807814687539774e-05,
      "loss": 0.6512,
      "step": 4009
    },
    {
      "epoch": 0.5100158982511924,
      "grad_norm": 2.427802562713623,
      "learning_rate": 9.805269186712486e-05,
      "loss": 0.9874,
      "step": 4010
    },
    {
      "epoch": 0.5101430842607313,
      "grad_norm": 1.7506847381591797,
      "learning_rate": 9.802723685885198e-05,
      "loss": 0.5743,
      "step": 4011
    },
    {
      "epoch": 0.5102702702702703,
      "grad_norm": 1.7773289680480957,
      "learning_rate": 9.800178185057911e-05,
      "loss": 0.4511,
      "step": 4012
    },
    {
      "epoch": 0.5103974562798093,
      "grad_norm": 1.8838062286376953,
      "learning_rate": 9.797632684230623e-05,
      "loss": 0.7381,
      "step": 4013
    },
    {
      "epoch": 0.5105246422893481,
      "grad_norm": 1.9037237167358398,
      "learning_rate": 9.795087183403335e-05,
      "loss": 0.7075,
      "step": 4014
    },
    {
      "epoch": 0.5106518282988871,
      "grad_norm": 1.6219534873962402,
      "learning_rate": 9.792541682576048e-05,
      "loss": 0.6578,
      "step": 4015
    },
    {
      "epoch": 0.5107790143084261,
      "grad_norm": 1.9089329242706299,
      "learning_rate": 9.78999618174876e-05,
      "loss": 0.6252,
      "step": 4016
    },
    {
      "epoch": 0.5109062003179651,
      "grad_norm": 2.219620943069458,
      "learning_rate": 9.787450680921472e-05,
      "loss": 0.7955,
      "step": 4017
    },
    {
      "epoch": 0.511033386327504,
      "grad_norm": 1.5130385160446167,
      "learning_rate": 9.784905180094184e-05,
      "loss": 0.7527,
      "step": 4018
    },
    {
      "epoch": 0.5111605723370429,
      "grad_norm": 1.9392246007919312,
      "learning_rate": 9.782359679266896e-05,
      "loss": 0.6205,
      "step": 4019
    },
    {
      "epoch": 0.5112877583465819,
      "grad_norm": 1.6724141836166382,
      "learning_rate": 9.779814178439607e-05,
      "loss": 0.436,
      "step": 4020
    },
    {
      "epoch": 0.5114149443561208,
      "grad_norm": 2.534201145172119,
      "learning_rate": 9.77726867761232e-05,
      "loss": 0.6419,
      "step": 4021
    },
    {
      "epoch": 0.5115421303656598,
      "grad_norm": 1.9895823001861572,
      "learning_rate": 9.774723176785034e-05,
      "loss": 0.5803,
      "step": 4022
    },
    {
      "epoch": 0.5116693163751987,
      "grad_norm": 2.131525754928589,
      "learning_rate": 9.772177675957746e-05,
      "loss": 0.5608,
      "step": 4023
    },
    {
      "epoch": 0.5117965023847377,
      "grad_norm": 2.248885154724121,
      "learning_rate": 9.769632175130458e-05,
      "loss": 0.5674,
      "step": 4024
    },
    {
      "epoch": 0.5119236883942766,
      "grad_norm": 2.2523887157440186,
      "learning_rate": 9.76708667430317e-05,
      "loss": 0.5435,
      "step": 4025
    },
    {
      "epoch": 0.5120508744038156,
      "grad_norm": 2.330390691757202,
      "learning_rate": 9.764541173475881e-05,
      "loss": 0.5937,
      "step": 4026
    },
    {
      "epoch": 0.5121780604133546,
      "grad_norm": 2.1089344024658203,
      "learning_rate": 9.761995672648595e-05,
      "loss": 0.6391,
      "step": 4027
    },
    {
      "epoch": 0.5123052464228934,
      "grad_norm": 1.9530601501464844,
      "learning_rate": 9.759450171821306e-05,
      "loss": 0.7803,
      "step": 4028
    },
    {
      "epoch": 0.5124324324324324,
      "grad_norm": 3.178800106048584,
      "learning_rate": 9.756904670994018e-05,
      "loss": 0.8413,
      "step": 4029
    },
    {
      "epoch": 0.5125596184419714,
      "grad_norm": 1.9734392166137695,
      "learning_rate": 9.75435917016673e-05,
      "loss": 0.8163,
      "step": 4030
    },
    {
      "epoch": 0.5126868044515104,
      "grad_norm": 3.2324936389923096,
      "learning_rate": 9.751813669339443e-05,
      "loss": 0.7395,
      "step": 4031
    },
    {
      "epoch": 0.5128139904610493,
      "grad_norm": 3.0130298137664795,
      "learning_rate": 9.749268168512155e-05,
      "loss": 0.7239,
      "step": 4032
    },
    {
      "epoch": 0.5129411764705882,
      "grad_norm": 1.386411190032959,
      "learning_rate": 9.746722667684867e-05,
      "loss": 0.3455,
      "step": 4033
    },
    {
      "epoch": 0.5130683624801272,
      "grad_norm": 3.2792751789093018,
      "learning_rate": 9.74417716685758e-05,
      "loss": 0.8733,
      "step": 4034
    },
    {
      "epoch": 0.5131955484896661,
      "grad_norm": 2.2124552726745605,
      "learning_rate": 9.741631666030292e-05,
      "loss": 0.6213,
      "step": 4035
    },
    {
      "epoch": 0.5133227344992051,
      "grad_norm": 2.1583778858184814,
      "learning_rate": 9.739086165203004e-05,
      "loss": 0.631,
      "step": 4036
    },
    {
      "epoch": 0.513449920508744,
      "grad_norm": 2.259793281555176,
      "learning_rate": 9.736540664375716e-05,
      "loss": 0.6349,
      "step": 4037
    },
    {
      "epoch": 0.513577106518283,
      "grad_norm": 2.9082741737365723,
      "learning_rate": 9.733995163548428e-05,
      "loss": 0.6859,
      "step": 4038
    },
    {
      "epoch": 0.5137042925278219,
      "grad_norm": 1.8489173650741577,
      "learning_rate": 9.73144966272114e-05,
      "loss": 0.3963,
      "step": 4039
    },
    {
      "epoch": 0.5138314785373609,
      "grad_norm": 1.5475645065307617,
      "learning_rate": 9.728904161893853e-05,
      "loss": 0.5398,
      "step": 4040
    },
    {
      "epoch": 0.5139586645468999,
      "grad_norm": 1.9259514808654785,
      "learning_rate": 9.726358661066566e-05,
      "loss": 0.5828,
      "step": 4041
    },
    {
      "epoch": 0.5140858505564387,
      "grad_norm": 2.1531918048858643,
      "learning_rate": 9.723813160239278e-05,
      "loss": 0.6202,
      "step": 4042
    },
    {
      "epoch": 0.5142130365659777,
      "grad_norm": 2.4620208740234375,
      "learning_rate": 9.72126765941199e-05,
      "loss": 0.654,
      "step": 4043
    },
    {
      "epoch": 0.5143402225755167,
      "grad_norm": 2.199563503265381,
      "learning_rate": 9.718722158584702e-05,
      "loss": 0.5013,
      "step": 4044
    },
    {
      "epoch": 0.5144674085850557,
      "grad_norm": 2.0112249851226807,
      "learning_rate": 9.716176657757414e-05,
      "loss": 0.68,
      "step": 4045
    },
    {
      "epoch": 0.5145945945945946,
      "grad_norm": 1.7123984098434448,
      "learning_rate": 9.713631156930127e-05,
      "loss": 0.8104,
      "step": 4046
    },
    {
      "epoch": 0.5147217806041335,
      "grad_norm": 1.4363317489624023,
      "learning_rate": 9.711085656102839e-05,
      "loss": 0.5476,
      "step": 4047
    },
    {
      "epoch": 0.5148489666136725,
      "grad_norm": 1.4103559255599976,
      "learning_rate": 9.708540155275551e-05,
      "loss": 0.4481,
      "step": 4048
    },
    {
      "epoch": 0.5149761526232114,
      "grad_norm": 1.541319727897644,
      "learning_rate": 9.705994654448263e-05,
      "loss": 0.4556,
      "step": 4049
    },
    {
      "epoch": 0.5151033386327504,
      "grad_norm": 2.5793323516845703,
      "learning_rate": 9.703449153620974e-05,
      "loss": 0.7992,
      "step": 4050
    },
    {
      "epoch": 0.5152305246422894,
      "grad_norm": 2.3368396759033203,
      "learning_rate": 9.700903652793688e-05,
      "loss": 0.476,
      "step": 4051
    },
    {
      "epoch": 0.5153577106518283,
      "grad_norm": 1.4836310148239136,
      "learning_rate": 9.6983581519664e-05,
      "loss": 0.5134,
      "step": 4052
    },
    {
      "epoch": 0.5154848966613672,
      "grad_norm": 1.4570807218551636,
      "learning_rate": 9.695812651139113e-05,
      "loss": 0.4108,
      "step": 4053
    },
    {
      "epoch": 0.5156120826709062,
      "grad_norm": 1.6952390670776367,
      "learning_rate": 9.693267150311825e-05,
      "loss": 0.4419,
      "step": 4054
    },
    {
      "epoch": 0.5157392686804452,
      "grad_norm": 1.6633422374725342,
      "learning_rate": 9.690721649484537e-05,
      "loss": 0.5756,
      "step": 4055
    },
    {
      "epoch": 0.5158664546899842,
      "grad_norm": 1.920578956604004,
      "learning_rate": 9.688176148657248e-05,
      "loss": 0.476,
      "step": 4056
    },
    {
      "epoch": 0.515993640699523,
      "grad_norm": 2.0453484058380127,
      "learning_rate": 9.68563064782996e-05,
      "loss": 0.9846,
      "step": 4057
    },
    {
      "epoch": 0.516120826709062,
      "grad_norm": 2.427619695663452,
      "learning_rate": 9.683085147002674e-05,
      "loss": 0.8212,
      "step": 4058
    },
    {
      "epoch": 0.516248012718601,
      "grad_norm": 2.4904327392578125,
      "learning_rate": 9.680539646175385e-05,
      "loss": 0.6382,
      "step": 4059
    },
    {
      "epoch": 0.5163751987281399,
      "grad_norm": 1.5525118112564087,
      "learning_rate": 9.677994145348099e-05,
      "loss": 0.5465,
      "step": 4060
    },
    {
      "epoch": 0.5165023847376788,
      "grad_norm": 2.557067632675171,
      "learning_rate": 9.67544864452081e-05,
      "loss": 0.698,
      "step": 4061
    },
    {
      "epoch": 0.5166295707472178,
      "grad_norm": 2.1020522117614746,
      "learning_rate": 9.672903143693522e-05,
      "loss": 0.6357,
      "step": 4062
    },
    {
      "epoch": 0.5167567567567568,
      "grad_norm": 2.4942708015441895,
      "learning_rate": 9.670357642866234e-05,
      "loss": 0.6402,
      "step": 4063
    },
    {
      "epoch": 0.5168839427662957,
      "grad_norm": 2.6903653144836426,
      "learning_rate": 9.667812142038946e-05,
      "loss": 0.5429,
      "step": 4064
    },
    {
      "epoch": 0.5170111287758347,
      "grad_norm": 2.1805484294891357,
      "learning_rate": 9.66526664121166e-05,
      "loss": 0.7358,
      "step": 4065
    },
    {
      "epoch": 0.5171383147853736,
      "grad_norm": 1.6661826372146606,
      "learning_rate": 9.662721140384371e-05,
      "loss": 0.5302,
      "step": 4066
    },
    {
      "epoch": 0.5172655007949125,
      "grad_norm": 2.1309187412261963,
      "learning_rate": 9.660175639557083e-05,
      "loss": 0.614,
      "step": 4067
    },
    {
      "epoch": 0.5173926868044515,
      "grad_norm": 1.9741926193237305,
      "learning_rate": 9.657630138729795e-05,
      "loss": 0.5732,
      "step": 4068
    },
    {
      "epoch": 0.5175198728139905,
      "grad_norm": 2.567063331604004,
      "learning_rate": 9.655084637902507e-05,
      "loss": 0.6965,
      "step": 4069
    },
    {
      "epoch": 0.5176470588235295,
      "grad_norm": 3.1128697395324707,
      "learning_rate": 9.65253913707522e-05,
      "loss": 0.8023,
      "step": 4070
    },
    {
      "epoch": 0.5177742448330683,
      "grad_norm": 1.8715455532073975,
      "learning_rate": 9.649993636247933e-05,
      "loss": 0.6309,
      "step": 4071
    },
    {
      "epoch": 0.5179014308426073,
      "grad_norm": 2.264928102493286,
      "learning_rate": 9.647448135420645e-05,
      "loss": 0.4905,
      "step": 4072
    },
    {
      "epoch": 0.5180286168521463,
      "grad_norm": 2.490684747695923,
      "learning_rate": 9.644902634593357e-05,
      "loss": 0.5154,
      "step": 4073
    },
    {
      "epoch": 0.5181558028616852,
      "grad_norm": 2.4532604217529297,
      "learning_rate": 9.642357133766069e-05,
      "loss": 0.7883,
      "step": 4074
    },
    {
      "epoch": 0.5182829888712241,
      "grad_norm": 2.286158323287964,
      "learning_rate": 9.639811632938781e-05,
      "loss": 0.6685,
      "step": 4075
    },
    {
      "epoch": 0.5184101748807631,
      "grad_norm": 1.7784899473190308,
      "learning_rate": 9.637266132111493e-05,
      "loss": 0.6624,
      "step": 4076
    },
    {
      "epoch": 0.5185373608903021,
      "grad_norm": 2.6274871826171875,
      "learning_rate": 9.634720631284206e-05,
      "loss": 0.7839,
      "step": 4077
    },
    {
      "epoch": 0.518664546899841,
      "grad_norm": 2.1685471534729004,
      "learning_rate": 9.632175130456918e-05,
      "loss": 1.1753,
      "step": 4078
    },
    {
      "epoch": 0.51879173290938,
      "grad_norm": 1.5439008474349976,
      "learning_rate": 9.62962962962963e-05,
      "loss": 0.4185,
      "step": 4079
    },
    {
      "epoch": 0.518918918918919,
      "grad_norm": 2.3592355251312256,
      "learning_rate": 9.627084128802343e-05,
      "loss": 0.6118,
      "step": 4080
    },
    {
      "epoch": 0.5190461049284578,
      "grad_norm": 1.9535036087036133,
      "learning_rate": 9.624538627975055e-05,
      "loss": 0.7526,
      "step": 4081
    },
    {
      "epoch": 0.5191732909379968,
      "grad_norm": 2.0583877563476562,
      "learning_rate": 9.621993127147767e-05,
      "loss": 0.4325,
      "step": 4082
    },
    {
      "epoch": 0.5193004769475358,
      "grad_norm": 2.12333607673645,
      "learning_rate": 9.619447626320478e-05,
      "loss": 0.613,
      "step": 4083
    },
    {
      "epoch": 0.5194276629570748,
      "grad_norm": 2.0072250366210938,
      "learning_rate": 9.616902125493192e-05,
      "loss": 0.7177,
      "step": 4084
    },
    {
      "epoch": 0.5195548489666136,
      "grad_norm": 1.7981270551681519,
      "learning_rate": 9.614356624665904e-05,
      "loss": 0.4639,
      "step": 4085
    },
    {
      "epoch": 0.5196820349761526,
      "grad_norm": 2.3751707077026367,
      "learning_rate": 9.611811123838615e-05,
      "loss": 0.7729,
      "step": 4086
    },
    {
      "epoch": 0.5198092209856916,
      "grad_norm": 2.1440110206604004,
      "learning_rate": 9.609265623011327e-05,
      "loss": 0.5978,
      "step": 4087
    },
    {
      "epoch": 0.5199364069952305,
      "grad_norm": 2.4514365196228027,
      "learning_rate": 9.606720122184039e-05,
      "loss": 0.8147,
      "step": 4088
    },
    {
      "epoch": 0.5200635930047695,
      "grad_norm": 2.0459468364715576,
      "learning_rate": 9.604174621356752e-05,
      "loss": 0.5046,
      "step": 4089
    },
    {
      "epoch": 0.5201907790143084,
      "grad_norm": 2.5043275356292725,
      "learning_rate": 9.601629120529466e-05,
      "loss": 0.6291,
      "step": 4090
    },
    {
      "epoch": 0.5203179650238474,
      "grad_norm": 1.9736942052841187,
      "learning_rate": 9.599083619702178e-05,
      "loss": 0.541,
      "step": 4091
    },
    {
      "epoch": 0.5204451510333863,
      "grad_norm": 2.174463987350464,
      "learning_rate": 9.59653811887489e-05,
      "loss": 0.6782,
      "step": 4092
    },
    {
      "epoch": 0.5205723370429253,
      "grad_norm": 1.7104580402374268,
      "learning_rate": 9.593992618047601e-05,
      "loss": 0.6689,
      "step": 4093
    },
    {
      "epoch": 0.5206995230524643,
      "grad_norm": 1.6667343378067017,
      "learning_rate": 9.591447117220313e-05,
      "loss": 0.6148,
      "step": 4094
    },
    {
      "epoch": 0.5208267090620032,
      "grad_norm": 2.1256933212280273,
      "learning_rate": 9.588901616393025e-05,
      "loss": 0.8235,
      "step": 4095
    },
    {
      "epoch": 0.5209538950715421,
      "grad_norm": 2.412701368331909,
      "learning_rate": 9.586356115565738e-05,
      "loss": 0.6373,
      "step": 4096
    },
    {
      "epoch": 0.5210810810810811,
      "grad_norm": 2.028468132019043,
      "learning_rate": 9.58381061473845e-05,
      "loss": 0.4705,
      "step": 4097
    },
    {
      "epoch": 0.5212082670906201,
      "grad_norm": 2.317636728286743,
      "learning_rate": 9.581265113911162e-05,
      "loss": 0.8668,
      "step": 4098
    },
    {
      "epoch": 0.5213354531001589,
      "grad_norm": 2.3296351432800293,
      "learning_rate": 9.578719613083875e-05,
      "loss": 0.6547,
      "step": 4099
    },
    {
      "epoch": 0.5214626391096979,
      "grad_norm": 2.689462423324585,
      "learning_rate": 9.576174112256587e-05,
      "loss": 0.8672,
      "step": 4100
    },
    {
      "epoch": 0.5215898251192369,
      "grad_norm": 1.5923025608062744,
      "learning_rate": 9.573628611429299e-05,
      "loss": 0.4591,
      "step": 4101
    },
    {
      "epoch": 0.5217170111287759,
      "grad_norm": 1.7632572650909424,
      "learning_rate": 9.571083110602012e-05,
      "loss": 0.4098,
      "step": 4102
    },
    {
      "epoch": 0.5218441971383148,
      "grad_norm": 2.107466697692871,
      "learning_rate": 9.568537609774724e-05,
      "loss": 0.5838,
      "step": 4103
    },
    {
      "epoch": 0.5219713831478537,
      "grad_norm": 2.171081781387329,
      "learning_rate": 9.565992108947436e-05,
      "loss": 0.5092,
      "step": 4104
    },
    {
      "epoch": 0.5220985691573927,
      "grad_norm": 2.0506842136383057,
      "learning_rate": 9.563446608120148e-05,
      "loss": 0.5559,
      "step": 4105
    },
    {
      "epoch": 0.5222257551669316,
      "grad_norm": 2.428767442703247,
      "learning_rate": 9.56090110729286e-05,
      "loss": 0.6717,
      "step": 4106
    },
    {
      "epoch": 0.5223529411764706,
      "grad_norm": 2.01751446723938,
      "learning_rate": 9.558355606465572e-05,
      "loss": 0.4714,
      "step": 4107
    },
    {
      "epoch": 0.5224801271860096,
      "grad_norm": 2.4306838512420654,
      "learning_rate": 9.555810105638285e-05,
      "loss": 0.7765,
      "step": 4108
    },
    {
      "epoch": 0.5226073131955485,
      "grad_norm": 2.0187549591064453,
      "learning_rate": 9.553264604810998e-05,
      "loss": 0.7142,
      "step": 4109
    },
    {
      "epoch": 0.5227344992050874,
      "grad_norm": 2.043025493621826,
      "learning_rate": 9.55071910398371e-05,
      "loss": 0.5698,
      "step": 4110
    },
    {
      "epoch": 0.5228616852146264,
      "grad_norm": 2.13655948638916,
      "learning_rate": 9.548173603156422e-05,
      "loss": 0.6038,
      "step": 4111
    },
    {
      "epoch": 0.5229888712241654,
      "grad_norm": 1.9434674978256226,
      "learning_rate": 9.545628102329134e-05,
      "loss": 0.5326,
      "step": 4112
    },
    {
      "epoch": 0.5231160572337042,
      "grad_norm": 1.6918361186981201,
      "learning_rate": 9.543082601501846e-05,
      "loss": 0.4251,
      "step": 4113
    },
    {
      "epoch": 0.5232432432432432,
      "grad_norm": 2.624270439147949,
      "learning_rate": 9.540537100674557e-05,
      "loss": 0.7914,
      "step": 4114
    },
    {
      "epoch": 0.5233704292527822,
      "grad_norm": 2.151930570602417,
      "learning_rate": 9.53799159984727e-05,
      "loss": 0.7285,
      "step": 4115
    },
    {
      "epoch": 0.5234976152623212,
      "grad_norm": 2.3283824920654297,
      "learning_rate": 9.535446099019983e-05,
      "loss": 0.6404,
      "step": 4116
    },
    {
      "epoch": 0.5236248012718601,
      "grad_norm": 1.8163530826568604,
      "learning_rate": 9.532900598192694e-05,
      "loss": 0.5939,
      "step": 4117
    },
    {
      "epoch": 0.523751987281399,
      "grad_norm": 1.6241655349731445,
      "learning_rate": 9.530355097365406e-05,
      "loss": 0.5683,
      "step": 4118
    },
    {
      "epoch": 0.523879173290938,
      "grad_norm": 2.0458385944366455,
      "learning_rate": 9.52780959653812e-05,
      "loss": 0.7941,
      "step": 4119
    },
    {
      "epoch": 0.5240063593004769,
      "grad_norm": 2.4503490924835205,
      "learning_rate": 9.525264095710831e-05,
      "loss": 0.6183,
      "step": 4120
    },
    {
      "epoch": 0.5241335453100159,
      "grad_norm": 1.5881520509719849,
      "learning_rate": 9.522718594883545e-05,
      "loss": 0.6931,
      "step": 4121
    },
    {
      "epoch": 0.5242607313195549,
      "grad_norm": 2.207310676574707,
      "learning_rate": 9.520173094056256e-05,
      "loss": 0.9255,
      "step": 4122
    },
    {
      "epoch": 0.5243879173290938,
      "grad_norm": 1.4687817096710205,
      "learning_rate": 9.517627593228968e-05,
      "loss": 0.6703,
      "step": 4123
    },
    {
      "epoch": 0.5245151033386327,
      "grad_norm": 2.98828125,
      "learning_rate": 9.51508209240168e-05,
      "loss": 0.6756,
      "step": 4124
    },
    {
      "epoch": 0.5246422893481717,
      "grad_norm": 2.199115037918091,
      "learning_rate": 9.512536591574392e-05,
      "loss": 0.5219,
      "step": 4125
    },
    {
      "epoch": 0.5247694753577107,
      "grad_norm": 1.5206598043441772,
      "learning_rate": 9.509991090747104e-05,
      "loss": 0.4843,
      "step": 4126
    },
    {
      "epoch": 0.5248966613672496,
      "grad_norm": 1.9424114227294922,
      "learning_rate": 9.507445589919817e-05,
      "loss": 0.5486,
      "step": 4127
    },
    {
      "epoch": 0.5250238473767885,
      "grad_norm": 1.7533727884292603,
      "learning_rate": 9.50490008909253e-05,
      "loss": 0.5083,
      "step": 4128
    },
    {
      "epoch": 0.5251510333863275,
      "grad_norm": 1.8066118955612183,
      "learning_rate": 9.502354588265242e-05,
      "loss": 0.65,
      "step": 4129
    },
    {
      "epoch": 0.5252782193958665,
      "grad_norm": 1.809515118598938,
      "learning_rate": 9.499809087437954e-05,
      "loss": 0.471,
      "step": 4130
    },
    {
      "epoch": 0.5254054054054054,
      "grad_norm": 1.706980586051941,
      "learning_rate": 9.497263586610666e-05,
      "loss": 0.455,
      "step": 4131
    },
    {
      "epoch": 0.5255325914149444,
      "grad_norm": 2.2954916954040527,
      "learning_rate": 9.494718085783378e-05,
      "loss": 0.7725,
      "step": 4132
    },
    {
      "epoch": 0.5256597774244833,
      "grad_norm": 2.185119390487671,
      "learning_rate": 9.492172584956091e-05,
      "loss": 0.6805,
      "step": 4133
    },
    {
      "epoch": 0.5257869634340222,
      "grad_norm": 1.7608063220977783,
      "learning_rate": 9.489627084128803e-05,
      "loss": 0.5432,
      "step": 4134
    },
    {
      "epoch": 0.5259141494435612,
      "grad_norm": 2.151085376739502,
      "learning_rate": 9.487081583301515e-05,
      "loss": 0.6333,
      "step": 4135
    },
    {
      "epoch": 0.5260413354531002,
      "grad_norm": 1.9571245908737183,
      "learning_rate": 9.484536082474227e-05,
      "loss": 0.6516,
      "step": 4136
    },
    {
      "epoch": 0.5261685214626391,
      "grad_norm": 1.6600043773651123,
      "learning_rate": 9.481990581646939e-05,
      "loss": 0.6451,
      "step": 4137
    },
    {
      "epoch": 0.526295707472178,
      "grad_norm": 2.1099138259887695,
      "learning_rate": 9.479445080819652e-05,
      "loss": 0.3159,
      "step": 4138
    },
    {
      "epoch": 0.526422893481717,
      "grad_norm": 3.4079439640045166,
      "learning_rate": 9.476899579992364e-05,
      "loss": 0.7051,
      "step": 4139
    },
    {
      "epoch": 0.526550079491256,
      "grad_norm": 1.809039831161499,
      "learning_rate": 9.474354079165077e-05,
      "loss": 0.5836,
      "step": 4140
    },
    {
      "epoch": 0.526677265500795,
      "grad_norm": 2.2124319076538086,
      "learning_rate": 9.471808578337789e-05,
      "loss": 0.6693,
      "step": 4141
    },
    {
      "epoch": 0.5268044515103338,
      "grad_norm": 2.382742404937744,
      "learning_rate": 9.469263077510501e-05,
      "loss": 0.6486,
      "step": 4142
    },
    {
      "epoch": 0.5269316375198728,
      "grad_norm": 2.6565513610839844,
      "learning_rate": 9.466717576683213e-05,
      "loss": 0.6436,
      "step": 4143
    },
    {
      "epoch": 0.5270588235294118,
      "grad_norm": 3.1882805824279785,
      "learning_rate": 9.464172075855924e-05,
      "loss": 0.6455,
      "step": 4144
    },
    {
      "epoch": 0.5271860095389507,
      "grad_norm": 2.1915647983551025,
      "learning_rate": 9.461626575028636e-05,
      "loss": 0.763,
      "step": 4145
    },
    {
      "epoch": 0.5273131955484897,
      "grad_norm": 2.04181170463562,
      "learning_rate": 9.45908107420135e-05,
      "loss": 0.8645,
      "step": 4146
    },
    {
      "epoch": 0.5274403815580286,
      "grad_norm": 4.090925216674805,
      "learning_rate": 9.456535573374061e-05,
      "loss": 0.5494,
      "step": 4147
    },
    {
      "epoch": 0.5275675675675676,
      "grad_norm": 2.0555338859558105,
      "learning_rate": 9.453990072546775e-05,
      "loss": 0.752,
      "step": 4148
    },
    {
      "epoch": 0.5276947535771065,
      "grad_norm": 2.132913589477539,
      "learning_rate": 9.451444571719487e-05,
      "loss": 0.685,
      "step": 4149
    },
    {
      "epoch": 0.5278219395866455,
      "grad_norm": 1.5300419330596924,
      "learning_rate": 9.448899070892198e-05,
      "loss": 0.4923,
      "step": 4150
    },
    {
      "epoch": 0.5279491255961845,
      "grad_norm": 2.552564859390259,
      "learning_rate": 9.44635357006491e-05,
      "loss": 0.4535,
      "step": 4151
    },
    {
      "epoch": 0.5280763116057233,
      "grad_norm": 3.013308048248291,
      "learning_rate": 9.443808069237624e-05,
      "loss": 0.8264,
      "step": 4152
    },
    {
      "epoch": 0.5282034976152623,
      "grad_norm": 2.4724807739257812,
      "learning_rate": 9.441262568410335e-05,
      "loss": 0.5039,
      "step": 4153
    },
    {
      "epoch": 0.5283306836248013,
      "grad_norm": 3.0066492557525635,
      "learning_rate": 9.438717067583047e-05,
      "loss": 0.5437,
      "step": 4154
    },
    {
      "epoch": 0.5284578696343403,
      "grad_norm": 2.6088602542877197,
      "learning_rate": 9.436171566755759e-05,
      "loss": 0.7818,
      "step": 4155
    },
    {
      "epoch": 0.5285850556438791,
      "grad_norm": 2.4484992027282715,
      "learning_rate": 9.433626065928471e-05,
      "loss": 0.4949,
      "step": 4156
    },
    {
      "epoch": 0.5287122416534181,
      "grad_norm": 1.690255045890808,
      "learning_rate": 9.431080565101184e-05,
      "loss": 0.5983,
      "step": 4157
    },
    {
      "epoch": 0.5288394276629571,
      "grad_norm": 2.416442394256592,
      "learning_rate": 9.428535064273896e-05,
      "loss": 0.7371,
      "step": 4158
    },
    {
      "epoch": 0.528966613672496,
      "grad_norm": 2.408923625946045,
      "learning_rate": 9.42598956344661e-05,
      "loss": 0.4427,
      "step": 4159
    },
    {
      "epoch": 0.529093799682035,
      "grad_norm": 2.994943380355835,
      "learning_rate": 9.423444062619321e-05,
      "loss": 0.6512,
      "step": 4160
    },
    {
      "epoch": 0.5292209856915739,
      "grad_norm": 3.417555570602417,
      "learning_rate": 9.420898561792033e-05,
      "loss": 0.572,
      "step": 4161
    },
    {
      "epoch": 0.5293481717011129,
      "grad_norm": 2.207538366317749,
      "learning_rate": 9.418353060964745e-05,
      "loss": 0.6585,
      "step": 4162
    },
    {
      "epoch": 0.5294753577106518,
      "grad_norm": 2.194622755050659,
      "learning_rate": 9.415807560137457e-05,
      "loss": 0.4066,
      "step": 4163
    },
    {
      "epoch": 0.5296025437201908,
      "grad_norm": 2.215301752090454,
      "learning_rate": 9.41326205931017e-05,
      "loss": 0.5453,
      "step": 4164
    },
    {
      "epoch": 0.5297297297297298,
      "grad_norm": 2.8719794750213623,
      "learning_rate": 9.410716558482882e-05,
      "loss": 0.6565,
      "step": 4165
    },
    {
      "epoch": 0.5298569157392686,
      "grad_norm": 2.2956926822662354,
      "learning_rate": 9.408171057655594e-05,
      "loss": 0.6557,
      "step": 4166
    },
    {
      "epoch": 0.5299841017488076,
      "grad_norm": 2.42143177986145,
      "learning_rate": 9.405625556828307e-05,
      "loss": 0.5623,
      "step": 4167
    },
    {
      "epoch": 0.5301112877583466,
      "grad_norm": 2.865600109100342,
      "learning_rate": 9.403080056001019e-05,
      "loss": 0.8642,
      "step": 4168
    },
    {
      "epoch": 0.5302384737678856,
      "grad_norm": 1.9925321340560913,
      "learning_rate": 9.400534555173731e-05,
      "loss": 0.5608,
      "step": 4169
    },
    {
      "epoch": 0.5303656597774244,
      "grad_norm": 2.296239137649536,
      "learning_rate": 9.397989054346443e-05,
      "loss": 0.8063,
      "step": 4170
    },
    {
      "epoch": 0.5304928457869634,
      "grad_norm": 2.1442768573760986,
      "learning_rate": 9.395443553519156e-05,
      "loss": 0.5797,
      "step": 4171
    },
    {
      "epoch": 0.5306200317965024,
      "grad_norm": 1.8859062194824219,
      "learning_rate": 9.392898052691868e-05,
      "loss": 0.5853,
      "step": 4172
    },
    {
      "epoch": 0.5307472178060413,
      "grad_norm": 2.858428955078125,
      "learning_rate": 9.39035255186458e-05,
      "loss": 0.5849,
      "step": 4173
    },
    {
      "epoch": 0.5308744038155803,
      "grad_norm": 2.290909767150879,
      "learning_rate": 9.387807051037292e-05,
      "loss": 0.7739,
      "step": 4174
    },
    {
      "epoch": 0.5310015898251192,
      "grad_norm": 2.8762190341949463,
      "learning_rate": 9.385261550210003e-05,
      "loss": 0.6224,
      "step": 4175
    },
    {
      "epoch": 0.5311287758346582,
      "grad_norm": 2.3109211921691895,
      "learning_rate": 9.382716049382717e-05,
      "loss": 0.5235,
      "step": 4176
    },
    {
      "epoch": 0.5312559618441971,
      "grad_norm": 2.7159311771392822,
      "learning_rate": 9.38017054855543e-05,
      "loss": 0.6869,
      "step": 4177
    },
    {
      "epoch": 0.5313831478537361,
      "grad_norm": 2.9293220043182373,
      "learning_rate": 9.377625047728142e-05,
      "loss": 0.6788,
      "step": 4178
    },
    {
      "epoch": 0.5315103338632751,
      "grad_norm": 2.554774284362793,
      "learning_rate": 9.375079546900854e-05,
      "loss": 0.6696,
      "step": 4179
    },
    {
      "epoch": 0.5316375198728139,
      "grad_norm": 2.174194574356079,
      "learning_rate": 9.372534046073566e-05,
      "loss": 0.7182,
      "step": 4180
    },
    {
      "epoch": 0.5317647058823529,
      "grad_norm": 1.9005976915359497,
      "learning_rate": 9.369988545246277e-05,
      "loss": 0.5801,
      "step": 4181
    },
    {
      "epoch": 0.5318918918918919,
      "grad_norm": 2.0404014587402344,
      "learning_rate": 9.367443044418989e-05,
      "loss": 0.8572,
      "step": 4182
    },
    {
      "epoch": 0.5320190779014309,
      "grad_norm": 2.4282784461975098,
      "learning_rate": 9.364897543591702e-05,
      "loss": 0.6205,
      "step": 4183
    },
    {
      "epoch": 0.5321462639109698,
      "grad_norm": 1.7991408109664917,
      "learning_rate": 9.362352042764414e-05,
      "loss": 0.4673,
      "step": 4184
    },
    {
      "epoch": 0.5322734499205087,
      "grad_norm": 1.8634262084960938,
      "learning_rate": 9.359806541937126e-05,
      "loss": 0.3772,
      "step": 4185
    },
    {
      "epoch": 0.5324006359300477,
      "grad_norm": 1.6080601215362549,
      "learning_rate": 9.357261041109838e-05,
      "loss": 0.552,
      "step": 4186
    },
    {
      "epoch": 0.5325278219395867,
      "grad_norm": 1.9305638074874878,
      "learning_rate": 9.354715540282551e-05,
      "loss": 0.7533,
      "step": 4187
    },
    {
      "epoch": 0.5326550079491256,
      "grad_norm": 2.4690983295440674,
      "learning_rate": 9.352170039455263e-05,
      "loss": 0.6408,
      "step": 4188
    },
    {
      "epoch": 0.5327821939586646,
      "grad_norm": 2.1747629642486572,
      "learning_rate": 9.349624538627975e-05,
      "loss": 0.717,
      "step": 4189
    },
    {
      "epoch": 0.5329093799682035,
      "grad_norm": 2.6271283626556396,
      "learning_rate": 9.347079037800688e-05,
      "loss": 0.7439,
      "step": 4190
    },
    {
      "epoch": 0.5330365659777424,
      "grad_norm": 1.506561279296875,
      "learning_rate": 9.3445335369734e-05,
      "loss": 0.7367,
      "step": 4191
    },
    {
      "epoch": 0.5331637519872814,
      "grad_norm": 1.924383521080017,
      "learning_rate": 9.341988036146112e-05,
      "loss": 0.7511,
      "step": 4192
    },
    {
      "epoch": 0.5332909379968204,
      "grad_norm": 1.4529926776885986,
      "learning_rate": 9.339442535318824e-05,
      "loss": 0.3801,
      "step": 4193
    },
    {
      "epoch": 0.5334181240063594,
      "grad_norm": 1.2465651035308838,
      "learning_rate": 9.336897034491536e-05,
      "loss": 0.3809,
      "step": 4194
    },
    {
      "epoch": 0.5335453100158982,
      "grad_norm": 1.845911979675293,
      "learning_rate": 9.334351533664249e-05,
      "loss": 0.5597,
      "step": 4195
    },
    {
      "epoch": 0.5336724960254372,
      "grad_norm": 2.392287254333496,
      "learning_rate": 9.331806032836962e-05,
      "loss": 0.8687,
      "step": 4196
    },
    {
      "epoch": 0.5337996820349762,
      "grad_norm": 2.211165428161621,
      "learning_rate": 9.329260532009674e-05,
      "loss": 0.5046,
      "step": 4197
    },
    {
      "epoch": 0.5339268680445151,
      "grad_norm": 2.5445897579193115,
      "learning_rate": 9.326715031182386e-05,
      "loss": 0.6128,
      "step": 4198
    },
    {
      "epoch": 0.534054054054054,
      "grad_norm": 1.9493228197097778,
      "learning_rate": 9.324169530355098e-05,
      "loss": 0.7671,
      "step": 4199
    },
    {
      "epoch": 0.534181240063593,
      "grad_norm": 2.5554559230804443,
      "learning_rate": 9.32162402952781e-05,
      "loss": 0.6407,
      "step": 4200
    },
    {
      "epoch": 0.534308426073132,
      "grad_norm": 2.003957986831665,
      "learning_rate": 9.319078528700522e-05,
      "loss": 0.4834,
      "step": 4201
    },
    {
      "epoch": 0.5344356120826709,
      "grad_norm": 2.308867931365967,
      "learning_rate": 9.316533027873235e-05,
      "loss": 0.6247,
      "step": 4202
    },
    {
      "epoch": 0.5345627980922099,
      "grad_norm": 1.4211270809173584,
      "learning_rate": 9.313987527045947e-05,
      "loss": 0.5622,
      "step": 4203
    },
    {
      "epoch": 0.5346899841017488,
      "grad_norm": 2.1659862995147705,
      "learning_rate": 9.311442026218659e-05,
      "loss": 0.5585,
      "step": 4204
    },
    {
      "epoch": 0.5348171701112877,
      "grad_norm": 1.7493586540222168,
      "learning_rate": 9.30889652539137e-05,
      "loss": 0.5366,
      "step": 4205
    },
    {
      "epoch": 0.5349443561208267,
      "grad_norm": 1.7220734357833862,
      "learning_rate": 9.306351024564084e-05,
      "loss": 0.3749,
      "step": 4206
    },
    {
      "epoch": 0.5350715421303657,
      "grad_norm": 1.8554996252059937,
      "learning_rate": 9.303805523736796e-05,
      "loss": 0.4365,
      "step": 4207
    },
    {
      "epoch": 0.5351987281399047,
      "grad_norm": 1.8668341636657715,
      "learning_rate": 9.301260022909509e-05,
      "loss": 0.585,
      "step": 4208
    },
    {
      "epoch": 0.5353259141494435,
      "grad_norm": 2.2740705013275146,
      "learning_rate": 9.298714522082221e-05,
      "loss": 0.7177,
      "step": 4209
    },
    {
      "epoch": 0.5354531001589825,
      "grad_norm": 2.377997636795044,
      "learning_rate": 9.296169021254933e-05,
      "loss": 0.7173,
      "step": 4210
    },
    {
      "epoch": 0.5355802861685215,
      "grad_norm": 2.1276328563690186,
      "learning_rate": 9.293623520427644e-05,
      "loss": 0.5668,
      "step": 4211
    },
    {
      "epoch": 0.5357074721780604,
      "grad_norm": 2.6043009757995605,
      "learning_rate": 9.291078019600356e-05,
      "loss": 0.7472,
      "step": 4212
    },
    {
      "epoch": 0.5358346581875993,
      "grad_norm": 2.7418923377990723,
      "learning_rate": 9.288532518773068e-05,
      "loss": 0.7361,
      "step": 4213
    },
    {
      "epoch": 0.5359618441971383,
      "grad_norm": 1.873802900314331,
      "learning_rate": 9.285987017945781e-05,
      "loss": 0.5374,
      "step": 4214
    },
    {
      "epoch": 0.5360890302066773,
      "grad_norm": 2.403029441833496,
      "learning_rate": 9.283441517118493e-05,
      "loss": 0.5987,
      "step": 4215
    },
    {
      "epoch": 0.5362162162162162,
      "grad_norm": 2.3132638931274414,
      "learning_rate": 9.280896016291207e-05,
      "loss": 0.7465,
      "step": 4216
    },
    {
      "epoch": 0.5363434022257552,
      "grad_norm": 3.3950228691101074,
      "learning_rate": 9.278350515463918e-05,
      "loss": 0.888,
      "step": 4217
    },
    {
      "epoch": 0.5364705882352941,
      "grad_norm": 2.8109400272369385,
      "learning_rate": 9.27580501463663e-05,
      "loss": 0.7515,
      "step": 4218
    },
    {
      "epoch": 0.536597774244833,
      "grad_norm": 4.19640588760376,
      "learning_rate": 9.273259513809342e-05,
      "loss": 0.624,
      "step": 4219
    },
    {
      "epoch": 0.536724960254372,
      "grad_norm": 3.085874557495117,
      "learning_rate": 9.270714012982054e-05,
      "loss": 0.6309,
      "step": 4220
    },
    {
      "epoch": 0.536852146263911,
      "grad_norm": 2.3271074295043945,
      "learning_rate": 9.268168512154767e-05,
      "loss": 0.4867,
      "step": 4221
    },
    {
      "epoch": 0.53697933227345,
      "grad_norm": 1.5545169115066528,
      "learning_rate": 9.265623011327479e-05,
      "loss": 0.5251,
      "step": 4222
    },
    {
      "epoch": 0.5371065182829888,
      "grad_norm": 2.9083216190338135,
      "learning_rate": 9.263077510500191e-05,
      "loss": 0.6326,
      "step": 4223
    },
    {
      "epoch": 0.5372337042925278,
      "grad_norm": 2.4293417930603027,
      "learning_rate": 9.260532009672903e-05,
      "loss": 0.6256,
      "step": 4224
    },
    {
      "epoch": 0.5373608903020668,
      "grad_norm": 1.9859825372695923,
      "learning_rate": 9.257986508845616e-05,
      "loss": 0.6998,
      "step": 4225
    },
    {
      "epoch": 0.5374880763116058,
      "grad_norm": 1.9531593322753906,
      "learning_rate": 9.255441008018328e-05,
      "loss": 0.6032,
      "step": 4226
    },
    {
      "epoch": 0.5376152623211446,
      "grad_norm": 2.1264796257019043,
      "learning_rate": 9.252895507191041e-05,
      "loss": 0.3412,
      "step": 4227
    },
    {
      "epoch": 0.5377424483306836,
      "grad_norm": 2.218942880630493,
      "learning_rate": 9.250350006363753e-05,
      "loss": 0.4534,
      "step": 4228
    },
    {
      "epoch": 0.5378696343402226,
      "grad_norm": 1.6948192119598389,
      "learning_rate": 9.247804505536465e-05,
      "loss": 0.4736,
      "step": 4229
    },
    {
      "epoch": 0.5379968203497615,
      "grad_norm": 1.9143977165222168,
      "learning_rate": 9.245259004709177e-05,
      "loss": 0.5642,
      "step": 4230
    },
    {
      "epoch": 0.5381240063593005,
      "grad_norm": 2.0442287921905518,
      "learning_rate": 9.242713503881889e-05,
      "loss": 0.6079,
      "step": 4231
    },
    {
      "epoch": 0.5382511923688394,
      "grad_norm": 1.893253207206726,
      "learning_rate": 9.2401680030546e-05,
      "loss": 0.6246,
      "step": 4232
    },
    {
      "epoch": 0.5383783783783784,
      "grad_norm": 2.0633296966552734,
      "learning_rate": 9.237622502227314e-05,
      "loss": 0.5725,
      "step": 4233
    },
    {
      "epoch": 0.5385055643879173,
      "grad_norm": 4.200961112976074,
      "learning_rate": 9.235077001400026e-05,
      "loss": 0.7228,
      "step": 4234
    },
    {
      "epoch": 0.5386327503974563,
      "grad_norm": 1.6979236602783203,
      "learning_rate": 9.232531500572739e-05,
      "loss": 0.3673,
      "step": 4235
    },
    {
      "epoch": 0.5387599364069953,
      "grad_norm": 2.0906822681427,
      "learning_rate": 9.229985999745451e-05,
      "loss": 0.6265,
      "step": 4236
    },
    {
      "epoch": 0.5388871224165341,
      "grad_norm": 2.389453887939453,
      "learning_rate": 9.227440498918163e-05,
      "loss": 0.7535,
      "step": 4237
    },
    {
      "epoch": 0.5390143084260731,
      "grad_norm": 1.596856951713562,
      "learning_rate": 9.224894998090875e-05,
      "loss": 0.3425,
      "step": 4238
    },
    {
      "epoch": 0.5391414944356121,
      "grad_norm": 2.0522003173828125,
      "learning_rate": 9.222349497263588e-05,
      "loss": 0.5723,
      "step": 4239
    },
    {
      "epoch": 0.5392686804451511,
      "grad_norm": 1.5109097957611084,
      "learning_rate": 9.2198039964363e-05,
      "loss": 0.6169,
      "step": 4240
    },
    {
      "epoch": 0.53939586645469,
      "grad_norm": 2.2082409858703613,
      "learning_rate": 9.217258495609011e-05,
      "loss": 0.5885,
      "step": 4241
    },
    {
      "epoch": 0.5395230524642289,
      "grad_norm": 2.1356353759765625,
      "learning_rate": 9.214712994781723e-05,
      "loss": 0.659,
      "step": 4242
    },
    {
      "epoch": 0.5396502384737679,
      "grad_norm": 1.7723909616470337,
      "learning_rate": 9.212167493954435e-05,
      "loss": 0.5472,
      "step": 4243
    },
    {
      "epoch": 0.5397774244833068,
      "grad_norm": 2.344637155532837,
      "learning_rate": 9.209621993127147e-05,
      "loss": 0.7447,
      "step": 4244
    },
    {
      "epoch": 0.5399046104928458,
      "grad_norm": 1.94408118724823,
      "learning_rate": 9.20707649229986e-05,
      "loss": 0.5693,
      "step": 4245
    },
    {
      "epoch": 0.5400317965023848,
      "grad_norm": 2.3729357719421387,
      "learning_rate": 9.204530991472574e-05,
      "loss": 0.77,
      "step": 4246
    },
    {
      "epoch": 0.5401589825119237,
      "grad_norm": 3.529755115509033,
      "learning_rate": 9.201985490645285e-05,
      "loss": 0.6112,
      "step": 4247
    },
    {
      "epoch": 0.5402861685214626,
      "grad_norm": 2.1189591884613037,
      "learning_rate": 9.199439989817997e-05,
      "loss": 0.621,
      "step": 4248
    },
    {
      "epoch": 0.5404133545310016,
      "grad_norm": 1.7008377313613892,
      "learning_rate": 9.196894488990709e-05,
      "loss": 0.4164,
      "step": 4249
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 2.287047863006592,
      "learning_rate": 9.194348988163421e-05,
      "loss": 0.7799,
      "step": 4250
    },
    {
      "epoch": 0.5406677265500794,
      "grad_norm": 3.0989980697631836,
      "learning_rate": 9.191803487336133e-05,
      "loss": 0.6645,
      "step": 4251
    },
    {
      "epoch": 0.5407949125596184,
      "grad_norm": 2.9948437213897705,
      "learning_rate": 9.189257986508846e-05,
      "loss": 0.7712,
      "step": 4252
    },
    {
      "epoch": 0.5409220985691574,
      "grad_norm": 1.8881993293762207,
      "learning_rate": 9.186712485681558e-05,
      "loss": 0.7014,
      "step": 4253
    },
    {
      "epoch": 0.5410492845786964,
      "grad_norm": 1.881866216659546,
      "learning_rate": 9.184166984854271e-05,
      "loss": 0.4511,
      "step": 4254
    },
    {
      "epoch": 0.5411764705882353,
      "grad_norm": 1.775600552558899,
      "learning_rate": 9.181621484026983e-05,
      "loss": 0.5982,
      "step": 4255
    },
    {
      "epoch": 0.5413036565977742,
      "grad_norm": 1.9510217905044556,
      "learning_rate": 9.179075983199695e-05,
      "loss": 0.6997,
      "step": 4256
    },
    {
      "epoch": 0.5414308426073132,
      "grad_norm": 1.9574583768844604,
      "learning_rate": 9.176530482372407e-05,
      "loss": 0.7814,
      "step": 4257
    },
    {
      "epoch": 0.5415580286168521,
      "grad_norm": 1.8806771039962769,
      "learning_rate": 9.17398498154512e-05,
      "loss": 0.3753,
      "step": 4258
    },
    {
      "epoch": 0.5416852146263911,
      "grad_norm": 2.5819618701934814,
      "learning_rate": 9.171439480717832e-05,
      "loss": 0.7016,
      "step": 4259
    },
    {
      "epoch": 0.5418124006359301,
      "grad_norm": 2.419180154800415,
      "learning_rate": 9.168893979890544e-05,
      "loss": 0.599,
      "step": 4260
    },
    {
      "epoch": 0.541939586645469,
      "grad_norm": 1.8535298109054565,
      "learning_rate": 9.166348479063256e-05,
      "loss": 0.6887,
      "step": 4261
    },
    {
      "epoch": 0.5420667726550079,
      "grad_norm": 1.7802966833114624,
      "learning_rate": 9.163802978235968e-05,
      "loss": 0.6168,
      "step": 4262
    },
    {
      "epoch": 0.5421939586645469,
      "grad_norm": 2.2677526473999023,
      "learning_rate": 9.16125747740868e-05,
      "loss": 0.7536,
      "step": 4263
    },
    {
      "epoch": 0.5423211446740859,
      "grad_norm": 1.5824743509292603,
      "learning_rate": 9.158711976581393e-05,
      "loss": 0.4971,
      "step": 4264
    },
    {
      "epoch": 0.5424483306836247,
      "grad_norm": 2.8093936443328857,
      "learning_rate": 9.156166475754106e-05,
      "loss": 0.9061,
      "step": 4265
    },
    {
      "epoch": 0.5425755166931637,
      "grad_norm": 1.7494196891784668,
      "learning_rate": 9.153620974926818e-05,
      "loss": 0.5784,
      "step": 4266
    },
    {
      "epoch": 0.5427027027027027,
      "grad_norm": 1.9820446968078613,
      "learning_rate": 9.15107547409953e-05,
      "loss": 0.6584,
      "step": 4267
    },
    {
      "epoch": 0.5428298887122417,
      "grad_norm": 1.7153739929199219,
      "learning_rate": 9.148529973272242e-05,
      "loss": 0.549,
      "step": 4268
    },
    {
      "epoch": 0.5429570747217806,
      "grad_norm": 1.698050856590271,
      "learning_rate": 9.145984472444953e-05,
      "loss": 0.4346,
      "step": 4269
    },
    {
      "epoch": 0.5430842607313195,
      "grad_norm": 1.9532760381698608,
      "learning_rate": 9.143438971617667e-05,
      "loss": 0.5924,
      "step": 4270
    },
    {
      "epoch": 0.5432114467408585,
      "grad_norm": 1.717980146408081,
      "learning_rate": 9.140893470790379e-05,
      "loss": 0.575,
      "step": 4271
    },
    {
      "epoch": 0.5433386327503975,
      "grad_norm": 2.902712821960449,
      "learning_rate": 9.13834796996309e-05,
      "loss": 0.6033,
      "step": 4272
    },
    {
      "epoch": 0.5434658187599364,
      "grad_norm": 2.266523599624634,
      "learning_rate": 9.135802469135802e-05,
      "loss": 0.7336,
      "step": 4273
    },
    {
      "epoch": 0.5435930047694754,
      "grad_norm": 2.6117727756500244,
      "learning_rate": 9.133256968308516e-05,
      "loss": 0.7231,
      "step": 4274
    },
    {
      "epoch": 0.5437201907790143,
      "grad_norm": 2.0457406044006348,
      "learning_rate": 9.130711467481227e-05,
      "loss": 0.615,
      "step": 4275
    },
    {
      "epoch": 0.5438473767885532,
      "grad_norm": 2.468305826187134,
      "learning_rate": 9.128165966653939e-05,
      "loss": 1.0447,
      "step": 4276
    },
    {
      "epoch": 0.5439745627980922,
      "grad_norm": 2.374516248703003,
      "learning_rate": 9.125620465826653e-05,
      "loss": 0.7032,
      "step": 4277
    },
    {
      "epoch": 0.5441017488076312,
      "grad_norm": 2.14432692527771,
      "learning_rate": 9.123074964999364e-05,
      "loss": 0.4131,
      "step": 4278
    },
    {
      "epoch": 0.5442289348171702,
      "grad_norm": 2.006566047668457,
      "learning_rate": 9.120529464172076e-05,
      "loss": 0.8225,
      "step": 4279
    },
    {
      "epoch": 0.544356120826709,
      "grad_norm": 1.9213547706604004,
      "learning_rate": 9.117983963344788e-05,
      "loss": 0.7108,
      "step": 4280
    },
    {
      "epoch": 0.544483306836248,
      "grad_norm": 2.1627726554870605,
      "learning_rate": 9.1154384625175e-05,
      "loss": 0.6491,
      "step": 4281
    },
    {
      "epoch": 0.544610492845787,
      "grad_norm": 1.6565165519714355,
      "learning_rate": 9.112892961690213e-05,
      "loss": 0.5634,
      "step": 4282
    },
    {
      "epoch": 0.5447376788553259,
      "grad_norm": 2.172717332839966,
      "learning_rate": 9.110347460862925e-05,
      "loss": 0.7744,
      "step": 4283
    },
    {
      "epoch": 0.5448648648648649,
      "grad_norm": 1.7701125144958496,
      "learning_rate": 9.107801960035638e-05,
      "loss": 0.5325,
      "step": 4284
    },
    {
      "epoch": 0.5449920508744038,
      "grad_norm": 1.3331555128097534,
      "learning_rate": 9.10525645920835e-05,
      "loss": 0.5936,
      "step": 4285
    },
    {
      "epoch": 0.5451192368839428,
      "grad_norm": 2.079005479812622,
      "learning_rate": 9.102710958381062e-05,
      "loss": 0.5977,
      "step": 4286
    },
    {
      "epoch": 0.5452464228934817,
      "grad_norm": 2.0477397441864014,
      "learning_rate": 9.100165457553774e-05,
      "loss": 0.4015,
      "step": 4287
    },
    {
      "epoch": 0.5453736089030207,
      "grad_norm": 2.1530652046203613,
      "learning_rate": 9.097619956726486e-05,
      "loss": 0.5052,
      "step": 4288
    },
    {
      "epoch": 0.5455007949125597,
      "grad_norm": 2.310152530670166,
      "learning_rate": 9.095074455899199e-05,
      "loss": 0.6268,
      "step": 4289
    },
    {
      "epoch": 0.5456279809220985,
      "grad_norm": 2.244072914123535,
      "learning_rate": 9.092528955071911e-05,
      "loss": 0.6631,
      "step": 4290
    },
    {
      "epoch": 0.5457551669316375,
      "grad_norm": 1.9703569412231445,
      "learning_rate": 9.089983454244623e-05,
      "loss": 0.5673,
      "step": 4291
    },
    {
      "epoch": 0.5458823529411765,
      "grad_norm": 2.578399419784546,
      "learning_rate": 9.087437953417335e-05,
      "loss": 0.7595,
      "step": 4292
    },
    {
      "epoch": 0.5460095389507155,
      "grad_norm": 7.303573131561279,
      "learning_rate": 9.084892452590048e-05,
      "loss": 0.8576,
      "step": 4293
    },
    {
      "epoch": 0.5461367249602543,
      "grad_norm": 2.2578799724578857,
      "learning_rate": 9.08234695176276e-05,
      "loss": 0.7303,
      "step": 4294
    },
    {
      "epoch": 0.5462639109697933,
      "grad_norm": 2.1593074798583984,
      "learning_rate": 9.079801450935472e-05,
      "loss": 0.8614,
      "step": 4295
    },
    {
      "epoch": 0.5463910969793323,
      "grad_norm": 2.215179681777954,
      "learning_rate": 9.077255950108185e-05,
      "loss": 0.5774,
      "step": 4296
    },
    {
      "epoch": 0.5465182829888712,
      "grad_norm": 1.5884547233581543,
      "learning_rate": 9.074710449280897e-05,
      "loss": 0.5832,
      "step": 4297
    },
    {
      "epoch": 0.5466454689984102,
      "grad_norm": 1.845487356185913,
      "learning_rate": 9.072164948453609e-05,
      "loss": 0.776,
      "step": 4298
    },
    {
      "epoch": 0.5467726550079491,
      "grad_norm": 1.8625237941741943,
      "learning_rate": 9.06961944762632e-05,
      "loss": 0.6699,
      "step": 4299
    },
    {
      "epoch": 0.5468998410174881,
      "grad_norm": 2.6479389667510986,
      "learning_rate": 9.067073946799032e-05,
      "loss": 0.6631,
      "step": 4300
    },
    {
      "epoch": 0.547027027027027,
      "grad_norm": 2.3654632568359375,
      "learning_rate": 9.064528445971746e-05,
      "loss": 0.796,
      "step": 4301
    },
    {
      "epoch": 0.547154213036566,
      "grad_norm": 2.109067916870117,
      "learning_rate": 9.061982945144457e-05,
      "loss": 0.6879,
      "step": 4302
    },
    {
      "epoch": 0.547281399046105,
      "grad_norm": 2.740575075149536,
      "learning_rate": 9.059437444317171e-05,
      "loss": 0.7429,
      "step": 4303
    },
    {
      "epoch": 0.5474085850556438,
      "grad_norm": 1.9767787456512451,
      "learning_rate": 9.056891943489883e-05,
      "loss": 0.648,
      "step": 4304
    },
    {
      "epoch": 0.5475357710651828,
      "grad_norm": 1.8463304042816162,
      "learning_rate": 9.054346442662594e-05,
      "loss": 0.6926,
      "step": 4305
    },
    {
      "epoch": 0.5476629570747218,
      "grad_norm": 1.707649827003479,
      "learning_rate": 9.051800941835306e-05,
      "loss": 0.5345,
      "step": 4306
    },
    {
      "epoch": 0.5477901430842608,
      "grad_norm": 2.8173158168792725,
      "learning_rate": 9.049255441008018e-05,
      "loss": 1.0805,
      "step": 4307
    },
    {
      "epoch": 0.5479173290937996,
      "grad_norm": 2.2175261974334717,
      "learning_rate": 9.046709940180731e-05,
      "loss": 1.0276,
      "step": 4308
    },
    {
      "epoch": 0.5480445151033386,
      "grad_norm": 2.158958673477173,
      "learning_rate": 9.044164439353443e-05,
      "loss": 0.8279,
      "step": 4309
    },
    {
      "epoch": 0.5481717011128776,
      "grad_norm": 2.4578027725219727,
      "learning_rate": 9.041618938526155e-05,
      "loss": 0.6292,
      "step": 4310
    },
    {
      "epoch": 0.5482988871224166,
      "grad_norm": 1.388582706451416,
      "learning_rate": 9.039073437698867e-05,
      "loss": 0.5438,
      "step": 4311
    },
    {
      "epoch": 0.5484260731319555,
      "grad_norm": 2.206134557723999,
      "learning_rate": 9.036527936871579e-05,
      "loss": 0.6099,
      "step": 4312
    },
    {
      "epoch": 0.5485532591414944,
      "grad_norm": 2.1439638137817383,
      "learning_rate": 9.033982436044292e-05,
      "loss": 0.5701,
      "step": 4313
    },
    {
      "epoch": 0.5486804451510334,
      "grad_norm": 2.273817300796509,
      "learning_rate": 9.031436935217005e-05,
      "loss": 0.7338,
      "step": 4314
    },
    {
      "epoch": 0.5488076311605723,
      "grad_norm": 2.2532958984375,
      "learning_rate": 9.028891434389717e-05,
      "loss": 0.4759,
      "step": 4315
    },
    {
      "epoch": 0.5489348171701113,
      "grad_norm": 2.622375726699829,
      "learning_rate": 9.026345933562429e-05,
      "loss": 0.7466,
      "step": 4316
    },
    {
      "epoch": 0.5490620031796503,
      "grad_norm": 2.207068681716919,
      "learning_rate": 9.023800432735141e-05,
      "loss": 0.575,
      "step": 4317
    },
    {
      "epoch": 0.5491891891891892,
      "grad_norm": 2.3832967281341553,
      "learning_rate": 9.021254931907853e-05,
      "loss": 0.4798,
      "step": 4318
    },
    {
      "epoch": 0.5493163751987281,
      "grad_norm": 1.887420654296875,
      "learning_rate": 9.018709431080565e-05,
      "loss": 0.757,
      "step": 4319
    },
    {
      "epoch": 0.5494435612082671,
      "grad_norm": 2.137979030609131,
      "learning_rate": 9.016163930253278e-05,
      "loss": 0.596,
      "step": 4320
    },
    {
      "epoch": 0.5495707472178061,
      "grad_norm": 1.6430646181106567,
      "learning_rate": 9.01361842942599e-05,
      "loss": 0.4481,
      "step": 4321
    },
    {
      "epoch": 0.549697933227345,
      "grad_norm": 2.3657326698303223,
      "learning_rate": 9.011072928598703e-05,
      "loss": 0.6872,
      "step": 4322
    },
    {
      "epoch": 0.5498251192368839,
      "grad_norm": 1.9213625192642212,
      "learning_rate": 9.008527427771415e-05,
      "loss": 0.7813,
      "step": 4323
    },
    {
      "epoch": 0.5499523052464229,
      "grad_norm": 1.939700961112976,
      "learning_rate": 9.005981926944127e-05,
      "loss": 0.5077,
      "step": 4324
    },
    {
      "epoch": 0.5500794912559619,
      "grad_norm": 2.836970806121826,
      "learning_rate": 9.003436426116839e-05,
      "loss": 0.5871,
      "step": 4325
    },
    {
      "epoch": 0.5502066772655008,
      "grad_norm": 2.044039726257324,
      "learning_rate": 9.00089092528955e-05,
      "loss": 0.5096,
      "step": 4326
    },
    {
      "epoch": 0.5503338632750397,
      "grad_norm": 1.9300576448440552,
      "learning_rate": 8.998345424462264e-05,
      "loss": 0.46,
      "step": 4327
    },
    {
      "epoch": 0.5504610492845787,
      "grad_norm": 2.146580457687378,
      "learning_rate": 8.995799923634976e-05,
      "loss": 0.6189,
      "step": 4328
    },
    {
      "epoch": 0.5505882352941176,
      "grad_norm": 2.1862826347351074,
      "learning_rate": 8.993254422807688e-05,
      "loss": 0.6667,
      "step": 4329
    },
    {
      "epoch": 0.5507154213036566,
      "grad_norm": 2.1591618061065674,
      "learning_rate": 8.9907089219804e-05,
      "loss": 0.5611,
      "step": 4330
    },
    {
      "epoch": 0.5508426073131956,
      "grad_norm": 1.8065731525421143,
      "learning_rate": 8.988163421153111e-05,
      "loss": 0.5431,
      "step": 4331
    },
    {
      "epoch": 0.5509697933227345,
      "grad_norm": 2.0679638385772705,
      "learning_rate": 8.985617920325825e-05,
      "loss": 0.6804,
      "step": 4332
    },
    {
      "epoch": 0.5510969793322734,
      "grad_norm": 2.8725697994232178,
      "learning_rate": 8.983072419498538e-05,
      "loss": 0.7161,
      "step": 4333
    },
    {
      "epoch": 0.5512241653418124,
      "grad_norm": 2.2737865447998047,
      "learning_rate": 8.98052691867125e-05,
      "loss": 0.5984,
      "step": 4334
    },
    {
      "epoch": 0.5513513513513514,
      "grad_norm": 2.660923957824707,
      "learning_rate": 8.977981417843962e-05,
      "loss": 0.608,
      "step": 4335
    },
    {
      "epoch": 0.5514785373608903,
      "grad_norm": 2.272797107696533,
      "learning_rate": 8.975435917016673e-05,
      "loss": 0.5484,
      "step": 4336
    },
    {
      "epoch": 0.5516057233704292,
      "grad_norm": 2.5690865516662598,
      "learning_rate": 8.972890416189385e-05,
      "loss": 0.7293,
      "step": 4337
    },
    {
      "epoch": 0.5517329093799682,
      "grad_norm": 1.5423208475112915,
      "learning_rate": 8.970344915362097e-05,
      "loss": 0.4624,
      "step": 4338
    },
    {
      "epoch": 0.5518600953895072,
      "grad_norm": 2.3282477855682373,
      "learning_rate": 8.96779941453481e-05,
      "loss": 0.7303,
      "step": 4339
    },
    {
      "epoch": 0.5519872813990461,
      "grad_norm": 1.9811034202575684,
      "learning_rate": 8.965253913707522e-05,
      "loss": 0.59,
      "step": 4340
    },
    {
      "epoch": 0.552114467408585,
      "grad_norm": 2.0773258209228516,
      "learning_rate": 8.962708412880234e-05,
      "loss": 0.3825,
      "step": 4341
    },
    {
      "epoch": 0.552241653418124,
      "grad_norm": 1.9989017248153687,
      "learning_rate": 8.960162912052947e-05,
      "loss": 0.5523,
      "step": 4342
    },
    {
      "epoch": 0.5523688394276629,
      "grad_norm": 1.631874680519104,
      "learning_rate": 8.957617411225659e-05,
      "loss": 0.544,
      "step": 4343
    },
    {
      "epoch": 0.5524960254372019,
      "grad_norm": 1.9594942331314087,
      "learning_rate": 8.955071910398371e-05,
      "loss": 0.4643,
      "step": 4344
    },
    {
      "epoch": 0.5526232114467409,
      "grad_norm": 2.120659828186035,
      "learning_rate": 8.952526409571084e-05,
      "loss": 0.6449,
      "step": 4345
    },
    {
      "epoch": 0.5527503974562799,
      "grad_norm": 2.302598714828491,
      "learning_rate": 8.949980908743796e-05,
      "loss": 0.6897,
      "step": 4346
    },
    {
      "epoch": 0.5528775834658187,
      "grad_norm": 3.353970527648926,
      "learning_rate": 8.947435407916508e-05,
      "loss": 0.4573,
      "step": 4347
    },
    {
      "epoch": 0.5530047694753577,
      "grad_norm": 2.0871574878692627,
      "learning_rate": 8.94488990708922e-05,
      "loss": 0.6173,
      "step": 4348
    },
    {
      "epoch": 0.5531319554848967,
      "grad_norm": 2.3607027530670166,
      "learning_rate": 8.942344406261932e-05,
      "loss": 0.5976,
      "step": 4349
    },
    {
      "epoch": 0.5532591414944356,
      "grad_norm": 2.483304500579834,
      "learning_rate": 8.939798905434644e-05,
      "loss": 0.41,
      "step": 4350
    },
    {
      "epoch": 0.5533863275039745,
      "grad_norm": 1.7192703485488892,
      "learning_rate": 8.937253404607357e-05,
      "loss": 0.2942,
      "step": 4351
    },
    {
      "epoch": 0.5535135135135135,
      "grad_norm": 1.7523136138916016,
      "learning_rate": 8.93470790378007e-05,
      "loss": 0.505,
      "step": 4352
    },
    {
      "epoch": 0.5536406995230525,
      "grad_norm": 1.5924394130706787,
      "learning_rate": 8.932162402952782e-05,
      "loss": 0.5818,
      "step": 4353
    },
    {
      "epoch": 0.5537678855325914,
      "grad_norm": 2.5598690509796143,
      "learning_rate": 8.929616902125494e-05,
      "loss": 0.8756,
      "step": 4354
    },
    {
      "epoch": 0.5538950715421304,
      "grad_norm": 2.5936572551727295,
      "learning_rate": 8.927071401298206e-05,
      "loss": 0.4335,
      "step": 4355
    },
    {
      "epoch": 0.5540222575516693,
      "grad_norm": 2.2072997093200684,
      "learning_rate": 8.924525900470918e-05,
      "loss": 0.6752,
      "step": 4356
    },
    {
      "epoch": 0.5541494435612083,
      "grad_norm": 4.071773052215576,
      "learning_rate": 8.92198039964363e-05,
      "loss": 0.876,
      "step": 4357
    },
    {
      "epoch": 0.5542766295707472,
      "grad_norm": 1.7421603202819824,
      "learning_rate": 8.919434898816343e-05,
      "loss": 0.4043,
      "step": 4358
    },
    {
      "epoch": 0.5544038155802862,
      "grad_norm": 3.7463080883026123,
      "learning_rate": 8.916889397989055e-05,
      "loss": 0.4746,
      "step": 4359
    },
    {
      "epoch": 0.5545310015898252,
      "grad_norm": 2.017970085144043,
      "learning_rate": 8.914343897161767e-05,
      "loss": 0.4928,
      "step": 4360
    },
    {
      "epoch": 0.554658187599364,
      "grad_norm": 2.8410656452178955,
      "learning_rate": 8.91179839633448e-05,
      "loss": 0.6031,
      "step": 4361
    },
    {
      "epoch": 0.554785373608903,
      "grad_norm": 2.058509111404419,
      "learning_rate": 8.909252895507192e-05,
      "loss": 0.7356,
      "step": 4362
    },
    {
      "epoch": 0.554912559618442,
      "grad_norm": 1.8974112272262573,
      "learning_rate": 8.906707394679903e-05,
      "loss": 0.6937,
      "step": 4363
    },
    {
      "epoch": 0.555039745627981,
      "grad_norm": 1.9173160791397095,
      "learning_rate": 8.904161893852617e-05,
      "loss": 0.5473,
      "step": 4364
    },
    {
      "epoch": 0.5551669316375198,
      "grad_norm": 1.7563101053237915,
      "learning_rate": 8.901616393025329e-05,
      "loss": 0.6281,
      "step": 4365
    },
    {
      "epoch": 0.5552941176470588,
      "grad_norm": 2.9799976348876953,
      "learning_rate": 8.89907089219804e-05,
      "loss": 0.6935,
      "step": 4366
    },
    {
      "epoch": 0.5554213036565978,
      "grad_norm": 1.9739556312561035,
      "learning_rate": 8.896525391370752e-05,
      "loss": 0.5996,
      "step": 4367
    },
    {
      "epoch": 0.5555484896661367,
      "grad_norm": 2.3632373809814453,
      "learning_rate": 8.893979890543464e-05,
      "loss": 0.523,
      "step": 4368
    },
    {
      "epoch": 0.5556756756756757,
      "grad_norm": 1.5042500495910645,
      "learning_rate": 8.891434389716176e-05,
      "loss": 0.5813,
      "step": 4369
    },
    {
      "epoch": 0.5558028616852146,
      "grad_norm": 1.9884209632873535,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.9161,
      "step": 4370
    },
    {
      "epoch": 0.5559300476947536,
      "grad_norm": 1.6973663568496704,
      "learning_rate": 8.886343388061603e-05,
      "loss": 0.5842,
      "step": 4371
    },
    {
      "epoch": 0.5560572337042925,
      "grad_norm": 1.745808720588684,
      "learning_rate": 8.883797887234314e-05,
      "loss": 0.5893,
      "step": 4372
    },
    {
      "epoch": 0.5561844197138315,
      "grad_norm": 2.3292760848999023,
      "learning_rate": 8.881252386407026e-05,
      "loss": 0.6454,
      "step": 4373
    },
    {
      "epoch": 0.5563116057233705,
      "grad_norm": 1.6233434677124023,
      "learning_rate": 8.878706885579738e-05,
      "loss": 0.595,
      "step": 4374
    },
    {
      "epoch": 0.5564387917329093,
      "grad_norm": 2.274055004119873,
      "learning_rate": 8.87616138475245e-05,
      "loss": 0.5616,
      "step": 4375
    },
    {
      "epoch": 0.5565659777424483,
      "grad_norm": 2.1317641735076904,
      "learning_rate": 8.873615883925163e-05,
      "loss": 0.7439,
      "step": 4376
    },
    {
      "epoch": 0.5566931637519873,
      "grad_norm": 1.7974790334701538,
      "learning_rate": 8.871070383097875e-05,
      "loss": 0.4903,
      "step": 4377
    },
    {
      "epoch": 0.5568203497615263,
      "grad_norm": 2.607116222381592,
      "learning_rate": 8.868524882270587e-05,
      "loss": 0.7449,
      "step": 4378
    },
    {
      "epoch": 0.5569475357710651,
      "grad_norm": 1.11594557762146,
      "learning_rate": 8.865979381443299e-05,
      "loss": 0.4314,
      "step": 4379
    },
    {
      "epoch": 0.5570747217806041,
      "grad_norm": 2.088221549987793,
      "learning_rate": 8.863433880616011e-05,
      "loss": 0.5214,
      "step": 4380
    },
    {
      "epoch": 0.5572019077901431,
      "grad_norm": 2.347912073135376,
      "learning_rate": 8.860888379788724e-05,
      "loss": 0.5951,
      "step": 4381
    },
    {
      "epoch": 0.557329093799682,
      "grad_norm": 2.105088710784912,
      "learning_rate": 8.858342878961436e-05,
      "loss": 0.5189,
      "step": 4382
    },
    {
      "epoch": 0.557456279809221,
      "grad_norm": 2.0422215461730957,
      "learning_rate": 8.855797378134149e-05,
      "loss": 0.7279,
      "step": 4383
    },
    {
      "epoch": 0.55758346581876,
      "grad_norm": 1.5504813194274902,
      "learning_rate": 8.853251877306861e-05,
      "loss": 0.5112,
      "step": 4384
    },
    {
      "epoch": 0.5577106518282989,
      "grad_norm": 1.677186369895935,
      "learning_rate": 8.850706376479573e-05,
      "loss": 0.5714,
      "step": 4385
    },
    {
      "epoch": 0.5578378378378378,
      "grad_norm": 1.8708622455596924,
      "learning_rate": 8.848160875652285e-05,
      "loss": 0.7023,
      "step": 4386
    },
    {
      "epoch": 0.5579650238473768,
      "grad_norm": 2.5174014568328857,
      "learning_rate": 8.845615374824997e-05,
      "loss": 0.4805,
      "step": 4387
    },
    {
      "epoch": 0.5580922098569158,
      "grad_norm": 1.8205575942993164,
      "learning_rate": 8.843069873997708e-05,
      "loss": 0.5437,
      "step": 4388
    },
    {
      "epoch": 0.5582193958664546,
      "grad_norm": 2.430067777633667,
      "learning_rate": 8.840524373170422e-05,
      "loss": 0.6483,
      "step": 4389
    },
    {
      "epoch": 0.5583465818759936,
      "grad_norm": 2.1166861057281494,
      "learning_rate": 8.837978872343135e-05,
      "loss": 0.8329,
      "step": 4390
    },
    {
      "epoch": 0.5584737678855326,
      "grad_norm": 2.1793558597564697,
      "learning_rate": 8.835433371515847e-05,
      "loss": 0.4405,
      "step": 4391
    },
    {
      "epoch": 0.5586009538950716,
      "grad_norm": 1.966016173362732,
      "learning_rate": 8.832887870688559e-05,
      "loss": 1.0002,
      "step": 4392
    },
    {
      "epoch": 0.5587281399046105,
      "grad_norm": 2.6051018238067627,
      "learning_rate": 8.83034236986127e-05,
      "loss": 0.6374,
      "step": 4393
    },
    {
      "epoch": 0.5588553259141494,
      "grad_norm": 2.990190267562866,
      "learning_rate": 8.827796869033982e-05,
      "loss": 0.898,
      "step": 4394
    },
    {
      "epoch": 0.5589825119236884,
      "grad_norm": 2.811736583709717,
      "learning_rate": 8.825251368206696e-05,
      "loss": 0.6081,
      "step": 4395
    },
    {
      "epoch": 0.5591096979332273,
      "grad_norm": 2.361301898956299,
      "learning_rate": 8.822705867379408e-05,
      "loss": 0.7302,
      "step": 4396
    },
    {
      "epoch": 0.5592368839427663,
      "grad_norm": 2.105586290359497,
      "learning_rate": 8.82016036655212e-05,
      "loss": 0.5591,
      "step": 4397
    },
    {
      "epoch": 0.5593640699523053,
      "grad_norm": 2.0684754848480225,
      "learning_rate": 8.817614865724831e-05,
      "loss": 0.7944,
      "step": 4398
    },
    {
      "epoch": 0.5594912559618442,
      "grad_norm": 1.7955350875854492,
      "learning_rate": 8.815069364897543e-05,
      "loss": 0.5472,
      "step": 4399
    },
    {
      "epoch": 0.5596184419713831,
      "grad_norm": 2.498924493789673,
      "learning_rate": 8.812523864070256e-05,
      "loss": 0.8928,
      "step": 4400
    },
    {
      "epoch": 0.5597456279809221,
      "grad_norm": 2.5639851093292236,
      "learning_rate": 8.809978363242968e-05,
      "loss": 0.7009,
      "step": 4401
    },
    {
      "epoch": 0.5598728139904611,
      "grad_norm": 2.321713924407959,
      "learning_rate": 8.807432862415681e-05,
      "loss": 0.622,
      "step": 4402
    },
    {
      "epoch": 0.56,
      "grad_norm": 2.087852716445923,
      "learning_rate": 8.804887361588393e-05,
      "loss": 0.711,
      "step": 4403
    },
    {
      "epoch": 0.5601271860095389,
      "grad_norm": 1.7815183401107788,
      "learning_rate": 8.802341860761105e-05,
      "loss": 0.59,
      "step": 4404
    },
    {
      "epoch": 0.5602543720190779,
      "grad_norm": 2.4008255004882812,
      "learning_rate": 8.799796359933817e-05,
      "loss": 0.7032,
      "step": 4405
    },
    {
      "epoch": 0.5603815580286169,
      "grad_norm": 1.9238632917404175,
      "learning_rate": 8.797250859106529e-05,
      "loss": 0.7268,
      "step": 4406
    },
    {
      "epoch": 0.5605087440381558,
      "grad_norm": 1.3534181118011475,
      "learning_rate": 8.794705358279242e-05,
      "loss": 0.36,
      "step": 4407
    },
    {
      "epoch": 0.5606359300476947,
      "grad_norm": 2.2856595516204834,
      "learning_rate": 8.792159857451954e-05,
      "loss": 0.492,
      "step": 4408
    },
    {
      "epoch": 0.5607631160572337,
      "grad_norm": 2.561272621154785,
      "learning_rate": 8.789614356624666e-05,
      "loss": 0.6201,
      "step": 4409
    },
    {
      "epoch": 0.5608903020667727,
      "grad_norm": 1.8672581911087036,
      "learning_rate": 8.787068855797379e-05,
      "loss": 0.4852,
      "step": 4410
    },
    {
      "epoch": 0.5610174880763116,
      "grad_norm": 2.5444579124450684,
      "learning_rate": 8.784523354970091e-05,
      "loss": 0.6499,
      "step": 4411
    },
    {
      "epoch": 0.5611446740858506,
      "grad_norm": 2.5414040088653564,
      "learning_rate": 8.781977854142803e-05,
      "loss": 0.6952,
      "step": 4412
    },
    {
      "epoch": 0.5612718600953895,
      "grad_norm": 1.357899785041809,
      "learning_rate": 8.779432353315515e-05,
      "loss": 0.5665,
      "step": 4413
    },
    {
      "epoch": 0.5613990461049284,
      "grad_norm": 3.3211419582366943,
      "learning_rate": 8.776886852488228e-05,
      "loss": 0.6815,
      "step": 4414
    },
    {
      "epoch": 0.5615262321144674,
      "grad_norm": 1.7874280214309692,
      "learning_rate": 8.77434135166094e-05,
      "loss": 0.5076,
      "step": 4415
    },
    {
      "epoch": 0.5616534181240064,
      "grad_norm": 2.0496668815612793,
      "learning_rate": 8.771795850833652e-05,
      "loss": 0.8803,
      "step": 4416
    },
    {
      "epoch": 0.5617806041335454,
      "grad_norm": 1.6218005418777466,
      "learning_rate": 8.769250350006364e-05,
      "loss": 0.5191,
      "step": 4417
    },
    {
      "epoch": 0.5619077901430842,
      "grad_norm": 1.784220814704895,
      "learning_rate": 8.766704849179076e-05,
      "loss": 0.6028,
      "step": 4418
    },
    {
      "epoch": 0.5620349761526232,
      "grad_norm": 2.3124921321868896,
      "learning_rate": 8.764159348351789e-05,
      "loss": 0.6979,
      "step": 4419
    },
    {
      "epoch": 0.5621621621621622,
      "grad_norm": 2.8321280479431152,
      "learning_rate": 8.761613847524502e-05,
      "loss": 0.629,
      "step": 4420
    },
    {
      "epoch": 0.5622893481717011,
      "grad_norm": 2.232311725616455,
      "learning_rate": 8.759068346697214e-05,
      "loss": 0.9275,
      "step": 4421
    },
    {
      "epoch": 0.56241653418124,
      "grad_norm": 2.1019039154052734,
      "learning_rate": 8.756522845869926e-05,
      "loss": 0.4336,
      "step": 4422
    },
    {
      "epoch": 0.562543720190779,
      "grad_norm": 1.9878902435302734,
      "learning_rate": 8.753977345042638e-05,
      "loss": 0.6827,
      "step": 4423
    },
    {
      "epoch": 0.562670906200318,
      "grad_norm": 2.63671875,
      "learning_rate": 8.75143184421535e-05,
      "loss": 0.5276,
      "step": 4424
    },
    {
      "epoch": 0.5627980922098569,
      "grad_norm": 3.3095834255218506,
      "learning_rate": 8.748886343388061e-05,
      "loss": 0.648,
      "step": 4425
    },
    {
      "epoch": 0.5629252782193959,
      "grad_norm": 2.3724935054779053,
      "learning_rate": 8.746340842560775e-05,
      "loss": 0.7126,
      "step": 4426
    },
    {
      "epoch": 0.5630524642289348,
      "grad_norm": 2.3240370750427246,
      "learning_rate": 8.743795341733486e-05,
      "loss": 0.528,
      "step": 4427
    },
    {
      "epoch": 0.5631796502384737,
      "grad_norm": 1.9347621202468872,
      "learning_rate": 8.741249840906198e-05,
      "loss": 0.7035,
      "step": 4428
    },
    {
      "epoch": 0.5633068362480127,
      "grad_norm": 1.5651142597198486,
      "learning_rate": 8.738704340078912e-05,
      "loss": 0.5712,
      "step": 4429
    },
    {
      "epoch": 0.5634340222575517,
      "grad_norm": 3.4034879207611084,
      "learning_rate": 8.736158839251623e-05,
      "loss": 0.5781,
      "step": 4430
    },
    {
      "epoch": 0.5635612082670907,
      "grad_norm": 2.956735134124756,
      "learning_rate": 8.733613338424335e-05,
      "loss": 0.6308,
      "step": 4431
    },
    {
      "epoch": 0.5636883942766295,
      "grad_norm": 2.1204493045806885,
      "learning_rate": 8.731067837597047e-05,
      "loss": 0.6671,
      "step": 4432
    },
    {
      "epoch": 0.5638155802861685,
      "grad_norm": 2.4356062412261963,
      "learning_rate": 8.72852233676976e-05,
      "loss": 0.6038,
      "step": 4433
    },
    {
      "epoch": 0.5639427662957075,
      "grad_norm": 2.4350850582122803,
      "learning_rate": 8.725976835942472e-05,
      "loss": 0.5328,
      "step": 4434
    },
    {
      "epoch": 0.5640699523052464,
      "grad_norm": 2.455375909805298,
      "learning_rate": 8.723431335115184e-05,
      "loss": 0.6898,
      "step": 4435
    },
    {
      "epoch": 0.5641971383147854,
      "grad_norm": 1.999748945236206,
      "learning_rate": 8.720885834287896e-05,
      "loss": 0.6155,
      "step": 4436
    },
    {
      "epoch": 0.5643243243243243,
      "grad_norm": 2.8676609992980957,
      "learning_rate": 8.718340333460608e-05,
      "loss": 0.585,
      "step": 4437
    },
    {
      "epoch": 0.5644515103338633,
      "grad_norm": 3.0281572341918945,
      "learning_rate": 8.715794832633321e-05,
      "loss": 0.8279,
      "step": 4438
    },
    {
      "epoch": 0.5645786963434022,
      "grad_norm": 1.7671151161193848,
      "learning_rate": 8.713249331806034e-05,
      "loss": 0.4119,
      "step": 4439
    },
    {
      "epoch": 0.5647058823529412,
      "grad_norm": 2.572014808654785,
      "learning_rate": 8.710703830978746e-05,
      "loss": 0.6137,
      "step": 4440
    },
    {
      "epoch": 0.5648330683624802,
      "grad_norm": 2.4688127040863037,
      "learning_rate": 8.708158330151458e-05,
      "loss": 0.6924,
      "step": 4441
    },
    {
      "epoch": 0.5649602543720191,
      "grad_norm": 1.3600057363510132,
      "learning_rate": 8.70561282932417e-05,
      "loss": 0.4875,
      "step": 4442
    },
    {
      "epoch": 0.565087440381558,
      "grad_norm": 2.3476665019989014,
      "learning_rate": 8.703067328496882e-05,
      "loss": 0.6545,
      "step": 4443
    },
    {
      "epoch": 0.565214626391097,
      "grad_norm": 2.0212740898132324,
      "learning_rate": 8.700521827669594e-05,
      "loss": 0.5016,
      "step": 4444
    },
    {
      "epoch": 0.565341812400636,
      "grad_norm": 2.3369076251983643,
      "learning_rate": 8.697976326842307e-05,
      "loss": 0.5586,
      "step": 4445
    },
    {
      "epoch": 0.5654689984101748,
      "grad_norm": 2.5007474422454834,
      "learning_rate": 8.695430826015019e-05,
      "loss": 0.6485,
      "step": 4446
    },
    {
      "epoch": 0.5655961844197138,
      "grad_norm": 2.243783950805664,
      "learning_rate": 8.692885325187731e-05,
      "loss": 0.7894,
      "step": 4447
    },
    {
      "epoch": 0.5657233704292528,
      "grad_norm": 1.8075168132781982,
      "learning_rate": 8.690339824360443e-05,
      "loss": 0.5424,
      "step": 4448
    },
    {
      "epoch": 0.5658505564387918,
      "grad_norm": 3.498445510864258,
      "learning_rate": 8.687794323533156e-05,
      "loss": 0.5764,
      "step": 4449
    },
    {
      "epoch": 0.5659777424483307,
      "grad_norm": 1.9670536518096924,
      "learning_rate": 8.685248822705868e-05,
      "loss": 0.4994,
      "step": 4450
    },
    {
      "epoch": 0.5661049284578696,
      "grad_norm": 1.7739884853363037,
      "learning_rate": 8.682703321878581e-05,
      "loss": 0.4099,
      "step": 4451
    },
    {
      "epoch": 0.5662321144674086,
      "grad_norm": 1.9427778720855713,
      "learning_rate": 8.680157821051293e-05,
      "loss": 0.6492,
      "step": 4452
    },
    {
      "epoch": 0.5663593004769475,
      "grad_norm": 1.8791227340698242,
      "learning_rate": 8.677612320224005e-05,
      "loss": 0.6344,
      "step": 4453
    },
    {
      "epoch": 0.5664864864864865,
      "grad_norm": 2.013197898864746,
      "learning_rate": 8.675066819396717e-05,
      "loss": 0.6862,
      "step": 4454
    },
    {
      "epoch": 0.5666136724960255,
      "grad_norm": 2.3316562175750732,
      "learning_rate": 8.672521318569428e-05,
      "loss": 0.6279,
      "step": 4455
    },
    {
      "epoch": 0.5667408585055644,
      "grad_norm": 1.865556240081787,
      "learning_rate": 8.66997581774214e-05,
      "loss": 0.4623,
      "step": 4456
    },
    {
      "epoch": 0.5668680445151033,
      "grad_norm": 1.8561670780181885,
      "learning_rate": 8.667430316914854e-05,
      "loss": 0.3689,
      "step": 4457
    },
    {
      "epoch": 0.5669952305246423,
      "grad_norm": 1.9750566482543945,
      "learning_rate": 8.664884816087567e-05,
      "loss": 0.4853,
      "step": 4458
    },
    {
      "epoch": 0.5671224165341813,
      "grad_norm": 2.031270742416382,
      "learning_rate": 8.662339315260279e-05,
      "loss": 0.4965,
      "step": 4459
    },
    {
      "epoch": 0.5672496025437201,
      "grad_norm": 1.7087132930755615,
      "learning_rate": 8.65979381443299e-05,
      "loss": 0.5172,
      "step": 4460
    },
    {
      "epoch": 0.5673767885532591,
      "grad_norm": 2.650996446609497,
      "learning_rate": 8.657248313605702e-05,
      "loss": 0.8086,
      "step": 4461
    },
    {
      "epoch": 0.5675039745627981,
      "grad_norm": 2.0571630001068115,
      "learning_rate": 8.654702812778414e-05,
      "loss": 0.5768,
      "step": 4462
    },
    {
      "epoch": 0.5676311605723371,
      "grad_norm": 2.2252542972564697,
      "learning_rate": 8.652157311951126e-05,
      "loss": 0.8101,
      "step": 4463
    },
    {
      "epoch": 0.567758346581876,
      "grad_norm": 1.9376165866851807,
      "learning_rate": 8.64961181112384e-05,
      "loss": 0.7042,
      "step": 4464
    },
    {
      "epoch": 0.5678855325914149,
      "grad_norm": 2.7468972206115723,
      "learning_rate": 8.647066310296551e-05,
      "loss": 0.7334,
      "step": 4465
    },
    {
      "epoch": 0.5680127186009539,
      "grad_norm": 2.4939093589782715,
      "learning_rate": 8.644520809469263e-05,
      "loss": 0.6174,
      "step": 4466
    },
    {
      "epoch": 0.5681399046104928,
      "grad_norm": 3.174673318862915,
      "learning_rate": 8.641975308641975e-05,
      "loss": 0.8385,
      "step": 4467
    },
    {
      "epoch": 0.5682670906200318,
      "grad_norm": 2.099853754043579,
      "learning_rate": 8.639429807814688e-05,
      "loss": 0.607,
      "step": 4468
    },
    {
      "epoch": 0.5683942766295708,
      "grad_norm": 2.6711530685424805,
      "learning_rate": 8.6368843069874e-05,
      "loss": 0.7224,
      "step": 4469
    },
    {
      "epoch": 0.5685214626391097,
      "grad_norm": 2.2398624420166016,
      "learning_rate": 8.634338806160113e-05,
      "loss": 0.6001,
      "step": 4470
    },
    {
      "epoch": 0.5686486486486486,
      "grad_norm": 2.4241974353790283,
      "learning_rate": 8.631793305332825e-05,
      "loss": 0.6749,
      "step": 4471
    },
    {
      "epoch": 0.5687758346581876,
      "grad_norm": 2.797060489654541,
      "learning_rate": 8.629247804505537e-05,
      "loss": 0.8082,
      "step": 4472
    },
    {
      "epoch": 0.5689030206677266,
      "grad_norm": 2.351172924041748,
      "learning_rate": 8.626702303678249e-05,
      "loss": 0.3737,
      "step": 4473
    },
    {
      "epoch": 0.5690302066772654,
      "grad_norm": 1.9793450832366943,
      "learning_rate": 8.624156802850961e-05,
      "loss": 0.3038,
      "step": 4474
    },
    {
      "epoch": 0.5691573926868044,
      "grad_norm": 2.252563714981079,
      "learning_rate": 8.621611302023673e-05,
      "loss": 0.8885,
      "step": 4475
    },
    {
      "epoch": 0.5692845786963434,
      "grad_norm": 2.1520376205444336,
      "learning_rate": 8.619065801196386e-05,
      "loss": 0.8656,
      "step": 4476
    },
    {
      "epoch": 0.5694117647058824,
      "grad_norm": 2.70224666595459,
      "learning_rate": 8.616520300369098e-05,
      "loss": 0.6822,
      "step": 4477
    },
    {
      "epoch": 0.5695389507154213,
      "grad_norm": 2.4767229557037354,
      "learning_rate": 8.613974799541811e-05,
      "loss": 0.7437,
      "step": 4478
    },
    {
      "epoch": 0.5696661367249602,
      "grad_norm": 2.329838275909424,
      "learning_rate": 8.611429298714523e-05,
      "loss": 0.4108,
      "step": 4479
    },
    {
      "epoch": 0.5697933227344992,
      "grad_norm": 2.1767234802246094,
      "learning_rate": 8.608883797887235e-05,
      "loss": 0.5179,
      "step": 4480
    },
    {
      "epoch": 0.5699205087440381,
      "grad_norm": 1.4237078428268433,
      "learning_rate": 8.606338297059947e-05,
      "loss": 0.5125,
      "step": 4481
    },
    {
      "epoch": 0.5700476947535771,
      "grad_norm": 2.289412260055542,
      "learning_rate": 8.60379279623266e-05,
      "loss": 0.7234,
      "step": 4482
    },
    {
      "epoch": 0.5701748807631161,
      "grad_norm": 2.2002034187316895,
      "learning_rate": 8.601247295405372e-05,
      "loss": 0.5383,
      "step": 4483
    },
    {
      "epoch": 0.570302066772655,
      "grad_norm": 1.9019320011138916,
      "learning_rate": 8.598701794578084e-05,
      "loss": 0.6903,
      "step": 4484
    },
    {
      "epoch": 0.5704292527821939,
      "grad_norm": 2.3481197357177734,
      "learning_rate": 8.596156293750795e-05,
      "loss": 0.8348,
      "step": 4485
    },
    {
      "epoch": 0.5705564387917329,
      "grad_norm": 2.388503074645996,
      "learning_rate": 8.593610792923507e-05,
      "loss": 0.4494,
      "step": 4486
    },
    {
      "epoch": 0.5706836248012719,
      "grad_norm": 3.88637375831604,
      "learning_rate": 8.591065292096219e-05,
      "loss": 0.7994,
      "step": 4487
    },
    {
      "epoch": 0.5708108108108109,
      "grad_norm": 2.490485668182373,
      "learning_rate": 8.588519791268932e-05,
      "loss": 0.8479,
      "step": 4488
    },
    {
      "epoch": 0.5709379968203497,
      "grad_norm": 3.4681098461151123,
      "learning_rate": 8.585974290441646e-05,
      "loss": 0.8957,
      "step": 4489
    },
    {
      "epoch": 0.5710651828298887,
      "grad_norm": 2.201526165008545,
      "learning_rate": 8.583428789614358e-05,
      "loss": 0.4227,
      "step": 4490
    },
    {
      "epoch": 0.5711923688394277,
      "grad_norm": 2.3595335483551025,
      "learning_rate": 8.58088328878707e-05,
      "loss": 0.6734,
      "step": 4491
    },
    {
      "epoch": 0.5713195548489666,
      "grad_norm": 2.461290121078491,
      "learning_rate": 8.578337787959781e-05,
      "loss": 0.7244,
      "step": 4492
    },
    {
      "epoch": 0.5714467408585056,
      "grad_norm": 3.0085670948028564,
      "learning_rate": 8.575792287132493e-05,
      "loss": 0.8217,
      "step": 4493
    },
    {
      "epoch": 0.5715739268680445,
      "grad_norm": 2.3288116455078125,
      "learning_rate": 8.573246786305205e-05,
      "loss": 0.6512,
      "step": 4494
    },
    {
      "epoch": 0.5717011128775835,
      "grad_norm": 1.6823883056640625,
      "learning_rate": 8.570701285477918e-05,
      "loss": 0.5276,
      "step": 4495
    },
    {
      "epoch": 0.5718282988871224,
      "grad_norm": 2.3982510566711426,
      "learning_rate": 8.56815578465063e-05,
      "loss": 0.7268,
      "step": 4496
    },
    {
      "epoch": 0.5719554848966614,
      "grad_norm": 2.341726303100586,
      "learning_rate": 8.565610283823343e-05,
      "loss": 0.6005,
      "step": 4497
    },
    {
      "epoch": 0.5720826709062004,
      "grad_norm": 1.9825563430786133,
      "learning_rate": 8.563064782996055e-05,
      "loss": 0.5963,
      "step": 4498
    },
    {
      "epoch": 0.5722098569157392,
      "grad_norm": 1.9082221984863281,
      "learning_rate": 8.560519282168767e-05,
      "loss": 0.5374,
      "step": 4499
    },
    {
      "epoch": 0.5723370429252782,
      "grad_norm": 2.5426294803619385,
      "learning_rate": 8.557973781341479e-05,
      "loss": 0.7448,
      "step": 4500
    },
    {
      "epoch": 0.5724642289348172,
      "grad_norm": 1.9911541938781738,
      "learning_rate": 8.555428280514192e-05,
      "loss": 0.7537,
      "step": 4501
    },
    {
      "epoch": 0.5725914149443562,
      "grad_norm": 1.9946860074996948,
      "learning_rate": 8.552882779686904e-05,
      "loss": 0.6267,
      "step": 4502
    },
    {
      "epoch": 0.572718600953895,
      "grad_norm": 1.7761889696121216,
      "learning_rate": 8.550337278859616e-05,
      "loss": 0.4661,
      "step": 4503
    },
    {
      "epoch": 0.572845786963434,
      "grad_norm": 3.3379571437835693,
      "learning_rate": 8.547791778032328e-05,
      "loss": 0.5736,
      "step": 4504
    },
    {
      "epoch": 0.572972972972973,
      "grad_norm": 1.074205994606018,
      "learning_rate": 8.54524627720504e-05,
      "loss": 0.3385,
      "step": 4505
    },
    {
      "epoch": 0.5731001589825119,
      "grad_norm": 6.392188549041748,
      "learning_rate": 8.542700776377752e-05,
      "loss": 0.7259,
      "step": 4506
    },
    {
      "epoch": 0.5732273449920509,
      "grad_norm": 2.01518177986145,
      "learning_rate": 8.540155275550465e-05,
      "loss": 0.5041,
      "step": 4507
    },
    {
      "epoch": 0.5733545310015898,
      "grad_norm": 2.424334764480591,
      "learning_rate": 8.537609774723178e-05,
      "loss": 0.7199,
      "step": 4508
    },
    {
      "epoch": 0.5734817170111288,
      "grad_norm": 1.5488455295562744,
      "learning_rate": 8.53506427389589e-05,
      "loss": 0.5446,
      "step": 4509
    },
    {
      "epoch": 0.5736089030206677,
      "grad_norm": 2.188331365585327,
      "learning_rate": 8.532518773068602e-05,
      "loss": 0.5807,
      "step": 4510
    },
    {
      "epoch": 0.5737360890302067,
      "grad_norm": 2.250375509262085,
      "learning_rate": 8.529973272241314e-05,
      "loss": 0.7599,
      "step": 4511
    },
    {
      "epoch": 0.5738632750397457,
      "grad_norm": 3.560807943344116,
      "learning_rate": 8.527427771414026e-05,
      "loss": 0.742,
      "step": 4512
    },
    {
      "epoch": 0.5739904610492845,
      "grad_norm": 2.524571418762207,
      "learning_rate": 8.524882270586739e-05,
      "loss": 0.5661,
      "step": 4513
    },
    {
      "epoch": 0.5741176470588235,
      "grad_norm": 2.089265823364258,
      "learning_rate": 8.52233676975945e-05,
      "loss": 0.4234,
      "step": 4514
    },
    {
      "epoch": 0.5742448330683625,
      "grad_norm": 2.087430953979492,
      "learning_rate": 8.519791268932163e-05,
      "loss": 0.4876,
      "step": 4515
    },
    {
      "epoch": 0.5743720190779015,
      "grad_norm": 3.2553157806396484,
      "learning_rate": 8.517245768104874e-05,
      "loss": 0.6006,
      "step": 4516
    },
    {
      "epoch": 0.5744992050874403,
      "grad_norm": 1.9457180500030518,
      "learning_rate": 8.514700267277588e-05,
      "loss": 0.6181,
      "step": 4517
    },
    {
      "epoch": 0.5746263910969793,
      "grad_norm": 2.2341413497924805,
      "learning_rate": 8.5121547664503e-05,
      "loss": 0.6982,
      "step": 4518
    },
    {
      "epoch": 0.5747535771065183,
      "grad_norm": 2.9075422286987305,
      "learning_rate": 8.509609265623011e-05,
      "loss": 0.5788,
      "step": 4519
    },
    {
      "epoch": 0.5748807631160572,
      "grad_norm": 1.7356961965560913,
      "learning_rate": 8.507063764795725e-05,
      "loss": 0.4703,
      "step": 4520
    },
    {
      "epoch": 0.5750079491255962,
      "grad_norm": 1.8166720867156982,
      "learning_rate": 8.504518263968436e-05,
      "loss": 0.5011,
      "step": 4521
    },
    {
      "epoch": 0.5751351351351351,
      "grad_norm": 1.571778655052185,
      "learning_rate": 8.501972763141148e-05,
      "loss": 0.6356,
      "step": 4522
    },
    {
      "epoch": 0.5752623211446741,
      "grad_norm": 2.4248428344726562,
      "learning_rate": 8.49942726231386e-05,
      "loss": 0.6286,
      "step": 4523
    },
    {
      "epoch": 0.575389507154213,
      "grad_norm": 1.9325904846191406,
      "learning_rate": 8.496881761486572e-05,
      "loss": 0.4945,
      "step": 4524
    },
    {
      "epoch": 0.575516693163752,
      "grad_norm": 1.8878631591796875,
      "learning_rate": 8.494336260659285e-05,
      "loss": 0.4733,
      "step": 4525
    },
    {
      "epoch": 0.575643879173291,
      "grad_norm": 1.6731764078140259,
      "learning_rate": 8.491790759831999e-05,
      "loss": 0.5273,
      "step": 4526
    },
    {
      "epoch": 0.5757710651828299,
      "grad_norm": 2.241239070892334,
      "learning_rate": 8.48924525900471e-05,
      "loss": 0.5006,
      "step": 4527
    },
    {
      "epoch": 0.5758982511923688,
      "grad_norm": 2.252094030380249,
      "learning_rate": 8.486699758177422e-05,
      "loss": 0.5438,
      "step": 4528
    },
    {
      "epoch": 0.5760254372019078,
      "grad_norm": 1.6868594884872437,
      "learning_rate": 8.484154257350134e-05,
      "loss": 0.3863,
      "step": 4529
    },
    {
      "epoch": 0.5761526232114468,
      "grad_norm": 1.772365927696228,
      "learning_rate": 8.481608756522846e-05,
      "loss": 0.6975,
      "step": 4530
    },
    {
      "epoch": 0.5762798092209857,
      "grad_norm": 2.3649260997772217,
      "learning_rate": 8.479063255695558e-05,
      "loss": 0.5965,
      "step": 4531
    },
    {
      "epoch": 0.5764069952305246,
      "grad_norm": 2.004197359085083,
      "learning_rate": 8.476517754868271e-05,
      "loss": 0.6295,
      "step": 4532
    },
    {
      "epoch": 0.5765341812400636,
      "grad_norm": 1.974456787109375,
      "learning_rate": 8.473972254040983e-05,
      "loss": 0.4654,
      "step": 4533
    },
    {
      "epoch": 0.5766613672496026,
      "grad_norm": 1.9656052589416504,
      "learning_rate": 8.471426753213695e-05,
      "loss": 0.539,
      "step": 4534
    },
    {
      "epoch": 0.5767885532591415,
      "grad_norm": 1.792048692703247,
      "learning_rate": 8.468881252386407e-05,
      "loss": 0.6229,
      "step": 4535
    },
    {
      "epoch": 0.5769157392686804,
      "grad_norm": 2.5353546142578125,
      "learning_rate": 8.46633575155912e-05,
      "loss": 0.6095,
      "step": 4536
    },
    {
      "epoch": 0.5770429252782194,
      "grad_norm": 3.007138252258301,
      "learning_rate": 8.463790250731832e-05,
      "loss": 0.6217,
      "step": 4537
    },
    {
      "epoch": 0.5771701112877583,
      "grad_norm": 2.394887924194336,
      "learning_rate": 8.461244749904544e-05,
      "loss": 0.601,
      "step": 4538
    },
    {
      "epoch": 0.5772972972972973,
      "grad_norm": 2.3696236610412598,
      "learning_rate": 8.458699249077257e-05,
      "loss": 0.5885,
      "step": 4539
    },
    {
      "epoch": 0.5774244833068363,
      "grad_norm": 2.7419283390045166,
      "learning_rate": 8.456153748249969e-05,
      "loss": 0.4531,
      "step": 4540
    },
    {
      "epoch": 0.5775516693163752,
      "grad_norm": 2.6435177326202393,
      "learning_rate": 8.453608247422681e-05,
      "loss": 0.603,
      "step": 4541
    },
    {
      "epoch": 0.5776788553259141,
      "grad_norm": 2.6976771354675293,
      "learning_rate": 8.451062746595393e-05,
      "loss": 0.5401,
      "step": 4542
    },
    {
      "epoch": 0.5778060413354531,
      "grad_norm": 2.9488089084625244,
      "learning_rate": 8.448517245768104e-05,
      "loss": 0.583,
      "step": 4543
    },
    {
      "epoch": 0.5779332273449921,
      "grad_norm": 1.6758289337158203,
      "learning_rate": 8.445971744940818e-05,
      "loss": 0.5391,
      "step": 4544
    },
    {
      "epoch": 0.578060413354531,
      "grad_norm": 2.137211561203003,
      "learning_rate": 8.44342624411353e-05,
      "loss": 0.53,
      "step": 4545
    },
    {
      "epoch": 0.5781875993640699,
      "grad_norm": 2.7306582927703857,
      "learning_rate": 8.440880743286243e-05,
      "loss": 0.6727,
      "step": 4546
    },
    {
      "epoch": 0.5783147853736089,
      "grad_norm": 2.482835054397583,
      "learning_rate": 8.438335242458955e-05,
      "loss": 0.5987,
      "step": 4547
    },
    {
      "epoch": 0.5784419713831479,
      "grad_norm": 2.149054527282715,
      "learning_rate": 8.435789741631667e-05,
      "loss": 0.5188,
      "step": 4548
    },
    {
      "epoch": 0.5785691573926868,
      "grad_norm": 3.1885759830474854,
      "learning_rate": 8.433244240804378e-05,
      "loss": 0.6383,
      "step": 4549
    },
    {
      "epoch": 0.5786963434022258,
      "grad_norm": 2.1356725692749023,
      "learning_rate": 8.43069873997709e-05,
      "loss": 0.4184,
      "step": 4550
    },
    {
      "epoch": 0.5788235294117647,
      "grad_norm": 3.3701279163360596,
      "learning_rate": 8.428153239149804e-05,
      "loss": 0.7098,
      "step": 4551
    },
    {
      "epoch": 0.5789507154213036,
      "grad_norm": 2.575122356414795,
      "learning_rate": 8.425607738322515e-05,
      "loss": 0.7551,
      "step": 4552
    },
    {
      "epoch": 0.5790779014308426,
      "grad_norm": 2.5622568130493164,
      "learning_rate": 8.423062237495227e-05,
      "loss": 0.8105,
      "step": 4553
    },
    {
      "epoch": 0.5792050874403816,
      "grad_norm": 3.0814621448516846,
      "learning_rate": 8.420516736667939e-05,
      "loss": 0.5468,
      "step": 4554
    },
    {
      "epoch": 0.5793322734499206,
      "grad_norm": 2.505455732345581,
      "learning_rate": 8.417971235840651e-05,
      "loss": 0.6474,
      "step": 4555
    },
    {
      "epoch": 0.5794594594594594,
      "grad_norm": 2.0717968940734863,
      "learning_rate": 8.415425735013364e-05,
      "loss": 0.548,
      "step": 4556
    },
    {
      "epoch": 0.5795866454689984,
      "grad_norm": 2.198596477508545,
      "learning_rate": 8.412880234186078e-05,
      "loss": 0.5319,
      "step": 4557
    },
    {
      "epoch": 0.5797138314785374,
      "grad_norm": 2.7451119422912598,
      "learning_rate": 8.41033473335879e-05,
      "loss": 0.5694,
      "step": 4558
    },
    {
      "epoch": 0.5798410174880763,
      "grad_norm": 2.3854146003723145,
      "learning_rate": 8.407789232531501e-05,
      "loss": 0.7737,
      "step": 4559
    },
    {
      "epoch": 0.5799682034976152,
      "grad_norm": 2.1999363899230957,
      "learning_rate": 8.405243731704213e-05,
      "loss": 0.5284,
      "step": 4560
    },
    {
      "epoch": 0.5800953895071542,
      "grad_norm": 2.0847654342651367,
      "learning_rate": 8.402698230876925e-05,
      "loss": 0.5618,
      "step": 4561
    },
    {
      "epoch": 0.5802225755166932,
      "grad_norm": 2.266606092453003,
      "learning_rate": 8.400152730049637e-05,
      "loss": 0.4601,
      "step": 4562
    },
    {
      "epoch": 0.5803497615262321,
      "grad_norm": 2.2561545372009277,
      "learning_rate": 8.39760722922235e-05,
      "loss": 0.766,
      "step": 4563
    },
    {
      "epoch": 0.5804769475357711,
      "grad_norm": 1.7771285772323608,
      "learning_rate": 8.395061728395062e-05,
      "loss": 0.6712,
      "step": 4564
    },
    {
      "epoch": 0.58060413354531,
      "grad_norm": 2.88435435295105,
      "learning_rate": 8.392516227567775e-05,
      "loss": 0.7689,
      "step": 4565
    },
    {
      "epoch": 0.5807313195548489,
      "grad_norm": 2.4778308868408203,
      "learning_rate": 8.389970726740487e-05,
      "loss": 0.8369,
      "step": 4566
    },
    {
      "epoch": 0.5808585055643879,
      "grad_norm": 2.4304330348968506,
      "learning_rate": 8.387425225913199e-05,
      "loss": 0.6245,
      "step": 4567
    },
    {
      "epoch": 0.5809856915739269,
      "grad_norm": 2.6896188259124756,
      "learning_rate": 8.384879725085911e-05,
      "loss": 0.5347,
      "step": 4568
    },
    {
      "epoch": 0.5811128775834659,
      "grad_norm": 2.307389736175537,
      "learning_rate": 8.382334224258623e-05,
      "loss": 0.5994,
      "step": 4569
    },
    {
      "epoch": 0.5812400635930047,
      "grad_norm": 2.609095335006714,
      "learning_rate": 8.379788723431336e-05,
      "loss": 0.7917,
      "step": 4570
    },
    {
      "epoch": 0.5813672496025437,
      "grad_norm": 2.0804319381713867,
      "learning_rate": 8.377243222604048e-05,
      "loss": 0.593,
      "step": 4571
    },
    {
      "epoch": 0.5814944356120827,
      "grad_norm": 2.3338632583618164,
      "learning_rate": 8.37469772177676e-05,
      "loss": 0.7699,
      "step": 4572
    },
    {
      "epoch": 0.5816216216216217,
      "grad_norm": 2.235506772994995,
      "learning_rate": 8.372152220949472e-05,
      "loss": 0.6388,
      "step": 4573
    },
    {
      "epoch": 0.5817488076311605,
      "grad_norm": 1.8876692056655884,
      "learning_rate": 8.369606720122183e-05,
      "loss": 0.5349,
      "step": 4574
    },
    {
      "epoch": 0.5818759936406995,
      "grad_norm": 2.2264983654022217,
      "learning_rate": 8.367061219294897e-05,
      "loss": 0.9811,
      "step": 4575
    },
    {
      "epoch": 0.5820031796502385,
      "grad_norm": 2.064716100692749,
      "learning_rate": 8.36451571846761e-05,
      "loss": 0.663,
      "step": 4576
    },
    {
      "epoch": 0.5821303656597774,
      "grad_norm": 1.7049070596694946,
      "learning_rate": 8.361970217640322e-05,
      "loss": 0.4297,
      "step": 4577
    },
    {
      "epoch": 0.5822575516693164,
      "grad_norm": 2.536285877227783,
      "learning_rate": 8.359424716813034e-05,
      "loss": 0.8123,
      "step": 4578
    },
    {
      "epoch": 0.5823847376788553,
      "grad_norm": 1.8898123502731323,
      "learning_rate": 8.356879215985746e-05,
      "loss": 0.5237,
      "step": 4579
    },
    {
      "epoch": 0.5825119236883943,
      "grad_norm": 2.477884292602539,
      "learning_rate": 8.354333715158457e-05,
      "loss": 0.6746,
      "step": 4580
    },
    {
      "epoch": 0.5826391096979332,
      "grad_norm": 2.6988563537597656,
      "learning_rate": 8.351788214331169e-05,
      "loss": 0.5669,
      "step": 4581
    },
    {
      "epoch": 0.5827662957074722,
      "grad_norm": 3.8511810302734375,
      "learning_rate": 8.349242713503882e-05,
      "loss": 0.5562,
      "step": 4582
    },
    {
      "epoch": 0.5828934817170112,
      "grad_norm": 2.2670061588287354,
      "learning_rate": 8.346697212676594e-05,
      "loss": 0.6047,
      "step": 4583
    },
    {
      "epoch": 0.58302066772655,
      "grad_norm": 1.5941786766052246,
      "learning_rate": 8.344151711849306e-05,
      "loss": 0.4509,
      "step": 4584
    },
    {
      "epoch": 0.583147853736089,
      "grad_norm": 2.5579800605773926,
      "learning_rate": 8.34160621102202e-05,
      "loss": 0.6972,
      "step": 4585
    },
    {
      "epoch": 0.583275039745628,
      "grad_norm": 2.655534505844116,
      "learning_rate": 8.339060710194731e-05,
      "loss": 0.6318,
      "step": 4586
    },
    {
      "epoch": 0.583402225755167,
      "grad_norm": 2.7558281421661377,
      "learning_rate": 8.336515209367443e-05,
      "loss": 0.9538,
      "step": 4587
    },
    {
      "epoch": 0.5835294117647059,
      "grad_norm": 1.6514307260513306,
      "learning_rate": 8.333969708540156e-05,
      "loss": 0.5828,
      "step": 4588
    },
    {
      "epoch": 0.5836565977742448,
      "grad_norm": 2.4739081859588623,
      "learning_rate": 8.331424207712868e-05,
      "loss": 0.6273,
      "step": 4589
    },
    {
      "epoch": 0.5837837837837838,
      "grad_norm": 2.243056058883667,
      "learning_rate": 8.32887870688558e-05,
      "loss": 0.5804,
      "step": 4590
    },
    {
      "epoch": 0.5839109697933227,
      "grad_norm": 1.982002854347229,
      "learning_rate": 8.326333206058292e-05,
      "loss": 0.5087,
      "step": 4591
    },
    {
      "epoch": 0.5840381558028617,
      "grad_norm": 2.1853246688842773,
      "learning_rate": 8.323787705231004e-05,
      "loss": 0.5488,
      "step": 4592
    },
    {
      "epoch": 0.5841653418124007,
      "grad_norm": 1.6831800937652588,
      "learning_rate": 8.321242204403716e-05,
      "loss": 0.3495,
      "step": 4593
    },
    {
      "epoch": 0.5842925278219396,
      "grad_norm": 1.8443597555160522,
      "learning_rate": 8.318696703576429e-05,
      "loss": 0.6418,
      "step": 4594
    },
    {
      "epoch": 0.5844197138314785,
      "grad_norm": 2.059122323989868,
      "learning_rate": 8.316151202749142e-05,
      "loss": 0.5,
      "step": 4595
    },
    {
      "epoch": 0.5845468998410175,
      "grad_norm": 2.224457025527954,
      "learning_rate": 8.313605701921854e-05,
      "loss": 0.5212,
      "step": 4596
    },
    {
      "epoch": 0.5846740858505565,
      "grad_norm": 2.2036705017089844,
      "learning_rate": 8.311060201094566e-05,
      "loss": 0.4668,
      "step": 4597
    },
    {
      "epoch": 0.5848012718600953,
      "grad_norm": 2.108149528503418,
      "learning_rate": 8.308514700267278e-05,
      "loss": 0.7824,
      "step": 4598
    },
    {
      "epoch": 0.5849284578696343,
      "grad_norm": 1.976388692855835,
      "learning_rate": 8.30596919943999e-05,
      "loss": 0.6155,
      "step": 4599
    },
    {
      "epoch": 0.5850556438791733,
      "grad_norm": 1.6854565143585205,
      "learning_rate": 8.303423698612702e-05,
      "loss": 0.4776,
      "step": 4600
    },
    {
      "epoch": 0.5851828298887123,
      "grad_norm": 2.3039345741271973,
      "learning_rate": 8.300878197785415e-05,
      "loss": 0.8015,
      "step": 4601
    },
    {
      "epoch": 0.5853100158982512,
      "grad_norm": 2.282305955886841,
      "learning_rate": 8.298332696958127e-05,
      "loss": 0.6669,
      "step": 4602
    },
    {
      "epoch": 0.5854372019077901,
      "grad_norm": 2.5151755809783936,
      "learning_rate": 8.295787196130839e-05,
      "loss": 0.7961,
      "step": 4603
    },
    {
      "epoch": 0.5855643879173291,
      "grad_norm": 2.4632513523101807,
      "learning_rate": 8.293241695303552e-05,
      "loss": 0.5797,
      "step": 4604
    },
    {
      "epoch": 0.585691573926868,
      "grad_norm": 3.196296453475952,
      "learning_rate": 8.290696194476264e-05,
      "loss": 0.8438,
      "step": 4605
    },
    {
      "epoch": 0.585818759936407,
      "grad_norm": 1.4869287014007568,
      "learning_rate": 8.288150693648976e-05,
      "loss": 0.5929,
      "step": 4606
    },
    {
      "epoch": 0.585945945945946,
      "grad_norm": 2.0450541973114014,
      "learning_rate": 8.285605192821689e-05,
      "loss": 0.6596,
      "step": 4607
    },
    {
      "epoch": 0.5860731319554849,
      "grad_norm": 2.221184015274048,
      "learning_rate": 8.283059691994401e-05,
      "loss": 0.6994,
      "step": 4608
    },
    {
      "epoch": 0.5862003179650238,
      "grad_norm": 1.7028567790985107,
      "learning_rate": 8.280514191167113e-05,
      "loss": 0.4091,
      "step": 4609
    },
    {
      "epoch": 0.5863275039745628,
      "grad_norm": 2.340496778488159,
      "learning_rate": 8.277968690339824e-05,
      "loss": 0.6282,
      "step": 4610
    },
    {
      "epoch": 0.5864546899841018,
      "grad_norm": 2.4669790267944336,
      "learning_rate": 8.275423189512536e-05,
      "loss": 0.6413,
      "step": 4611
    },
    {
      "epoch": 0.5865818759936406,
      "grad_norm": 2.2180140018463135,
      "learning_rate": 8.272877688685248e-05,
      "loss": 0.5338,
      "step": 4612
    },
    {
      "epoch": 0.5867090620031796,
      "grad_norm": 2.5524613857269287,
      "learning_rate": 8.270332187857961e-05,
      "loss": 0.515,
      "step": 4613
    },
    {
      "epoch": 0.5868362480127186,
      "grad_norm": 2.204258918762207,
      "learning_rate": 8.267786687030675e-05,
      "loss": 0.6063,
      "step": 4614
    },
    {
      "epoch": 0.5869634340222576,
      "grad_norm": 2.8497302532196045,
      "learning_rate": 8.265241186203387e-05,
      "loss": 0.6753,
      "step": 4615
    },
    {
      "epoch": 0.5870906200317965,
      "grad_norm": 1.4240424633026123,
      "learning_rate": 8.262695685376098e-05,
      "loss": 0.3042,
      "step": 4616
    },
    {
      "epoch": 0.5872178060413354,
      "grad_norm": 1.807859182357788,
      "learning_rate": 8.26015018454881e-05,
      "loss": 0.5918,
      "step": 4617
    },
    {
      "epoch": 0.5873449920508744,
      "grad_norm": 1.9208502769470215,
      "learning_rate": 8.257604683721522e-05,
      "loss": 0.4473,
      "step": 4618
    },
    {
      "epoch": 0.5874721780604134,
      "grad_norm": 3.3964850902557373,
      "learning_rate": 8.255059182894235e-05,
      "loss": 0.6414,
      "step": 4619
    },
    {
      "epoch": 0.5875993640699523,
      "grad_norm": 1.852555751800537,
      "learning_rate": 8.252513682066947e-05,
      "loss": 0.5488,
      "step": 4620
    },
    {
      "epoch": 0.5877265500794913,
      "grad_norm": 2.43575382232666,
      "learning_rate": 8.249968181239659e-05,
      "loss": 0.6716,
      "step": 4621
    },
    {
      "epoch": 0.5878537360890302,
      "grad_norm": 2.620816707611084,
      "learning_rate": 8.247422680412371e-05,
      "loss": 0.3547,
      "step": 4622
    },
    {
      "epoch": 0.5879809220985691,
      "grad_norm": 2.106414318084717,
      "learning_rate": 8.244877179585083e-05,
      "loss": 0.5617,
      "step": 4623
    },
    {
      "epoch": 0.5881081081081081,
      "grad_norm": 2.6231322288513184,
      "learning_rate": 8.242331678757796e-05,
      "loss": 0.8421,
      "step": 4624
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 2.046509265899658,
      "learning_rate": 8.239786177930508e-05,
      "loss": 0.3836,
      "step": 4625
    },
    {
      "epoch": 0.5883624801271861,
      "grad_norm": 2.4664385318756104,
      "learning_rate": 8.237240677103221e-05,
      "loss": 0.4225,
      "step": 4626
    },
    {
      "epoch": 0.5884896661367249,
      "grad_norm": 2.6519722938537598,
      "learning_rate": 8.234695176275933e-05,
      "loss": 0.5829,
      "step": 4627
    },
    {
      "epoch": 0.5886168521462639,
      "grad_norm": 2.0380654335021973,
      "learning_rate": 8.232149675448645e-05,
      "loss": 0.6003,
      "step": 4628
    },
    {
      "epoch": 0.5887440381558029,
      "grad_norm": 1.9539159536361694,
      "learning_rate": 8.229604174621357e-05,
      "loss": 0.6006,
      "step": 4629
    },
    {
      "epoch": 0.5888712241653418,
      "grad_norm": 2.8418757915496826,
      "learning_rate": 8.227058673794069e-05,
      "loss": 0.8124,
      "step": 4630
    },
    {
      "epoch": 0.5889984101748807,
      "grad_norm": 1.8788081407546997,
      "learning_rate": 8.224513172966782e-05,
      "loss": 0.5108,
      "step": 4631
    },
    {
      "epoch": 0.5891255961844197,
      "grad_norm": 1.8331115245819092,
      "learning_rate": 8.221967672139494e-05,
      "loss": 0.5618,
      "step": 4632
    },
    {
      "epoch": 0.5892527821939587,
      "grad_norm": 2.145414113998413,
      "learning_rate": 8.219422171312207e-05,
      "loss": 0.6019,
      "step": 4633
    },
    {
      "epoch": 0.5893799682034976,
      "grad_norm": 2.5961503982543945,
      "learning_rate": 8.216876670484919e-05,
      "loss": 0.4432,
      "step": 4634
    },
    {
      "epoch": 0.5895071542130366,
      "grad_norm": 2.2142906188964844,
      "learning_rate": 8.214331169657631e-05,
      "loss": 0.8527,
      "step": 4635
    },
    {
      "epoch": 0.5896343402225755,
      "grad_norm": 2.7137489318847656,
      "learning_rate": 8.211785668830343e-05,
      "loss": 0.3801,
      "step": 4636
    },
    {
      "epoch": 0.5897615262321144,
      "grad_norm": 1.730683445930481,
      "learning_rate": 8.209240168003055e-05,
      "loss": 0.4894,
      "step": 4637
    },
    {
      "epoch": 0.5898887122416534,
      "grad_norm": 3.313361883163452,
      "learning_rate": 8.206694667175768e-05,
      "loss": 0.85,
      "step": 4638
    },
    {
      "epoch": 0.5900158982511924,
      "grad_norm": 3.260589122772217,
      "learning_rate": 8.20414916634848e-05,
      "loss": 0.8811,
      "step": 4639
    },
    {
      "epoch": 0.5901430842607314,
      "grad_norm": 2.637683868408203,
      "learning_rate": 8.201603665521192e-05,
      "loss": 0.76,
      "step": 4640
    },
    {
      "epoch": 0.5902702702702702,
      "grad_norm": 1.9407997131347656,
      "learning_rate": 8.199058164693903e-05,
      "loss": 0.5678,
      "step": 4641
    },
    {
      "epoch": 0.5903974562798092,
      "grad_norm": 2.007593870162964,
      "learning_rate": 8.196512663866615e-05,
      "loss": 0.5302,
      "step": 4642
    },
    {
      "epoch": 0.5905246422893482,
      "grad_norm": 2.225985050201416,
      "learning_rate": 8.193967163039328e-05,
      "loss": 0.7303,
      "step": 4643
    },
    {
      "epoch": 0.5906518282988871,
      "grad_norm": 2.4603962898254395,
      "learning_rate": 8.19142166221204e-05,
      "loss": 0.8886,
      "step": 4644
    },
    {
      "epoch": 0.590779014308426,
      "grad_norm": 2.184547185897827,
      "learning_rate": 8.188876161384754e-05,
      "loss": 0.4856,
      "step": 4645
    },
    {
      "epoch": 0.590906200317965,
      "grad_norm": 1.6199687719345093,
      "learning_rate": 8.186330660557465e-05,
      "loss": 0.4108,
      "step": 4646
    },
    {
      "epoch": 0.591033386327504,
      "grad_norm": 2.3991758823394775,
      "learning_rate": 8.183785159730177e-05,
      "loss": 0.6876,
      "step": 4647
    },
    {
      "epoch": 0.5911605723370429,
      "grad_norm": 1.7736483812332153,
      "learning_rate": 8.181239658902889e-05,
      "loss": 0.5161,
      "step": 4648
    },
    {
      "epoch": 0.5912877583465819,
      "grad_norm": 1.6946052312850952,
      "learning_rate": 8.178694158075601e-05,
      "loss": 0.4114,
      "step": 4649
    },
    {
      "epoch": 0.5914149443561209,
      "grad_norm": 1.588307499885559,
      "learning_rate": 8.176148657248314e-05,
      "loss": 0.4361,
      "step": 4650
    },
    {
      "epoch": 0.5915421303656597,
      "grad_norm": 1.6827433109283447,
      "learning_rate": 8.173603156421026e-05,
      "loss": 0.4352,
      "step": 4651
    },
    {
      "epoch": 0.5916693163751987,
      "grad_norm": 1.9997776746749878,
      "learning_rate": 8.171057655593738e-05,
      "loss": 0.4869,
      "step": 4652
    },
    {
      "epoch": 0.5917965023847377,
      "grad_norm": 1.9294558763504028,
      "learning_rate": 8.168512154766451e-05,
      "loss": 0.5096,
      "step": 4653
    },
    {
      "epoch": 0.5919236883942767,
      "grad_norm": 2.8116347789764404,
      "learning_rate": 8.165966653939163e-05,
      "loss": 0.6465,
      "step": 4654
    },
    {
      "epoch": 0.5920508744038155,
      "grad_norm": 1.5755008459091187,
      "learning_rate": 8.163421153111875e-05,
      "loss": 0.4628,
      "step": 4655
    },
    {
      "epoch": 0.5921780604133545,
      "grad_norm": 1.7599869966506958,
      "learning_rate": 8.160875652284587e-05,
      "loss": 0.5833,
      "step": 4656
    },
    {
      "epoch": 0.5923052464228935,
      "grad_norm": 3.5557680130004883,
      "learning_rate": 8.1583301514573e-05,
      "loss": 0.6204,
      "step": 4657
    },
    {
      "epoch": 0.5924324324324325,
      "grad_norm": 2.2051892280578613,
      "learning_rate": 8.155784650630012e-05,
      "loss": 0.4078,
      "step": 4658
    },
    {
      "epoch": 0.5925596184419714,
      "grad_norm": 2.439863681793213,
      "learning_rate": 8.153239149802724e-05,
      "loss": 0.643,
      "step": 4659
    },
    {
      "epoch": 0.5926868044515103,
      "grad_norm": 2.46171498298645,
      "learning_rate": 8.150693648975436e-05,
      "loss": 0.5213,
      "step": 4660
    },
    {
      "epoch": 0.5928139904610493,
      "grad_norm": 2.276620626449585,
      "learning_rate": 8.148148148148148e-05,
      "loss": 0.694,
      "step": 4661
    },
    {
      "epoch": 0.5929411764705882,
      "grad_norm": 2.5739269256591797,
      "learning_rate": 8.145602647320861e-05,
      "loss": 0.9232,
      "step": 4662
    },
    {
      "epoch": 0.5930683624801272,
      "grad_norm": 2.2797772884368896,
      "learning_rate": 8.143057146493574e-05,
      "loss": 0.6144,
      "step": 4663
    },
    {
      "epoch": 0.5931955484896662,
      "grad_norm": 1.920318365097046,
      "learning_rate": 8.140511645666286e-05,
      "loss": 0.6668,
      "step": 4664
    },
    {
      "epoch": 0.5933227344992051,
      "grad_norm": 2.749154567718506,
      "learning_rate": 8.137966144838998e-05,
      "loss": 0.7693,
      "step": 4665
    },
    {
      "epoch": 0.593449920508744,
      "grad_norm": 1.672420620918274,
      "learning_rate": 8.13542064401171e-05,
      "loss": 0.5723,
      "step": 4666
    },
    {
      "epoch": 0.593577106518283,
      "grad_norm": 1.951305866241455,
      "learning_rate": 8.132875143184422e-05,
      "loss": 0.5658,
      "step": 4667
    },
    {
      "epoch": 0.593704292527822,
      "grad_norm": 2.111525535583496,
      "learning_rate": 8.130329642357133e-05,
      "loss": 0.3668,
      "step": 4668
    },
    {
      "epoch": 0.5938314785373608,
      "grad_norm": 2.905775547027588,
      "learning_rate": 8.127784141529847e-05,
      "loss": 0.775,
      "step": 4669
    },
    {
      "epoch": 0.5939586645468998,
      "grad_norm": 2.204606771469116,
      "learning_rate": 8.125238640702559e-05,
      "loss": 0.6735,
      "step": 4670
    },
    {
      "epoch": 0.5940858505564388,
      "grad_norm": 2.0010130405426025,
      "learning_rate": 8.12269313987527e-05,
      "loss": 0.3838,
      "step": 4671
    },
    {
      "epoch": 0.5942130365659778,
      "grad_norm": 2.377114772796631,
      "learning_rate": 8.120147639047984e-05,
      "loss": 0.7909,
      "step": 4672
    },
    {
      "epoch": 0.5943402225755167,
      "grad_norm": 2.3194375038146973,
      "learning_rate": 8.117602138220696e-05,
      "loss": 0.7457,
      "step": 4673
    },
    {
      "epoch": 0.5944674085850556,
      "grad_norm": 1.7828073501586914,
      "learning_rate": 8.115056637393407e-05,
      "loss": 0.6605,
      "step": 4674
    },
    {
      "epoch": 0.5945945945945946,
      "grad_norm": 2.053199291229248,
      "learning_rate": 8.112511136566119e-05,
      "loss": 0.8911,
      "step": 4675
    },
    {
      "epoch": 0.5947217806041335,
      "grad_norm": 2.456631660461426,
      "learning_rate": 8.109965635738833e-05,
      "loss": 0.63,
      "step": 4676
    },
    {
      "epoch": 0.5948489666136725,
      "grad_norm": 2.0921525955200195,
      "learning_rate": 8.107420134911544e-05,
      "loss": 0.5865,
      "step": 4677
    },
    {
      "epoch": 0.5949761526232115,
      "grad_norm": 1.5220867395401,
      "learning_rate": 8.104874634084256e-05,
      "loss": 0.4898,
      "step": 4678
    },
    {
      "epoch": 0.5951033386327504,
      "grad_norm": 1.7192316055297852,
      "learning_rate": 8.102329133256968e-05,
      "loss": 0.4983,
      "step": 4679
    },
    {
      "epoch": 0.5952305246422893,
      "grad_norm": 2.3369526863098145,
      "learning_rate": 8.09978363242968e-05,
      "loss": 0.6937,
      "step": 4680
    },
    {
      "epoch": 0.5953577106518283,
      "grad_norm": 2.539280652999878,
      "learning_rate": 8.097238131602393e-05,
      "loss": 0.5378,
      "step": 4681
    },
    {
      "epoch": 0.5954848966613673,
      "grad_norm": 1.4762818813323975,
      "learning_rate": 8.094692630775106e-05,
      "loss": 0.4521,
      "step": 4682
    },
    {
      "epoch": 0.5956120826709062,
      "grad_norm": 1.851385235786438,
      "learning_rate": 8.092147129947818e-05,
      "loss": 0.6058,
      "step": 4683
    },
    {
      "epoch": 0.5957392686804451,
      "grad_norm": 1.7306593656539917,
      "learning_rate": 8.08960162912053e-05,
      "loss": 0.8,
      "step": 4684
    },
    {
      "epoch": 0.5958664546899841,
      "grad_norm": 3.466061592102051,
      "learning_rate": 8.087056128293242e-05,
      "loss": 0.7889,
      "step": 4685
    },
    {
      "epoch": 0.5959936406995231,
      "grad_norm": 1.5164252519607544,
      "learning_rate": 8.084510627465954e-05,
      "loss": 0.5334,
      "step": 4686
    },
    {
      "epoch": 0.596120826709062,
      "grad_norm": 2.1799488067626953,
      "learning_rate": 8.081965126638666e-05,
      "loss": 0.5353,
      "step": 4687
    },
    {
      "epoch": 0.596248012718601,
      "grad_norm": 2.576054334640503,
      "learning_rate": 8.079419625811379e-05,
      "loss": 0.7145,
      "step": 4688
    },
    {
      "epoch": 0.5963751987281399,
      "grad_norm": 2.4362475872039795,
      "learning_rate": 8.076874124984091e-05,
      "loss": 0.5099,
      "step": 4689
    },
    {
      "epoch": 0.5965023847376788,
      "grad_norm": 1.379380226135254,
      "learning_rate": 8.074328624156803e-05,
      "loss": 0.4069,
      "step": 4690
    },
    {
      "epoch": 0.5966295707472178,
      "grad_norm": 1.7528764009475708,
      "learning_rate": 8.071783123329515e-05,
      "loss": 0.3874,
      "step": 4691
    },
    {
      "epoch": 0.5967567567567568,
      "grad_norm": 2.421617269515991,
      "learning_rate": 8.069237622502228e-05,
      "loss": 0.5375,
      "step": 4692
    },
    {
      "epoch": 0.5968839427662957,
      "grad_norm": 3.318265914916992,
      "learning_rate": 8.06669212167494e-05,
      "loss": 0.6996,
      "step": 4693
    },
    {
      "epoch": 0.5970111287758346,
      "grad_norm": 2.215731620788574,
      "learning_rate": 8.064146620847653e-05,
      "loss": 0.396,
      "step": 4694
    },
    {
      "epoch": 0.5971383147853736,
      "grad_norm": 2.1118879318237305,
      "learning_rate": 8.061601120020365e-05,
      "loss": 0.6373,
      "step": 4695
    },
    {
      "epoch": 0.5972655007949126,
      "grad_norm": 1.9511609077453613,
      "learning_rate": 8.059055619193077e-05,
      "loss": 0.5899,
      "step": 4696
    },
    {
      "epoch": 0.5973926868044515,
      "grad_norm": 1.9610987901687622,
      "learning_rate": 8.056510118365789e-05,
      "loss": 0.742,
      "step": 4697
    },
    {
      "epoch": 0.5975198728139904,
      "grad_norm": 1.8059768676757812,
      "learning_rate": 8.0539646175385e-05,
      "loss": 0.5909,
      "step": 4698
    },
    {
      "epoch": 0.5976470588235294,
      "grad_norm": 2.6600711345672607,
      "learning_rate": 8.051419116711212e-05,
      "loss": 0.6291,
      "step": 4699
    },
    {
      "epoch": 0.5977742448330684,
      "grad_norm": 1.835028052330017,
      "learning_rate": 8.048873615883926e-05,
      "loss": 0.6105,
      "step": 4700
    },
    {
      "epoch": 0.5979014308426073,
      "grad_norm": 2.4055769443511963,
      "learning_rate": 8.046328115056639e-05,
      "loss": 0.7019,
      "step": 4701
    },
    {
      "epoch": 0.5980286168521463,
      "grad_norm": 2.2315621376037598,
      "learning_rate": 8.043782614229351e-05,
      "loss": 0.6793,
      "step": 4702
    },
    {
      "epoch": 0.5981558028616852,
      "grad_norm": 1.7193316221237183,
      "learning_rate": 8.041237113402063e-05,
      "loss": 0.4943,
      "step": 4703
    },
    {
      "epoch": 0.5982829888712242,
      "grad_norm": 2.335261583328247,
      "learning_rate": 8.038691612574774e-05,
      "loss": 0.6343,
      "step": 4704
    },
    {
      "epoch": 0.5984101748807631,
      "grad_norm": 2.714715003967285,
      "learning_rate": 8.036146111747486e-05,
      "loss": 0.6266,
      "step": 4705
    },
    {
      "epoch": 0.5985373608903021,
      "grad_norm": 1.771504282951355,
      "learning_rate": 8.033600610920198e-05,
      "loss": 0.4594,
      "step": 4706
    },
    {
      "epoch": 0.598664546899841,
      "grad_norm": 1.4112012386322021,
      "learning_rate": 8.031055110092911e-05,
      "loss": 0.3553,
      "step": 4707
    },
    {
      "epoch": 0.5987917329093799,
      "grad_norm": 2.1276042461395264,
      "learning_rate": 8.028509609265623e-05,
      "loss": 0.5509,
      "step": 4708
    },
    {
      "epoch": 0.5989189189189189,
      "grad_norm": 3.1732826232910156,
      "learning_rate": 8.025964108438335e-05,
      "loss": 0.6914,
      "step": 4709
    },
    {
      "epoch": 0.5990461049284579,
      "grad_norm": 2.8678290843963623,
      "learning_rate": 8.023418607611047e-05,
      "loss": 0.8547,
      "step": 4710
    },
    {
      "epoch": 0.5991732909379969,
      "grad_norm": 2.043578863143921,
      "learning_rate": 8.02087310678376e-05,
      "loss": 0.5736,
      "step": 4711
    },
    {
      "epoch": 0.5993004769475357,
      "grad_norm": 3.0072405338287354,
      "learning_rate": 8.018327605956472e-05,
      "loss": 0.5691,
      "step": 4712
    },
    {
      "epoch": 0.5994276629570747,
      "grad_norm": 1.4798839092254639,
      "learning_rate": 8.015782105129185e-05,
      "loss": 0.6939,
      "step": 4713
    },
    {
      "epoch": 0.5995548489666137,
      "grad_norm": 2.3807804584503174,
      "learning_rate": 8.013236604301897e-05,
      "loss": 0.4882,
      "step": 4714
    },
    {
      "epoch": 0.5996820349761526,
      "grad_norm": 2.7654690742492676,
      "learning_rate": 8.010691103474609e-05,
      "loss": 0.5846,
      "step": 4715
    },
    {
      "epoch": 0.5998092209856916,
      "grad_norm": 1.7818794250488281,
      "learning_rate": 8.008145602647321e-05,
      "loss": 0.6059,
      "step": 4716
    },
    {
      "epoch": 0.5999364069952305,
      "grad_norm": 2.506953001022339,
      "learning_rate": 8.005600101820033e-05,
      "loss": 0.5859,
      "step": 4717
    },
    {
      "epoch": 0.6000635930047695,
      "grad_norm": 2.490525722503662,
      "learning_rate": 8.003054600992745e-05,
      "loss": 0.6948,
      "step": 4718
    },
    {
      "epoch": 0.6001907790143084,
      "grad_norm": 1.8716095685958862,
      "learning_rate": 8.000509100165458e-05,
      "loss": 0.6907,
      "step": 4719
    },
    {
      "epoch": 0.6003179650238474,
      "grad_norm": 1.9532723426818848,
      "learning_rate": 7.99796359933817e-05,
      "loss": 0.427,
      "step": 4720
    },
    {
      "epoch": 0.6004451510333864,
      "grad_norm": 1.752504587173462,
      "learning_rate": 7.995418098510883e-05,
      "loss": 0.6546,
      "step": 4721
    },
    {
      "epoch": 0.6005723370429252,
      "grad_norm": 2.3258893489837646,
      "learning_rate": 7.992872597683595e-05,
      "loss": 0.5822,
      "step": 4722
    },
    {
      "epoch": 0.6006995230524642,
      "grad_norm": 1.7127163410186768,
      "learning_rate": 7.990327096856307e-05,
      "loss": 0.3668,
      "step": 4723
    },
    {
      "epoch": 0.6008267090620032,
      "grad_norm": 2.6991078853607178,
      "learning_rate": 7.987781596029019e-05,
      "loss": 0.7177,
      "step": 4724
    },
    {
      "epoch": 0.6009538950715422,
      "grad_norm": 1.9012258052825928,
      "learning_rate": 7.985236095201732e-05,
      "loss": 0.5544,
      "step": 4725
    },
    {
      "epoch": 0.601081081081081,
      "grad_norm": 1.8643320798873901,
      "learning_rate": 7.982690594374444e-05,
      "loss": 0.5245,
      "step": 4726
    },
    {
      "epoch": 0.60120826709062,
      "grad_norm": 2.3502132892608643,
      "learning_rate": 7.980145093547156e-05,
      "loss": 0.472,
      "step": 4727
    },
    {
      "epoch": 0.601335453100159,
      "grad_norm": 1.880172848701477,
      "learning_rate": 7.977599592719868e-05,
      "loss": 0.3393,
      "step": 4728
    },
    {
      "epoch": 0.6014626391096979,
      "grad_norm": 2.5093798637390137,
      "learning_rate": 7.97505409189258e-05,
      "loss": 0.4064,
      "step": 4729
    },
    {
      "epoch": 0.6015898251192369,
      "grad_norm": 2.4712908267974854,
      "learning_rate": 7.972508591065293e-05,
      "loss": 0.701,
      "step": 4730
    },
    {
      "epoch": 0.6017170111287758,
      "grad_norm": 2.4525458812713623,
      "learning_rate": 7.969963090238005e-05,
      "loss": 0.6367,
      "step": 4731
    },
    {
      "epoch": 0.6018441971383148,
      "grad_norm": 1.839776635169983,
      "learning_rate": 7.967417589410718e-05,
      "loss": 0.5646,
      "step": 4732
    },
    {
      "epoch": 0.6019713831478537,
      "grad_norm": 2.1640796661376953,
      "learning_rate": 7.96487208858343e-05,
      "loss": 0.5915,
      "step": 4733
    },
    {
      "epoch": 0.6020985691573927,
      "grad_norm": 3.4861273765563965,
      "learning_rate": 7.962326587756142e-05,
      "loss": 0.5092,
      "step": 4734
    },
    {
      "epoch": 0.6022257551669317,
      "grad_norm": 2.1305906772613525,
      "learning_rate": 7.959781086928853e-05,
      "loss": 0.6083,
      "step": 4735
    },
    {
      "epoch": 0.6023529411764705,
      "grad_norm": 1.6129916906356812,
      "learning_rate": 7.957235586101565e-05,
      "loss": 0.6816,
      "step": 4736
    },
    {
      "epoch": 0.6024801271860095,
      "grad_norm": 2.4016895294189453,
      "learning_rate": 7.954690085274277e-05,
      "loss": 0.6334,
      "step": 4737
    },
    {
      "epoch": 0.6026073131955485,
      "grad_norm": 1.8519431352615356,
      "learning_rate": 7.95214458444699e-05,
      "loss": 0.4524,
      "step": 4738
    },
    {
      "epoch": 0.6027344992050875,
      "grad_norm": 1.5087518692016602,
      "learning_rate": 7.949599083619702e-05,
      "loss": 0.7769,
      "step": 4739
    },
    {
      "epoch": 0.6028616852146264,
      "grad_norm": 2.1214818954467773,
      "learning_rate": 7.947053582792415e-05,
      "loss": 0.5485,
      "step": 4740
    },
    {
      "epoch": 0.6029888712241653,
      "grad_norm": 2.4594240188598633,
      "learning_rate": 7.944508081965127e-05,
      "loss": 0.5916,
      "step": 4741
    },
    {
      "epoch": 0.6031160572337043,
      "grad_norm": 1.9483137130737305,
      "learning_rate": 7.941962581137839e-05,
      "loss": 0.7399,
      "step": 4742
    },
    {
      "epoch": 0.6032432432432432,
      "grad_norm": 1.8914251327514648,
      "learning_rate": 7.939417080310551e-05,
      "loss": 0.3812,
      "step": 4743
    },
    {
      "epoch": 0.6033704292527822,
      "grad_norm": 2.2423996925354004,
      "learning_rate": 7.936871579483264e-05,
      "loss": 0.8873,
      "step": 4744
    },
    {
      "epoch": 0.6034976152623212,
      "grad_norm": 2.5069570541381836,
      "learning_rate": 7.934326078655976e-05,
      "loss": 0.4797,
      "step": 4745
    },
    {
      "epoch": 0.6036248012718601,
      "grad_norm": 2.6023752689361572,
      "learning_rate": 7.931780577828688e-05,
      "loss": 0.76,
      "step": 4746
    },
    {
      "epoch": 0.603751987281399,
      "grad_norm": 3.0647099018096924,
      "learning_rate": 7.9292350770014e-05,
      "loss": 0.7307,
      "step": 4747
    },
    {
      "epoch": 0.603879173290938,
      "grad_norm": 2.4572620391845703,
      "learning_rate": 7.926689576174112e-05,
      "loss": 0.6258,
      "step": 4748
    },
    {
      "epoch": 0.604006359300477,
      "grad_norm": 3.0625338554382324,
      "learning_rate": 7.924144075346824e-05,
      "loss": 0.8032,
      "step": 4749
    },
    {
      "epoch": 0.604133545310016,
      "grad_norm": 2.0028722286224365,
      "learning_rate": 7.921598574519537e-05,
      "loss": 0.4196,
      "step": 4750
    },
    {
      "epoch": 0.6042607313195548,
      "grad_norm": 2.2279791831970215,
      "learning_rate": 7.91905307369225e-05,
      "loss": 0.5948,
      "step": 4751
    },
    {
      "epoch": 0.6043879173290938,
      "grad_norm": 1.7201160192489624,
      "learning_rate": 7.916507572864962e-05,
      "loss": 0.4316,
      "step": 4752
    },
    {
      "epoch": 0.6045151033386328,
      "grad_norm": 1.8301582336425781,
      "learning_rate": 7.913962072037674e-05,
      "loss": 0.7885,
      "step": 4753
    },
    {
      "epoch": 0.6046422893481717,
      "grad_norm": 2.0791053771972656,
      "learning_rate": 7.911416571210386e-05,
      "loss": 0.6675,
      "step": 4754
    },
    {
      "epoch": 0.6047694753577106,
      "grad_norm": 1.8903660774230957,
      "learning_rate": 7.908871070383098e-05,
      "loss": 0.3922,
      "step": 4755
    },
    {
      "epoch": 0.6048966613672496,
      "grad_norm": 2.0224392414093018,
      "learning_rate": 7.906325569555811e-05,
      "loss": 0.6524,
      "step": 4756
    },
    {
      "epoch": 0.6050238473767886,
      "grad_norm": 1.7656233310699463,
      "learning_rate": 7.903780068728523e-05,
      "loss": 0.4665,
      "step": 4757
    },
    {
      "epoch": 0.6051510333863275,
      "grad_norm": 1.807643175125122,
      "learning_rate": 7.901234567901235e-05,
      "loss": 0.6771,
      "step": 4758
    },
    {
      "epoch": 0.6052782193958665,
      "grad_norm": 2.7584986686706543,
      "learning_rate": 7.898689067073947e-05,
      "loss": 0.7742,
      "step": 4759
    },
    {
      "epoch": 0.6054054054054054,
      "grad_norm": 2.399489402770996,
      "learning_rate": 7.89614356624666e-05,
      "loss": 0.5832,
      "step": 4760
    },
    {
      "epoch": 0.6055325914149443,
      "grad_norm": 1.725934624671936,
      "learning_rate": 7.893598065419372e-05,
      "loss": 0.3077,
      "step": 4761
    },
    {
      "epoch": 0.6056597774244833,
      "grad_norm": 1.61764657497406,
      "learning_rate": 7.891052564592083e-05,
      "loss": 0.6308,
      "step": 4762
    },
    {
      "epoch": 0.6057869634340223,
      "grad_norm": 1.939286708831787,
      "learning_rate": 7.888507063764797e-05,
      "loss": 0.6385,
      "step": 4763
    },
    {
      "epoch": 0.6059141494435613,
      "grad_norm": 2.0239503383636475,
      "learning_rate": 7.885961562937509e-05,
      "loss": 0.6525,
      "step": 4764
    },
    {
      "epoch": 0.6060413354531001,
      "grad_norm": 2.6703526973724365,
      "learning_rate": 7.88341606211022e-05,
      "loss": 0.5469,
      "step": 4765
    },
    {
      "epoch": 0.6061685214626391,
      "grad_norm": 3.652905225753784,
      "learning_rate": 7.880870561282932e-05,
      "loss": 0.5642,
      "step": 4766
    },
    {
      "epoch": 0.6062957074721781,
      "grad_norm": 2.8419151306152344,
      "learning_rate": 7.878325060455644e-05,
      "loss": 0.5025,
      "step": 4767
    },
    {
      "epoch": 0.606422893481717,
      "grad_norm": 2.3958609104156494,
      "learning_rate": 7.875779559628357e-05,
      "loss": 0.5386,
      "step": 4768
    },
    {
      "epoch": 0.606550079491256,
      "grad_norm": 3.1731364727020264,
      "learning_rate": 7.873234058801071e-05,
      "loss": 0.7981,
      "step": 4769
    },
    {
      "epoch": 0.6066772655007949,
      "grad_norm": 1.727805733680725,
      "learning_rate": 7.870688557973783e-05,
      "loss": 0.5544,
      "step": 4770
    },
    {
      "epoch": 0.6068044515103339,
      "grad_norm": 2.6149349212646484,
      "learning_rate": 7.868143057146494e-05,
      "loss": 0.879,
      "step": 4771
    },
    {
      "epoch": 0.6069316375198728,
      "grad_norm": 1.7588939666748047,
      "learning_rate": 7.865597556319206e-05,
      "loss": 0.6746,
      "step": 4772
    },
    {
      "epoch": 0.6070588235294118,
      "grad_norm": 1.8730127811431885,
      "learning_rate": 7.863052055491918e-05,
      "loss": 0.5253,
      "step": 4773
    },
    {
      "epoch": 0.6071860095389507,
      "grad_norm": 2.12961483001709,
      "learning_rate": 7.86050655466463e-05,
      "loss": 0.5819,
      "step": 4774
    },
    {
      "epoch": 0.6073131955484896,
      "grad_norm": 3.8494718074798584,
      "learning_rate": 7.857961053837343e-05,
      "loss": 0.9418,
      "step": 4775
    },
    {
      "epoch": 0.6074403815580286,
      "grad_norm": 1.6913533210754395,
      "learning_rate": 7.855415553010055e-05,
      "loss": 0.7217,
      "step": 4776
    },
    {
      "epoch": 0.6075675675675676,
      "grad_norm": 2.3967630863189697,
      "learning_rate": 7.852870052182767e-05,
      "loss": 0.6802,
      "step": 4777
    },
    {
      "epoch": 0.6076947535771066,
      "grad_norm": 1.9227205514907837,
      "learning_rate": 7.850324551355479e-05,
      "loss": 0.4127,
      "step": 4778
    },
    {
      "epoch": 0.6078219395866454,
      "grad_norm": 2.590113401412964,
      "learning_rate": 7.847779050528192e-05,
      "loss": 0.6796,
      "step": 4779
    },
    {
      "epoch": 0.6079491255961844,
      "grad_norm": 2.3240597248077393,
      "learning_rate": 7.845233549700904e-05,
      "loss": 0.5325,
      "step": 4780
    },
    {
      "epoch": 0.6080763116057234,
      "grad_norm": 2.0633130073547363,
      "learning_rate": 7.842688048873616e-05,
      "loss": 0.5129,
      "step": 4781
    },
    {
      "epoch": 0.6082034976152623,
      "grad_norm": 1.9293819665908813,
      "learning_rate": 7.840142548046329e-05,
      "loss": 0.5703,
      "step": 4782
    },
    {
      "epoch": 0.6083306836248012,
      "grad_norm": 2.3045575618743896,
      "learning_rate": 7.837597047219041e-05,
      "loss": 0.5921,
      "step": 4783
    },
    {
      "epoch": 0.6084578696343402,
      "grad_norm": 1.960731029510498,
      "learning_rate": 7.835051546391753e-05,
      "loss": 0.6009,
      "step": 4784
    },
    {
      "epoch": 0.6085850556438792,
      "grad_norm": 1.8547329902648926,
      "learning_rate": 7.832506045564465e-05,
      "loss": 0.685,
      "step": 4785
    },
    {
      "epoch": 0.6087122416534181,
      "grad_norm": 1.9823379516601562,
      "learning_rate": 7.829960544737177e-05,
      "loss": 0.6636,
      "step": 4786
    },
    {
      "epoch": 0.6088394276629571,
      "grad_norm": 2.555558204650879,
      "learning_rate": 7.82741504390989e-05,
      "loss": 1.0323,
      "step": 4787
    },
    {
      "epoch": 0.608966613672496,
      "grad_norm": 2.7288131713867188,
      "learning_rate": 7.824869543082602e-05,
      "loss": 0.5558,
      "step": 4788
    },
    {
      "epoch": 0.609093799682035,
      "grad_norm": 2.865774154663086,
      "learning_rate": 7.822324042255315e-05,
      "loss": 0.5142,
      "step": 4789
    },
    {
      "epoch": 0.6092209856915739,
      "grad_norm": 1.7093580961227417,
      "learning_rate": 7.819778541428027e-05,
      "loss": 0.5324,
      "step": 4790
    },
    {
      "epoch": 0.6093481717011129,
      "grad_norm": 1.9879392385482788,
      "learning_rate": 7.817233040600739e-05,
      "loss": 0.6261,
      "step": 4791
    },
    {
      "epoch": 0.6094753577106519,
      "grad_norm": 2.3168399333953857,
      "learning_rate": 7.81468753977345e-05,
      "loss": 0.4904,
      "step": 4792
    },
    {
      "epoch": 0.6096025437201907,
      "grad_norm": 2.0089519023895264,
      "learning_rate": 7.812142038946162e-05,
      "loss": 0.5564,
      "step": 4793
    },
    {
      "epoch": 0.6097297297297297,
      "grad_norm": 2.278223991394043,
      "learning_rate": 7.809596538118876e-05,
      "loss": 0.556,
      "step": 4794
    },
    {
      "epoch": 0.6098569157392687,
      "grad_norm": 2.293825626373291,
      "learning_rate": 7.807051037291588e-05,
      "loss": 0.6881,
      "step": 4795
    },
    {
      "epoch": 0.6099841017488077,
      "grad_norm": 2.2130212783813477,
      "learning_rate": 7.8045055364643e-05,
      "loss": 0.5882,
      "step": 4796
    },
    {
      "epoch": 0.6101112877583466,
      "grad_norm": 1.7213999032974243,
      "learning_rate": 7.801960035637011e-05,
      "loss": 0.571,
      "step": 4797
    },
    {
      "epoch": 0.6102384737678855,
      "grad_norm": 2.232297897338867,
      "learning_rate": 7.799414534809725e-05,
      "loss": 0.5631,
      "step": 4798
    },
    {
      "epoch": 0.6103656597774245,
      "grad_norm": 3.1095285415649414,
      "learning_rate": 7.796869033982436e-05,
      "loss": 0.5174,
      "step": 4799
    },
    {
      "epoch": 0.6104928457869634,
      "grad_norm": 2.105454683303833,
      "learning_rate": 7.79432353315515e-05,
      "loss": 0.4478,
      "step": 4800
    },
    {
      "epoch": 0.6106200317965024,
      "grad_norm": 3.0676686763763428,
      "learning_rate": 7.791778032327861e-05,
      "loss": 0.5094,
      "step": 4801
    },
    {
      "epoch": 0.6107472178060414,
      "grad_norm": 3.0200281143188477,
      "learning_rate": 7.789232531500573e-05,
      "loss": 0.5082,
      "step": 4802
    },
    {
      "epoch": 0.6108744038155803,
      "grad_norm": 1.8846535682678223,
      "learning_rate": 7.786687030673285e-05,
      "loss": 0.4431,
      "step": 4803
    },
    {
      "epoch": 0.6110015898251192,
      "grad_norm": 1.968518614768982,
      "learning_rate": 7.784141529845997e-05,
      "loss": 0.5015,
      "step": 4804
    },
    {
      "epoch": 0.6111287758346582,
      "grad_norm": 2.1811482906341553,
      "learning_rate": 7.781596029018709e-05,
      "loss": 0.9048,
      "step": 4805
    },
    {
      "epoch": 0.6112559618441972,
      "grad_norm": 2.1940746307373047,
      "learning_rate": 7.779050528191422e-05,
      "loss": 0.7857,
      "step": 4806
    },
    {
      "epoch": 0.611383147853736,
      "grad_norm": 2.7551116943359375,
      "learning_rate": 7.776505027364134e-05,
      "loss": 0.5007,
      "step": 4807
    },
    {
      "epoch": 0.611510333863275,
      "grad_norm": 2.256077289581299,
      "learning_rate": 7.773959526536847e-05,
      "loss": 0.6243,
      "step": 4808
    },
    {
      "epoch": 0.611637519872814,
      "grad_norm": 2.7414345741271973,
      "learning_rate": 7.771414025709559e-05,
      "loss": 0.665,
      "step": 4809
    },
    {
      "epoch": 0.611764705882353,
      "grad_norm": 2.1532881259918213,
      "learning_rate": 7.768868524882271e-05,
      "loss": 0.5723,
      "step": 4810
    },
    {
      "epoch": 0.6118918918918919,
      "grad_norm": 1.630450963973999,
      "learning_rate": 7.766323024054983e-05,
      "loss": 0.7378,
      "step": 4811
    },
    {
      "epoch": 0.6120190779014308,
      "grad_norm": 2.0232012271881104,
      "learning_rate": 7.763777523227695e-05,
      "loss": 0.5702,
      "step": 4812
    },
    {
      "epoch": 0.6121462639109698,
      "grad_norm": 2.4047465324401855,
      "learning_rate": 7.761232022400408e-05,
      "loss": 0.5878,
      "step": 4813
    },
    {
      "epoch": 0.6122734499205087,
      "grad_norm": 1.909482717514038,
      "learning_rate": 7.75868652157312e-05,
      "loss": 0.5708,
      "step": 4814
    },
    {
      "epoch": 0.6124006359300477,
      "grad_norm": 2.3064522743225098,
      "learning_rate": 7.756141020745832e-05,
      "loss": 0.5469,
      "step": 4815
    },
    {
      "epoch": 0.6125278219395867,
      "grad_norm": 2.371065378189087,
      "learning_rate": 7.753595519918544e-05,
      "loss": 0.6518,
      "step": 4816
    },
    {
      "epoch": 0.6126550079491256,
      "grad_norm": 2.732692241668701,
      "learning_rate": 7.751050019091256e-05,
      "loss": 0.4313,
      "step": 4817
    },
    {
      "epoch": 0.6127821939586645,
      "grad_norm": 1.765238642692566,
      "learning_rate": 7.748504518263969e-05,
      "loss": 0.5687,
      "step": 4818
    },
    {
      "epoch": 0.6129093799682035,
      "grad_norm": 2.2150890827178955,
      "learning_rate": 7.745959017436682e-05,
      "loss": 0.5081,
      "step": 4819
    },
    {
      "epoch": 0.6130365659777425,
      "grad_norm": 2.4305803775787354,
      "learning_rate": 7.743413516609394e-05,
      "loss": 0.6388,
      "step": 4820
    },
    {
      "epoch": 0.6131637519872813,
      "grad_norm": 1.798388123512268,
      "learning_rate": 7.740868015782106e-05,
      "loss": 0.5386,
      "step": 4821
    },
    {
      "epoch": 0.6132909379968203,
      "grad_norm": 1.9901094436645508,
      "learning_rate": 7.738322514954818e-05,
      "loss": 0.6246,
      "step": 4822
    },
    {
      "epoch": 0.6134181240063593,
      "grad_norm": 2.016864538192749,
      "learning_rate": 7.73577701412753e-05,
      "loss": 0.8037,
      "step": 4823
    },
    {
      "epoch": 0.6135453100158983,
      "grad_norm": 1.5619980096817017,
      "learning_rate": 7.733231513300241e-05,
      "loss": 0.5051,
      "step": 4824
    },
    {
      "epoch": 0.6136724960254372,
      "grad_norm": 1.7549575567245483,
      "learning_rate": 7.730686012472955e-05,
      "loss": 0.5925,
      "step": 4825
    },
    {
      "epoch": 0.6137996820349761,
      "grad_norm": 2.5007212162017822,
      "learning_rate": 7.728140511645666e-05,
      "loss": 0.8541,
      "step": 4826
    },
    {
      "epoch": 0.6139268680445151,
      "grad_norm": 2.145582914352417,
      "learning_rate": 7.725595010818378e-05,
      "loss": 0.5666,
      "step": 4827
    },
    {
      "epoch": 0.614054054054054,
      "grad_norm": 1.842081904411316,
      "learning_rate": 7.723049509991092e-05,
      "loss": 0.5501,
      "step": 4828
    },
    {
      "epoch": 0.614181240063593,
      "grad_norm": 2.305260181427002,
      "learning_rate": 7.720504009163803e-05,
      "loss": 0.7061,
      "step": 4829
    },
    {
      "epoch": 0.614308426073132,
      "grad_norm": 1.9879345893859863,
      "learning_rate": 7.717958508336515e-05,
      "loss": 0.5104,
      "step": 4830
    },
    {
      "epoch": 0.614435612082671,
      "grad_norm": 1.5956799983978271,
      "learning_rate": 7.715413007509229e-05,
      "loss": 0.5054,
      "step": 4831
    },
    {
      "epoch": 0.6145627980922098,
      "grad_norm": 2.205958843231201,
      "learning_rate": 7.71286750668194e-05,
      "loss": 0.6155,
      "step": 4832
    },
    {
      "epoch": 0.6146899841017488,
      "grad_norm": 2.229173421859741,
      "learning_rate": 7.710322005854652e-05,
      "loss": 0.4635,
      "step": 4833
    },
    {
      "epoch": 0.6148171701112878,
      "grad_norm": 2.5783774852752686,
      "learning_rate": 7.707776505027364e-05,
      "loss": 0.6994,
      "step": 4834
    },
    {
      "epoch": 0.6149443561208268,
      "grad_norm": 2.2025115489959717,
      "learning_rate": 7.705231004200076e-05,
      "loss": 0.5135,
      "step": 4835
    },
    {
      "epoch": 0.6150715421303656,
      "grad_norm": 2.160209894180298,
      "learning_rate": 7.702685503372788e-05,
      "loss": 0.7325,
      "step": 4836
    },
    {
      "epoch": 0.6151987281399046,
      "grad_norm": 2.7318129539489746,
      "learning_rate": 7.700140002545501e-05,
      "loss": 0.8267,
      "step": 4837
    },
    {
      "epoch": 0.6153259141494436,
      "grad_norm": 2.005061149597168,
      "learning_rate": 7.697594501718214e-05,
      "loss": 0.6348,
      "step": 4838
    },
    {
      "epoch": 0.6154531001589825,
      "grad_norm": 2.172376871109009,
      "learning_rate": 7.695049000890926e-05,
      "loss": 0.7135,
      "step": 4839
    },
    {
      "epoch": 0.6155802861685215,
      "grad_norm": 2.4311416149139404,
      "learning_rate": 7.692503500063638e-05,
      "loss": 0.8212,
      "step": 4840
    },
    {
      "epoch": 0.6157074721780604,
      "grad_norm": 2.437102794647217,
      "learning_rate": 7.68995799923635e-05,
      "loss": 0.3474,
      "step": 4841
    },
    {
      "epoch": 0.6158346581875994,
      "grad_norm": 2.118082046508789,
      "learning_rate": 7.687412498409062e-05,
      "loss": 0.5948,
      "step": 4842
    },
    {
      "epoch": 0.6159618441971383,
      "grad_norm": 2.2147648334503174,
      "learning_rate": 7.684866997581774e-05,
      "loss": 0.4597,
      "step": 4843
    },
    {
      "epoch": 0.6160890302066773,
      "grad_norm": 1.5325558185577393,
      "learning_rate": 7.682321496754487e-05,
      "loss": 0.4341,
      "step": 4844
    },
    {
      "epoch": 0.6162162162162163,
      "grad_norm": 2.29573655128479,
      "learning_rate": 7.679775995927199e-05,
      "loss": 0.6722,
      "step": 4845
    },
    {
      "epoch": 0.6163434022257551,
      "grad_norm": 2.647480010986328,
      "learning_rate": 7.677230495099911e-05,
      "loss": 0.7113,
      "step": 4846
    },
    {
      "epoch": 0.6164705882352941,
      "grad_norm": 2.585928440093994,
      "learning_rate": 7.674684994272624e-05,
      "loss": 0.6694,
      "step": 4847
    },
    {
      "epoch": 0.6165977742448331,
      "grad_norm": 1.7972885370254517,
      "learning_rate": 7.672139493445336e-05,
      "loss": 0.8562,
      "step": 4848
    },
    {
      "epoch": 0.6167249602543721,
      "grad_norm": 2.8022663593292236,
      "learning_rate": 7.669593992618048e-05,
      "loss": 0.6806,
      "step": 4849
    },
    {
      "epoch": 0.6168521462639109,
      "grad_norm": 2.599668264389038,
      "learning_rate": 7.667048491790761e-05,
      "loss": 0.5276,
      "step": 4850
    },
    {
      "epoch": 0.6169793322734499,
      "grad_norm": 2.4683480262756348,
      "learning_rate": 7.664502990963473e-05,
      "loss": 0.6259,
      "step": 4851
    },
    {
      "epoch": 0.6171065182829889,
      "grad_norm": 2.252896785736084,
      "learning_rate": 7.661957490136185e-05,
      "loss": 0.5148,
      "step": 4852
    },
    {
      "epoch": 0.6172337042925278,
      "grad_norm": 2.0970704555511475,
      "learning_rate": 7.659411989308897e-05,
      "loss": 0.386,
      "step": 4853
    },
    {
      "epoch": 0.6173608903020668,
      "grad_norm": 1.844767689704895,
      "learning_rate": 7.656866488481608e-05,
      "loss": 0.4154,
      "step": 4854
    },
    {
      "epoch": 0.6174880763116057,
      "grad_norm": 1.4941251277923584,
      "learning_rate": 7.65432098765432e-05,
      "loss": 0.5115,
      "step": 4855
    },
    {
      "epoch": 0.6176152623211447,
      "grad_norm": 2.243483543395996,
      "learning_rate": 7.651775486827034e-05,
      "loss": 0.5527,
      "step": 4856
    },
    {
      "epoch": 0.6177424483306836,
      "grad_norm": 1.960530161857605,
      "learning_rate": 7.649229985999747e-05,
      "loss": 0.4401,
      "step": 4857
    },
    {
      "epoch": 0.6178696343402226,
      "grad_norm": 1.367146372795105,
      "learning_rate": 7.646684485172459e-05,
      "loss": 0.3892,
      "step": 4858
    },
    {
      "epoch": 0.6179968203497616,
      "grad_norm": 1.6597181558609009,
      "learning_rate": 7.64413898434517e-05,
      "loss": 0.3681,
      "step": 4859
    },
    {
      "epoch": 0.6181240063593004,
      "grad_norm": 1.6697025299072266,
      "learning_rate": 7.641593483517882e-05,
      "loss": 0.3986,
      "step": 4860
    },
    {
      "epoch": 0.6182511923688394,
      "grad_norm": 1.9868313074111938,
      "learning_rate": 7.639047982690594e-05,
      "loss": 0.4538,
      "step": 4861
    },
    {
      "epoch": 0.6183783783783784,
      "grad_norm": 1.662086009979248,
      "learning_rate": 7.636502481863307e-05,
      "loss": 0.4669,
      "step": 4862
    },
    {
      "epoch": 0.6185055643879174,
      "grad_norm": 2.0122995376586914,
      "learning_rate": 7.63395698103602e-05,
      "loss": 0.6404,
      "step": 4863
    },
    {
      "epoch": 0.6186327503974562,
      "grad_norm": 2.6956357955932617,
      "learning_rate": 7.631411480208731e-05,
      "loss": 0.4398,
      "step": 4864
    },
    {
      "epoch": 0.6187599364069952,
      "grad_norm": 2.4485669136047363,
      "learning_rate": 7.628865979381443e-05,
      "loss": 0.4042,
      "step": 4865
    },
    {
      "epoch": 0.6188871224165342,
      "grad_norm": 2.7307894229888916,
      "learning_rate": 7.626320478554156e-05,
      "loss": 0.5777,
      "step": 4866
    },
    {
      "epoch": 0.6190143084260731,
      "grad_norm": 2.086042642593384,
      "learning_rate": 7.623774977726868e-05,
      "loss": 0.4695,
      "step": 4867
    },
    {
      "epoch": 0.6191414944356121,
      "grad_norm": 2.423579454421997,
      "learning_rate": 7.62122947689958e-05,
      "loss": 0.6105,
      "step": 4868
    },
    {
      "epoch": 0.619268680445151,
      "grad_norm": 1.8122344017028809,
      "learning_rate": 7.618683976072293e-05,
      "loss": 0.5779,
      "step": 4869
    },
    {
      "epoch": 0.61939586645469,
      "grad_norm": 2.3073570728302,
      "learning_rate": 7.616138475245005e-05,
      "loss": 0.46,
      "step": 4870
    },
    {
      "epoch": 0.6195230524642289,
      "grad_norm": 1.8586034774780273,
      "learning_rate": 7.613592974417717e-05,
      "loss": 0.4959,
      "step": 4871
    },
    {
      "epoch": 0.6196502384737679,
      "grad_norm": 1.9154860973358154,
      "learning_rate": 7.611047473590429e-05,
      "loss": 0.5223,
      "step": 4872
    },
    {
      "epoch": 0.6197774244833069,
      "grad_norm": 1.6564507484436035,
      "learning_rate": 7.608501972763141e-05,
      "loss": 0.5861,
      "step": 4873
    },
    {
      "epoch": 0.6199046104928458,
      "grad_norm": 1.9709899425506592,
      "learning_rate": 7.605956471935854e-05,
      "loss": 0.5622,
      "step": 4874
    },
    {
      "epoch": 0.6200317965023847,
      "grad_norm": 1.5861150026321411,
      "learning_rate": 7.603410971108566e-05,
      "loss": 0.4267,
      "step": 4875
    },
    {
      "epoch": 0.6201589825119237,
      "grad_norm": 1.7591426372528076,
      "learning_rate": 7.600865470281279e-05,
      "loss": 0.6056,
      "step": 4876
    },
    {
      "epoch": 0.6202861685214627,
      "grad_norm": 1.9921921491622925,
      "learning_rate": 7.598319969453991e-05,
      "loss": 0.6598,
      "step": 4877
    },
    {
      "epoch": 0.6204133545310015,
      "grad_norm": 2.167546272277832,
      "learning_rate": 7.595774468626703e-05,
      "loss": 0.5353,
      "step": 4878
    },
    {
      "epoch": 0.6205405405405405,
      "grad_norm": 1.5688400268554688,
      "learning_rate": 7.593228967799415e-05,
      "loss": 0.5209,
      "step": 4879
    },
    {
      "epoch": 0.6206677265500795,
      "grad_norm": 1.979849100112915,
      "learning_rate": 7.590683466972127e-05,
      "loss": 0.4458,
      "step": 4880
    },
    {
      "epoch": 0.6207949125596185,
      "grad_norm": 2.8916232585906982,
      "learning_rate": 7.58813796614484e-05,
      "loss": 0.7958,
      "step": 4881
    },
    {
      "epoch": 0.6209220985691574,
      "grad_norm": 3.338409185409546,
      "learning_rate": 7.585592465317552e-05,
      "loss": 0.7867,
      "step": 4882
    },
    {
      "epoch": 0.6210492845786963,
      "grad_norm": 2.0939483642578125,
      "learning_rate": 7.583046964490264e-05,
      "loss": 0.7898,
      "step": 4883
    },
    {
      "epoch": 0.6211764705882353,
      "grad_norm": 1.5138059854507446,
      "learning_rate": 7.580501463662975e-05,
      "loss": 0.3996,
      "step": 4884
    },
    {
      "epoch": 0.6213036565977742,
      "grad_norm": 2.6643450260162354,
      "learning_rate": 7.577955962835687e-05,
      "loss": 0.6466,
      "step": 4885
    },
    {
      "epoch": 0.6214308426073132,
      "grad_norm": 2.547884702682495,
      "learning_rate": 7.5754104620084e-05,
      "loss": 0.7478,
      "step": 4886
    },
    {
      "epoch": 0.6215580286168522,
      "grad_norm": 1.9826525449752808,
      "learning_rate": 7.572864961181112e-05,
      "loss": 0.522,
      "step": 4887
    },
    {
      "epoch": 0.6216852146263911,
      "grad_norm": 2.558011054992676,
      "learning_rate": 7.570319460353826e-05,
      "loss": 0.5555,
      "step": 4888
    },
    {
      "epoch": 0.62181240063593,
      "grad_norm": 1.6027207374572754,
      "learning_rate": 7.567773959526538e-05,
      "loss": 0.3466,
      "step": 4889
    },
    {
      "epoch": 0.621939586645469,
      "grad_norm": 2.3693594932556152,
      "learning_rate": 7.56522845869925e-05,
      "loss": 0.6407,
      "step": 4890
    },
    {
      "epoch": 0.622066772655008,
      "grad_norm": 1.6742792129516602,
      "learning_rate": 7.562682957871961e-05,
      "loss": 0.4397,
      "step": 4891
    },
    {
      "epoch": 0.6221939586645469,
      "grad_norm": 2.3501620292663574,
      "learning_rate": 7.560137457044673e-05,
      "loss": 0.5367,
      "step": 4892
    },
    {
      "epoch": 0.6223211446740858,
      "grad_norm": 2.4033689498901367,
      "learning_rate": 7.557591956217386e-05,
      "loss": 0.6369,
      "step": 4893
    },
    {
      "epoch": 0.6224483306836248,
      "grad_norm": 3.360076427459717,
      "learning_rate": 7.555046455390098e-05,
      "loss": 1.0994,
      "step": 4894
    },
    {
      "epoch": 0.6225755166931638,
      "grad_norm": 2.381707191467285,
      "learning_rate": 7.552500954562812e-05,
      "loss": 0.5676,
      "step": 4895
    },
    {
      "epoch": 0.6227027027027027,
      "grad_norm": 1.390089988708496,
      "learning_rate": 7.549955453735523e-05,
      "loss": 0.4165,
      "step": 4896
    },
    {
      "epoch": 0.6228298887122417,
      "grad_norm": 2.6113474369049072,
      "learning_rate": 7.547409952908235e-05,
      "loss": 0.5419,
      "step": 4897
    },
    {
      "epoch": 0.6229570747217806,
      "grad_norm": 1.6307995319366455,
      "learning_rate": 7.544864452080947e-05,
      "loss": 0.3715,
      "step": 4898
    },
    {
      "epoch": 0.6230842607313195,
      "grad_norm": 2.09519100189209,
      "learning_rate": 7.542318951253659e-05,
      "loss": 0.4726,
      "step": 4899
    },
    {
      "epoch": 0.6232114467408585,
      "grad_norm": 2.1269187927246094,
      "learning_rate": 7.539773450426372e-05,
      "loss": 0.4649,
      "step": 4900
    },
    {
      "epoch": 0.6233386327503975,
      "grad_norm": 2.6852457523345947,
      "learning_rate": 7.537227949599084e-05,
      "loss": 0.5831,
      "step": 4901
    },
    {
      "epoch": 0.6234658187599365,
      "grad_norm": 1.9720147848129272,
      "learning_rate": 7.534682448771796e-05,
      "loss": 0.5906,
      "step": 4902
    },
    {
      "epoch": 0.6235930047694753,
      "grad_norm": 2.2622456550598145,
      "learning_rate": 7.532136947944508e-05,
      "loss": 0.4969,
      "step": 4903
    },
    {
      "epoch": 0.6237201907790143,
      "grad_norm": 1.7711073160171509,
      "learning_rate": 7.52959144711722e-05,
      "loss": 0.352,
      "step": 4904
    },
    {
      "epoch": 0.6238473767885533,
      "grad_norm": 2.506078004837036,
      "learning_rate": 7.527045946289933e-05,
      "loss": 0.5387,
      "step": 4905
    },
    {
      "epoch": 0.6239745627980922,
      "grad_norm": 1.8014309406280518,
      "learning_rate": 7.524500445462646e-05,
      "loss": 0.7335,
      "step": 4906
    },
    {
      "epoch": 0.6241017488076311,
      "grad_norm": 2.8349361419677734,
      "learning_rate": 7.521954944635358e-05,
      "loss": 0.6994,
      "step": 4907
    },
    {
      "epoch": 0.6242289348171701,
      "grad_norm": 2.389758825302124,
      "learning_rate": 7.51940944380807e-05,
      "loss": 0.681,
      "step": 4908
    },
    {
      "epoch": 0.6243561208267091,
      "grad_norm": 2.446944236755371,
      "learning_rate": 7.516863942980782e-05,
      "loss": 0.4562,
      "step": 4909
    },
    {
      "epoch": 0.624483306836248,
      "grad_norm": 2.553783655166626,
      "learning_rate": 7.514318442153494e-05,
      "loss": 0.6583,
      "step": 4910
    },
    {
      "epoch": 0.624610492845787,
      "grad_norm": 2.1408772468566895,
      "learning_rate": 7.511772941326206e-05,
      "loss": 0.7576,
      "step": 4911
    },
    {
      "epoch": 0.6247376788553259,
      "grad_norm": 1.6486682891845703,
      "learning_rate": 7.509227440498919e-05,
      "loss": 0.3607,
      "step": 4912
    },
    {
      "epoch": 0.6248648648648648,
      "grad_norm": 2.6283252239227295,
      "learning_rate": 7.50668193967163e-05,
      "loss": 0.6487,
      "step": 4913
    },
    {
      "epoch": 0.6249920508744038,
      "grad_norm": 2.729536533355713,
      "learning_rate": 7.504136438844343e-05,
      "loss": 0.7175,
      "step": 4914
    },
    {
      "epoch": 0.6251192368839428,
      "grad_norm": 1.8313673734664917,
      "learning_rate": 7.501590938017056e-05,
      "loss": 0.7797,
      "step": 4915
    },
    {
      "epoch": 0.6252464228934818,
      "grad_norm": 1.9403141736984253,
      "learning_rate": 7.499045437189768e-05,
      "loss": 0.5621,
      "step": 4916
    },
    {
      "epoch": 0.6253736089030206,
      "grad_norm": 2.1524059772491455,
      "learning_rate": 7.49649993636248e-05,
      "loss": 0.4268,
      "step": 4917
    },
    {
      "epoch": 0.6255007949125596,
      "grad_norm": 2.5303359031677246,
      "learning_rate": 7.493954435535191e-05,
      "loss": 0.6572,
      "step": 4918
    },
    {
      "epoch": 0.6256279809220986,
      "grad_norm": 1.8802357912063599,
      "learning_rate": 7.491408934707905e-05,
      "loss": 0.4358,
      "step": 4919
    },
    {
      "epoch": 0.6257551669316376,
      "grad_norm": 1.7485430240631104,
      "learning_rate": 7.488863433880617e-05,
      "loss": 0.7473,
      "step": 4920
    },
    {
      "epoch": 0.6258823529411764,
      "grad_norm": 2.35833740234375,
      "learning_rate": 7.486317933053328e-05,
      "loss": 0.7698,
      "step": 4921
    },
    {
      "epoch": 0.6260095389507154,
      "grad_norm": 1.8229525089263916,
      "learning_rate": 7.48377243222604e-05,
      "loss": 0.7003,
      "step": 4922
    },
    {
      "epoch": 0.6261367249602544,
      "grad_norm": 1.3152756690979004,
      "learning_rate": 7.481226931398752e-05,
      "loss": 0.4871,
      "step": 4923
    },
    {
      "epoch": 0.6262639109697933,
      "grad_norm": 2.599860429763794,
      "learning_rate": 7.478681430571465e-05,
      "loss": 0.5377,
      "step": 4924
    },
    {
      "epoch": 0.6263910969793323,
      "grad_norm": 1.9897310733795166,
      "learning_rate": 7.476135929744179e-05,
      "loss": 0.7601,
      "step": 4925
    },
    {
      "epoch": 0.6265182829888712,
      "grad_norm": 2.112654685974121,
      "learning_rate": 7.47359042891689e-05,
      "loss": 0.5265,
      "step": 4926
    },
    {
      "epoch": 0.6266454689984102,
      "grad_norm": 2.9840340614318848,
      "learning_rate": 7.471044928089602e-05,
      "loss": 0.5056,
      "step": 4927
    },
    {
      "epoch": 0.6267726550079491,
      "grad_norm": 1.8992842435836792,
      "learning_rate": 7.468499427262314e-05,
      "loss": 0.77,
      "step": 4928
    },
    {
      "epoch": 0.6268998410174881,
      "grad_norm": 1.5178751945495605,
      "learning_rate": 7.465953926435026e-05,
      "loss": 0.3224,
      "step": 4929
    },
    {
      "epoch": 0.6270270270270271,
      "grad_norm": 1.613110065460205,
      "learning_rate": 7.463408425607738e-05,
      "loss": 0.6137,
      "step": 4930
    },
    {
      "epoch": 0.6271542130365659,
      "grad_norm": 1.7097141742706299,
      "learning_rate": 7.460862924780451e-05,
      "loss": 0.54,
      "step": 4931
    },
    {
      "epoch": 0.6272813990461049,
      "grad_norm": 2.211712598800659,
      "learning_rate": 7.458317423953163e-05,
      "loss": 0.6435,
      "step": 4932
    },
    {
      "epoch": 0.6274085850556439,
      "grad_norm": 2.636378765106201,
      "learning_rate": 7.455771923125875e-05,
      "loss": 0.7072,
      "step": 4933
    },
    {
      "epoch": 0.6275357710651829,
      "grad_norm": 2.072995662689209,
      "learning_rate": 7.453226422298588e-05,
      "loss": 0.7305,
      "step": 4934
    },
    {
      "epoch": 0.6276629570747218,
      "grad_norm": 1.9285944700241089,
      "learning_rate": 7.4506809214713e-05,
      "loss": 0.5584,
      "step": 4935
    },
    {
      "epoch": 0.6277901430842607,
      "grad_norm": 2.839421510696411,
      "learning_rate": 7.448135420644012e-05,
      "loss": 0.6893,
      "step": 4936
    },
    {
      "epoch": 0.6279173290937997,
      "grad_norm": 2.788465976715088,
      "learning_rate": 7.445589919816725e-05,
      "loss": 0.6118,
      "step": 4937
    },
    {
      "epoch": 0.6280445151033386,
      "grad_norm": 2.2027101516723633,
      "learning_rate": 7.443044418989437e-05,
      "loss": 0.8766,
      "step": 4938
    },
    {
      "epoch": 0.6281717011128776,
      "grad_norm": 1.9529790878295898,
      "learning_rate": 7.440498918162149e-05,
      "loss": 0.4453,
      "step": 4939
    },
    {
      "epoch": 0.6282988871224165,
      "grad_norm": 2.0005669593811035,
      "learning_rate": 7.437953417334861e-05,
      "loss": 0.572,
      "step": 4940
    },
    {
      "epoch": 0.6284260731319555,
      "grad_norm": 1.8408057689666748,
      "learning_rate": 7.435407916507573e-05,
      "loss": 0.6331,
      "step": 4941
    },
    {
      "epoch": 0.6285532591414944,
      "grad_norm": 1.933380126953125,
      "learning_rate": 7.432862415680285e-05,
      "loss": 0.6957,
      "step": 4942
    },
    {
      "epoch": 0.6286804451510334,
      "grad_norm": 1.955060362815857,
      "learning_rate": 7.430316914852998e-05,
      "loss": 0.517,
      "step": 4943
    },
    {
      "epoch": 0.6288076311605724,
      "grad_norm": 2.8801145553588867,
      "learning_rate": 7.427771414025711e-05,
      "loss": 0.7136,
      "step": 4944
    },
    {
      "epoch": 0.6289348171701112,
      "grad_norm": 2.02752423286438,
      "learning_rate": 7.425225913198423e-05,
      "loss": 0.4875,
      "step": 4945
    },
    {
      "epoch": 0.6290620031796502,
      "grad_norm": 2.2635958194732666,
      "learning_rate": 7.422680412371135e-05,
      "loss": 0.6391,
      "step": 4946
    },
    {
      "epoch": 0.6291891891891892,
      "grad_norm": 2.139169692993164,
      "learning_rate": 7.420134911543847e-05,
      "loss": 0.6389,
      "step": 4947
    },
    {
      "epoch": 0.6293163751987282,
      "grad_norm": 2.0586414337158203,
      "learning_rate": 7.417589410716558e-05,
      "loss": 0.8982,
      "step": 4948
    },
    {
      "epoch": 0.629443561208267,
      "grad_norm": 2.61946702003479,
      "learning_rate": 7.41504390988927e-05,
      "loss": 0.5309,
      "step": 4949
    },
    {
      "epoch": 0.629570747217806,
      "grad_norm": 1.9648386240005493,
      "learning_rate": 7.412498409061984e-05,
      "loss": 0.6333,
      "step": 4950
    },
    {
      "epoch": 0.629697933227345,
      "grad_norm": 2.3970611095428467,
      "learning_rate": 7.409952908234695e-05,
      "loss": 0.7818,
      "step": 4951
    },
    {
      "epoch": 0.6298251192368839,
      "grad_norm": 1.7392046451568604,
      "learning_rate": 7.407407407407407e-05,
      "loss": 0.5406,
      "step": 4952
    },
    {
      "epoch": 0.6299523052464229,
      "grad_norm": 2.167659282684326,
      "learning_rate": 7.404861906580119e-05,
      "loss": 0.6076,
      "step": 4953
    },
    {
      "epoch": 0.6300794912559619,
      "grad_norm": 1.9467860460281372,
      "learning_rate": 7.402316405752832e-05,
      "loss": 0.4636,
      "step": 4954
    },
    {
      "epoch": 0.6302066772655008,
      "grad_norm": 2.0667195320129395,
      "learning_rate": 7.399770904925544e-05,
      "loss": 0.4628,
      "step": 4955
    },
    {
      "epoch": 0.6303338632750397,
      "grad_norm": 1.9283584356307983,
      "learning_rate": 7.397225404098258e-05,
      "loss": 0.8165,
      "step": 4956
    },
    {
      "epoch": 0.6304610492845787,
      "grad_norm": 2.0109333992004395,
      "learning_rate": 7.39467990327097e-05,
      "loss": 0.4206,
      "step": 4957
    },
    {
      "epoch": 0.6305882352941177,
      "grad_norm": 3.0371997356414795,
      "learning_rate": 7.392134402443681e-05,
      "loss": 0.5343,
      "step": 4958
    },
    {
      "epoch": 0.6307154213036565,
      "grad_norm": 1.6858534812927246,
      "learning_rate": 7.389588901616393e-05,
      "loss": 0.553,
      "step": 4959
    },
    {
      "epoch": 0.6308426073131955,
      "grad_norm": 3.219679594039917,
      "learning_rate": 7.387043400789105e-05,
      "loss": 0.6098,
      "step": 4960
    },
    {
      "epoch": 0.6309697933227345,
      "grad_norm": 2.1172945499420166,
      "learning_rate": 7.384497899961817e-05,
      "loss": 0.6213,
      "step": 4961
    },
    {
      "epoch": 0.6310969793322735,
      "grad_norm": 3.2283034324645996,
      "learning_rate": 7.38195239913453e-05,
      "loss": 0.8015,
      "step": 4962
    },
    {
      "epoch": 0.6312241653418124,
      "grad_norm": 1.5875755548477173,
      "learning_rate": 7.379406898307243e-05,
      "loss": 0.4503,
      "step": 4963
    },
    {
      "epoch": 0.6313513513513513,
      "grad_norm": 2.1207499504089355,
      "learning_rate": 7.376861397479955e-05,
      "loss": 0.7165,
      "step": 4964
    },
    {
      "epoch": 0.6314785373608903,
      "grad_norm": 1.7962943315505981,
      "learning_rate": 7.374315896652667e-05,
      "loss": 0.5138,
      "step": 4965
    },
    {
      "epoch": 0.6316057233704293,
      "grad_norm": 2.2401978969573975,
      "learning_rate": 7.371770395825379e-05,
      "loss": 0.5046,
      "step": 4966
    },
    {
      "epoch": 0.6317329093799682,
      "grad_norm": 1.7217363119125366,
      "learning_rate": 7.369224894998091e-05,
      "loss": 0.4945,
      "step": 4967
    },
    {
      "epoch": 0.6318600953895072,
      "grad_norm": 2.6094324588775635,
      "learning_rate": 7.366679394170804e-05,
      "loss": 0.4946,
      "step": 4968
    },
    {
      "epoch": 0.6319872813990461,
      "grad_norm": 2.096073627471924,
      "learning_rate": 7.364133893343516e-05,
      "loss": 0.6314,
      "step": 4969
    },
    {
      "epoch": 0.632114467408585,
      "grad_norm": 3.904860496520996,
      "learning_rate": 7.361588392516228e-05,
      "loss": 0.5209,
      "step": 4970
    },
    {
      "epoch": 0.632241653418124,
      "grad_norm": 3.0127830505371094,
      "learning_rate": 7.35904289168894e-05,
      "loss": 0.7754,
      "step": 4971
    },
    {
      "epoch": 0.632368839427663,
      "grad_norm": 2.169807195663452,
      "learning_rate": 7.356497390861652e-05,
      "loss": 0.3959,
      "step": 4972
    },
    {
      "epoch": 0.632496025437202,
      "grad_norm": 1.7970170974731445,
      "learning_rate": 7.353951890034365e-05,
      "loss": 0.5179,
      "step": 4973
    },
    {
      "epoch": 0.6326232114467408,
      "grad_norm": 2.4448609352111816,
      "learning_rate": 7.351406389207077e-05,
      "loss": 0.5104,
      "step": 4974
    },
    {
      "epoch": 0.6327503974562798,
      "grad_norm": 1.6633155345916748,
      "learning_rate": 7.34886088837979e-05,
      "loss": 0.5111,
      "step": 4975
    },
    {
      "epoch": 0.6328775834658188,
      "grad_norm": 1.9735615253448486,
      "learning_rate": 7.346315387552502e-05,
      "loss": 0.398,
      "step": 4976
    },
    {
      "epoch": 0.6330047694753577,
      "grad_norm": 2.2632691860198975,
      "learning_rate": 7.343769886725214e-05,
      "loss": 0.5877,
      "step": 4977
    },
    {
      "epoch": 0.6331319554848966,
      "grad_norm": 2.1848442554473877,
      "learning_rate": 7.341224385897926e-05,
      "loss": 0.6845,
      "step": 4978
    },
    {
      "epoch": 0.6332591414944356,
      "grad_norm": 3.1494882106781006,
      "learning_rate": 7.338678885070637e-05,
      "loss": 0.5847,
      "step": 4979
    },
    {
      "epoch": 0.6333863275039746,
      "grad_norm": 2.5184805393218994,
      "learning_rate": 7.33613338424335e-05,
      "loss": 0.6937,
      "step": 4980
    },
    {
      "epoch": 0.6335135135135135,
      "grad_norm": 3.088630199432373,
      "learning_rate": 7.333587883416062e-05,
      "loss": 0.6632,
      "step": 4981
    },
    {
      "epoch": 0.6336406995230525,
      "grad_norm": 4.059723377227783,
      "learning_rate": 7.331042382588774e-05,
      "loss": 0.8867,
      "step": 4982
    },
    {
      "epoch": 0.6337678855325914,
      "grad_norm": 2.0009615421295166,
      "learning_rate": 7.328496881761488e-05,
      "loss": 0.5165,
      "step": 4983
    },
    {
      "epoch": 0.6338950715421303,
      "grad_norm": 2.1817212104797363,
      "learning_rate": 7.3259513809342e-05,
      "loss": 0.5113,
      "step": 4984
    },
    {
      "epoch": 0.6340222575516693,
      "grad_norm": 2.180237293243408,
      "learning_rate": 7.323405880106911e-05,
      "loss": 0.4674,
      "step": 4985
    },
    {
      "epoch": 0.6341494435612083,
      "grad_norm": 1.557178258895874,
      "learning_rate": 7.320860379279623e-05,
      "loss": 0.3837,
      "step": 4986
    },
    {
      "epoch": 0.6342766295707473,
      "grad_norm": 3.0530636310577393,
      "learning_rate": 7.318314878452336e-05,
      "loss": 0.7689,
      "step": 4987
    },
    {
      "epoch": 0.6344038155802861,
      "grad_norm": 2.1717615127563477,
      "learning_rate": 7.315769377625048e-05,
      "loss": 0.6902,
      "step": 4988
    },
    {
      "epoch": 0.6345310015898251,
      "grad_norm": 2.513012170791626,
      "learning_rate": 7.31322387679776e-05,
      "loss": 0.5446,
      "step": 4989
    },
    {
      "epoch": 0.6346581875993641,
      "grad_norm": 2.491342544555664,
      "learning_rate": 7.310678375970472e-05,
      "loss": 0.7194,
      "step": 4990
    },
    {
      "epoch": 0.634785373608903,
      "grad_norm": 2.0493390560150146,
      "learning_rate": 7.308132875143184e-05,
      "loss": 0.5149,
      "step": 4991
    },
    {
      "epoch": 0.634912559618442,
      "grad_norm": 1.6437870264053345,
      "learning_rate": 7.305587374315896e-05,
      "loss": 0.5294,
      "step": 4992
    },
    {
      "epoch": 0.6350397456279809,
      "grad_norm": 1.7553396224975586,
      "learning_rate": 7.303041873488609e-05,
      "loss": 0.7503,
      "step": 4993
    },
    {
      "epoch": 0.6351669316375199,
      "grad_norm": 2.403181314468384,
      "learning_rate": 7.300496372661322e-05,
      "loss": 0.6274,
      "step": 4994
    },
    {
      "epoch": 0.6352941176470588,
      "grad_norm": 1.904827356338501,
      "learning_rate": 7.297950871834034e-05,
      "loss": 0.471,
      "step": 4995
    },
    {
      "epoch": 0.6354213036565978,
      "grad_norm": 1.547608733177185,
      "learning_rate": 7.295405371006746e-05,
      "loss": 0.4113,
      "step": 4996
    },
    {
      "epoch": 0.6355484896661368,
      "grad_norm": 2.870107650756836,
      "learning_rate": 7.292859870179458e-05,
      "loss": 0.7356,
      "step": 4997
    },
    {
      "epoch": 0.6356756756756756,
      "grad_norm": 2.6109535694122314,
      "learning_rate": 7.29031436935217e-05,
      "loss": 0.7801,
      "step": 4998
    },
    {
      "epoch": 0.6358028616852146,
      "grad_norm": 2.378281593322754,
      "learning_rate": 7.287768868524883e-05,
      "loss": 0.6519,
      "step": 4999
    },
    {
      "epoch": 0.6359300476947536,
      "grad_norm": 3.431251287460327,
      "learning_rate": 7.285223367697595e-05,
      "loss": 0.551,
      "step": 5000
    },
    {
      "epoch": 0.6360572337042926,
      "grad_norm": 2.6939656734466553,
      "learning_rate": 7.282677866870307e-05,
      "loss": 0.6297,
      "step": 5001
    },
    {
      "epoch": 0.6361844197138314,
      "grad_norm": 2.5610108375549316,
      "learning_rate": 7.28013236604302e-05,
      "loss": 0.5707,
      "step": 5002
    },
    {
      "epoch": 0.6363116057233704,
      "grad_norm": 3.1439552307128906,
      "learning_rate": 7.277586865215732e-05,
      "loss": 0.662,
      "step": 5003
    },
    {
      "epoch": 0.6364387917329094,
      "grad_norm": 1.6963324546813965,
      "learning_rate": 7.275041364388444e-05,
      "loss": 0.6316,
      "step": 5004
    },
    {
      "epoch": 0.6365659777424484,
      "grad_norm": 1.6197868585586548,
      "learning_rate": 7.272495863561156e-05,
      "loss": 0.4007,
      "step": 5005
    },
    {
      "epoch": 0.6366931637519873,
      "grad_norm": 2.4095523357391357,
      "learning_rate": 7.269950362733869e-05,
      "loss": 0.4328,
      "step": 5006
    },
    {
      "epoch": 0.6368203497615262,
      "grad_norm": 2.3196167945861816,
      "learning_rate": 7.267404861906581e-05,
      "loss": 0.7814,
      "step": 5007
    },
    {
      "epoch": 0.6369475357710652,
      "grad_norm": 1.9588687419891357,
      "learning_rate": 7.264859361079293e-05,
      "loss": 0.64,
      "step": 5008
    },
    {
      "epoch": 0.6370747217806041,
      "grad_norm": 1.643039345741272,
      "learning_rate": 7.262313860252004e-05,
      "loss": 0.5838,
      "step": 5009
    },
    {
      "epoch": 0.6372019077901431,
      "grad_norm": 1.9837560653686523,
      "learning_rate": 7.259768359424716e-05,
      "loss": 0.9141,
      "step": 5010
    },
    {
      "epoch": 0.6373290937996821,
      "grad_norm": 2.70759654045105,
      "learning_rate": 7.25722285859743e-05,
      "loss": 0.6588,
      "step": 5011
    },
    {
      "epoch": 0.637456279809221,
      "grad_norm": 1.8029414415359497,
      "learning_rate": 7.254677357770143e-05,
      "loss": 0.3552,
      "step": 5012
    },
    {
      "epoch": 0.6375834658187599,
      "grad_norm": 2.217331886291504,
      "learning_rate": 7.252131856942855e-05,
      "loss": 0.5939,
      "step": 5013
    },
    {
      "epoch": 0.6377106518282989,
      "grad_norm": 2.157778024673462,
      "learning_rate": 7.249586356115567e-05,
      "loss": 0.6502,
      "step": 5014
    },
    {
      "epoch": 0.6378378378378379,
      "grad_norm": 1.8657232522964478,
      "learning_rate": 7.247040855288278e-05,
      "loss": 0.5675,
      "step": 5015
    },
    {
      "epoch": 0.6379650238473767,
      "grad_norm": 2.216182231903076,
      "learning_rate": 7.24449535446099e-05,
      "loss": 0.5366,
      "step": 5016
    },
    {
      "epoch": 0.6380922098569157,
      "grad_norm": 2.0596232414245605,
      "learning_rate": 7.241949853633702e-05,
      "loss": 0.4474,
      "step": 5017
    },
    {
      "epoch": 0.6382193958664547,
      "grad_norm": 2.243124008178711,
      "learning_rate": 7.239404352806415e-05,
      "loss": 0.3716,
      "step": 5018
    },
    {
      "epoch": 0.6383465818759937,
      "grad_norm": 1.7680894136428833,
      "learning_rate": 7.236858851979127e-05,
      "loss": 0.5925,
      "step": 5019
    },
    {
      "epoch": 0.6384737678855326,
      "grad_norm": 1.9825018644332886,
      "learning_rate": 7.234313351151839e-05,
      "loss": 0.5934,
      "step": 5020
    },
    {
      "epoch": 0.6386009538950715,
      "grad_norm": 2.0093743801116943,
      "learning_rate": 7.231767850324551e-05,
      "loss": 0.4654,
      "step": 5021
    },
    {
      "epoch": 0.6387281399046105,
      "grad_norm": 1.5355134010314941,
      "learning_rate": 7.229222349497264e-05,
      "loss": 0.4775,
      "step": 5022
    },
    {
      "epoch": 0.6388553259141494,
      "grad_norm": 2.0327837467193604,
      "learning_rate": 7.226676848669976e-05,
      "loss": 0.6709,
      "step": 5023
    },
    {
      "epoch": 0.6389825119236884,
      "grad_norm": 2.1965603828430176,
      "learning_rate": 7.224131347842688e-05,
      "loss": 0.5762,
      "step": 5024
    },
    {
      "epoch": 0.6391096979332274,
      "grad_norm": 2.5861616134643555,
      "learning_rate": 7.221585847015401e-05,
      "loss": 0.5384,
      "step": 5025
    },
    {
      "epoch": 0.6392368839427663,
      "grad_norm": 1.8172643184661865,
      "learning_rate": 7.219040346188113e-05,
      "loss": 0.643,
      "step": 5026
    },
    {
      "epoch": 0.6393640699523052,
      "grad_norm": 3.2728989124298096,
      "learning_rate": 7.216494845360825e-05,
      "loss": 0.7859,
      "step": 5027
    },
    {
      "epoch": 0.6394912559618442,
      "grad_norm": 2.3946187496185303,
      "learning_rate": 7.213949344533537e-05,
      "loss": 0.6058,
      "step": 5028
    },
    {
      "epoch": 0.6396184419713832,
      "grad_norm": 2.569838285446167,
      "learning_rate": 7.211403843706249e-05,
      "loss": 0.65,
      "step": 5029
    },
    {
      "epoch": 0.639745627980922,
      "grad_norm": 2.960477828979492,
      "learning_rate": 7.208858342878962e-05,
      "loss": 0.8714,
      "step": 5030
    },
    {
      "epoch": 0.639872813990461,
      "grad_norm": 2.3372931480407715,
      "learning_rate": 7.206312842051675e-05,
      "loss": 0.4774,
      "step": 5031
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.099242687225342,
      "learning_rate": 7.203767341224387e-05,
      "loss": 0.806,
      "step": 5032
    },
    {
      "epoch": 0.640127186009539,
      "grad_norm": 1.7716914415359497,
      "learning_rate": 7.201221840397099e-05,
      "loss": 0.6432,
      "step": 5033
    },
    {
      "epoch": 0.6402543720190779,
      "grad_norm": 2.904855251312256,
      "learning_rate": 7.198676339569811e-05,
      "loss": 0.5722,
      "step": 5034
    },
    {
      "epoch": 0.6403815580286168,
      "grad_norm": 2.414102554321289,
      "learning_rate": 7.196130838742523e-05,
      "loss": 0.8772,
      "step": 5035
    },
    {
      "epoch": 0.6405087440381558,
      "grad_norm": 2.059635877609253,
      "learning_rate": 7.193585337915235e-05,
      "loss": 0.6685,
      "step": 5036
    },
    {
      "epoch": 0.6406359300476947,
      "grad_norm": 1.9056429862976074,
      "learning_rate": 7.191039837087948e-05,
      "loss": 0.6072,
      "step": 5037
    },
    {
      "epoch": 0.6407631160572337,
      "grad_norm": 3.0183558464050293,
      "learning_rate": 7.18849433626066e-05,
      "loss": 0.5949,
      "step": 5038
    },
    {
      "epoch": 0.6408903020667727,
      "grad_norm": 3.1159634590148926,
      "learning_rate": 7.185948835433372e-05,
      "loss": 0.6632,
      "step": 5039
    },
    {
      "epoch": 0.6410174880763116,
      "grad_norm": 1.748400330543518,
      "learning_rate": 7.183403334606083e-05,
      "loss": 0.372,
      "step": 5040
    },
    {
      "epoch": 0.6411446740858505,
      "grad_norm": 1.8083219528198242,
      "learning_rate": 7.180857833778797e-05,
      "loss": 0.5271,
      "step": 5041
    },
    {
      "epoch": 0.6412718600953895,
      "grad_norm": 2.3034446239471436,
      "learning_rate": 7.178312332951508e-05,
      "loss": 0.6327,
      "step": 5042
    },
    {
      "epoch": 0.6413990461049285,
      "grad_norm": 2.3282766342163086,
      "learning_rate": 7.175766832124222e-05,
      "loss": 0.8075,
      "step": 5043
    },
    {
      "epoch": 0.6415262321144674,
      "grad_norm": 1.934191346168518,
      "learning_rate": 7.173221331296934e-05,
      "loss": 0.4497,
      "step": 5044
    },
    {
      "epoch": 0.6416534181240063,
      "grad_norm": 2.1208066940307617,
      "learning_rate": 7.170675830469645e-05,
      "loss": 0.5919,
      "step": 5045
    },
    {
      "epoch": 0.6417806041335453,
      "grad_norm": 3.7401442527770996,
      "learning_rate": 7.168130329642357e-05,
      "loss": 1.0345,
      "step": 5046
    },
    {
      "epoch": 0.6419077901430843,
      "grad_norm": 2.839611768722534,
      "learning_rate": 7.165584828815069e-05,
      "loss": 0.446,
      "step": 5047
    },
    {
      "epoch": 0.6420349761526232,
      "grad_norm": 2.083076000213623,
      "learning_rate": 7.163039327987781e-05,
      "loss": 0.6704,
      "step": 5048
    },
    {
      "epoch": 0.6421621621621622,
      "grad_norm": 1.7459378242492676,
      "learning_rate": 7.160493827160494e-05,
      "loss": 0.5548,
      "step": 5049
    },
    {
      "epoch": 0.6422893481717011,
      "grad_norm": 2.5721633434295654,
      "learning_rate": 7.157948326333206e-05,
      "loss": 0.6149,
      "step": 5050
    },
    {
      "epoch": 0.6424165341812401,
      "grad_norm": 2.951329469680786,
      "learning_rate": 7.15540282550592e-05,
      "loss": 0.7082,
      "step": 5051
    },
    {
      "epoch": 0.642543720190779,
      "grad_norm": 2.7904372215270996,
      "learning_rate": 7.152857324678631e-05,
      "loss": 0.6566,
      "step": 5052
    },
    {
      "epoch": 0.642670906200318,
      "grad_norm": 1.8206586837768555,
      "learning_rate": 7.150311823851343e-05,
      "loss": 0.4626,
      "step": 5053
    },
    {
      "epoch": 0.642798092209857,
      "grad_norm": 1.6331491470336914,
      "learning_rate": 7.147766323024055e-05,
      "loss": 0.5799,
      "step": 5054
    },
    {
      "epoch": 0.6429252782193958,
      "grad_norm": 2.471879243850708,
      "learning_rate": 7.145220822196767e-05,
      "loss": 0.684,
      "step": 5055
    },
    {
      "epoch": 0.6430524642289348,
      "grad_norm": 2.1109862327575684,
      "learning_rate": 7.14267532136948e-05,
      "loss": 0.5018,
      "step": 5056
    },
    {
      "epoch": 0.6431796502384738,
      "grad_norm": 2.1137301921844482,
      "learning_rate": 7.140129820542192e-05,
      "loss": 0.5608,
      "step": 5057
    },
    {
      "epoch": 0.6433068362480128,
      "grad_norm": 1.6101226806640625,
      "learning_rate": 7.137584319714904e-05,
      "loss": 0.4747,
      "step": 5058
    },
    {
      "epoch": 0.6434340222575516,
      "grad_norm": 2.331331729888916,
      "learning_rate": 7.135038818887616e-05,
      "loss": 0.4804,
      "step": 5059
    },
    {
      "epoch": 0.6435612082670906,
      "grad_norm": 3.0481739044189453,
      "learning_rate": 7.132493318060328e-05,
      "loss": 0.6379,
      "step": 5060
    },
    {
      "epoch": 0.6436883942766296,
      "grad_norm": 1.928717851638794,
      "learning_rate": 7.129947817233041e-05,
      "loss": 0.5331,
      "step": 5061
    },
    {
      "epoch": 0.6438155802861685,
      "grad_norm": 1.8307334184646606,
      "learning_rate": 7.127402316405754e-05,
      "loss": 0.5494,
      "step": 5062
    },
    {
      "epoch": 0.6439427662957075,
      "grad_norm": 1.9856560230255127,
      "learning_rate": 7.124856815578466e-05,
      "loss": 0.5819,
      "step": 5063
    },
    {
      "epoch": 0.6440699523052464,
      "grad_norm": 2.7260966300964355,
      "learning_rate": 7.122311314751178e-05,
      "loss": 0.5138,
      "step": 5064
    },
    {
      "epoch": 0.6441971383147854,
      "grad_norm": 2.339953660964966,
      "learning_rate": 7.11976581392389e-05,
      "loss": 0.8253,
      "step": 5065
    },
    {
      "epoch": 0.6443243243243243,
      "grad_norm": 2.3512814044952393,
      "learning_rate": 7.117220313096602e-05,
      "loss": 0.5394,
      "step": 5066
    },
    {
      "epoch": 0.6444515103338633,
      "grad_norm": 2.219369649887085,
      "learning_rate": 7.114674812269313e-05,
      "loss": 0.5307,
      "step": 5067
    },
    {
      "epoch": 0.6445786963434023,
      "grad_norm": 2.5742204189300537,
      "learning_rate": 7.112129311442027e-05,
      "loss": 0.5415,
      "step": 5068
    },
    {
      "epoch": 0.6447058823529411,
      "grad_norm": 2.1664910316467285,
      "learning_rate": 7.109583810614739e-05,
      "loss": 0.6601,
      "step": 5069
    },
    {
      "epoch": 0.6448330683624801,
      "grad_norm": 1.9348188638687134,
      "learning_rate": 7.107038309787452e-05,
      "loss": 0.5257,
      "step": 5070
    },
    {
      "epoch": 0.6449602543720191,
      "grad_norm": 2.3119449615478516,
      "learning_rate": 7.104492808960164e-05,
      "loss": 0.6303,
      "step": 5071
    },
    {
      "epoch": 0.6450874403815581,
      "grad_norm": 2.5501959323883057,
      "learning_rate": 7.101947308132876e-05,
      "loss": 0.5905,
      "step": 5072
    },
    {
      "epoch": 0.645214626391097,
      "grad_norm": 1.7057517766952515,
      "learning_rate": 7.099401807305587e-05,
      "loss": 0.2882,
      "step": 5073
    },
    {
      "epoch": 0.6453418124006359,
      "grad_norm": 2.8479437828063965,
      "learning_rate": 7.0968563064783e-05,
      "loss": 0.6469,
      "step": 5074
    },
    {
      "epoch": 0.6454689984101749,
      "grad_norm": 2.3914217948913574,
      "learning_rate": 7.094310805651013e-05,
      "loss": 0.7786,
      "step": 5075
    },
    {
      "epoch": 0.6455961844197138,
      "grad_norm": 1.873704195022583,
      "learning_rate": 7.091765304823724e-05,
      "loss": 0.4884,
      "step": 5076
    },
    {
      "epoch": 0.6457233704292528,
      "grad_norm": 1.6543083190917969,
      "learning_rate": 7.089219803996436e-05,
      "loss": 0.4186,
      "step": 5077
    },
    {
      "epoch": 0.6458505564387917,
      "grad_norm": 1.8137762546539307,
      "learning_rate": 7.086674303169148e-05,
      "loss": 0.6016,
      "step": 5078
    },
    {
      "epoch": 0.6459777424483307,
      "grad_norm": 1.5380122661590576,
      "learning_rate": 7.08412880234186e-05,
      "loss": 0.4094,
      "step": 5079
    },
    {
      "epoch": 0.6461049284578696,
      "grad_norm": 2.263253927230835,
      "learning_rate": 7.081583301514573e-05,
      "loss": 0.4034,
      "step": 5080
    },
    {
      "epoch": 0.6462321144674086,
      "grad_norm": 2.418192148208618,
      "learning_rate": 7.079037800687286e-05,
      "loss": 0.759,
      "step": 5081
    },
    {
      "epoch": 0.6463593004769476,
      "grad_norm": 1.7495537996292114,
      "learning_rate": 7.076492299859998e-05,
      "loss": 0.5446,
      "step": 5082
    },
    {
      "epoch": 0.6464864864864864,
      "grad_norm": 2.5577523708343506,
      "learning_rate": 7.07394679903271e-05,
      "loss": 0.5936,
      "step": 5083
    },
    {
      "epoch": 0.6466136724960254,
      "grad_norm": 2.184408187866211,
      "learning_rate": 7.071401298205422e-05,
      "loss": 0.6164,
      "step": 5084
    },
    {
      "epoch": 0.6467408585055644,
      "grad_norm": 2.229681968688965,
      "learning_rate": 7.068855797378134e-05,
      "loss": 0.8173,
      "step": 5085
    },
    {
      "epoch": 0.6468680445151034,
      "grad_norm": 1.5495401620864868,
      "learning_rate": 7.066310296550846e-05,
      "loss": 0.3396,
      "step": 5086
    },
    {
      "epoch": 0.6469952305246423,
      "grad_norm": 2.4861443042755127,
      "learning_rate": 7.063764795723559e-05,
      "loss": 0.5624,
      "step": 5087
    },
    {
      "epoch": 0.6471224165341812,
      "grad_norm": 2.484651803970337,
      "learning_rate": 7.061219294896271e-05,
      "loss": 0.4411,
      "step": 5088
    },
    {
      "epoch": 0.6472496025437202,
      "grad_norm": 2.0082504749298096,
      "learning_rate": 7.058673794068983e-05,
      "loss": 0.6327,
      "step": 5089
    },
    {
      "epoch": 0.6473767885532591,
      "grad_norm": 3.278324604034424,
      "learning_rate": 7.056128293241696e-05,
      "loss": 0.6469,
      "step": 5090
    },
    {
      "epoch": 0.6475039745627981,
      "grad_norm": 2.832031488418579,
      "learning_rate": 7.053582792414408e-05,
      "loss": 0.6241,
      "step": 5091
    },
    {
      "epoch": 0.647631160572337,
      "grad_norm": 2.334041118621826,
      "learning_rate": 7.05103729158712e-05,
      "loss": 0.8015,
      "step": 5092
    },
    {
      "epoch": 0.647758346581876,
      "grad_norm": 3.2083518505096436,
      "learning_rate": 7.048491790759833e-05,
      "loss": 0.5669,
      "step": 5093
    },
    {
      "epoch": 0.6478855325914149,
      "grad_norm": 2.1878368854522705,
      "learning_rate": 7.045946289932545e-05,
      "loss": 0.7916,
      "step": 5094
    },
    {
      "epoch": 0.6480127186009539,
      "grad_norm": 1.6847198009490967,
      "learning_rate": 7.043400789105257e-05,
      "loss": 0.4088,
      "step": 5095
    },
    {
      "epoch": 0.6481399046104929,
      "grad_norm": 2.8388051986694336,
      "learning_rate": 7.040855288277969e-05,
      "loss": 0.8298,
      "step": 5096
    },
    {
      "epoch": 0.6482670906200318,
      "grad_norm": 2.252748489379883,
      "learning_rate": 7.03830978745068e-05,
      "loss": 0.5158,
      "step": 5097
    },
    {
      "epoch": 0.6483942766295707,
      "grad_norm": 2.0519254207611084,
      "learning_rate": 7.035764286623392e-05,
      "loss": 0.7956,
      "step": 5098
    },
    {
      "epoch": 0.6485214626391097,
      "grad_norm": 1.7597501277923584,
      "learning_rate": 7.033218785796106e-05,
      "loss": 0.3576,
      "step": 5099
    },
    {
      "epoch": 0.6486486486486487,
      "grad_norm": 2.8812882900238037,
      "learning_rate": 7.030673284968819e-05,
      "loss": 0.46,
      "step": 5100
    },
    {
      "epoch": 0.6487758346581876,
      "grad_norm": 1.9388599395751953,
      "learning_rate": 7.028127784141531e-05,
      "loss": 0.4436,
      "step": 5101
    },
    {
      "epoch": 0.6489030206677265,
      "grad_norm": 1.8290178775787354,
      "learning_rate": 7.025582283314243e-05,
      "loss": 0.4933,
      "step": 5102
    },
    {
      "epoch": 0.6490302066772655,
      "grad_norm": 1.8270106315612793,
      "learning_rate": 7.023036782486954e-05,
      "loss": 0.496,
      "step": 5103
    },
    {
      "epoch": 0.6491573926868045,
      "grad_norm": 2.0406012535095215,
      "learning_rate": 7.020491281659666e-05,
      "loss": 0.4754,
      "step": 5104
    },
    {
      "epoch": 0.6492845786963434,
      "grad_norm": 2.5355260372161865,
      "learning_rate": 7.01794578083238e-05,
      "loss": 0.6734,
      "step": 5105
    },
    {
      "epoch": 0.6494117647058824,
      "grad_norm": 2.0026533603668213,
      "learning_rate": 7.015400280005091e-05,
      "loss": 0.4009,
      "step": 5106
    },
    {
      "epoch": 0.6495389507154213,
      "grad_norm": 1.6441267728805542,
      "learning_rate": 7.012854779177803e-05,
      "loss": 0.6782,
      "step": 5107
    },
    {
      "epoch": 0.6496661367249602,
      "grad_norm": 1.9849761724472046,
      "learning_rate": 7.010309278350515e-05,
      "loss": 0.6208,
      "step": 5108
    },
    {
      "epoch": 0.6497933227344992,
      "grad_norm": 2.1107492446899414,
      "learning_rate": 7.007763777523228e-05,
      "loss": 0.51,
      "step": 5109
    },
    {
      "epoch": 0.6499205087440382,
      "grad_norm": 1.9534916877746582,
      "learning_rate": 7.00521827669594e-05,
      "loss": 0.5175,
      "step": 5110
    },
    {
      "epoch": 0.6500476947535772,
      "grad_norm": 1.777592658996582,
      "learning_rate": 7.002672775868652e-05,
      "loss": 0.405,
      "step": 5111
    },
    {
      "epoch": 0.650174880763116,
      "grad_norm": 2.029331684112549,
      "learning_rate": 7.000127275041365e-05,
      "loss": 0.6321,
      "step": 5112
    },
    {
      "epoch": 0.650302066772655,
      "grad_norm": 2.3871219158172607,
      "learning_rate": 6.997581774214077e-05,
      "loss": 0.5368,
      "step": 5113
    },
    {
      "epoch": 0.650429252782194,
      "grad_norm": 2.2807393074035645,
      "learning_rate": 6.995036273386789e-05,
      "loss": 0.5621,
      "step": 5114
    },
    {
      "epoch": 0.6505564387917329,
      "grad_norm": 2.2433855533599854,
      "learning_rate": 6.992490772559501e-05,
      "loss": 0.7563,
      "step": 5115
    },
    {
      "epoch": 0.6506836248012718,
      "grad_norm": 2.311471700668335,
      "learning_rate": 6.989945271732213e-05,
      "loss": 0.4741,
      "step": 5116
    },
    {
      "epoch": 0.6508108108108108,
      "grad_norm": 1.4031513929367065,
      "learning_rate": 6.987399770904926e-05,
      "loss": 0.4084,
      "step": 5117
    },
    {
      "epoch": 0.6509379968203498,
      "grad_norm": 1.8728930950164795,
      "learning_rate": 6.984854270077638e-05,
      "loss": 0.3601,
      "step": 5118
    },
    {
      "epoch": 0.6510651828298887,
      "grad_norm": 2.335381507873535,
      "learning_rate": 6.982308769250351e-05,
      "loss": 0.8531,
      "step": 5119
    },
    {
      "epoch": 0.6511923688394277,
      "grad_norm": 2.357773542404175,
      "learning_rate": 6.979763268423063e-05,
      "loss": 0.7478,
      "step": 5120
    },
    {
      "epoch": 0.6513195548489666,
      "grad_norm": 2.6234323978424072,
      "learning_rate": 6.977217767595775e-05,
      "loss": 0.5551,
      "step": 5121
    },
    {
      "epoch": 0.6514467408585055,
      "grad_norm": 2.2647104263305664,
      "learning_rate": 6.974672266768487e-05,
      "loss": 0.5425,
      "step": 5122
    },
    {
      "epoch": 0.6515739268680445,
      "grad_norm": 2.3925936222076416,
      "learning_rate": 6.972126765941199e-05,
      "loss": 0.6069,
      "step": 5123
    },
    {
      "epoch": 0.6517011128775835,
      "grad_norm": 3.018927812576294,
      "learning_rate": 6.969581265113912e-05,
      "loss": 0.7078,
      "step": 5124
    },
    {
      "epoch": 0.6518282988871225,
      "grad_norm": 2.28613018989563,
      "learning_rate": 6.967035764286624e-05,
      "loss": 0.6625,
      "step": 5125
    },
    {
      "epoch": 0.6519554848966613,
      "grad_norm": 2.32771897315979,
      "learning_rate": 6.964490263459336e-05,
      "loss": 0.428,
      "step": 5126
    },
    {
      "epoch": 0.6520826709062003,
      "grad_norm": 2.239307165145874,
      "learning_rate": 6.961944762632048e-05,
      "loss": 0.4683,
      "step": 5127
    },
    {
      "epoch": 0.6522098569157393,
      "grad_norm": 2.6543469429016113,
      "learning_rate": 6.95939926180476e-05,
      "loss": 0.5531,
      "step": 5128
    },
    {
      "epoch": 0.6523370429252782,
      "grad_norm": 2.015594244003296,
      "learning_rate": 6.956853760977473e-05,
      "loss": 0.3194,
      "step": 5129
    },
    {
      "epoch": 0.6524642289348171,
      "grad_norm": 2.562654495239258,
      "learning_rate": 6.954308260150185e-05,
      "loss": 0.5578,
      "step": 5130
    },
    {
      "epoch": 0.6525914149443561,
      "grad_norm": 2.120194435119629,
      "learning_rate": 6.951762759322898e-05,
      "loss": 0.6447,
      "step": 5131
    },
    {
      "epoch": 0.6527186009538951,
      "grad_norm": 2.538156509399414,
      "learning_rate": 6.94921725849561e-05,
      "loss": 0.5015,
      "step": 5132
    },
    {
      "epoch": 0.652845786963434,
      "grad_norm": 2.8971002101898193,
      "learning_rate": 6.946671757668322e-05,
      "loss": 0.4835,
      "step": 5133
    },
    {
      "epoch": 0.652972972972973,
      "grad_norm": 1.9985219240188599,
      "learning_rate": 6.944126256841033e-05,
      "loss": 0.4309,
      "step": 5134
    },
    {
      "epoch": 0.653100158982512,
      "grad_norm": 2.1872246265411377,
      "learning_rate": 6.941580756013745e-05,
      "loss": 0.419,
      "step": 5135
    },
    {
      "epoch": 0.6532273449920509,
      "grad_norm": 2.9757821559906006,
      "learning_rate": 6.939035255186459e-05,
      "loss": 0.6333,
      "step": 5136
    },
    {
      "epoch": 0.6533545310015898,
      "grad_norm": 2.6951186656951904,
      "learning_rate": 6.93648975435917e-05,
      "loss": 0.5839,
      "step": 5137
    },
    {
      "epoch": 0.6534817170111288,
      "grad_norm": 2.0296645164489746,
      "learning_rate": 6.933944253531884e-05,
      "loss": 0.3175,
      "step": 5138
    },
    {
      "epoch": 0.6536089030206678,
      "grad_norm": 2.1762120723724365,
      "learning_rate": 6.931398752704596e-05,
      "loss": 0.6644,
      "step": 5139
    },
    {
      "epoch": 0.6537360890302066,
      "grad_norm": 2.0293667316436768,
      "learning_rate": 6.928853251877307e-05,
      "loss": 0.2265,
      "step": 5140
    },
    {
      "epoch": 0.6538632750397456,
      "grad_norm": 2.3957178592681885,
      "learning_rate": 6.926307751050019e-05,
      "loss": 0.6871,
      "step": 5141
    },
    {
      "epoch": 0.6539904610492846,
      "grad_norm": 3.010939598083496,
      "learning_rate": 6.923762250222731e-05,
      "loss": 0.6262,
      "step": 5142
    },
    {
      "epoch": 0.6541176470588236,
      "grad_norm": 2.4548964500427246,
      "learning_rate": 6.921216749395444e-05,
      "loss": 0.633,
      "step": 5143
    },
    {
      "epoch": 0.6542448330683625,
      "grad_norm": 2.8637144565582275,
      "learning_rate": 6.918671248568156e-05,
      "loss": 0.8472,
      "step": 5144
    },
    {
      "epoch": 0.6543720190779014,
      "grad_norm": 2.545917272567749,
      "learning_rate": 6.916125747740868e-05,
      "loss": 0.448,
      "step": 5145
    },
    {
      "epoch": 0.6544992050874404,
      "grad_norm": 2.0692155361175537,
      "learning_rate": 6.91358024691358e-05,
      "loss": 0.4235,
      "step": 5146
    },
    {
      "epoch": 0.6546263910969793,
      "grad_norm": 2.9929909706115723,
      "learning_rate": 6.911034746086292e-05,
      "loss": 0.5989,
      "step": 5147
    },
    {
      "epoch": 0.6547535771065183,
      "grad_norm": 2.439915895462036,
      "learning_rate": 6.908489245259005e-05,
      "loss": 0.8266,
      "step": 5148
    },
    {
      "epoch": 0.6548807631160573,
      "grad_norm": 2.7526984214782715,
      "learning_rate": 6.905943744431718e-05,
      "loss": 0.7418,
      "step": 5149
    },
    {
      "epoch": 0.6550079491255962,
      "grad_norm": 2.540487051010132,
      "learning_rate": 6.90339824360443e-05,
      "loss": 0.7253,
      "step": 5150
    },
    {
      "epoch": 0.6551351351351351,
      "grad_norm": 2.756570339202881,
      "learning_rate": 6.900852742777142e-05,
      "loss": 0.9293,
      "step": 5151
    },
    {
      "epoch": 0.6552623211446741,
      "grad_norm": 2.578767776489258,
      "learning_rate": 6.898307241949854e-05,
      "loss": 0.4187,
      "step": 5152
    },
    {
      "epoch": 0.6553895071542131,
      "grad_norm": 1.6707624197006226,
      "learning_rate": 6.895761741122566e-05,
      "loss": 0.5079,
      "step": 5153
    },
    {
      "epoch": 0.6555166931637519,
      "grad_norm": 2.3146891593933105,
      "learning_rate": 6.893216240295278e-05,
      "loss": 0.5326,
      "step": 5154
    },
    {
      "epoch": 0.6556438791732909,
      "grad_norm": 2.0882680416107178,
      "learning_rate": 6.890670739467991e-05,
      "loss": 1.348,
      "step": 5155
    },
    {
      "epoch": 0.6557710651828299,
      "grad_norm": 1.7823517322540283,
      "learning_rate": 6.888125238640703e-05,
      "loss": 0.4361,
      "step": 5156
    },
    {
      "epoch": 0.6558982511923689,
      "grad_norm": 2.043808698654175,
      "learning_rate": 6.885579737813415e-05,
      "loss": 0.5462,
      "step": 5157
    },
    {
      "epoch": 0.6560254372019078,
      "grad_norm": 2.8602044582366943,
      "learning_rate": 6.883034236986128e-05,
      "loss": 0.7267,
      "step": 5158
    },
    {
      "epoch": 0.6561526232114467,
      "grad_norm": 1.5323466062545776,
      "learning_rate": 6.88048873615884e-05,
      "loss": 0.4605,
      "step": 5159
    },
    {
      "epoch": 0.6562798092209857,
      "grad_norm": 1.9774036407470703,
      "learning_rate": 6.877943235331552e-05,
      "loss": 0.9047,
      "step": 5160
    },
    {
      "epoch": 0.6564069952305246,
      "grad_norm": 1.915351152420044,
      "learning_rate": 6.875397734504264e-05,
      "loss": 0.5312,
      "step": 5161
    },
    {
      "epoch": 0.6565341812400636,
      "grad_norm": 1.655684232711792,
      "learning_rate": 6.872852233676977e-05,
      "loss": 0.4919,
      "step": 5162
    },
    {
      "epoch": 0.6566613672496026,
      "grad_norm": 2.754176616668701,
      "learning_rate": 6.870306732849689e-05,
      "loss": 0.7777,
      "step": 5163
    },
    {
      "epoch": 0.6567885532591415,
      "grad_norm": 1.865204095840454,
      "learning_rate": 6.8677612320224e-05,
      "loss": 0.4256,
      "step": 5164
    },
    {
      "epoch": 0.6569157392686804,
      "grad_norm": 1.444685459136963,
      "learning_rate": 6.865215731195112e-05,
      "loss": 0.3923,
      "step": 5165
    },
    {
      "epoch": 0.6570429252782194,
      "grad_norm": 1.8758045434951782,
      "learning_rate": 6.862670230367824e-05,
      "loss": 0.5473,
      "step": 5166
    },
    {
      "epoch": 0.6571701112877584,
      "grad_norm": 2.4321651458740234,
      "learning_rate": 6.860124729540537e-05,
      "loss": 0.6933,
      "step": 5167
    },
    {
      "epoch": 0.6572972972972972,
      "grad_norm": 1.632880687713623,
      "learning_rate": 6.857579228713251e-05,
      "loss": 0.6478,
      "step": 5168
    },
    {
      "epoch": 0.6574244833068362,
      "grad_norm": 1.770822525024414,
      "learning_rate": 6.855033727885963e-05,
      "loss": 0.4907,
      "step": 5169
    },
    {
      "epoch": 0.6575516693163752,
      "grad_norm": 2.331632375717163,
      "learning_rate": 6.852488227058674e-05,
      "loss": 0.6807,
      "step": 5170
    },
    {
      "epoch": 0.6576788553259142,
      "grad_norm": 1.7687345743179321,
      "learning_rate": 6.849942726231386e-05,
      "loss": 0.4646,
      "step": 5171
    },
    {
      "epoch": 0.6578060413354531,
      "grad_norm": 1.9959406852722168,
      "learning_rate": 6.847397225404098e-05,
      "loss": 0.6945,
      "step": 5172
    },
    {
      "epoch": 0.657933227344992,
      "grad_norm": 1.6348832845687866,
      "learning_rate": 6.84485172457681e-05,
      "loss": 0.4706,
      "step": 5173
    },
    {
      "epoch": 0.658060413354531,
      "grad_norm": 1.5028825998306274,
      "learning_rate": 6.842306223749523e-05,
      "loss": 0.5252,
      "step": 5174
    },
    {
      "epoch": 0.6581875993640699,
      "grad_norm": 2.6432509422302246,
      "learning_rate": 6.839760722922235e-05,
      "loss": 0.7303,
      "step": 5175
    },
    {
      "epoch": 0.6583147853736089,
      "grad_norm": 1.9365649223327637,
      "learning_rate": 6.837215222094947e-05,
      "loss": 0.3863,
      "step": 5176
    },
    {
      "epoch": 0.6584419713831479,
      "grad_norm": 3.2314298152923584,
      "learning_rate": 6.83466972126766e-05,
      "loss": 1.0096,
      "step": 5177
    },
    {
      "epoch": 0.6585691573926868,
      "grad_norm": 2.248363971710205,
      "learning_rate": 6.832124220440372e-05,
      "loss": 0.5372,
      "step": 5178
    },
    {
      "epoch": 0.6586963434022257,
      "grad_norm": 2.15915846824646,
      "learning_rate": 6.829578719613084e-05,
      "loss": 0.6493,
      "step": 5179
    },
    {
      "epoch": 0.6588235294117647,
      "grad_norm": 2.1569900512695312,
      "learning_rate": 6.827033218785797e-05,
      "loss": 0.4549,
      "step": 5180
    },
    {
      "epoch": 0.6589507154213037,
      "grad_norm": 2.0775420665740967,
      "learning_rate": 6.824487717958509e-05,
      "loss": 0.423,
      "step": 5181
    },
    {
      "epoch": 0.6590779014308427,
      "grad_norm": 3.0213842391967773,
      "learning_rate": 6.821942217131221e-05,
      "loss": 0.7767,
      "step": 5182
    },
    {
      "epoch": 0.6592050874403815,
      "grad_norm": 2.656123399734497,
      "learning_rate": 6.819396716303933e-05,
      "loss": 0.8466,
      "step": 5183
    },
    {
      "epoch": 0.6593322734499205,
      "grad_norm": 1.8244340419769287,
      "learning_rate": 6.816851215476645e-05,
      "loss": 0.4513,
      "step": 5184
    },
    {
      "epoch": 0.6594594594594595,
      "grad_norm": 2.1521811485290527,
      "learning_rate": 6.814305714649357e-05,
      "loss": 0.4799,
      "step": 5185
    },
    {
      "epoch": 0.6595866454689984,
      "grad_norm": 2.40389347076416,
      "learning_rate": 6.81176021382207e-05,
      "loss": 0.5144,
      "step": 5186
    },
    {
      "epoch": 0.6597138314785373,
      "grad_norm": 2.422701597213745,
      "learning_rate": 6.809214712994783e-05,
      "loss": 0.9718,
      "step": 5187
    },
    {
      "epoch": 0.6598410174880763,
      "grad_norm": 1.7880507707595825,
      "learning_rate": 6.806669212167495e-05,
      "loss": 0.2489,
      "step": 5188
    },
    {
      "epoch": 0.6599682034976153,
      "grad_norm": 1.7925612926483154,
      "learning_rate": 6.804123711340207e-05,
      "loss": 0.4703,
      "step": 5189
    },
    {
      "epoch": 0.6600953895071542,
      "grad_norm": 1.7487008571624756,
      "learning_rate": 6.801578210512919e-05,
      "loss": 0.4828,
      "step": 5190
    },
    {
      "epoch": 0.6602225755166932,
      "grad_norm": 2.278843641281128,
      "learning_rate": 6.79903270968563e-05,
      "loss": 0.4313,
      "step": 5191
    },
    {
      "epoch": 0.6603497615262321,
      "grad_norm": 2.3217577934265137,
      "learning_rate": 6.796487208858342e-05,
      "loss": 0.6077,
      "step": 5192
    },
    {
      "epoch": 0.660476947535771,
      "grad_norm": 1.8791277408599854,
      "learning_rate": 6.793941708031056e-05,
      "loss": 0.5184,
      "step": 5193
    },
    {
      "epoch": 0.66060413354531,
      "grad_norm": 2.2052907943725586,
      "learning_rate": 6.791396207203768e-05,
      "loss": 0.6064,
      "step": 5194
    },
    {
      "epoch": 0.660731319554849,
      "grad_norm": 2.695157766342163,
      "learning_rate": 6.78885070637648e-05,
      "loss": 0.823,
      "step": 5195
    },
    {
      "epoch": 0.660858505564388,
      "grad_norm": 2.8042349815368652,
      "learning_rate": 6.786305205549191e-05,
      "loss": 0.6236,
      "step": 5196
    },
    {
      "epoch": 0.6609856915739268,
      "grad_norm": 2.4866280555725098,
      "learning_rate": 6.783759704721905e-05,
      "loss": 0.7475,
      "step": 5197
    },
    {
      "epoch": 0.6611128775834658,
      "grad_norm": 2.544440507888794,
      "learning_rate": 6.781214203894616e-05,
      "loss": 0.506,
      "step": 5198
    },
    {
      "epoch": 0.6612400635930048,
      "grad_norm": 3.0052413940429688,
      "learning_rate": 6.77866870306733e-05,
      "loss": 0.6686,
      "step": 5199
    },
    {
      "epoch": 0.6613672496025437,
      "grad_norm": 2.5716283321380615,
      "learning_rate": 6.776123202240041e-05,
      "loss": 0.4538,
      "step": 5200
    },
    {
      "epoch": 0.6614944356120827,
      "grad_norm": 1.4721078872680664,
      "learning_rate": 6.773577701412753e-05,
      "loss": 0.3777,
      "step": 5201
    },
    {
      "epoch": 0.6616216216216216,
      "grad_norm": 3.0401337146759033,
      "learning_rate": 6.771032200585465e-05,
      "loss": 0.8859,
      "step": 5202
    },
    {
      "epoch": 0.6617488076311606,
      "grad_norm": 2.3984177112579346,
      "learning_rate": 6.768486699758177e-05,
      "loss": 0.5161,
      "step": 5203
    },
    {
      "epoch": 0.6618759936406995,
      "grad_norm": 2.262833595275879,
      "learning_rate": 6.765941198930889e-05,
      "loss": 0.3919,
      "step": 5204
    },
    {
      "epoch": 0.6620031796502385,
      "grad_norm": 2.634291410446167,
      "learning_rate": 6.763395698103602e-05,
      "loss": 0.6229,
      "step": 5205
    },
    {
      "epoch": 0.6621303656597775,
      "grad_norm": 1.3940494060516357,
      "learning_rate": 6.760850197276315e-05,
      "loss": 0.3922,
      "step": 5206
    },
    {
      "epoch": 0.6622575516693163,
      "grad_norm": 1.8210283517837524,
      "learning_rate": 6.758304696449027e-05,
      "loss": 0.6356,
      "step": 5207
    },
    {
      "epoch": 0.6623847376788553,
      "grad_norm": 2.2399914264678955,
      "learning_rate": 6.755759195621739e-05,
      "loss": 0.4488,
      "step": 5208
    },
    {
      "epoch": 0.6625119236883943,
      "grad_norm": 2.8854286670684814,
      "learning_rate": 6.753213694794451e-05,
      "loss": 0.56,
      "step": 5209
    },
    {
      "epoch": 0.6626391096979333,
      "grad_norm": 1.6146010160446167,
      "learning_rate": 6.750668193967163e-05,
      "loss": 0.5466,
      "step": 5210
    },
    {
      "epoch": 0.6627662957074721,
      "grad_norm": 2.552699327468872,
      "learning_rate": 6.748122693139876e-05,
      "loss": 0.63,
      "step": 5211
    },
    {
      "epoch": 0.6628934817170111,
      "grad_norm": 2.1178176403045654,
      "learning_rate": 6.745577192312588e-05,
      "loss": 0.709,
      "step": 5212
    },
    {
      "epoch": 0.6630206677265501,
      "grad_norm": 3.14687180519104,
      "learning_rate": 6.7430316914853e-05,
      "loss": 0.9869,
      "step": 5213
    },
    {
      "epoch": 0.663147853736089,
      "grad_norm": 2.7064931392669678,
      "learning_rate": 6.740486190658012e-05,
      "loss": 0.5968,
      "step": 5214
    },
    {
      "epoch": 0.663275039745628,
      "grad_norm": 1.5311452150344849,
      "learning_rate": 6.737940689830724e-05,
      "loss": 0.4339,
      "step": 5215
    },
    {
      "epoch": 0.6634022257551669,
      "grad_norm": 2.6589841842651367,
      "learning_rate": 6.735395189003437e-05,
      "loss": 0.8279,
      "step": 5216
    },
    {
      "epoch": 0.6635294117647059,
      "grad_norm": 2.638324022293091,
      "learning_rate": 6.732849688176149e-05,
      "loss": 0.731,
      "step": 5217
    },
    {
      "epoch": 0.6636565977742448,
      "grad_norm": 1.6774452924728394,
      "learning_rate": 6.730304187348862e-05,
      "loss": 0.4625,
      "step": 5218
    },
    {
      "epoch": 0.6637837837837838,
      "grad_norm": 2.411710500717163,
      "learning_rate": 6.727758686521574e-05,
      "loss": 0.7241,
      "step": 5219
    },
    {
      "epoch": 0.6639109697933228,
      "grad_norm": 1.840686321258545,
      "learning_rate": 6.725213185694286e-05,
      "loss": 0.6537,
      "step": 5220
    },
    {
      "epoch": 0.6640381558028617,
      "grad_norm": 2.473867177963257,
      "learning_rate": 6.722667684866998e-05,
      "loss": 0.6885,
      "step": 5221
    },
    {
      "epoch": 0.6641653418124006,
      "grad_norm": 2.279717445373535,
      "learning_rate": 6.72012218403971e-05,
      "loss": 0.7353,
      "step": 5222
    },
    {
      "epoch": 0.6642925278219396,
      "grad_norm": 2.1522583961486816,
      "learning_rate": 6.717576683212423e-05,
      "loss": 0.5499,
      "step": 5223
    },
    {
      "epoch": 0.6644197138314786,
      "grad_norm": 1.6202094554901123,
      "learning_rate": 6.715031182385135e-05,
      "loss": 0.5752,
      "step": 5224
    },
    {
      "epoch": 0.6645468998410174,
      "grad_norm": 1.8292752504348755,
      "learning_rate": 6.712485681557846e-05,
      "loss": 0.4581,
      "step": 5225
    },
    {
      "epoch": 0.6646740858505564,
      "grad_norm": 2.377561330795288,
      "learning_rate": 6.70994018073056e-05,
      "loss": 0.6602,
      "step": 5226
    },
    {
      "epoch": 0.6648012718600954,
      "grad_norm": 2.1905646324157715,
      "learning_rate": 6.707394679903272e-05,
      "loss": 0.7253,
      "step": 5227
    },
    {
      "epoch": 0.6649284578696344,
      "grad_norm": 2.2320399284362793,
      "learning_rate": 6.704849179075983e-05,
      "loss": 0.6734,
      "step": 5228
    },
    {
      "epoch": 0.6650556438791733,
      "grad_norm": 1.6698099374771118,
      "learning_rate": 6.702303678248695e-05,
      "loss": 0.5209,
      "step": 5229
    },
    {
      "epoch": 0.6651828298887122,
      "grad_norm": 1.6861096620559692,
      "learning_rate": 6.699758177421409e-05,
      "loss": 0.4509,
      "step": 5230
    },
    {
      "epoch": 0.6653100158982512,
      "grad_norm": 1.9715259075164795,
      "learning_rate": 6.69721267659412e-05,
      "loss": 0.4016,
      "step": 5231
    },
    {
      "epoch": 0.6654372019077901,
      "grad_norm": 1.6729027032852173,
      "learning_rate": 6.694667175766832e-05,
      "loss": 0.4641,
      "step": 5232
    },
    {
      "epoch": 0.6655643879173291,
      "grad_norm": 2.5168216228485107,
      "learning_rate": 6.692121674939544e-05,
      "loss": 0.8346,
      "step": 5233
    },
    {
      "epoch": 0.6656915739268681,
      "grad_norm": 2.372563362121582,
      "learning_rate": 6.689576174112256e-05,
      "loss": 0.5508,
      "step": 5234
    },
    {
      "epoch": 0.665818759936407,
      "grad_norm": 2.4124603271484375,
      "learning_rate": 6.687030673284969e-05,
      "loss": 0.7596,
      "step": 5235
    },
    {
      "epoch": 0.6659459459459459,
      "grad_norm": 2.101222276687622,
      "learning_rate": 6.684485172457681e-05,
      "loss": 0.7705,
      "step": 5236
    },
    {
      "epoch": 0.6660731319554849,
      "grad_norm": 2.4002110958099365,
      "learning_rate": 6.681939671630394e-05,
      "loss": 0.4911,
      "step": 5237
    },
    {
      "epoch": 0.6662003179650239,
      "grad_norm": 1.7921290397644043,
      "learning_rate": 6.679394170803106e-05,
      "loss": 0.8179,
      "step": 5238
    },
    {
      "epoch": 0.6663275039745628,
      "grad_norm": 1.6325825452804565,
      "learning_rate": 6.676848669975818e-05,
      "loss": 0.4476,
      "step": 5239
    },
    {
      "epoch": 0.6664546899841017,
      "grad_norm": 2.145275831222534,
      "learning_rate": 6.67430316914853e-05,
      "loss": 0.8103,
      "step": 5240
    },
    {
      "epoch": 0.6665818759936407,
      "grad_norm": 1.961515188217163,
      "learning_rate": 6.671757668321242e-05,
      "loss": 0.4225,
      "step": 5241
    },
    {
      "epoch": 0.6667090620031797,
      "grad_norm": 2.0495057106018066,
      "learning_rate": 6.669212167493955e-05,
      "loss": 0.5904,
      "step": 5242
    },
    {
      "epoch": 0.6668362480127186,
      "grad_norm": 1.7959092855453491,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.6162,
      "step": 5243
    },
    {
      "epoch": 0.6669634340222576,
      "grad_norm": 2.7894415855407715,
      "learning_rate": 6.664121165839379e-05,
      "loss": 0.6256,
      "step": 5244
    },
    {
      "epoch": 0.6670906200317965,
      "grad_norm": 1.6944462060928345,
      "learning_rate": 6.661575665012092e-05,
      "loss": 0.6243,
      "step": 5245
    },
    {
      "epoch": 0.6672178060413354,
      "grad_norm": 1.8020139932632446,
      "learning_rate": 6.659030164184804e-05,
      "loss": 0.7191,
      "step": 5246
    },
    {
      "epoch": 0.6673449920508744,
      "grad_norm": 2.521425485610962,
      "learning_rate": 6.656484663357516e-05,
      "loss": 0.8775,
      "step": 5247
    },
    {
      "epoch": 0.6674721780604134,
      "grad_norm": 2.5669360160827637,
      "learning_rate": 6.653939162530228e-05,
      "loss": 0.5114,
      "step": 5248
    },
    {
      "epoch": 0.6675993640699524,
      "grad_norm": 2.6964921951293945,
      "learning_rate": 6.651393661702941e-05,
      "loss": 0.7027,
      "step": 5249
    },
    {
      "epoch": 0.6677265500794912,
      "grad_norm": 1.953513503074646,
      "learning_rate": 6.648848160875653e-05,
      "loss": 0.4215,
      "step": 5250
    },
    {
      "epoch": 0.6678537360890302,
      "grad_norm": 2.535073757171631,
      "learning_rate": 6.646302660048365e-05,
      "loss": 0.6537,
      "step": 5251
    },
    {
      "epoch": 0.6679809220985692,
      "grad_norm": 2.91910457611084,
      "learning_rate": 6.643757159221077e-05,
      "loss": 0.8283,
      "step": 5252
    },
    {
      "epoch": 0.6681081081081081,
      "grad_norm": 2.240711212158203,
      "learning_rate": 6.641211658393788e-05,
      "loss": 0.5922,
      "step": 5253
    },
    {
      "epoch": 0.668235294117647,
      "grad_norm": 1.8350119590759277,
      "learning_rate": 6.638666157566502e-05,
      "loss": 0.7163,
      "step": 5254
    },
    {
      "epoch": 0.668362480127186,
      "grad_norm": 1.9854621887207031,
      "learning_rate": 6.636120656739215e-05,
      "loss": 0.5581,
      "step": 5255
    },
    {
      "epoch": 0.668489666136725,
      "grad_norm": 2.583437442779541,
      "learning_rate": 6.633575155911927e-05,
      "loss": 0.4692,
      "step": 5256
    },
    {
      "epoch": 0.6686168521462639,
      "grad_norm": 2.27650785446167,
      "learning_rate": 6.631029655084639e-05,
      "loss": 0.65,
      "step": 5257
    },
    {
      "epoch": 0.6687440381558029,
      "grad_norm": 2.0940945148468018,
      "learning_rate": 6.62848415425735e-05,
      "loss": 0.457,
      "step": 5258
    },
    {
      "epoch": 0.6688712241653418,
      "grad_norm": 1.9040523767471313,
      "learning_rate": 6.625938653430062e-05,
      "loss": 0.3586,
      "step": 5259
    },
    {
      "epoch": 0.6689984101748807,
      "grad_norm": 2.2077176570892334,
      "learning_rate": 6.623393152602774e-05,
      "loss": 0.4963,
      "step": 5260
    },
    {
      "epoch": 0.6691255961844197,
      "grad_norm": 1.9635614156723022,
      "learning_rate": 6.620847651775487e-05,
      "loss": 0.4772,
      "step": 5261
    },
    {
      "epoch": 0.6692527821939587,
      "grad_norm": 2.7347288131713867,
      "learning_rate": 6.6183021509482e-05,
      "loss": 0.6357,
      "step": 5262
    },
    {
      "epoch": 0.6693799682034977,
      "grad_norm": 2.3562171459198,
      "learning_rate": 6.615756650120911e-05,
      "loss": 0.5616,
      "step": 5263
    },
    {
      "epoch": 0.6695071542130365,
      "grad_norm": 2.9008305072784424,
      "learning_rate": 6.613211149293623e-05,
      "loss": 0.551,
      "step": 5264
    },
    {
      "epoch": 0.6696343402225755,
      "grad_norm": 1.7839289903640747,
      "learning_rate": 6.610665648466336e-05,
      "loss": 0.5572,
      "step": 5265
    },
    {
      "epoch": 0.6697615262321145,
      "grad_norm": 2.097405195236206,
      "learning_rate": 6.608120147639048e-05,
      "loss": 0.5432,
      "step": 5266
    },
    {
      "epoch": 0.6698887122416535,
      "grad_norm": 2.3426902294158936,
      "learning_rate": 6.60557464681176e-05,
      "loss": 0.5632,
      "step": 5267
    },
    {
      "epoch": 0.6700158982511923,
      "grad_norm": 2.223374605178833,
      "learning_rate": 6.603029145984473e-05,
      "loss": 0.5037,
      "step": 5268
    },
    {
      "epoch": 0.6701430842607313,
      "grad_norm": 2.229680299758911,
      "learning_rate": 6.600483645157185e-05,
      "loss": 0.5114,
      "step": 5269
    },
    {
      "epoch": 0.6702702702702703,
      "grad_norm": 2.450780153274536,
      "learning_rate": 6.597938144329897e-05,
      "loss": 0.8352,
      "step": 5270
    },
    {
      "epoch": 0.6703974562798092,
      "grad_norm": 2.5556414127349854,
      "learning_rate": 6.595392643502609e-05,
      "loss": 0.642,
      "step": 5271
    },
    {
      "epoch": 0.6705246422893482,
      "grad_norm": 2.1931872367858887,
      "learning_rate": 6.592847142675321e-05,
      "loss": 0.6039,
      "step": 5272
    },
    {
      "epoch": 0.6706518282988871,
      "grad_norm": 2.477386474609375,
      "learning_rate": 6.590301641848034e-05,
      "loss": 0.8103,
      "step": 5273
    },
    {
      "epoch": 0.6707790143084261,
      "grad_norm": 3.2341690063476562,
      "learning_rate": 6.587756141020747e-05,
      "loss": 0.5752,
      "step": 5274
    },
    {
      "epoch": 0.670906200317965,
      "grad_norm": 1.847279667854309,
      "learning_rate": 6.585210640193459e-05,
      "loss": 0.4445,
      "step": 5275
    },
    {
      "epoch": 0.671033386327504,
      "grad_norm": 2.0452942848205566,
      "learning_rate": 6.582665139366171e-05,
      "loss": 0.6019,
      "step": 5276
    },
    {
      "epoch": 0.671160572337043,
      "grad_norm": 1.8006868362426758,
      "learning_rate": 6.580119638538883e-05,
      "loss": 0.4856,
      "step": 5277
    },
    {
      "epoch": 0.6712877583465818,
      "grad_norm": 2.198967456817627,
      "learning_rate": 6.577574137711595e-05,
      "loss": 0.7595,
      "step": 5278
    },
    {
      "epoch": 0.6714149443561208,
      "grad_norm": 3.478806972503662,
      "learning_rate": 6.575028636884307e-05,
      "loss": 0.5679,
      "step": 5279
    },
    {
      "epoch": 0.6715421303656598,
      "grad_norm": 1.8977104425430298,
      "learning_rate": 6.57248313605702e-05,
      "loss": 0.4934,
      "step": 5280
    },
    {
      "epoch": 0.6716693163751988,
      "grad_norm": 1.8496642112731934,
      "learning_rate": 6.569937635229732e-05,
      "loss": 0.5422,
      "step": 5281
    },
    {
      "epoch": 0.6717965023847376,
      "grad_norm": 2.6255288124084473,
      "learning_rate": 6.567392134402444e-05,
      "loss": 0.5504,
      "step": 5282
    },
    {
      "epoch": 0.6719236883942766,
      "grad_norm": 2.5908517837524414,
      "learning_rate": 6.564846633575155e-05,
      "loss": 0.6595,
      "step": 5283
    },
    {
      "epoch": 0.6720508744038156,
      "grad_norm": 1.9175132513046265,
      "learning_rate": 6.562301132747869e-05,
      "loss": 0.4443,
      "step": 5284
    },
    {
      "epoch": 0.6721780604133545,
      "grad_norm": 2.0714473724365234,
      "learning_rate": 6.55975563192058e-05,
      "loss": 0.6147,
      "step": 5285
    },
    {
      "epoch": 0.6723052464228935,
      "grad_norm": 3.2199909687042236,
      "learning_rate": 6.557210131093294e-05,
      "loss": 0.5101,
      "step": 5286
    },
    {
      "epoch": 0.6724324324324324,
      "grad_norm": 2.28882098197937,
      "learning_rate": 6.554664630266006e-05,
      "loss": 0.654,
      "step": 5287
    },
    {
      "epoch": 0.6725596184419714,
      "grad_norm": 2.022789478302002,
      "learning_rate": 6.552119129438718e-05,
      "loss": 0.4643,
      "step": 5288
    },
    {
      "epoch": 0.6726868044515103,
      "grad_norm": 1.6072638034820557,
      "learning_rate": 6.54957362861143e-05,
      "loss": 0.3699,
      "step": 5289
    },
    {
      "epoch": 0.6728139904610493,
      "grad_norm": 1.985214114189148,
      "learning_rate": 6.547028127784141e-05,
      "loss": 0.4944,
      "step": 5290
    },
    {
      "epoch": 0.6729411764705883,
      "grad_norm": 2.286072015762329,
      "learning_rate": 6.544482626956853e-05,
      "loss": 0.5813,
      "step": 5291
    },
    {
      "epoch": 0.6730683624801271,
      "grad_norm": 2.15704607963562,
      "learning_rate": 6.541937126129566e-05,
      "loss": 0.5582,
      "step": 5292
    },
    {
      "epoch": 0.6731955484896661,
      "grad_norm": 1.7924124002456665,
      "learning_rate": 6.539391625302278e-05,
      "loss": 0.4968,
      "step": 5293
    },
    {
      "epoch": 0.6733227344992051,
      "grad_norm": 2.809819459915161,
      "learning_rate": 6.536846124474992e-05,
      "loss": 0.6545,
      "step": 5294
    },
    {
      "epoch": 0.6734499205087441,
      "grad_norm": 1.5595461130142212,
      "learning_rate": 6.534300623647703e-05,
      "loss": 0.5799,
      "step": 5295
    },
    {
      "epoch": 0.673577106518283,
      "grad_norm": 1.606716513633728,
      "learning_rate": 6.531755122820415e-05,
      "loss": 0.5181,
      "step": 5296
    },
    {
      "epoch": 0.6737042925278219,
      "grad_norm": 2.8414618968963623,
      "learning_rate": 6.529209621993127e-05,
      "loss": 0.898,
      "step": 5297
    },
    {
      "epoch": 0.6738314785373609,
      "grad_norm": 1.6411696672439575,
      "learning_rate": 6.526664121165839e-05,
      "loss": 0.7243,
      "step": 5298
    },
    {
      "epoch": 0.6739586645468998,
      "grad_norm": 2.223788261413574,
      "learning_rate": 6.524118620338552e-05,
      "loss": 0.5971,
      "step": 5299
    },
    {
      "epoch": 0.6740858505564388,
      "grad_norm": 1.9401766061782837,
      "learning_rate": 6.521573119511264e-05,
      "loss": 0.9195,
      "step": 5300
    },
    {
      "epoch": 0.6742130365659778,
      "grad_norm": 2.1549794673919678,
      "learning_rate": 6.519027618683976e-05,
      "loss": 0.4456,
      "step": 5301
    },
    {
      "epoch": 0.6743402225755167,
      "grad_norm": 1.8518847227096558,
      "learning_rate": 6.516482117856688e-05,
      "loss": 0.3429,
      "step": 5302
    },
    {
      "epoch": 0.6744674085850556,
      "grad_norm": 2.371591329574585,
      "learning_rate": 6.513936617029401e-05,
      "loss": 0.5144,
      "step": 5303
    },
    {
      "epoch": 0.6745945945945946,
      "grad_norm": 1.6954345703125,
      "learning_rate": 6.511391116202113e-05,
      "loss": 0.3709,
      "step": 5304
    },
    {
      "epoch": 0.6747217806041336,
      "grad_norm": 2.789283275604248,
      "learning_rate": 6.508845615374826e-05,
      "loss": 0.738,
      "step": 5305
    },
    {
      "epoch": 0.6748489666136724,
      "grad_norm": 2.984220504760742,
      "learning_rate": 6.506300114547538e-05,
      "loss": 0.6433,
      "step": 5306
    },
    {
      "epoch": 0.6749761526232114,
      "grad_norm": 2.135143280029297,
      "learning_rate": 6.50375461372025e-05,
      "loss": 0.6537,
      "step": 5307
    },
    {
      "epoch": 0.6751033386327504,
      "grad_norm": 2.9258320331573486,
      "learning_rate": 6.501209112892962e-05,
      "loss": 0.8262,
      "step": 5308
    },
    {
      "epoch": 0.6752305246422894,
      "grad_norm": 2.358736038208008,
      "learning_rate": 6.498663612065674e-05,
      "loss": 0.6632,
      "step": 5309
    },
    {
      "epoch": 0.6753577106518283,
      "grad_norm": 1.6873060464859009,
      "learning_rate": 6.496118111238386e-05,
      "loss": 0.6261,
      "step": 5310
    },
    {
      "epoch": 0.6754848966613672,
      "grad_norm": 2.4057748317718506,
      "learning_rate": 6.493572610411099e-05,
      "loss": 0.4735,
      "step": 5311
    },
    {
      "epoch": 0.6756120826709062,
      "grad_norm": 1.9893320798873901,
      "learning_rate": 6.491027109583811e-05,
      "loss": 0.4894,
      "step": 5312
    },
    {
      "epoch": 0.6757392686804452,
      "grad_norm": 2.5407986640930176,
      "learning_rate": 6.488481608756524e-05,
      "loss": 0.6978,
      "step": 5313
    },
    {
      "epoch": 0.6758664546899841,
      "grad_norm": 2.092345714569092,
      "learning_rate": 6.485936107929236e-05,
      "loss": 0.4074,
      "step": 5314
    },
    {
      "epoch": 0.6759936406995231,
      "grad_norm": 1.9937896728515625,
      "learning_rate": 6.483390607101948e-05,
      "loss": 0.6466,
      "step": 5315
    },
    {
      "epoch": 0.676120826709062,
      "grad_norm": 2.3228070735931396,
      "learning_rate": 6.48084510627466e-05,
      "loss": 0.4934,
      "step": 5316
    },
    {
      "epoch": 0.6762480127186009,
      "grad_norm": 2.10373854637146,
      "learning_rate": 6.478299605447373e-05,
      "loss": 0.6001,
      "step": 5317
    },
    {
      "epoch": 0.6763751987281399,
      "grad_norm": 2.229238748550415,
      "learning_rate": 6.475754104620085e-05,
      "loss": 0.3908,
      "step": 5318
    },
    {
      "epoch": 0.6765023847376789,
      "grad_norm": 1.9506840705871582,
      "learning_rate": 6.473208603792797e-05,
      "loss": 0.6221,
      "step": 5319
    },
    {
      "epoch": 0.6766295707472179,
      "grad_norm": 2.4703781604766846,
      "learning_rate": 6.470663102965508e-05,
      "loss": 0.4626,
      "step": 5320
    },
    {
      "epoch": 0.6767567567567567,
      "grad_norm": 2.6831977367401123,
      "learning_rate": 6.46811760213822e-05,
      "loss": 0.7258,
      "step": 5321
    },
    {
      "epoch": 0.6768839427662957,
      "grad_norm": 2.582188606262207,
      "learning_rate": 6.465572101310932e-05,
      "loss": 0.5273,
      "step": 5322
    },
    {
      "epoch": 0.6770111287758347,
      "grad_norm": 2.139237642288208,
      "learning_rate": 6.463026600483645e-05,
      "loss": 0.6609,
      "step": 5323
    },
    {
      "epoch": 0.6771383147853736,
      "grad_norm": 2.126530408859253,
      "learning_rate": 6.460481099656359e-05,
      "loss": 0.6522,
      "step": 5324
    },
    {
      "epoch": 0.6772655007949125,
      "grad_norm": 2.282681465148926,
      "learning_rate": 6.45793559882907e-05,
      "loss": 0.3624,
      "step": 5325
    },
    {
      "epoch": 0.6773926868044515,
      "grad_norm": 1.7829246520996094,
      "learning_rate": 6.455390098001782e-05,
      "loss": 0.4013,
      "step": 5326
    },
    {
      "epoch": 0.6775198728139905,
      "grad_norm": 1.8542884588241577,
      "learning_rate": 6.452844597174494e-05,
      "loss": 0.6168,
      "step": 5327
    },
    {
      "epoch": 0.6776470588235294,
      "grad_norm": 1.6910436153411865,
      "learning_rate": 6.450299096347206e-05,
      "loss": 0.4751,
      "step": 5328
    },
    {
      "epoch": 0.6777742448330684,
      "grad_norm": 2.545480251312256,
      "learning_rate": 6.447753595519919e-05,
      "loss": 0.4634,
      "step": 5329
    },
    {
      "epoch": 0.6779014308426073,
      "grad_norm": 1.977250337600708,
      "learning_rate": 6.445208094692631e-05,
      "loss": 0.6829,
      "step": 5330
    },
    {
      "epoch": 0.6780286168521462,
      "grad_norm": 2.3697397708892822,
      "learning_rate": 6.442662593865343e-05,
      "loss": 0.6863,
      "step": 5331
    },
    {
      "epoch": 0.6781558028616852,
      "grad_norm": 2.715852737426758,
      "learning_rate": 6.440117093038055e-05,
      "loss": 0.6875,
      "step": 5332
    },
    {
      "epoch": 0.6782829888712242,
      "grad_norm": 2.0851547718048096,
      "learning_rate": 6.437571592210768e-05,
      "loss": 0.6078,
      "step": 5333
    },
    {
      "epoch": 0.6784101748807632,
      "grad_norm": 2.6018946170806885,
      "learning_rate": 6.43502609138348e-05,
      "loss": 0.5914,
      "step": 5334
    },
    {
      "epoch": 0.678537360890302,
      "grad_norm": 1.8604406118392944,
      "learning_rate": 6.432480590556192e-05,
      "loss": 0.5894,
      "step": 5335
    },
    {
      "epoch": 0.678664546899841,
      "grad_norm": 2.3451313972473145,
      "learning_rate": 6.429935089728905e-05,
      "loss": 0.5682,
      "step": 5336
    },
    {
      "epoch": 0.67879173290938,
      "grad_norm": 2.4913954734802246,
      "learning_rate": 6.427389588901617e-05,
      "loss": 0.5203,
      "step": 5337
    },
    {
      "epoch": 0.6789189189189189,
      "grad_norm": 1.5705249309539795,
      "learning_rate": 6.424844088074329e-05,
      "loss": 0.4668,
      "step": 5338
    },
    {
      "epoch": 0.6790461049284578,
      "grad_norm": 2.010953426361084,
      "learning_rate": 6.422298587247041e-05,
      "loss": 0.612,
      "step": 5339
    },
    {
      "epoch": 0.6791732909379968,
      "grad_norm": 2.045802116394043,
      "learning_rate": 6.419753086419753e-05,
      "loss": 0.6301,
      "step": 5340
    },
    {
      "epoch": 0.6793004769475358,
      "grad_norm": 2.5949320793151855,
      "learning_rate": 6.417207585592465e-05,
      "loss": 0.796,
      "step": 5341
    },
    {
      "epoch": 0.6794276629570747,
      "grad_norm": 2.726654529571533,
      "learning_rate": 6.414662084765178e-05,
      "loss": 0.5459,
      "step": 5342
    },
    {
      "epoch": 0.6795548489666137,
      "grad_norm": 2.5845015048980713,
      "learning_rate": 6.412116583937891e-05,
      "loss": 0.3888,
      "step": 5343
    },
    {
      "epoch": 0.6796820349761526,
      "grad_norm": 1.347054123878479,
      "learning_rate": 6.409571083110603e-05,
      "loss": 0.3881,
      "step": 5344
    },
    {
      "epoch": 0.6798092209856915,
      "grad_norm": 1.6793566942214966,
      "learning_rate": 6.407025582283315e-05,
      "loss": 0.4517,
      "step": 5345
    },
    {
      "epoch": 0.6799364069952305,
      "grad_norm": 2.1932878494262695,
      "learning_rate": 6.404480081456027e-05,
      "loss": 0.6425,
      "step": 5346
    },
    {
      "epoch": 0.6800635930047695,
      "grad_norm": 2.163646936416626,
      "learning_rate": 6.401934580628738e-05,
      "loss": 0.6563,
      "step": 5347
    },
    {
      "epoch": 0.6801907790143085,
      "grad_norm": 2.2978415489196777,
      "learning_rate": 6.399389079801452e-05,
      "loss": 0.6815,
      "step": 5348
    },
    {
      "epoch": 0.6803179650238473,
      "grad_norm": 1.8197740316390991,
      "learning_rate": 6.396843578974164e-05,
      "loss": 0.5796,
      "step": 5349
    },
    {
      "epoch": 0.6804451510333863,
      "grad_norm": 1.6225627660751343,
      "learning_rate": 6.394298078146875e-05,
      "loss": 0.4274,
      "step": 5350
    },
    {
      "epoch": 0.6805723370429253,
      "grad_norm": 1.9937264919281006,
      "learning_rate": 6.391752577319587e-05,
      "loss": 0.5768,
      "step": 5351
    },
    {
      "epoch": 0.6806995230524643,
      "grad_norm": 2.0799853801727295,
      "learning_rate": 6.3892070764923e-05,
      "loss": 0.538,
      "step": 5352
    },
    {
      "epoch": 0.6808267090620032,
      "grad_norm": 2.5977437496185303,
      "learning_rate": 6.386661575665012e-05,
      "loss": 0.4272,
      "step": 5353
    },
    {
      "epoch": 0.6809538950715421,
      "grad_norm": 1.917716383934021,
      "learning_rate": 6.384116074837724e-05,
      "loss": 0.4673,
      "step": 5354
    },
    {
      "epoch": 0.6810810810810811,
      "grad_norm": 2.074714422225952,
      "learning_rate": 6.381570574010438e-05,
      "loss": 0.5212,
      "step": 5355
    },
    {
      "epoch": 0.68120826709062,
      "grad_norm": 2.7952609062194824,
      "learning_rate": 6.37902507318315e-05,
      "loss": 0.749,
      "step": 5356
    },
    {
      "epoch": 0.681335453100159,
      "grad_norm": 1.7201727628707886,
      "learning_rate": 6.376479572355861e-05,
      "loss": 0.4316,
      "step": 5357
    },
    {
      "epoch": 0.681462639109698,
      "grad_norm": 2.2663567066192627,
      "learning_rate": 6.373934071528573e-05,
      "loss": 0.3945,
      "step": 5358
    },
    {
      "epoch": 0.6815898251192369,
      "grad_norm": 2.181901216506958,
      "learning_rate": 6.371388570701285e-05,
      "loss": 0.4082,
      "step": 5359
    },
    {
      "epoch": 0.6817170111287758,
      "grad_norm": 2.056459665298462,
      "learning_rate": 6.368843069873998e-05,
      "loss": 0.5833,
      "step": 5360
    },
    {
      "epoch": 0.6818441971383148,
      "grad_norm": 2.691866397857666,
      "learning_rate": 6.36629756904671e-05,
      "loss": 0.4437,
      "step": 5361
    },
    {
      "epoch": 0.6819713831478538,
      "grad_norm": 3.2759406566619873,
      "learning_rate": 6.363752068219423e-05,
      "loss": 0.7389,
      "step": 5362
    },
    {
      "epoch": 0.6820985691573926,
      "grad_norm": 2.6378676891326904,
      "learning_rate": 6.361206567392135e-05,
      "loss": 0.588,
      "step": 5363
    },
    {
      "epoch": 0.6822257551669316,
      "grad_norm": 2.223691463470459,
      "learning_rate": 6.358661066564847e-05,
      "loss": 0.5892,
      "step": 5364
    },
    {
      "epoch": 0.6823529411764706,
      "grad_norm": 2.6894047260284424,
      "learning_rate": 6.356115565737559e-05,
      "loss": 0.4724,
      "step": 5365
    },
    {
      "epoch": 0.6824801271860096,
      "grad_norm": 2.3249261379241943,
      "learning_rate": 6.353570064910271e-05,
      "loss": 0.424,
      "step": 5366
    },
    {
      "epoch": 0.6826073131955485,
      "grad_norm": 1.9508588314056396,
      "learning_rate": 6.351024564082984e-05,
      "loss": 0.5041,
      "step": 5367
    },
    {
      "epoch": 0.6827344992050874,
      "grad_norm": 2.6062910556793213,
      "learning_rate": 6.348479063255696e-05,
      "loss": 0.7191,
      "step": 5368
    },
    {
      "epoch": 0.6828616852146264,
      "grad_norm": 2.230064868927002,
      "learning_rate": 6.345933562428408e-05,
      "loss": 0.3957,
      "step": 5369
    },
    {
      "epoch": 0.6829888712241653,
      "grad_norm": 2.228760242462158,
      "learning_rate": 6.34338806160112e-05,
      "loss": 0.6034,
      "step": 5370
    },
    {
      "epoch": 0.6831160572337043,
      "grad_norm": 2.030855894088745,
      "learning_rate": 6.340842560773833e-05,
      "loss": 0.5161,
      "step": 5371
    },
    {
      "epoch": 0.6832432432432433,
      "grad_norm": 1.6367545127868652,
      "learning_rate": 6.338297059946545e-05,
      "loss": 0.6367,
      "step": 5372
    },
    {
      "epoch": 0.6833704292527822,
      "grad_norm": 2.44952654838562,
      "learning_rate": 6.335751559119257e-05,
      "loss": 0.6397,
      "step": 5373
    },
    {
      "epoch": 0.6834976152623211,
      "grad_norm": 2.0676863193511963,
      "learning_rate": 6.33320605829197e-05,
      "loss": 0.4686,
      "step": 5374
    },
    {
      "epoch": 0.6836248012718601,
      "grad_norm": 2.32637619972229,
      "learning_rate": 6.330660557464682e-05,
      "loss": 0.6235,
      "step": 5375
    },
    {
      "epoch": 0.6837519872813991,
      "grad_norm": 1.9930012226104736,
      "learning_rate": 6.328115056637394e-05,
      "loss": 0.5995,
      "step": 5376
    },
    {
      "epoch": 0.683879173290938,
      "grad_norm": 3.1289241313934326,
      "learning_rate": 6.325569555810106e-05,
      "loss": 0.6921,
      "step": 5377
    },
    {
      "epoch": 0.6840063593004769,
      "grad_norm": 2.578150749206543,
      "learning_rate": 6.323024054982817e-05,
      "loss": 0.7903,
      "step": 5378
    },
    {
      "epoch": 0.6841335453100159,
      "grad_norm": 1.8999865055084229,
      "learning_rate": 6.32047855415553e-05,
      "loss": 0.5156,
      "step": 5379
    },
    {
      "epoch": 0.6842607313195549,
      "grad_norm": 2.107099771499634,
      "learning_rate": 6.317933053328243e-05,
      "loss": 0.5775,
      "step": 5380
    },
    {
      "epoch": 0.6843879173290938,
      "grad_norm": 2.5153517723083496,
      "learning_rate": 6.315387552500956e-05,
      "loss": 0.6728,
      "step": 5381
    },
    {
      "epoch": 0.6845151033386327,
      "grad_norm": 3.032930612564087,
      "learning_rate": 6.312842051673668e-05,
      "loss": 0.8195,
      "step": 5382
    },
    {
      "epoch": 0.6846422893481717,
      "grad_norm": 2.4000086784362793,
      "learning_rate": 6.31029655084638e-05,
      "loss": 0.7729,
      "step": 5383
    },
    {
      "epoch": 0.6847694753577106,
      "grad_norm": 1.737504243850708,
      "learning_rate": 6.307751050019091e-05,
      "loss": 0.4433,
      "step": 5384
    },
    {
      "epoch": 0.6848966613672496,
      "grad_norm": 1.929711103439331,
      "learning_rate": 6.305205549191803e-05,
      "loss": 0.5389,
      "step": 5385
    },
    {
      "epoch": 0.6850238473767886,
      "grad_norm": 2.5576140880584717,
      "learning_rate": 6.302660048364516e-05,
      "loss": 0.7544,
      "step": 5386
    },
    {
      "epoch": 0.6851510333863275,
      "grad_norm": 1.6694490909576416,
      "learning_rate": 6.300114547537228e-05,
      "loss": 0.6232,
      "step": 5387
    },
    {
      "epoch": 0.6852782193958664,
      "grad_norm": 1.7013354301452637,
      "learning_rate": 6.29756904670994e-05,
      "loss": 0.5153,
      "step": 5388
    },
    {
      "epoch": 0.6854054054054054,
      "grad_norm": 2.727231979370117,
      "learning_rate": 6.295023545882652e-05,
      "loss": 0.5478,
      "step": 5389
    },
    {
      "epoch": 0.6855325914149444,
      "grad_norm": 1.4939205646514893,
      "learning_rate": 6.292478045055364e-05,
      "loss": 0.4256,
      "step": 5390
    },
    {
      "epoch": 0.6856597774244833,
      "grad_norm": 2.121396541595459,
      "learning_rate": 6.289932544228077e-05,
      "loss": 0.4245,
      "step": 5391
    },
    {
      "epoch": 0.6857869634340222,
      "grad_norm": 2.1628754138946533,
      "learning_rate": 6.28738704340079e-05,
      "loss": 0.4741,
      "step": 5392
    },
    {
      "epoch": 0.6859141494435612,
      "grad_norm": 2.3520705699920654,
      "learning_rate": 6.284841542573502e-05,
      "loss": 0.7157,
      "step": 5393
    },
    {
      "epoch": 0.6860413354531002,
      "grad_norm": 1.8765350580215454,
      "learning_rate": 6.282296041746214e-05,
      "loss": 0.3586,
      "step": 5394
    },
    {
      "epoch": 0.6861685214626391,
      "grad_norm": 3.3455889225006104,
      "learning_rate": 6.279750540918926e-05,
      "loss": 0.7519,
      "step": 5395
    },
    {
      "epoch": 0.686295707472178,
      "grad_norm": 1.9860522747039795,
      "learning_rate": 6.277205040091638e-05,
      "loss": 0.6458,
      "step": 5396
    },
    {
      "epoch": 0.686422893481717,
      "grad_norm": 2.832815170288086,
      "learning_rate": 6.27465953926435e-05,
      "loss": 0.666,
      "step": 5397
    },
    {
      "epoch": 0.686550079491256,
      "grad_norm": 3.483774423599243,
      "learning_rate": 6.272114038437063e-05,
      "loss": 0.5998,
      "step": 5398
    },
    {
      "epoch": 0.6866772655007949,
      "grad_norm": 2.616530179977417,
      "learning_rate": 6.269568537609775e-05,
      "loss": 0.7548,
      "step": 5399
    },
    {
      "epoch": 0.6868044515103339,
      "grad_norm": 2.494570255279541,
      "learning_rate": 6.267023036782487e-05,
      "loss": 0.8043,
      "step": 5400
    },
    {
      "epoch": 0.6869316375198729,
      "grad_norm": 2.421506881713867,
      "learning_rate": 6.2644775359552e-05,
      "loss": 0.677,
      "step": 5401
    },
    {
      "epoch": 0.6870588235294117,
      "grad_norm": 1.784486174583435,
      "learning_rate": 6.261932035127912e-05,
      "loss": 0.4549,
      "step": 5402
    },
    {
      "epoch": 0.6871860095389507,
      "grad_norm": 2.1650662422180176,
      "learning_rate": 6.259386534300624e-05,
      "loss": 0.4827,
      "step": 5403
    },
    {
      "epoch": 0.6873131955484897,
      "grad_norm": 2.1612236499786377,
      "learning_rate": 6.256841033473336e-05,
      "loss": 0.5662,
      "step": 5404
    },
    {
      "epoch": 0.6874403815580287,
      "grad_norm": 1.870291829109192,
      "learning_rate": 6.254295532646049e-05,
      "loss": 0.4976,
      "step": 5405
    },
    {
      "epoch": 0.6875675675675675,
      "grad_norm": 2.4812564849853516,
      "learning_rate": 6.251750031818761e-05,
      "loss": 0.6948,
      "step": 5406
    },
    {
      "epoch": 0.6876947535771065,
      "grad_norm": 2.318251371383667,
      "learning_rate": 6.249204530991473e-05,
      "loss": 0.466,
      "step": 5407
    },
    {
      "epoch": 0.6878219395866455,
      "grad_norm": 2.1604557037353516,
      "learning_rate": 6.246659030164184e-05,
      "loss": 0.5699,
      "step": 5408
    },
    {
      "epoch": 0.6879491255961844,
      "grad_norm": 1.4591249227523804,
      "learning_rate": 6.244113529336896e-05,
      "loss": 0.4923,
      "step": 5409
    },
    {
      "epoch": 0.6880763116057234,
      "grad_norm": 2.289872884750366,
      "learning_rate": 6.24156802850961e-05,
      "loss": 0.6149,
      "step": 5410
    },
    {
      "epoch": 0.6882034976152623,
      "grad_norm": 1.6823174953460693,
      "learning_rate": 6.239022527682323e-05,
      "loss": 0.5514,
      "step": 5411
    },
    {
      "epoch": 0.6883306836248013,
      "grad_norm": 1.958799958229065,
      "learning_rate": 6.236477026855035e-05,
      "loss": 0.6503,
      "step": 5412
    },
    {
      "epoch": 0.6884578696343402,
      "grad_norm": 3.2244677543640137,
      "learning_rate": 6.233931526027747e-05,
      "loss": 0.5536,
      "step": 5413
    },
    {
      "epoch": 0.6885850556438792,
      "grad_norm": 1.9537014961242676,
      "learning_rate": 6.231386025200458e-05,
      "loss": 0.5633,
      "step": 5414
    },
    {
      "epoch": 0.6887122416534182,
      "grad_norm": 1.7717394828796387,
      "learning_rate": 6.22884052437317e-05,
      "loss": 0.6095,
      "step": 5415
    },
    {
      "epoch": 0.688839427662957,
      "grad_norm": 2.402982234954834,
      "learning_rate": 6.226295023545882e-05,
      "loss": 0.3755,
      "step": 5416
    },
    {
      "epoch": 0.688966613672496,
      "grad_norm": 2.063852071762085,
      "learning_rate": 6.223749522718595e-05,
      "loss": 0.5402,
      "step": 5417
    },
    {
      "epoch": 0.689093799682035,
      "grad_norm": 2.8433213233947754,
      "learning_rate": 6.221204021891307e-05,
      "loss": 0.5185,
      "step": 5418
    },
    {
      "epoch": 0.689220985691574,
      "grad_norm": 3.4467685222625732,
      "learning_rate": 6.218658521064019e-05,
      "loss": 0.5649,
      "step": 5419
    },
    {
      "epoch": 0.6893481717011128,
      "grad_norm": 3.1764447689056396,
      "learning_rate": 6.216113020236732e-05,
      "loss": 0.6772,
      "step": 5420
    },
    {
      "epoch": 0.6894753577106518,
      "grad_norm": 2.05732798576355,
      "learning_rate": 6.213567519409444e-05,
      "loss": 0.4397,
      "step": 5421
    },
    {
      "epoch": 0.6896025437201908,
      "grad_norm": 1.7351149320602417,
      "learning_rate": 6.211022018582156e-05,
      "loss": 0.3726,
      "step": 5422
    },
    {
      "epoch": 0.6897297297297297,
      "grad_norm": 1.6509143114089966,
      "learning_rate": 6.20847651775487e-05,
      "loss": 0.5067,
      "step": 5423
    },
    {
      "epoch": 0.6898569157392687,
      "grad_norm": 2.3052775859832764,
      "learning_rate": 6.205931016927581e-05,
      "loss": 0.4638,
      "step": 5424
    },
    {
      "epoch": 0.6899841017488076,
      "grad_norm": 1.6820241212844849,
      "learning_rate": 6.203385516100293e-05,
      "loss": 0.408,
      "step": 5425
    },
    {
      "epoch": 0.6901112877583466,
      "grad_norm": 2.4376022815704346,
      "learning_rate": 6.200840015273005e-05,
      "loss": 0.6234,
      "step": 5426
    },
    {
      "epoch": 0.6902384737678855,
      "grad_norm": 1.8678252696990967,
      "learning_rate": 6.198294514445717e-05,
      "loss": 0.5942,
      "step": 5427
    },
    {
      "epoch": 0.6903656597774245,
      "grad_norm": 2.4078168869018555,
      "learning_rate": 6.195749013618429e-05,
      "loss": 0.5086,
      "step": 5428
    },
    {
      "epoch": 0.6904928457869635,
      "grad_norm": 2.235555648803711,
      "learning_rate": 6.193203512791142e-05,
      "loss": 0.5771,
      "step": 5429
    },
    {
      "epoch": 0.6906200317965023,
      "grad_norm": 1.9343609809875488,
      "learning_rate": 6.190658011963855e-05,
      "loss": 0.4771,
      "step": 5430
    },
    {
      "epoch": 0.6907472178060413,
      "grad_norm": 3.067218542098999,
      "learning_rate": 6.188112511136567e-05,
      "loss": 0.7764,
      "step": 5431
    },
    {
      "epoch": 0.6908744038155803,
      "grad_norm": 3.0284340381622314,
      "learning_rate": 6.185567010309279e-05,
      "loss": 0.4633,
      "step": 5432
    },
    {
      "epoch": 0.6910015898251193,
      "grad_norm": 2.82842755317688,
      "learning_rate": 6.183021509481991e-05,
      "loss": 0.4029,
      "step": 5433
    },
    {
      "epoch": 0.6911287758346581,
      "grad_norm": 2.302098512649536,
      "learning_rate": 6.180476008654703e-05,
      "loss": 0.4741,
      "step": 5434
    },
    {
      "epoch": 0.6912559618441971,
      "grad_norm": 1.944333791732788,
      "learning_rate": 6.177930507827416e-05,
      "loss": 0.3407,
      "step": 5435
    },
    {
      "epoch": 0.6913831478537361,
      "grad_norm": 2.765167474746704,
      "learning_rate": 6.175385007000128e-05,
      "loss": 0.4845,
      "step": 5436
    },
    {
      "epoch": 0.6915103338632751,
      "grad_norm": 2.4453773498535156,
      "learning_rate": 6.17283950617284e-05,
      "loss": 0.315,
      "step": 5437
    },
    {
      "epoch": 0.691637519872814,
      "grad_norm": 2.5595333576202393,
      "learning_rate": 6.170294005345552e-05,
      "loss": 0.5648,
      "step": 5438
    },
    {
      "epoch": 0.691764705882353,
      "grad_norm": 3.759535312652588,
      "learning_rate": 6.167748504518265e-05,
      "loss": 0.7278,
      "step": 5439
    },
    {
      "epoch": 0.6918918918918919,
      "grad_norm": 1.9078824520111084,
      "learning_rate": 6.165203003690977e-05,
      "loss": 0.5042,
      "step": 5440
    },
    {
      "epoch": 0.6920190779014308,
      "grad_norm": 2.3608553409576416,
      "learning_rate": 6.162657502863689e-05,
      "loss": 0.4236,
      "step": 5441
    },
    {
      "epoch": 0.6921462639109698,
      "grad_norm": 1.9948595762252808,
      "learning_rate": 6.160112002036402e-05,
      "loss": 0.6253,
      "step": 5442
    },
    {
      "epoch": 0.6922734499205088,
      "grad_norm": 3.1080360412597656,
      "learning_rate": 6.157566501209114e-05,
      "loss": 0.4742,
      "step": 5443
    },
    {
      "epoch": 0.6924006359300477,
      "grad_norm": 1.9518163204193115,
      "learning_rate": 6.155021000381825e-05,
      "loss": 0.3088,
      "step": 5444
    },
    {
      "epoch": 0.6925278219395866,
      "grad_norm": 2.690164089202881,
      "learning_rate": 6.152475499554537e-05,
      "loss": 0.6819,
      "step": 5445
    },
    {
      "epoch": 0.6926550079491256,
      "grad_norm": 2.5631966590881348,
      "learning_rate": 6.149929998727249e-05,
      "loss": 0.694,
      "step": 5446
    },
    {
      "epoch": 0.6927821939586646,
      "grad_norm": 2.371727228164673,
      "learning_rate": 6.147384497899961e-05,
      "loss": 0.7769,
      "step": 5447
    },
    {
      "epoch": 0.6929093799682035,
      "grad_norm": 2.3873541355133057,
      "learning_rate": 6.144838997072674e-05,
      "loss": 0.403,
      "step": 5448
    },
    {
      "epoch": 0.6930365659777424,
      "grad_norm": 2.937572479248047,
      "learning_rate": 6.142293496245388e-05,
      "loss": 0.6783,
      "step": 5449
    },
    {
      "epoch": 0.6931637519872814,
      "grad_norm": 2.436431407928467,
      "learning_rate": 6.1397479954181e-05,
      "loss": 0.4963,
      "step": 5450
    },
    {
      "epoch": 0.6932909379968204,
      "grad_norm": 3.9373762607574463,
      "learning_rate": 6.137202494590811e-05,
      "loss": 0.6747,
      "step": 5451
    },
    {
      "epoch": 0.6934181240063593,
      "grad_norm": 2.4391798973083496,
      "learning_rate": 6.134656993763523e-05,
      "loss": 0.6514,
      "step": 5452
    },
    {
      "epoch": 0.6935453100158983,
      "grad_norm": 2.0898172855377197,
      "learning_rate": 6.132111492936235e-05,
      "loss": 0.5345,
      "step": 5453
    },
    {
      "epoch": 0.6936724960254372,
      "grad_norm": 1.397110939025879,
      "learning_rate": 6.129565992108948e-05,
      "loss": 0.3526,
      "step": 5454
    },
    {
      "epoch": 0.6937996820349761,
      "grad_norm": 2.398456573486328,
      "learning_rate": 6.12702049128166e-05,
      "loss": 0.6969,
      "step": 5455
    },
    {
      "epoch": 0.6939268680445151,
      "grad_norm": 2.219635009765625,
      "learning_rate": 6.124474990454372e-05,
      "loss": 0.7335,
      "step": 5456
    },
    {
      "epoch": 0.6940540540540541,
      "grad_norm": 2.191608428955078,
      "learning_rate": 6.121929489627084e-05,
      "loss": 0.5147,
      "step": 5457
    },
    {
      "epoch": 0.694181240063593,
      "grad_norm": 2.04672908782959,
      "learning_rate": 6.119383988799796e-05,
      "loss": 0.4568,
      "step": 5458
    },
    {
      "epoch": 0.6943084260731319,
      "grad_norm": 2.3833656311035156,
      "learning_rate": 6.116838487972509e-05,
      "loss": 0.5933,
      "step": 5459
    },
    {
      "epoch": 0.6944356120826709,
      "grad_norm": 1.6061780452728271,
      "learning_rate": 6.114292987145221e-05,
      "loss": 0.3715,
      "step": 5460
    },
    {
      "epoch": 0.6945627980922099,
      "grad_norm": 2.0061898231506348,
      "learning_rate": 6.111747486317934e-05,
      "loss": 0.4187,
      "step": 5461
    },
    {
      "epoch": 0.6946899841017488,
      "grad_norm": 2.683192729949951,
      "learning_rate": 6.109201985490646e-05,
      "loss": 0.5115,
      "step": 5462
    },
    {
      "epoch": 0.6948171701112877,
      "grad_norm": 1.7994773387908936,
      "learning_rate": 6.106656484663358e-05,
      "loss": 0.3536,
      "step": 5463
    },
    {
      "epoch": 0.6949443561208267,
      "grad_norm": 2.444676399230957,
      "learning_rate": 6.10411098383607e-05,
      "loss": 0.8261,
      "step": 5464
    },
    {
      "epoch": 0.6950715421303657,
      "grad_norm": 2.130319356918335,
      "learning_rate": 6.101565483008782e-05,
      "loss": 0.4382,
      "step": 5465
    },
    {
      "epoch": 0.6951987281399046,
      "grad_norm": 1.6337014436721802,
      "learning_rate": 6.099019982181494e-05,
      "loss": 0.3713,
      "step": 5466
    },
    {
      "epoch": 0.6953259141494436,
      "grad_norm": 2.098379611968994,
      "learning_rate": 6.096474481354206e-05,
      "loss": 0.5135,
      "step": 5467
    },
    {
      "epoch": 0.6954531001589825,
      "grad_norm": 2.006108522415161,
      "learning_rate": 6.093928980526919e-05,
      "loss": 0.4816,
      "step": 5468
    },
    {
      "epoch": 0.6955802861685214,
      "grad_norm": 3.0004189014434814,
      "learning_rate": 6.091383479699632e-05,
      "loss": 0.534,
      "step": 5469
    },
    {
      "epoch": 0.6957074721780604,
      "grad_norm": 2.1070640087127686,
      "learning_rate": 6.088837978872344e-05,
      "loss": 0.4631,
      "step": 5470
    },
    {
      "epoch": 0.6958346581875994,
      "grad_norm": 1.6861085891723633,
      "learning_rate": 6.0862924780450556e-05,
      "loss": 0.3159,
      "step": 5471
    },
    {
      "epoch": 0.6959618441971384,
      "grad_norm": 1.9978184700012207,
      "learning_rate": 6.083746977217768e-05,
      "loss": 0.7121,
      "step": 5472
    },
    {
      "epoch": 0.6960890302066772,
      "grad_norm": 3.3457937240600586,
      "learning_rate": 6.08120147639048e-05,
      "loss": 0.7294,
      "step": 5473
    },
    {
      "epoch": 0.6962162162162162,
      "grad_norm": 1.971684217453003,
      "learning_rate": 6.0786559755631925e-05,
      "loss": 0.4325,
      "step": 5474
    },
    {
      "epoch": 0.6963434022257552,
      "grad_norm": 2.221733808517456,
      "learning_rate": 6.0761104747359044e-05,
      "loss": 0.5793,
      "step": 5475
    },
    {
      "epoch": 0.6964705882352941,
      "grad_norm": 2.616032600402832,
      "learning_rate": 6.073564973908616e-05,
      "loss": 0.84,
      "step": 5476
    },
    {
      "epoch": 0.696597774244833,
      "grad_norm": 1.7474318742752075,
      "learning_rate": 6.071019473081329e-05,
      "loss": 0.5803,
      "step": 5477
    },
    {
      "epoch": 0.696724960254372,
      "grad_norm": 2.581468105316162,
      "learning_rate": 6.068473972254042e-05,
      "loss": 0.6677,
      "step": 5478
    },
    {
      "epoch": 0.696852146263911,
      "grad_norm": 2.8615143299102783,
      "learning_rate": 6.065928471426754e-05,
      "loss": 0.7654,
      "step": 5479
    },
    {
      "epoch": 0.6969793322734499,
      "grad_norm": 3.3916869163513184,
      "learning_rate": 6.063382970599466e-05,
      "loss": 0.3916,
      "step": 5480
    },
    {
      "epoch": 0.6971065182829889,
      "grad_norm": 1.715464472770691,
      "learning_rate": 6.0608374697721784e-05,
      "loss": 0.5857,
      "step": 5481
    },
    {
      "epoch": 0.6972337042925278,
      "grad_norm": 1.88027024269104,
      "learning_rate": 6.05829196894489e-05,
      "loss": 0.5187,
      "step": 5482
    },
    {
      "epoch": 0.6973608903020668,
      "grad_norm": 2.603020429611206,
      "learning_rate": 6.055746468117602e-05,
      "loss": 0.5358,
      "step": 5483
    },
    {
      "epoch": 0.6974880763116057,
      "grad_norm": 2.386507749557495,
      "learning_rate": 6.053200967290315e-05,
      "loss": 0.8416,
      "step": 5484
    },
    {
      "epoch": 0.6976152623211447,
      "grad_norm": 2.3419065475463867,
      "learning_rate": 6.0506554664630265e-05,
      "loss": 0.543,
      "step": 5485
    },
    {
      "epoch": 0.6977424483306837,
      "grad_norm": 2.7571611404418945,
      "learning_rate": 6.0481099656357384e-05,
      "loss": 0.5337,
      "step": 5486
    },
    {
      "epoch": 0.6978696343402225,
      "grad_norm": 1.7023953199386597,
      "learning_rate": 6.045564464808451e-05,
      "loss": 0.3686,
      "step": 5487
    },
    {
      "epoch": 0.6979968203497615,
      "grad_norm": 2.6323235034942627,
      "learning_rate": 6.043018963981164e-05,
      "loss": 1.1024,
      "step": 5488
    },
    {
      "epoch": 0.6981240063593005,
      "grad_norm": 2.871966600418091,
      "learning_rate": 6.040473463153876e-05,
      "loss": 0.5169,
      "step": 5489
    },
    {
      "epoch": 0.6982511923688395,
      "grad_norm": 2.2133944034576416,
      "learning_rate": 6.0379279623265886e-05,
      "loss": 0.4999,
      "step": 5490
    },
    {
      "epoch": 0.6983783783783784,
      "grad_norm": 2.148651123046875,
      "learning_rate": 6.0353824614993005e-05,
      "loss": 0.759,
      "step": 5491
    },
    {
      "epoch": 0.6985055643879173,
      "grad_norm": 2.1162335872650146,
      "learning_rate": 6.0328369606720124e-05,
      "loss": 0.5931,
      "step": 5492
    },
    {
      "epoch": 0.6986327503974563,
      "grad_norm": 1.9720364809036255,
      "learning_rate": 6.030291459844725e-05,
      "loss": 0.5157,
      "step": 5493
    },
    {
      "epoch": 0.6987599364069952,
      "grad_norm": 2.132298707962036,
      "learning_rate": 6.027745959017437e-05,
      "loss": 0.4023,
      "step": 5494
    },
    {
      "epoch": 0.6988871224165342,
      "grad_norm": 2.8409016132354736,
      "learning_rate": 6.025200458190149e-05,
      "loss": 0.8118,
      "step": 5495
    },
    {
      "epoch": 0.6990143084260731,
      "grad_norm": 2.9794719219207764,
      "learning_rate": 6.022654957362861e-05,
      "loss": 0.6469,
      "step": 5496
    },
    {
      "epoch": 0.6991414944356121,
      "grad_norm": 2.4745376110076904,
      "learning_rate": 6.020109456535573e-05,
      "loss": 0.4672,
      "step": 5497
    },
    {
      "epoch": 0.699268680445151,
      "grad_norm": 2.308242082595825,
      "learning_rate": 6.017563955708286e-05,
      "loss": 0.8212,
      "step": 5498
    },
    {
      "epoch": 0.69939586645469,
      "grad_norm": 3.0034821033477783,
      "learning_rate": 6.015018454880998e-05,
      "loss": 0.7347,
      "step": 5499
    },
    {
      "epoch": 0.699523052464229,
      "grad_norm": 1.728977084159851,
      "learning_rate": 6.012472954053711e-05,
      "loss": 0.3684,
      "step": 5500
    },
    {
      "epoch": 0.6996502384737678,
      "grad_norm": 2.116943836212158,
      "learning_rate": 6.0099274532264226e-05,
      "loss": 0.5274,
      "step": 5501
    },
    {
      "epoch": 0.6997774244833068,
      "grad_norm": 2.3873579502105713,
      "learning_rate": 6.0073819523991345e-05,
      "loss": 0.5057,
      "step": 5502
    },
    {
      "epoch": 0.6999046104928458,
      "grad_norm": 2.881471872329712,
      "learning_rate": 6.004836451571847e-05,
      "loss": 0.8684,
      "step": 5503
    },
    {
      "epoch": 0.7000317965023848,
      "grad_norm": 2.227187156677246,
      "learning_rate": 6.002290950744559e-05,
      "loss": 0.602,
      "step": 5504
    },
    {
      "epoch": 0.7001589825119237,
      "grad_norm": 2.433041572570801,
      "learning_rate": 5.9997454499172715e-05,
      "loss": 0.6098,
      "step": 5505
    },
    {
      "epoch": 0.7002861685214626,
      "grad_norm": 2.4426093101501465,
      "learning_rate": 5.9971999490899833e-05,
      "loss": 0.8353,
      "step": 5506
    },
    {
      "epoch": 0.7004133545310016,
      "grad_norm": 2.3353209495544434,
      "learning_rate": 5.9946544482626966e-05,
      "loss": 0.5022,
      "step": 5507
    },
    {
      "epoch": 0.7005405405405405,
      "grad_norm": 2.699345588684082,
      "learning_rate": 5.9921089474354085e-05,
      "loss": 0.8314,
      "step": 5508
    },
    {
      "epoch": 0.7006677265500795,
      "grad_norm": 2.32285475730896,
      "learning_rate": 5.989563446608121e-05,
      "loss": 0.6832,
      "step": 5509
    },
    {
      "epoch": 0.7007949125596185,
      "grad_norm": 3.085359573364258,
      "learning_rate": 5.987017945780833e-05,
      "loss": 0.8835,
      "step": 5510
    },
    {
      "epoch": 0.7009220985691574,
      "grad_norm": 1.5715957880020142,
      "learning_rate": 5.984472444953545e-05,
      "loss": 0.4463,
      "step": 5511
    },
    {
      "epoch": 0.7010492845786963,
      "grad_norm": 2.705683708190918,
      "learning_rate": 5.981926944126257e-05,
      "loss": 0.4284,
      "step": 5512
    },
    {
      "epoch": 0.7011764705882353,
      "grad_norm": 1.5801098346710205,
      "learning_rate": 5.979381443298969e-05,
      "loss": 0.4588,
      "step": 5513
    },
    {
      "epoch": 0.7013036565977743,
      "grad_norm": 2.506405830383301,
      "learning_rate": 5.976835942471681e-05,
      "loss": 0.6713,
      "step": 5514
    },
    {
      "epoch": 0.7014308426073131,
      "grad_norm": 2.5683434009552,
      "learning_rate": 5.9742904416443936e-05,
      "loss": 0.9041,
      "step": 5515
    },
    {
      "epoch": 0.7015580286168521,
      "grad_norm": 2.3215978145599365,
      "learning_rate": 5.9717449408171055e-05,
      "loss": 0.4131,
      "step": 5516
    },
    {
      "epoch": 0.7016852146263911,
      "grad_norm": 2.101485013961792,
      "learning_rate": 5.969199439989819e-05,
      "loss": 0.4879,
      "step": 5517
    },
    {
      "epoch": 0.7018124006359301,
      "grad_norm": 1.3959673643112183,
      "learning_rate": 5.9666539391625306e-05,
      "loss": 0.5219,
      "step": 5518
    },
    {
      "epoch": 0.701939586645469,
      "grad_norm": 1.5652737617492676,
      "learning_rate": 5.964108438335243e-05,
      "loss": 0.4106,
      "step": 5519
    },
    {
      "epoch": 0.7020667726550079,
      "grad_norm": 2.1613848209381104,
      "learning_rate": 5.961562937507955e-05,
      "loss": 0.4214,
      "step": 5520
    },
    {
      "epoch": 0.7021939586645469,
      "grad_norm": 2.220242977142334,
      "learning_rate": 5.9590174366806676e-05,
      "loss": 0.5432,
      "step": 5521
    },
    {
      "epoch": 0.7023211446740858,
      "grad_norm": 2.538607358932495,
      "learning_rate": 5.9564719358533794e-05,
      "loss": 0.5821,
      "step": 5522
    },
    {
      "epoch": 0.7024483306836248,
      "grad_norm": 2.12813663482666,
      "learning_rate": 5.953926435026091e-05,
      "loss": 0.5879,
      "step": 5523
    },
    {
      "epoch": 0.7025755166931638,
      "grad_norm": 2.3786964416503906,
      "learning_rate": 5.951380934198804e-05,
      "loss": 0.6057,
      "step": 5524
    },
    {
      "epoch": 0.7027027027027027,
      "grad_norm": 2.1221537590026855,
      "learning_rate": 5.948835433371516e-05,
      "loss": 0.5062,
      "step": 5525
    },
    {
      "epoch": 0.7028298887122416,
      "grad_norm": 3.062178134918213,
      "learning_rate": 5.9462899325442276e-05,
      "loss": 0.6492,
      "step": 5526
    },
    {
      "epoch": 0.7029570747217806,
      "grad_norm": 2.31959867477417,
      "learning_rate": 5.943744431716941e-05,
      "loss": 0.4146,
      "step": 5527
    },
    {
      "epoch": 0.7030842607313196,
      "grad_norm": 2.178450584411621,
      "learning_rate": 5.9411989308896534e-05,
      "loss": 0.795,
      "step": 5528
    },
    {
      "epoch": 0.7032114467408586,
      "grad_norm": 2.152470350265503,
      "learning_rate": 5.938653430062365e-05,
      "loss": 0.4576,
      "step": 5529
    },
    {
      "epoch": 0.7033386327503974,
      "grad_norm": 2.0165371894836426,
      "learning_rate": 5.936107929235077e-05,
      "loss": 0.5713,
      "step": 5530
    },
    {
      "epoch": 0.7034658187599364,
      "grad_norm": 2.6771252155303955,
      "learning_rate": 5.93356242840779e-05,
      "loss": 0.7515,
      "step": 5531
    },
    {
      "epoch": 0.7035930047694754,
      "grad_norm": 1.8750303983688354,
      "learning_rate": 5.9310169275805016e-05,
      "loss": 0.4558,
      "step": 5532
    },
    {
      "epoch": 0.7037201907790143,
      "grad_norm": 2.212704658508301,
      "learning_rate": 5.9284714267532134e-05,
      "loss": 0.6565,
      "step": 5533
    },
    {
      "epoch": 0.7038473767885532,
      "grad_norm": 2.1014304161071777,
      "learning_rate": 5.925925925925926e-05,
      "loss": 0.4615,
      "step": 5534
    },
    {
      "epoch": 0.7039745627980922,
      "grad_norm": 1.2814381122589111,
      "learning_rate": 5.923380425098638e-05,
      "loss": 0.2667,
      "step": 5535
    },
    {
      "epoch": 0.7041017488076312,
      "grad_norm": 2.296597480773926,
      "learning_rate": 5.920834924271351e-05,
      "loss": 0.6002,
      "step": 5536
    },
    {
      "epoch": 0.7042289348171701,
      "grad_norm": 1.6373143196105957,
      "learning_rate": 5.9182894234440636e-05,
      "loss": 0.4419,
      "step": 5537
    },
    {
      "epoch": 0.7043561208267091,
      "grad_norm": 2.82313871383667,
      "learning_rate": 5.9157439226167755e-05,
      "loss": 0.8462,
      "step": 5538
    },
    {
      "epoch": 0.704483306836248,
      "grad_norm": 2.6762008666992188,
      "learning_rate": 5.9131984217894874e-05,
      "loss": 0.5503,
      "step": 5539
    },
    {
      "epoch": 0.7046104928457869,
      "grad_norm": 2.454960823059082,
      "learning_rate": 5.9106529209622e-05,
      "loss": 0.6788,
      "step": 5540
    },
    {
      "epoch": 0.7047376788553259,
      "grad_norm": 2.150211811065674,
      "learning_rate": 5.908107420134912e-05,
      "loss": 0.3702,
      "step": 5541
    },
    {
      "epoch": 0.7048648648648649,
      "grad_norm": 3.08324933052063,
      "learning_rate": 5.905561919307624e-05,
      "loss": 0.6176,
      "step": 5542
    },
    {
      "epoch": 0.7049920508744039,
      "grad_norm": 2.102910280227661,
      "learning_rate": 5.903016418480336e-05,
      "loss": 0.4364,
      "step": 5543
    },
    {
      "epoch": 0.7051192368839427,
      "grad_norm": 1.9656480550765991,
      "learning_rate": 5.900470917653048e-05,
      "loss": 0.3529,
      "step": 5544
    },
    {
      "epoch": 0.7052464228934817,
      "grad_norm": 4.103466033935547,
      "learning_rate": 5.89792541682576e-05,
      "loss": 0.8358,
      "step": 5545
    },
    {
      "epoch": 0.7053736089030207,
      "grad_norm": 2.826707363128662,
      "learning_rate": 5.895379915998473e-05,
      "loss": 0.5953,
      "step": 5546
    },
    {
      "epoch": 0.7055007949125596,
      "grad_norm": 2.3453683853149414,
      "learning_rate": 5.892834415171186e-05,
      "loss": 0.6179,
      "step": 5547
    },
    {
      "epoch": 0.7056279809220986,
      "grad_norm": 2.646327495574951,
      "learning_rate": 5.8902889143438976e-05,
      "loss": 0.6538,
      "step": 5548
    },
    {
      "epoch": 0.7057551669316375,
      "grad_norm": 2.5830821990966797,
      "learning_rate": 5.8877434135166095e-05,
      "loss": 0.4491,
      "step": 5549
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 2.571185350418091,
      "learning_rate": 5.885197912689322e-05,
      "loss": 0.4221,
      "step": 5550
    },
    {
      "epoch": 0.7060095389507154,
      "grad_norm": 3.260023832321167,
      "learning_rate": 5.882652411862034e-05,
      "loss": 0.7633,
      "step": 5551
    },
    {
      "epoch": 0.7061367249602544,
      "grad_norm": 2.184908628463745,
      "learning_rate": 5.8801069110347465e-05,
      "loss": 0.6785,
      "step": 5552
    },
    {
      "epoch": 0.7062639109697934,
      "grad_norm": 3.625121831893921,
      "learning_rate": 5.8775614102074584e-05,
      "loss": 0.731,
      "step": 5553
    },
    {
      "epoch": 0.7063910969793322,
      "grad_norm": 1.9353406429290771,
      "learning_rate": 5.87501590938017e-05,
      "loss": 0.5002,
      "step": 5554
    },
    {
      "epoch": 0.7065182829888712,
      "grad_norm": 2.4476168155670166,
      "learning_rate": 5.872470408552883e-05,
      "loss": 0.7787,
      "step": 5555
    },
    {
      "epoch": 0.7066454689984102,
      "grad_norm": 2.031043767929077,
      "learning_rate": 5.869924907725596e-05,
      "loss": 0.5258,
      "step": 5556
    },
    {
      "epoch": 0.7067726550079492,
      "grad_norm": 1.9946630001068115,
      "learning_rate": 5.867379406898308e-05,
      "loss": 0.357,
      "step": 5557
    },
    {
      "epoch": 0.706899841017488,
      "grad_norm": 2.29341721534729,
      "learning_rate": 5.86483390607102e-05,
      "loss": 0.8087,
      "step": 5558
    },
    {
      "epoch": 0.707027027027027,
      "grad_norm": 2.1339845657348633,
      "learning_rate": 5.862288405243732e-05,
      "loss": 0.5362,
      "step": 5559
    },
    {
      "epoch": 0.707154213036566,
      "grad_norm": 2.223175048828125,
      "learning_rate": 5.859742904416444e-05,
      "loss": 0.6227,
      "step": 5560
    },
    {
      "epoch": 0.7072813990461049,
      "grad_norm": 2.0563971996307373,
      "learning_rate": 5.857197403589156e-05,
      "loss": 0.7869,
      "step": 5561
    },
    {
      "epoch": 0.7074085850556439,
      "grad_norm": 2.3040695190429688,
      "learning_rate": 5.8546519027618686e-05,
      "loss": 0.364,
      "step": 5562
    },
    {
      "epoch": 0.7075357710651828,
      "grad_norm": 2.4790148735046387,
      "learning_rate": 5.8521064019345805e-05,
      "loss": 0.5707,
      "step": 5563
    },
    {
      "epoch": 0.7076629570747218,
      "grad_norm": 3.963177442550659,
      "learning_rate": 5.849560901107293e-05,
      "loss": 0.4252,
      "step": 5564
    },
    {
      "epoch": 0.7077901430842607,
      "grad_norm": 2.0304362773895264,
      "learning_rate": 5.847015400280005e-05,
      "loss": 0.4941,
      "step": 5565
    },
    {
      "epoch": 0.7079173290937997,
      "grad_norm": 1.9493448734283447,
      "learning_rate": 5.844469899452718e-05,
      "loss": 0.4481,
      "step": 5566
    },
    {
      "epoch": 0.7080445151033387,
      "grad_norm": 2.6884210109710693,
      "learning_rate": 5.84192439862543e-05,
      "loss": 0.4681,
      "step": 5567
    },
    {
      "epoch": 0.7081717011128776,
      "grad_norm": 2.392601490020752,
      "learning_rate": 5.8393788977981426e-05,
      "loss": 0.5656,
      "step": 5568
    },
    {
      "epoch": 0.7082988871224165,
      "grad_norm": 1.817012071609497,
      "learning_rate": 5.8368333969708544e-05,
      "loss": 0.5068,
      "step": 5569
    },
    {
      "epoch": 0.7084260731319555,
      "grad_norm": 2.476247787475586,
      "learning_rate": 5.834287896143566e-05,
      "loss": 0.7343,
      "step": 5570
    },
    {
      "epoch": 0.7085532591414945,
      "grad_norm": 1.7244597673416138,
      "learning_rate": 5.831742395316279e-05,
      "loss": 0.6255,
      "step": 5571
    },
    {
      "epoch": 0.7086804451510333,
      "grad_norm": 1.828331470489502,
      "learning_rate": 5.829196894488991e-05,
      "loss": 0.5423,
      "step": 5572
    },
    {
      "epoch": 0.7088076311605723,
      "grad_norm": 1.7797565460205078,
      "learning_rate": 5.8266513936617026e-05,
      "loss": 0.5038,
      "step": 5573
    },
    {
      "epoch": 0.7089348171701113,
      "grad_norm": 1.1771360635757446,
      "learning_rate": 5.824105892834415e-05,
      "loss": 0.4244,
      "step": 5574
    },
    {
      "epoch": 0.7090620031796503,
      "grad_norm": 1.9988147020339966,
      "learning_rate": 5.8215603920071284e-05,
      "loss": 0.4695,
      "step": 5575
    },
    {
      "epoch": 0.7091891891891892,
      "grad_norm": 2.596390724182129,
      "learning_rate": 5.81901489117984e-05,
      "loss": 0.4549,
      "step": 5576
    },
    {
      "epoch": 0.7093163751987281,
      "grad_norm": 2.7754931449890137,
      "learning_rate": 5.816469390352552e-05,
      "loss": 0.5864,
      "step": 5577
    },
    {
      "epoch": 0.7094435612082671,
      "grad_norm": 2.0955636501312256,
      "learning_rate": 5.813923889525265e-05,
      "loss": 0.589,
      "step": 5578
    },
    {
      "epoch": 0.709570747217806,
      "grad_norm": 2.079951286315918,
      "learning_rate": 5.8113783886979766e-05,
      "loss": 0.5859,
      "step": 5579
    },
    {
      "epoch": 0.709697933227345,
      "grad_norm": 3.7764575481414795,
      "learning_rate": 5.808832887870689e-05,
      "loss": 0.7613,
      "step": 5580
    },
    {
      "epoch": 0.709825119236884,
      "grad_norm": 2.5877175331115723,
      "learning_rate": 5.806287387043401e-05,
      "loss": 0.563,
      "step": 5581
    },
    {
      "epoch": 0.7099523052464229,
      "grad_norm": 2.1377625465393066,
      "learning_rate": 5.803741886216113e-05,
      "loss": 0.5468,
      "step": 5582
    },
    {
      "epoch": 0.7100794912559618,
      "grad_norm": 2.6487889289855957,
      "learning_rate": 5.8011963853888254e-05,
      "loss": 0.5952,
      "step": 5583
    },
    {
      "epoch": 0.7102066772655008,
      "grad_norm": 2.917985200881958,
      "learning_rate": 5.798650884561537e-05,
      "loss": 0.7023,
      "step": 5584
    },
    {
      "epoch": 0.7103338632750398,
      "grad_norm": 2.230949878692627,
      "learning_rate": 5.7961053837342505e-05,
      "loss": 0.4432,
      "step": 5585
    },
    {
      "epoch": 0.7104610492845786,
      "grad_norm": 2.935612201690674,
      "learning_rate": 5.7935598829069624e-05,
      "loss": 0.7277,
      "step": 5586
    },
    {
      "epoch": 0.7105882352941176,
      "grad_norm": 2.023017644882202,
      "learning_rate": 5.791014382079675e-05,
      "loss": 0.5006,
      "step": 5587
    },
    {
      "epoch": 0.7107154213036566,
      "grad_norm": 2.697768449783325,
      "learning_rate": 5.788468881252387e-05,
      "loss": 0.5084,
      "step": 5588
    },
    {
      "epoch": 0.7108426073131956,
      "grad_norm": 2.665006399154663,
      "learning_rate": 5.785923380425099e-05,
      "loss": 0.6145,
      "step": 5589
    },
    {
      "epoch": 0.7109697933227345,
      "grad_norm": 2.6856184005737305,
      "learning_rate": 5.783377879597811e-05,
      "loss": 0.4353,
      "step": 5590
    },
    {
      "epoch": 0.7110969793322734,
      "grad_norm": 2.7736074924468994,
      "learning_rate": 5.780832378770523e-05,
      "loss": 0.6133,
      "step": 5591
    },
    {
      "epoch": 0.7112241653418124,
      "grad_norm": 1.7642379999160767,
      "learning_rate": 5.778286877943235e-05,
      "loss": 0.3565,
      "step": 5592
    },
    {
      "epoch": 0.7113513513513513,
      "grad_norm": 1.9368535280227661,
      "learning_rate": 5.7757413771159476e-05,
      "loss": 0.4951,
      "step": 5593
    },
    {
      "epoch": 0.7114785373608903,
      "grad_norm": 2.8425116539001465,
      "learning_rate": 5.7731958762886594e-05,
      "loss": 0.571,
      "step": 5594
    },
    {
      "epoch": 0.7116057233704293,
      "grad_norm": 2.202455520629883,
      "learning_rate": 5.7706503754613727e-05,
      "loss": 0.5939,
      "step": 5595
    },
    {
      "epoch": 0.7117329093799682,
      "grad_norm": 2.197228193283081,
      "learning_rate": 5.768104874634085e-05,
      "loss": 0.6693,
      "step": 5596
    },
    {
      "epoch": 0.7118600953895071,
      "grad_norm": 2.29154109954834,
      "learning_rate": 5.765559373806797e-05,
      "loss": 0.4263,
      "step": 5597
    },
    {
      "epoch": 0.7119872813990461,
      "grad_norm": 1.651027798652649,
      "learning_rate": 5.763013872979509e-05,
      "loss": 0.4909,
      "step": 5598
    },
    {
      "epoch": 0.7121144674085851,
      "grad_norm": 2.280672073364258,
      "learning_rate": 5.7604683721522215e-05,
      "loss": 0.6693,
      "step": 5599
    },
    {
      "epoch": 0.712241653418124,
      "grad_norm": 2.2456655502319336,
      "learning_rate": 5.7579228713249334e-05,
      "loss": 0.5517,
      "step": 5600
    },
    {
      "epoch": 0.7123688394276629,
      "grad_norm": 2.4246137142181396,
      "learning_rate": 5.755377370497645e-05,
      "loss": 0.6926,
      "step": 5601
    },
    {
      "epoch": 0.7124960254372019,
      "grad_norm": 1.860888957977295,
      "learning_rate": 5.752831869670358e-05,
      "loss": 0.5431,
      "step": 5602
    },
    {
      "epoch": 0.7126232114467409,
      "grad_norm": 1.5326054096221924,
      "learning_rate": 5.75028636884307e-05,
      "loss": 0.5579,
      "step": 5603
    },
    {
      "epoch": 0.7127503974562798,
      "grad_norm": 2.079657793045044,
      "learning_rate": 5.747740868015783e-05,
      "loss": 0.6562,
      "step": 5604
    },
    {
      "epoch": 0.7128775834658188,
      "grad_norm": 2.3105616569519043,
      "learning_rate": 5.745195367188495e-05,
      "loss": 0.3676,
      "step": 5605
    },
    {
      "epoch": 0.7130047694753577,
      "grad_norm": 1.6762148141860962,
      "learning_rate": 5.742649866361207e-05,
      "loss": 0.6525,
      "step": 5606
    },
    {
      "epoch": 0.7131319554848966,
      "grad_norm": 2.2605879306793213,
      "learning_rate": 5.740104365533919e-05,
      "loss": 0.8184,
      "step": 5607
    },
    {
      "epoch": 0.7132591414944356,
      "grad_norm": 3.072235584259033,
      "learning_rate": 5.737558864706631e-05,
      "loss": 0.991,
      "step": 5608
    },
    {
      "epoch": 0.7133863275039746,
      "grad_norm": 1.8703101873397827,
      "learning_rate": 5.7350133638793436e-05,
      "loss": 0.5652,
      "step": 5609
    },
    {
      "epoch": 0.7135135135135136,
      "grad_norm": 2.174487352371216,
      "learning_rate": 5.7324678630520555e-05,
      "loss": 0.3653,
      "step": 5610
    },
    {
      "epoch": 0.7136406995230524,
      "grad_norm": 2.9505438804626465,
      "learning_rate": 5.729922362224768e-05,
      "loss": 0.4838,
      "step": 5611
    },
    {
      "epoch": 0.7137678855325914,
      "grad_norm": 1.9657976627349854,
      "learning_rate": 5.72737686139748e-05,
      "loss": 0.4852,
      "step": 5612
    },
    {
      "epoch": 0.7138950715421304,
      "grad_norm": 2.582587718963623,
      "learning_rate": 5.724831360570192e-05,
      "loss": 0.7127,
      "step": 5613
    },
    {
      "epoch": 0.7140222575516694,
      "grad_norm": 2.1675682067871094,
      "learning_rate": 5.722285859742905e-05,
      "loss": 0.4971,
      "step": 5614
    },
    {
      "epoch": 0.7141494435612082,
      "grad_norm": 2.00476336479187,
      "learning_rate": 5.7197403589156176e-05,
      "loss": 0.514,
      "step": 5615
    },
    {
      "epoch": 0.7142766295707472,
      "grad_norm": 3.008803129196167,
      "learning_rate": 5.7171948580883295e-05,
      "loss": 0.5884,
      "step": 5616
    },
    {
      "epoch": 0.7144038155802862,
      "grad_norm": 2.108811855316162,
      "learning_rate": 5.714649357261041e-05,
      "loss": 0.3737,
      "step": 5617
    },
    {
      "epoch": 0.7145310015898251,
      "grad_norm": 2.9376299381256104,
      "learning_rate": 5.712103856433754e-05,
      "loss": 0.8748,
      "step": 5618
    },
    {
      "epoch": 0.7146581875993641,
      "grad_norm": 2.370922803878784,
      "learning_rate": 5.709558355606466e-05,
      "loss": 0.5346,
      "step": 5619
    },
    {
      "epoch": 0.714785373608903,
      "grad_norm": 3.111170530319214,
      "learning_rate": 5.7070128547791776e-05,
      "loss": 0.6588,
      "step": 5620
    },
    {
      "epoch": 0.714912559618442,
      "grad_norm": 2.302551031112671,
      "learning_rate": 5.70446735395189e-05,
      "loss": 0.7384,
      "step": 5621
    },
    {
      "epoch": 0.7150397456279809,
      "grad_norm": 2.134988784790039,
      "learning_rate": 5.701921853124602e-05,
      "loss": 0.4717,
      "step": 5622
    },
    {
      "epoch": 0.7151669316375199,
      "grad_norm": 1.687092661857605,
      "learning_rate": 5.699376352297314e-05,
      "loss": 0.5283,
      "step": 5623
    },
    {
      "epoch": 0.7152941176470589,
      "grad_norm": 2.6276156902313232,
      "learning_rate": 5.696830851470027e-05,
      "loss": 0.7319,
      "step": 5624
    },
    {
      "epoch": 0.7154213036565977,
      "grad_norm": 2.1555674076080322,
      "learning_rate": 5.69428535064274e-05,
      "loss": 0.6213,
      "step": 5625
    },
    {
      "epoch": 0.7155484896661367,
      "grad_norm": 1.5220743417739868,
      "learning_rate": 5.6917398498154516e-05,
      "loss": 0.3659,
      "step": 5626
    },
    {
      "epoch": 0.7156756756756757,
      "grad_norm": 2.3332176208496094,
      "learning_rate": 5.689194348988164e-05,
      "loss": 0.4785,
      "step": 5627
    },
    {
      "epoch": 0.7158028616852147,
      "grad_norm": 1.520959496498108,
      "learning_rate": 5.686648848160876e-05,
      "loss": 0.3432,
      "step": 5628
    },
    {
      "epoch": 0.7159300476947535,
      "grad_norm": 2.0067639350891113,
      "learning_rate": 5.684103347333588e-05,
      "loss": 0.6361,
      "step": 5629
    },
    {
      "epoch": 0.7160572337042925,
      "grad_norm": 2.7242305278778076,
      "learning_rate": 5.6815578465063004e-05,
      "loss": 0.5758,
      "step": 5630
    },
    {
      "epoch": 0.7161844197138315,
      "grad_norm": 2.945650577545166,
      "learning_rate": 5.679012345679012e-05,
      "loss": 0.7471,
      "step": 5631
    },
    {
      "epoch": 0.7163116057233704,
      "grad_norm": 2.2510006427764893,
      "learning_rate": 5.676466844851724e-05,
      "loss": 0.4318,
      "step": 5632
    },
    {
      "epoch": 0.7164387917329094,
      "grad_norm": 2.2763750553131104,
      "learning_rate": 5.673921344024437e-05,
      "loss": 0.6543,
      "step": 5633
    },
    {
      "epoch": 0.7165659777424483,
      "grad_norm": 2.21197247505188,
      "learning_rate": 5.67137584319715e-05,
      "loss": 0.5703,
      "step": 5634
    },
    {
      "epoch": 0.7166931637519873,
      "grad_norm": 1.5475972890853882,
      "learning_rate": 5.668830342369862e-05,
      "loss": 0.5356,
      "step": 5635
    },
    {
      "epoch": 0.7168203497615262,
      "grad_norm": 2.490841865539551,
      "learning_rate": 5.666284841542574e-05,
      "loss": 0.5669,
      "step": 5636
    },
    {
      "epoch": 0.7169475357710652,
      "grad_norm": 1.8528118133544922,
      "learning_rate": 5.663739340715286e-05,
      "loss": 0.6656,
      "step": 5637
    },
    {
      "epoch": 0.7170747217806042,
      "grad_norm": 2.1309890747070312,
      "learning_rate": 5.661193839887998e-05,
      "loss": 0.5336,
      "step": 5638
    },
    {
      "epoch": 0.717201907790143,
      "grad_norm": 2.701633930206299,
      "learning_rate": 5.65864833906071e-05,
      "loss": 0.4206,
      "step": 5639
    },
    {
      "epoch": 0.717329093799682,
      "grad_norm": 2.6801259517669678,
      "learning_rate": 5.6561028382334226e-05,
      "loss": 0.8051,
      "step": 5640
    },
    {
      "epoch": 0.717456279809221,
      "grad_norm": 2.5409555435180664,
      "learning_rate": 5.6535573374061344e-05,
      "loss": 0.6223,
      "step": 5641
    },
    {
      "epoch": 0.71758346581876,
      "grad_norm": 1.963691234588623,
      "learning_rate": 5.651011836578847e-05,
      "loss": 0.4539,
      "step": 5642
    },
    {
      "epoch": 0.7177106518282989,
      "grad_norm": 1.9220983982086182,
      "learning_rate": 5.64846633575156e-05,
      "loss": 0.4695,
      "step": 5643
    },
    {
      "epoch": 0.7178378378378378,
      "grad_norm": 2.0385043621063232,
      "learning_rate": 5.645920834924272e-05,
      "loss": 0.3675,
      "step": 5644
    },
    {
      "epoch": 0.7179650238473768,
      "grad_norm": 2.722419500350952,
      "learning_rate": 5.643375334096984e-05,
      "loss": 0.8306,
      "step": 5645
    },
    {
      "epoch": 0.7180922098569157,
      "grad_norm": 2.149730920791626,
      "learning_rate": 5.6408298332696965e-05,
      "loss": 0.6752,
      "step": 5646
    },
    {
      "epoch": 0.7182193958664547,
      "grad_norm": 1.5864683389663696,
      "learning_rate": 5.6382843324424084e-05,
      "loss": 0.5476,
      "step": 5647
    },
    {
      "epoch": 0.7183465818759937,
      "grad_norm": 2.15765118598938,
      "learning_rate": 5.63573883161512e-05,
      "loss": 0.5819,
      "step": 5648
    },
    {
      "epoch": 0.7184737678855326,
      "grad_norm": 1.879051923751831,
      "learning_rate": 5.633193330787833e-05,
      "loss": 0.5017,
      "step": 5649
    },
    {
      "epoch": 0.7186009538950715,
      "grad_norm": 2.4119937419891357,
      "learning_rate": 5.630647829960545e-05,
      "loss": 0.6282,
      "step": 5650
    },
    {
      "epoch": 0.7187281399046105,
      "grad_norm": 2.889697551727295,
      "learning_rate": 5.6281023291332566e-05,
      "loss": 0.6948,
      "step": 5651
    },
    {
      "epoch": 0.7188553259141495,
      "grad_norm": 2.2538225650787354,
      "learning_rate": 5.625556828305969e-05,
      "loss": 0.6691,
      "step": 5652
    },
    {
      "epoch": 0.7189825119236883,
      "grad_norm": 1.628153681755066,
      "learning_rate": 5.6230113274786824e-05,
      "loss": 0.3685,
      "step": 5653
    },
    {
      "epoch": 0.7191096979332273,
      "grad_norm": 2.1817057132720947,
      "learning_rate": 5.620465826651394e-05,
      "loss": 0.3284,
      "step": 5654
    },
    {
      "epoch": 0.7192368839427663,
      "grad_norm": 2.2674059867858887,
      "learning_rate": 5.617920325824106e-05,
      "loss": 0.6142,
      "step": 5655
    },
    {
      "epoch": 0.7193640699523053,
      "grad_norm": 2.241176128387451,
      "learning_rate": 5.6153748249968187e-05,
      "loss": 0.6212,
      "step": 5656
    },
    {
      "epoch": 0.7194912559618442,
      "grad_norm": 2.1218457221984863,
      "learning_rate": 5.6128293241695305e-05,
      "loss": 0.3764,
      "step": 5657
    },
    {
      "epoch": 0.7196184419713831,
      "grad_norm": 2.064774990081787,
      "learning_rate": 5.610283823342243e-05,
      "loss": 0.5173,
      "step": 5658
    },
    {
      "epoch": 0.7197456279809221,
      "grad_norm": 2.2118420600891113,
      "learning_rate": 5.607738322514955e-05,
      "loss": 0.378,
      "step": 5659
    },
    {
      "epoch": 0.7198728139904611,
      "grad_norm": 2.592181444168091,
      "learning_rate": 5.605192821687667e-05,
      "loss": 0.5582,
      "step": 5660
    },
    {
      "epoch": 0.72,
      "grad_norm": 2.9131505489349365,
      "learning_rate": 5.6026473208603794e-05,
      "loss": 0.5114,
      "step": 5661
    },
    {
      "epoch": 0.720127186009539,
      "grad_norm": 2.633347272872925,
      "learning_rate": 5.600101820033091e-05,
      "loss": 0.496,
      "step": 5662
    },
    {
      "epoch": 0.7202543720190779,
      "grad_norm": 2.9195711612701416,
      "learning_rate": 5.5975563192058045e-05,
      "loss": 0.6501,
      "step": 5663
    },
    {
      "epoch": 0.7203815580286168,
      "grad_norm": 2.250664710998535,
      "learning_rate": 5.5950108183785164e-05,
      "loss": 0.5365,
      "step": 5664
    },
    {
      "epoch": 0.7205087440381558,
      "grad_norm": 2.111048698425293,
      "learning_rate": 5.592465317551229e-05,
      "loss": 0.4984,
      "step": 5665
    },
    {
      "epoch": 0.7206359300476948,
      "grad_norm": 1.790875792503357,
      "learning_rate": 5.589919816723941e-05,
      "loss": 0.5864,
      "step": 5666
    },
    {
      "epoch": 0.7207631160572338,
      "grad_norm": 2.1535918712615967,
      "learning_rate": 5.5873743158966527e-05,
      "loss": 0.676,
      "step": 5667
    },
    {
      "epoch": 0.7208903020667726,
      "grad_norm": 1.7185604572296143,
      "learning_rate": 5.584828815069365e-05,
      "loss": 0.4266,
      "step": 5668
    },
    {
      "epoch": 0.7210174880763116,
      "grad_norm": 1.613713026046753,
      "learning_rate": 5.582283314242077e-05,
      "loss": 0.5348,
      "step": 5669
    },
    {
      "epoch": 0.7211446740858506,
      "grad_norm": 2.3491268157958984,
      "learning_rate": 5.5797378134147896e-05,
      "loss": 0.3243,
      "step": 5670
    },
    {
      "epoch": 0.7212718600953895,
      "grad_norm": 1.7954314947128296,
      "learning_rate": 5.5771923125875015e-05,
      "loss": 0.4569,
      "step": 5671
    },
    {
      "epoch": 0.7213990461049284,
      "grad_norm": 2.89289927482605,
      "learning_rate": 5.574646811760215e-05,
      "loss": 1.1162,
      "step": 5672
    },
    {
      "epoch": 0.7215262321144674,
      "grad_norm": 2.641794443130493,
      "learning_rate": 5.5721013109329266e-05,
      "loss": 0.9055,
      "step": 5673
    },
    {
      "epoch": 0.7216534181240064,
      "grad_norm": 1.2642074823379517,
      "learning_rate": 5.569555810105639e-05,
      "loss": 0.3092,
      "step": 5674
    },
    {
      "epoch": 0.7217806041335453,
      "grad_norm": 1.9003233909606934,
      "learning_rate": 5.567010309278351e-05,
      "loss": 0.5345,
      "step": 5675
    },
    {
      "epoch": 0.7219077901430843,
      "grad_norm": 2.405226945877075,
      "learning_rate": 5.564464808451063e-05,
      "loss": 0.395,
      "step": 5676
    },
    {
      "epoch": 0.7220349761526232,
      "grad_norm": 2.314823627471924,
      "learning_rate": 5.5619193076237755e-05,
      "loss": 0.4472,
      "step": 5677
    },
    {
      "epoch": 0.7221621621621621,
      "grad_norm": 2.306318759918213,
      "learning_rate": 5.559373806796487e-05,
      "loss": 0.5188,
      "step": 5678
    },
    {
      "epoch": 0.7222893481717011,
      "grad_norm": 2.655679702758789,
      "learning_rate": 5.556828305969199e-05,
      "loss": 0.4797,
      "step": 5679
    },
    {
      "epoch": 0.7224165341812401,
      "grad_norm": 2.5121848583221436,
      "learning_rate": 5.554282805141912e-05,
      "loss": 0.804,
      "step": 5680
    },
    {
      "epoch": 0.7225437201907791,
      "grad_norm": 1.9769995212554932,
      "learning_rate": 5.5517373043146236e-05,
      "loss": 0.5117,
      "step": 5681
    },
    {
      "epoch": 0.7226709062003179,
      "grad_norm": 1.6239728927612305,
      "learning_rate": 5.549191803487337e-05,
      "loss": 0.5389,
      "step": 5682
    },
    {
      "epoch": 0.7227980922098569,
      "grad_norm": 1.772682547569275,
      "learning_rate": 5.546646302660049e-05,
      "loss": 0.3552,
      "step": 5683
    },
    {
      "epoch": 0.7229252782193959,
      "grad_norm": 2.0450103282928467,
      "learning_rate": 5.544100801832761e-05,
      "loss": 0.4944,
      "step": 5684
    },
    {
      "epoch": 0.7230524642289348,
      "grad_norm": 2.0304253101348877,
      "learning_rate": 5.541555301005473e-05,
      "loss": 0.3333,
      "step": 5685
    },
    {
      "epoch": 0.7231796502384737,
      "grad_norm": 2.364003896713257,
      "learning_rate": 5.539009800178186e-05,
      "loss": 0.5729,
      "step": 5686
    },
    {
      "epoch": 0.7233068362480127,
      "grad_norm": 2.2402114868164062,
      "learning_rate": 5.5364642993508976e-05,
      "loss": 0.5387,
      "step": 5687
    },
    {
      "epoch": 0.7234340222575517,
      "grad_norm": 1.7244031429290771,
      "learning_rate": 5.5339187985236095e-05,
      "loss": 0.277,
      "step": 5688
    },
    {
      "epoch": 0.7235612082670906,
      "grad_norm": 2.1051995754241943,
      "learning_rate": 5.531373297696322e-05,
      "loss": 0.4572,
      "step": 5689
    },
    {
      "epoch": 0.7236883942766296,
      "grad_norm": 1.266600489616394,
      "learning_rate": 5.528827796869034e-05,
      "loss": 0.386,
      "step": 5690
    },
    {
      "epoch": 0.7238155802861685,
      "grad_norm": 2.870461940765381,
      "learning_rate": 5.526282296041746e-05,
      "loss": 0.386,
      "step": 5691
    },
    {
      "epoch": 0.7239427662957074,
      "grad_norm": 3.207277774810791,
      "learning_rate": 5.523736795214459e-05,
      "loss": 0.7112,
      "step": 5692
    },
    {
      "epoch": 0.7240699523052464,
      "grad_norm": 1.985178828239441,
      "learning_rate": 5.5211912943871715e-05,
      "loss": 0.5755,
      "step": 5693
    },
    {
      "epoch": 0.7241971383147854,
      "grad_norm": 1.6328790187835693,
      "learning_rate": 5.5186457935598834e-05,
      "loss": 0.5905,
      "step": 5694
    },
    {
      "epoch": 0.7243243243243244,
      "grad_norm": 2.5575735569000244,
      "learning_rate": 5.516100292732595e-05,
      "loss": 0.8016,
      "step": 5695
    },
    {
      "epoch": 0.7244515103338632,
      "grad_norm": 1.3551892042160034,
      "learning_rate": 5.513554791905308e-05,
      "loss": 0.3788,
      "step": 5696
    },
    {
      "epoch": 0.7245786963434022,
      "grad_norm": 2.82253360748291,
      "learning_rate": 5.51100929107802e-05,
      "loss": 0.6461,
      "step": 5697
    },
    {
      "epoch": 0.7247058823529412,
      "grad_norm": 1.9969733953475952,
      "learning_rate": 5.5084637902507316e-05,
      "loss": 0.6094,
      "step": 5698
    },
    {
      "epoch": 0.7248330683624802,
      "grad_norm": 2.6558496952056885,
      "learning_rate": 5.505918289423444e-05,
      "loss": 0.339,
      "step": 5699
    },
    {
      "epoch": 0.724960254372019,
      "grad_norm": 2.1003153324127197,
      "learning_rate": 5.503372788596156e-05,
      "loss": 0.6083,
      "step": 5700
    },
    {
      "epoch": 0.725087440381558,
      "grad_norm": 2.8414599895477295,
      "learning_rate": 5.5008272877688686e-05,
      "loss": 0.644,
      "step": 5701
    },
    {
      "epoch": 0.725214626391097,
      "grad_norm": 2.5232765674591064,
      "learning_rate": 5.498281786941582e-05,
      "loss": 0.6481,
      "step": 5702
    },
    {
      "epoch": 0.7253418124006359,
      "grad_norm": 1.6912293434143066,
      "learning_rate": 5.495736286114294e-05,
      "loss": 0.3407,
      "step": 5703
    },
    {
      "epoch": 0.7254689984101749,
      "grad_norm": 2.300100564956665,
      "learning_rate": 5.4931907852870055e-05,
      "loss": 0.789,
      "step": 5704
    },
    {
      "epoch": 0.7255961844197139,
      "grad_norm": 2.654453992843628,
      "learning_rate": 5.490645284459718e-05,
      "loss": 0.6077,
      "step": 5705
    },
    {
      "epoch": 0.7257233704292528,
      "grad_norm": 3.3719677925109863,
      "learning_rate": 5.48809978363243e-05,
      "loss": 0.886,
      "step": 5706
    },
    {
      "epoch": 0.7258505564387917,
      "grad_norm": 2.0878348350524902,
      "learning_rate": 5.485554282805142e-05,
      "loss": 0.3276,
      "step": 5707
    },
    {
      "epoch": 0.7259777424483307,
      "grad_norm": 2.676823854446411,
      "learning_rate": 5.4830087819778544e-05,
      "loss": 0.5729,
      "step": 5708
    },
    {
      "epoch": 0.7261049284578697,
      "grad_norm": 1.5176167488098145,
      "learning_rate": 5.480463281150566e-05,
      "loss": 0.3642,
      "step": 5709
    },
    {
      "epoch": 0.7262321144674085,
      "grad_norm": 2.861140251159668,
      "learning_rate": 5.477917780323278e-05,
      "loss": 0.7411,
      "step": 5710
    },
    {
      "epoch": 0.7263593004769475,
      "grad_norm": 3.203589916229248,
      "learning_rate": 5.4753722794959914e-05,
      "loss": 0.8243,
      "step": 5711
    },
    {
      "epoch": 0.7264864864864865,
      "grad_norm": 2.1842384338378906,
      "learning_rate": 5.472826778668704e-05,
      "loss": 0.5248,
      "step": 5712
    },
    {
      "epoch": 0.7266136724960255,
      "grad_norm": 2.3361215591430664,
      "learning_rate": 5.470281277841416e-05,
      "loss": 0.6035,
      "step": 5713
    },
    {
      "epoch": 0.7267408585055644,
      "grad_norm": 1.9611146450042725,
      "learning_rate": 5.467735777014128e-05,
      "loss": 0.5186,
      "step": 5714
    },
    {
      "epoch": 0.7268680445151033,
      "grad_norm": 2.6532089710235596,
      "learning_rate": 5.46519027618684e-05,
      "loss": 0.6221,
      "step": 5715
    },
    {
      "epoch": 0.7269952305246423,
      "grad_norm": 1.9851410388946533,
      "learning_rate": 5.462644775359552e-05,
      "loss": 0.4227,
      "step": 5716
    },
    {
      "epoch": 0.7271224165341812,
      "grad_norm": 2.6149425506591797,
      "learning_rate": 5.4600992745322646e-05,
      "loss": 0.5815,
      "step": 5717
    },
    {
      "epoch": 0.7272496025437202,
      "grad_norm": 2.9164726734161377,
      "learning_rate": 5.4575537737049765e-05,
      "loss": 0.4576,
      "step": 5718
    },
    {
      "epoch": 0.7273767885532592,
      "grad_norm": 1.7279162406921387,
      "learning_rate": 5.4550082728776884e-05,
      "loss": 0.6207,
      "step": 5719
    },
    {
      "epoch": 0.7275039745627981,
      "grad_norm": 2.087954521179199,
      "learning_rate": 5.452462772050401e-05,
      "loss": 0.593,
      "step": 5720
    },
    {
      "epoch": 0.727631160572337,
      "grad_norm": 3.200150728225708,
      "learning_rate": 5.449917271223114e-05,
      "loss": 0.6048,
      "step": 5721
    },
    {
      "epoch": 0.727758346581876,
      "grad_norm": 1.8219952583312988,
      "learning_rate": 5.447371770395826e-05,
      "loss": 0.4085,
      "step": 5722
    },
    {
      "epoch": 0.727885532591415,
      "grad_norm": 1.9267747402191162,
      "learning_rate": 5.444826269568538e-05,
      "loss": 0.3797,
      "step": 5723
    },
    {
      "epoch": 0.7280127186009538,
      "grad_norm": 2.1375808715820312,
      "learning_rate": 5.4422807687412505e-05,
      "loss": 0.6142,
      "step": 5724
    },
    {
      "epoch": 0.7281399046104928,
      "grad_norm": 2.0656354427337646,
      "learning_rate": 5.4397352679139623e-05,
      "loss": 0.5124,
      "step": 5725
    },
    {
      "epoch": 0.7282670906200318,
      "grad_norm": 2.7052106857299805,
      "learning_rate": 5.437189767086674e-05,
      "loss": 0.7775,
      "step": 5726
    },
    {
      "epoch": 0.7283942766295708,
      "grad_norm": 2.377938985824585,
      "learning_rate": 5.434644266259387e-05,
      "loss": 0.7085,
      "step": 5727
    },
    {
      "epoch": 0.7285214626391097,
      "grad_norm": 2.5854756832122803,
      "learning_rate": 5.4320987654320986e-05,
      "loss": 0.5102,
      "step": 5728
    },
    {
      "epoch": 0.7286486486486486,
      "grad_norm": 2.4615578651428223,
      "learning_rate": 5.4295532646048105e-05,
      "loss": 0.7625,
      "step": 5729
    },
    {
      "epoch": 0.7287758346581876,
      "grad_norm": 1.8774248361587524,
      "learning_rate": 5.427007763777523e-05,
      "loss": 0.539,
      "step": 5730
    },
    {
      "epoch": 0.7289030206677265,
      "grad_norm": 1.7339041233062744,
      "learning_rate": 5.424462262950236e-05,
      "loss": 0.4852,
      "step": 5731
    },
    {
      "epoch": 0.7290302066772655,
      "grad_norm": 2.544926166534424,
      "learning_rate": 5.421916762122948e-05,
      "loss": 0.566,
      "step": 5732
    },
    {
      "epoch": 0.7291573926868045,
      "grad_norm": 1.5746433734893799,
      "learning_rate": 5.419371261295661e-05,
      "loss": 0.4255,
      "step": 5733
    },
    {
      "epoch": 0.7292845786963434,
      "grad_norm": 1.9538359642028809,
      "learning_rate": 5.4168257604683726e-05,
      "loss": 0.7185,
      "step": 5734
    },
    {
      "epoch": 0.7294117647058823,
      "grad_norm": 2.8901023864746094,
      "learning_rate": 5.4142802596410845e-05,
      "loss": 0.6686,
      "step": 5735
    },
    {
      "epoch": 0.7295389507154213,
      "grad_norm": 1.762414813041687,
      "learning_rate": 5.411734758813797e-05,
      "loss": 0.6292,
      "step": 5736
    },
    {
      "epoch": 0.7296661367249603,
      "grad_norm": 2.8375296592712402,
      "learning_rate": 5.409189257986509e-05,
      "loss": 0.471,
      "step": 5737
    },
    {
      "epoch": 0.7297933227344992,
      "grad_norm": 1.9599072933197021,
      "learning_rate": 5.406643757159221e-05,
      "loss": 0.4143,
      "step": 5738
    },
    {
      "epoch": 0.7299205087440381,
      "grad_norm": 2.838163375854492,
      "learning_rate": 5.404098256331933e-05,
      "loss": 0.5284,
      "step": 5739
    },
    {
      "epoch": 0.7300476947535771,
      "grad_norm": 2.3597536087036133,
      "learning_rate": 5.4015527555046466e-05,
      "loss": 0.5838,
      "step": 5740
    },
    {
      "epoch": 0.7301748807631161,
      "grad_norm": 2.65767765045166,
      "learning_rate": 5.3990072546773584e-05,
      "loss": 0.7447,
      "step": 5741
    },
    {
      "epoch": 0.730302066772655,
      "grad_norm": 2.248875141143799,
      "learning_rate": 5.39646175385007e-05,
      "loss": 0.5558,
      "step": 5742
    },
    {
      "epoch": 0.730429252782194,
      "grad_norm": 1.9116026163101196,
      "learning_rate": 5.393916253022783e-05,
      "loss": 0.4628,
      "step": 5743
    },
    {
      "epoch": 0.7305564387917329,
      "grad_norm": 1.860456943511963,
      "learning_rate": 5.391370752195495e-05,
      "loss": 0.6707,
      "step": 5744
    },
    {
      "epoch": 0.7306836248012719,
      "grad_norm": 2.2965335845947266,
      "learning_rate": 5.3888252513682066e-05,
      "loss": 0.6472,
      "step": 5745
    },
    {
      "epoch": 0.7308108108108108,
      "grad_norm": 2.9819533824920654,
      "learning_rate": 5.386279750540919e-05,
      "loss": 0.5172,
      "step": 5746
    },
    {
      "epoch": 0.7309379968203498,
      "grad_norm": 2.200627326965332,
      "learning_rate": 5.383734249713631e-05,
      "loss": 0.5809,
      "step": 5747
    },
    {
      "epoch": 0.7310651828298887,
      "grad_norm": 2.180116891860962,
      "learning_rate": 5.3811887488863436e-05,
      "loss": 0.4554,
      "step": 5748
    },
    {
      "epoch": 0.7311923688394276,
      "grad_norm": 1.8635914325714111,
      "learning_rate": 5.3786432480590555e-05,
      "loss": 0.5877,
      "step": 5749
    },
    {
      "epoch": 0.7313195548489666,
      "grad_norm": 1.8730731010437012,
      "learning_rate": 5.376097747231769e-05,
      "loss": 0.5594,
      "step": 5750
    },
    {
      "epoch": 0.7314467408585056,
      "grad_norm": 2.0563299655914307,
      "learning_rate": 5.3735522464044806e-05,
      "loss": 0.6921,
      "step": 5751
    },
    {
      "epoch": 0.7315739268680446,
      "grad_norm": 1.832389235496521,
      "learning_rate": 5.371006745577193e-05,
      "loss": 0.4332,
      "step": 5752
    },
    {
      "epoch": 0.7317011128775834,
      "grad_norm": 2.3767223358154297,
      "learning_rate": 5.368461244749905e-05,
      "loss": 0.6927,
      "step": 5753
    },
    {
      "epoch": 0.7318282988871224,
      "grad_norm": 2.193406820297241,
      "learning_rate": 5.365915743922617e-05,
      "loss": 0.519,
      "step": 5754
    },
    {
      "epoch": 0.7319554848966614,
      "grad_norm": 2.061901807785034,
      "learning_rate": 5.3633702430953294e-05,
      "loss": 0.4546,
      "step": 5755
    },
    {
      "epoch": 0.7320826709062003,
      "grad_norm": 1.6603353023529053,
      "learning_rate": 5.360824742268041e-05,
      "loss": 0.5086,
      "step": 5756
    },
    {
      "epoch": 0.7322098569157393,
      "grad_norm": 4.283333778381348,
      "learning_rate": 5.358279241440753e-05,
      "loss": 0.7038,
      "step": 5757
    },
    {
      "epoch": 0.7323370429252782,
      "grad_norm": 2.648601531982422,
      "learning_rate": 5.355733740613466e-05,
      "loss": 0.6825,
      "step": 5758
    },
    {
      "epoch": 0.7324642289348172,
      "grad_norm": 2.464294195175171,
      "learning_rate": 5.3531882397861776e-05,
      "loss": 0.5594,
      "step": 5759
    },
    {
      "epoch": 0.7325914149443561,
      "grad_norm": 2.246706962585449,
      "learning_rate": 5.350642738958891e-05,
      "loss": 0.5795,
      "step": 5760
    },
    {
      "epoch": 0.7327186009538951,
      "grad_norm": 2.1840972900390625,
      "learning_rate": 5.348097238131603e-05,
      "loss": 0.6848,
      "step": 5761
    },
    {
      "epoch": 0.732845786963434,
      "grad_norm": 2.667517900466919,
      "learning_rate": 5.345551737304315e-05,
      "loss": 0.7605,
      "step": 5762
    },
    {
      "epoch": 0.7329729729729729,
      "grad_norm": 2.718240737915039,
      "learning_rate": 5.343006236477027e-05,
      "loss": 0.8237,
      "step": 5763
    },
    {
      "epoch": 0.7331001589825119,
      "grad_norm": 2.180612325668335,
      "learning_rate": 5.34046073564974e-05,
      "loss": 0.534,
      "step": 5764
    },
    {
      "epoch": 0.7332273449920509,
      "grad_norm": 1.5308188199996948,
      "learning_rate": 5.3379152348224515e-05,
      "loss": 0.3338,
      "step": 5765
    },
    {
      "epoch": 0.7333545310015899,
      "grad_norm": 2.940234899520874,
      "learning_rate": 5.3353697339951634e-05,
      "loss": 0.6463,
      "step": 5766
    },
    {
      "epoch": 0.7334817170111287,
      "grad_norm": 1.7971054315567017,
      "learning_rate": 5.332824233167876e-05,
      "loss": 0.6356,
      "step": 5767
    },
    {
      "epoch": 0.7336089030206677,
      "grad_norm": 1.931194543838501,
      "learning_rate": 5.330278732340588e-05,
      "loss": 0.4803,
      "step": 5768
    },
    {
      "epoch": 0.7337360890302067,
      "grad_norm": 2.495171308517456,
      "learning_rate": 5.3277332315133e-05,
      "loss": 0.4869,
      "step": 5769
    },
    {
      "epoch": 0.7338632750397456,
      "grad_norm": 1.709662914276123,
      "learning_rate": 5.325187730686013e-05,
      "loss": 0.4303,
      "step": 5770
    },
    {
      "epoch": 0.7339904610492846,
      "grad_norm": 1.7192960977554321,
      "learning_rate": 5.3226422298587255e-05,
      "loss": 0.4997,
      "step": 5771
    },
    {
      "epoch": 0.7341176470588235,
      "grad_norm": 1.6420923471450806,
      "learning_rate": 5.3200967290314374e-05,
      "loss": 0.3291,
      "step": 5772
    },
    {
      "epoch": 0.7342448330683625,
      "grad_norm": 2.086334466934204,
      "learning_rate": 5.317551228204149e-05,
      "loss": 0.3686,
      "step": 5773
    },
    {
      "epoch": 0.7343720190779014,
      "grad_norm": 2.224992275238037,
      "learning_rate": 5.315005727376862e-05,
      "loss": 0.4361,
      "step": 5774
    },
    {
      "epoch": 0.7344992050874404,
      "grad_norm": 2.2755014896392822,
      "learning_rate": 5.312460226549574e-05,
      "loss": 0.4011,
      "step": 5775
    },
    {
      "epoch": 0.7346263910969794,
      "grad_norm": 2.9811506271362305,
      "learning_rate": 5.3099147257222855e-05,
      "loss": 0.6395,
      "step": 5776
    },
    {
      "epoch": 0.7347535771065182,
      "grad_norm": 1.7245445251464844,
      "learning_rate": 5.307369224894998e-05,
      "loss": 0.5167,
      "step": 5777
    },
    {
      "epoch": 0.7348807631160572,
      "grad_norm": 2.064389228820801,
      "learning_rate": 5.30482372406771e-05,
      "loss": 0.696,
      "step": 5778
    },
    {
      "epoch": 0.7350079491255962,
      "grad_norm": 2.2868943214416504,
      "learning_rate": 5.302278223240423e-05,
      "loss": 0.8262,
      "step": 5779
    },
    {
      "epoch": 0.7351351351351352,
      "grad_norm": 2.8638365268707275,
      "learning_rate": 5.299732722413136e-05,
      "loss": 0.5407,
      "step": 5780
    },
    {
      "epoch": 0.735262321144674,
      "grad_norm": 2.530914306640625,
      "learning_rate": 5.2971872215858476e-05,
      "loss": 0.5928,
      "step": 5781
    },
    {
      "epoch": 0.735389507154213,
      "grad_norm": 2.1796064376831055,
      "learning_rate": 5.2946417207585595e-05,
      "loss": 0.5503,
      "step": 5782
    },
    {
      "epoch": 0.735516693163752,
      "grad_norm": 2.796093225479126,
      "learning_rate": 5.292096219931272e-05,
      "loss": 0.4766,
      "step": 5783
    },
    {
      "epoch": 0.735643879173291,
      "grad_norm": 2.1750876903533936,
      "learning_rate": 5.289550719103984e-05,
      "loss": 0.51,
      "step": 5784
    },
    {
      "epoch": 0.7357710651828299,
      "grad_norm": 2.248833417892456,
      "learning_rate": 5.287005218276696e-05,
      "loss": 0.4998,
      "step": 5785
    },
    {
      "epoch": 0.7358982511923688,
      "grad_norm": 2.227102756500244,
      "learning_rate": 5.2844597174494083e-05,
      "loss": 0.7154,
      "step": 5786
    },
    {
      "epoch": 0.7360254372019078,
      "grad_norm": 2.5671260356903076,
      "learning_rate": 5.28191421662212e-05,
      "loss": 0.8516,
      "step": 5787
    },
    {
      "epoch": 0.7361526232114467,
      "grad_norm": 2.3962018489837646,
      "learning_rate": 5.279368715794832e-05,
      "loss": 0.6271,
      "step": 5788
    },
    {
      "epoch": 0.7362798092209857,
      "grad_norm": 3.0240390300750732,
      "learning_rate": 5.276823214967545e-05,
      "loss": 0.6732,
      "step": 5789
    },
    {
      "epoch": 0.7364069952305247,
      "grad_norm": 1.9307132959365845,
      "learning_rate": 5.274277714140258e-05,
      "loss": 0.4507,
      "step": 5790
    },
    {
      "epoch": 0.7365341812400636,
      "grad_norm": 2.5407907962799072,
      "learning_rate": 5.27173221331297e-05,
      "loss": 0.4575,
      "step": 5791
    },
    {
      "epoch": 0.7366613672496025,
      "grad_norm": 1.6476470232009888,
      "learning_rate": 5.269186712485682e-05,
      "loss": 0.472,
      "step": 5792
    },
    {
      "epoch": 0.7367885532591415,
      "grad_norm": 1.72784423828125,
      "learning_rate": 5.266641211658394e-05,
      "loss": 0.4164,
      "step": 5793
    },
    {
      "epoch": 0.7369157392686805,
      "grad_norm": 1.435138463973999,
      "learning_rate": 5.264095710831106e-05,
      "loss": 0.4517,
      "step": 5794
    },
    {
      "epoch": 0.7370429252782194,
      "grad_norm": 2.2068159580230713,
      "learning_rate": 5.2615502100038186e-05,
      "loss": 0.5619,
      "step": 5795
    },
    {
      "epoch": 0.7371701112877583,
      "grad_norm": 2.605395793914795,
      "learning_rate": 5.2590047091765305e-05,
      "loss": 0.4958,
      "step": 5796
    },
    {
      "epoch": 0.7372972972972973,
      "grad_norm": 2.160207509994507,
      "learning_rate": 5.2564592083492423e-05,
      "loss": 0.462,
      "step": 5797
    },
    {
      "epoch": 0.7374244833068363,
      "grad_norm": 2.005342483520508,
      "learning_rate": 5.253913707521955e-05,
      "loss": 0.6202,
      "step": 5798
    },
    {
      "epoch": 0.7375516693163752,
      "grad_norm": 2.408867597579956,
      "learning_rate": 5.251368206694668e-05,
      "loss": 0.4229,
      "step": 5799
    },
    {
      "epoch": 0.7376788553259142,
      "grad_norm": 3.0275392532348633,
      "learning_rate": 5.24882270586738e-05,
      "loss": 0.4203,
      "step": 5800
    },
    {
      "epoch": 0.7378060413354531,
      "grad_norm": 2.9401559829711914,
      "learning_rate": 5.246277205040092e-05,
      "loss": 0.6228,
      "step": 5801
    },
    {
      "epoch": 0.737933227344992,
      "grad_norm": 2.336014747619629,
      "learning_rate": 5.2437317042128044e-05,
      "loss": 0.4758,
      "step": 5802
    },
    {
      "epoch": 0.738060413354531,
      "grad_norm": 2.1750526428222656,
      "learning_rate": 5.241186203385516e-05,
      "loss": 0.478,
      "step": 5803
    },
    {
      "epoch": 0.73818759936407,
      "grad_norm": 2.2159204483032227,
      "learning_rate": 5.238640702558228e-05,
      "loss": 0.5027,
      "step": 5804
    },
    {
      "epoch": 0.738314785373609,
      "grad_norm": 2.5009236335754395,
      "learning_rate": 5.236095201730941e-05,
      "loss": 0.4165,
      "step": 5805
    },
    {
      "epoch": 0.7384419713831478,
      "grad_norm": 1.9077364206314087,
      "learning_rate": 5.2335497009036526e-05,
      "loss": 0.5146,
      "step": 5806
    },
    {
      "epoch": 0.7385691573926868,
      "grad_norm": 1.7969322204589844,
      "learning_rate": 5.231004200076365e-05,
      "loss": 0.4565,
      "step": 5807
    },
    {
      "epoch": 0.7386963434022258,
      "grad_norm": 2.8053886890411377,
      "learning_rate": 5.2284586992490784e-05,
      "loss": 0.5844,
      "step": 5808
    },
    {
      "epoch": 0.7388235294117647,
      "grad_norm": 2.5459213256835938,
      "learning_rate": 5.22591319842179e-05,
      "loss": 0.5809,
      "step": 5809
    },
    {
      "epoch": 0.7389507154213036,
      "grad_norm": 1.8127082586288452,
      "learning_rate": 5.223367697594502e-05,
      "loss": 0.3716,
      "step": 5810
    },
    {
      "epoch": 0.7390779014308426,
      "grad_norm": 3.2660794258117676,
      "learning_rate": 5.220822196767215e-05,
      "loss": 0.5466,
      "step": 5811
    },
    {
      "epoch": 0.7392050874403816,
      "grad_norm": 2.495419502258301,
      "learning_rate": 5.2182766959399266e-05,
      "loss": 0.5351,
      "step": 5812
    },
    {
      "epoch": 0.7393322734499205,
      "grad_norm": 1.5945638418197632,
      "learning_rate": 5.2157311951126384e-05,
      "loss": 0.5679,
      "step": 5813
    },
    {
      "epoch": 0.7394594594594595,
      "grad_norm": 2.9829835891723633,
      "learning_rate": 5.213185694285351e-05,
      "loss": 0.7585,
      "step": 5814
    },
    {
      "epoch": 0.7395866454689984,
      "grad_norm": 2.061622381210327,
      "learning_rate": 5.210640193458063e-05,
      "loss": 0.5258,
      "step": 5815
    },
    {
      "epoch": 0.7397138314785373,
      "grad_norm": 2.541919469833374,
      "learning_rate": 5.208094692630775e-05,
      "loss": 0.5103,
      "step": 5816
    },
    {
      "epoch": 0.7398410174880763,
      "grad_norm": 1.7560168504714966,
      "learning_rate": 5.205549191803487e-05,
      "loss": 0.38,
      "step": 5817
    },
    {
      "epoch": 0.7399682034976153,
      "grad_norm": 1.9970378875732422,
      "learning_rate": 5.2030036909762005e-05,
      "loss": 0.5412,
      "step": 5818
    },
    {
      "epoch": 0.7400953895071543,
      "grad_norm": 2.570340871810913,
      "learning_rate": 5.2004581901489124e-05,
      "loss": 0.5292,
      "step": 5819
    },
    {
      "epoch": 0.7402225755166931,
      "grad_norm": 2.4284255504608154,
      "learning_rate": 5.197912689321624e-05,
      "loss": 0.4234,
      "step": 5820
    },
    {
      "epoch": 0.7403497615262321,
      "grad_norm": 2.4368045330047607,
      "learning_rate": 5.195367188494337e-05,
      "loss": 0.483,
      "step": 5821
    },
    {
      "epoch": 0.7404769475357711,
      "grad_norm": 4.169674873352051,
      "learning_rate": 5.192821687667049e-05,
      "loss": 0.7055,
      "step": 5822
    },
    {
      "epoch": 0.74060413354531,
      "grad_norm": 2.2657032012939453,
      "learning_rate": 5.190276186839761e-05,
      "loss": 0.6208,
      "step": 5823
    },
    {
      "epoch": 0.7407313195548489,
      "grad_norm": 1.9849913120269775,
      "learning_rate": 5.187730686012473e-05,
      "loss": 0.4207,
      "step": 5824
    },
    {
      "epoch": 0.7408585055643879,
      "grad_norm": 2.5783421993255615,
      "learning_rate": 5.185185185185185e-05,
      "loss": 0.6069,
      "step": 5825
    },
    {
      "epoch": 0.7409856915739269,
      "grad_norm": 2.291585922241211,
      "learning_rate": 5.1826396843578975e-05,
      "loss": 0.5069,
      "step": 5826
    },
    {
      "epoch": 0.7411128775834658,
      "grad_norm": 2.917633295059204,
      "learning_rate": 5.1800941835306094e-05,
      "loss": 0.5194,
      "step": 5827
    },
    {
      "epoch": 0.7412400635930048,
      "grad_norm": 2.87046480178833,
      "learning_rate": 5.1775486827033226e-05,
      "loss": 0.6165,
      "step": 5828
    },
    {
      "epoch": 0.7413672496025437,
      "grad_norm": 1.8709908723831177,
      "learning_rate": 5.1750031818760345e-05,
      "loss": 0.6106,
      "step": 5829
    },
    {
      "epoch": 0.7414944356120827,
      "grad_norm": 2.093808174133301,
      "learning_rate": 5.172457681048747e-05,
      "loss": 0.3955,
      "step": 5830
    },
    {
      "epoch": 0.7416216216216216,
      "grad_norm": 3.9832968711853027,
      "learning_rate": 5.169912180221459e-05,
      "loss": 0.643,
      "step": 5831
    },
    {
      "epoch": 0.7417488076311606,
      "grad_norm": 1.7730507850646973,
      "learning_rate": 5.167366679394171e-05,
      "loss": 0.5918,
      "step": 5832
    },
    {
      "epoch": 0.7418759936406996,
      "grad_norm": 2.4302659034729004,
      "learning_rate": 5.1648211785668834e-05,
      "loss": 0.6088,
      "step": 5833
    },
    {
      "epoch": 0.7420031796502384,
      "grad_norm": 1.7140717506408691,
      "learning_rate": 5.162275677739595e-05,
      "loss": 0.2785,
      "step": 5834
    },
    {
      "epoch": 0.7421303656597774,
      "grad_norm": 1.7966527938842773,
      "learning_rate": 5.159730176912307e-05,
      "loss": 0.5999,
      "step": 5835
    },
    {
      "epoch": 0.7422575516693164,
      "grad_norm": 1.650187373161316,
      "learning_rate": 5.1571846760850197e-05,
      "loss": 0.4157,
      "step": 5836
    },
    {
      "epoch": 0.7423847376788554,
      "grad_norm": 2.185311794281006,
      "learning_rate": 5.1546391752577315e-05,
      "loss": 0.7933,
      "step": 5837
    },
    {
      "epoch": 0.7425119236883942,
      "grad_norm": 2.9259185791015625,
      "learning_rate": 5.152093674430445e-05,
      "loss": 0.7904,
      "step": 5838
    },
    {
      "epoch": 0.7426391096979332,
      "grad_norm": 2.525538206100464,
      "learning_rate": 5.149548173603157e-05,
      "loss": 0.5342,
      "step": 5839
    },
    {
      "epoch": 0.7427662957074722,
      "grad_norm": 1.7549984455108643,
      "learning_rate": 5.147002672775869e-05,
      "loss": 0.413,
      "step": 5840
    },
    {
      "epoch": 0.7428934817170111,
      "grad_norm": 2.5548360347747803,
      "learning_rate": 5.144457171948581e-05,
      "loss": 0.6011,
      "step": 5841
    },
    {
      "epoch": 0.7430206677265501,
      "grad_norm": 2.2503819465637207,
      "learning_rate": 5.1419116711212936e-05,
      "loss": 0.5064,
      "step": 5842
    },
    {
      "epoch": 0.743147853736089,
      "grad_norm": 2.8863682746887207,
      "learning_rate": 5.1393661702940055e-05,
      "loss": 0.5008,
      "step": 5843
    },
    {
      "epoch": 0.743275039745628,
      "grad_norm": 2.67928147315979,
      "learning_rate": 5.1368206694667174e-05,
      "loss": 0.7765,
      "step": 5844
    },
    {
      "epoch": 0.7434022257551669,
      "grad_norm": 2.1479082107543945,
      "learning_rate": 5.13427516863943e-05,
      "loss": 0.3542,
      "step": 5845
    },
    {
      "epoch": 0.7435294117647059,
      "grad_norm": 2.1414897441864014,
      "learning_rate": 5.131729667812142e-05,
      "loss": 0.6823,
      "step": 5846
    },
    {
      "epoch": 0.7436565977742449,
      "grad_norm": 2.735980272293091,
      "learning_rate": 5.129184166984855e-05,
      "loss": 0.7529,
      "step": 5847
    },
    {
      "epoch": 0.7437837837837837,
      "grad_norm": 1.8799265623092651,
      "learning_rate": 5.126638666157567e-05,
      "loss": 0.3798,
      "step": 5848
    },
    {
      "epoch": 0.7439109697933227,
      "grad_norm": 2.5686464309692383,
      "learning_rate": 5.1240931653302794e-05,
      "loss": 0.4936,
      "step": 5849
    },
    {
      "epoch": 0.7440381558028617,
      "grad_norm": 2.314488410949707,
      "learning_rate": 5.121547664502991e-05,
      "loss": 0.4626,
      "step": 5850
    },
    {
      "epoch": 0.7441653418124007,
      "grad_norm": 2.4707539081573486,
      "learning_rate": 5.119002163675703e-05,
      "loss": 0.6185,
      "step": 5851
    },
    {
      "epoch": 0.7442925278219396,
      "grad_norm": 1.9120383262634277,
      "learning_rate": 5.116456662848416e-05,
      "loss": 0.514,
      "step": 5852
    },
    {
      "epoch": 0.7444197138314785,
      "grad_norm": 1.942642092704773,
      "learning_rate": 5.1139111620211276e-05,
      "loss": 0.4383,
      "step": 5853
    },
    {
      "epoch": 0.7445468998410175,
      "grad_norm": 2.8310210704803467,
      "learning_rate": 5.11136566119384e-05,
      "loss": 0.7193,
      "step": 5854
    },
    {
      "epoch": 0.7446740858505564,
      "grad_norm": 2.887462854385376,
      "learning_rate": 5.108820160366552e-05,
      "loss": 0.6888,
      "step": 5855
    },
    {
      "epoch": 0.7448012718600954,
      "grad_norm": 2.286848545074463,
      "learning_rate": 5.106274659539264e-05,
      "loss": 0.5536,
      "step": 5856
    },
    {
      "epoch": 0.7449284578696344,
      "grad_norm": 3.335592031478882,
      "learning_rate": 5.103729158711977e-05,
      "loss": 0.5691,
      "step": 5857
    },
    {
      "epoch": 0.7450556438791733,
      "grad_norm": 2.764267683029175,
      "learning_rate": 5.10118365788469e-05,
      "loss": 0.7311,
      "step": 5858
    },
    {
      "epoch": 0.7451828298887122,
      "grad_norm": 2.034109592437744,
      "learning_rate": 5.0986381570574016e-05,
      "loss": 0.6635,
      "step": 5859
    },
    {
      "epoch": 0.7453100158982512,
      "grad_norm": 3.087129592895508,
      "learning_rate": 5.0960926562301134e-05,
      "loss": 0.4318,
      "step": 5860
    },
    {
      "epoch": 0.7454372019077902,
      "grad_norm": 2.720170021057129,
      "learning_rate": 5.093547155402826e-05,
      "loss": 0.783,
      "step": 5861
    },
    {
      "epoch": 0.745564387917329,
      "grad_norm": 1.84943687915802,
      "learning_rate": 5.091001654575538e-05,
      "loss": 0.4887,
      "step": 5862
    },
    {
      "epoch": 0.745691573926868,
      "grad_norm": 2.9749388694763184,
      "learning_rate": 5.08845615374825e-05,
      "loss": 0.6355,
      "step": 5863
    },
    {
      "epoch": 0.745818759936407,
      "grad_norm": 2.715602159500122,
      "learning_rate": 5.085910652920962e-05,
      "loss": 0.6992,
      "step": 5864
    },
    {
      "epoch": 0.745945945945946,
      "grad_norm": 2.7208139896392822,
      "learning_rate": 5.083365152093674e-05,
      "loss": 0.5343,
      "step": 5865
    },
    {
      "epoch": 0.7460731319554849,
      "grad_norm": 1.7379788160324097,
      "learning_rate": 5.080819651266386e-05,
      "loss": 0.3881,
      "step": 5866
    },
    {
      "epoch": 0.7462003179650238,
      "grad_norm": 2.1680948734283447,
      "learning_rate": 5.078274150439099e-05,
      "loss": 0.532,
      "step": 5867
    },
    {
      "epoch": 0.7463275039745628,
      "grad_norm": 2.8969759941101074,
      "learning_rate": 5.075728649611812e-05,
      "loss": 0.6991,
      "step": 5868
    },
    {
      "epoch": 0.7464546899841017,
      "grad_norm": 2.1688246726989746,
      "learning_rate": 5.073183148784524e-05,
      "loss": 0.4616,
      "step": 5869
    },
    {
      "epoch": 0.7465818759936407,
      "grad_norm": 2.0740134716033936,
      "learning_rate": 5.070637647957236e-05,
      "loss": 0.5945,
      "step": 5870
    },
    {
      "epoch": 0.7467090620031797,
      "grad_norm": 3.608266830444336,
      "learning_rate": 5.068092147129948e-05,
      "loss": 0.6898,
      "step": 5871
    },
    {
      "epoch": 0.7468362480127186,
      "grad_norm": 3.0227067470550537,
      "learning_rate": 5.06554664630266e-05,
      "loss": 0.7094,
      "step": 5872
    },
    {
      "epoch": 0.7469634340222575,
      "grad_norm": 1.9552345275878906,
      "learning_rate": 5.0630011454753726e-05,
      "loss": 0.4282,
      "step": 5873
    },
    {
      "epoch": 0.7470906200317965,
      "grad_norm": 2.070727825164795,
      "learning_rate": 5.0604556446480844e-05,
      "loss": 0.7825,
      "step": 5874
    },
    {
      "epoch": 0.7472178060413355,
      "grad_norm": 2.6159045696258545,
      "learning_rate": 5.057910143820796e-05,
      "loss": 0.5993,
      "step": 5875
    },
    {
      "epoch": 0.7473449920508745,
      "grad_norm": 3.010161876678467,
      "learning_rate": 5.0553646429935095e-05,
      "loss": 0.7375,
      "step": 5876
    },
    {
      "epoch": 0.7474721780604133,
      "grad_norm": 3.442145586013794,
      "learning_rate": 5.052819142166222e-05,
      "loss": 0.5944,
      "step": 5877
    },
    {
      "epoch": 0.7475993640699523,
      "grad_norm": 2.1559183597564697,
      "learning_rate": 5.050273641338934e-05,
      "loss": 0.5048,
      "step": 5878
    },
    {
      "epoch": 0.7477265500794913,
      "grad_norm": 2.330467462539673,
      "learning_rate": 5.047728140511646e-05,
      "loss": 0.7043,
      "step": 5879
    },
    {
      "epoch": 0.7478537360890302,
      "grad_norm": 2.7858312129974365,
      "learning_rate": 5.0451826396843584e-05,
      "loss": 0.6045,
      "step": 5880
    },
    {
      "epoch": 0.7479809220985691,
      "grad_norm": 2.8915090560913086,
      "learning_rate": 5.04263713885707e-05,
      "loss": 0.5806,
      "step": 5881
    },
    {
      "epoch": 0.7481081081081081,
      "grad_norm": 2.0913162231445312,
      "learning_rate": 5.040091638029782e-05,
      "loss": 0.5376,
      "step": 5882
    },
    {
      "epoch": 0.7482352941176471,
      "grad_norm": 1.96828031539917,
      "learning_rate": 5.037546137202495e-05,
      "loss": 0.4605,
      "step": 5883
    },
    {
      "epoch": 0.748362480127186,
      "grad_norm": 2.8389744758605957,
      "learning_rate": 5.0350006363752066e-05,
      "loss": 0.6078,
      "step": 5884
    },
    {
      "epoch": 0.748489666136725,
      "grad_norm": 2.652663230895996,
      "learning_rate": 5.032455135547919e-05,
      "loss": 0.6154,
      "step": 5885
    },
    {
      "epoch": 0.748616852146264,
      "grad_norm": 2.1897690296173096,
      "learning_rate": 5.029909634720632e-05,
      "loss": 0.5236,
      "step": 5886
    },
    {
      "epoch": 0.7487440381558028,
      "grad_norm": 1.992144227027893,
      "learning_rate": 5.027364133893344e-05,
      "loss": 0.5044,
      "step": 5887
    },
    {
      "epoch": 0.7488712241653418,
      "grad_norm": 1.7279726266860962,
      "learning_rate": 5.024818633066056e-05,
      "loss": 0.5858,
      "step": 5888
    },
    {
      "epoch": 0.7489984101748808,
      "grad_norm": 2.143190622329712,
      "learning_rate": 5.0222731322387686e-05,
      "loss": 0.6398,
      "step": 5889
    },
    {
      "epoch": 0.7491255961844198,
      "grad_norm": 2.7144923210144043,
      "learning_rate": 5.0197276314114805e-05,
      "loss": 0.8064,
      "step": 5890
    },
    {
      "epoch": 0.7492527821939586,
      "grad_norm": 2.311537981033325,
      "learning_rate": 5.0171821305841924e-05,
      "loss": 0.8108,
      "step": 5891
    },
    {
      "epoch": 0.7493799682034976,
      "grad_norm": 2.392965793609619,
      "learning_rate": 5.014636629756905e-05,
      "loss": 0.5774,
      "step": 5892
    },
    {
      "epoch": 0.7495071542130366,
      "grad_norm": 2.515460729598999,
      "learning_rate": 5.012091128929617e-05,
      "loss": 0.5671,
      "step": 5893
    },
    {
      "epoch": 0.7496343402225755,
      "grad_norm": 1.8957661390304565,
      "learning_rate": 5.009545628102329e-05,
      "loss": 0.3638,
      "step": 5894
    },
    {
      "epoch": 0.7497615262321145,
      "grad_norm": 1.9634212255477905,
      "learning_rate": 5.007000127275041e-05,
      "loss": 0.5126,
      "step": 5895
    },
    {
      "epoch": 0.7498887122416534,
      "grad_norm": 1.454146385192871,
      "learning_rate": 5.0044546264477545e-05,
      "loss": 0.3436,
      "step": 5896
    },
    {
      "epoch": 0.7500158982511924,
      "grad_norm": 1.601804256439209,
      "learning_rate": 5.001909125620466e-05,
      "loss": 0.5984,
      "step": 5897
    },
    {
      "epoch": 0.7501430842607313,
      "grad_norm": 2.8573882579803467,
      "learning_rate": 4.999363624793178e-05,
      "loss": 0.4756,
      "step": 5898
    },
    {
      "epoch": 0.7502702702702703,
      "grad_norm": 1.698302149772644,
      "learning_rate": 4.996818123965891e-05,
      "loss": 0.3989,
      "step": 5899
    },
    {
      "epoch": 0.7503974562798092,
      "grad_norm": 2.5253143310546875,
      "learning_rate": 4.9942726231386026e-05,
      "loss": 0.5304,
      "step": 5900
    },
    {
      "epoch": 0.7505246422893481,
      "grad_norm": 1.7800992727279663,
      "learning_rate": 4.991727122311315e-05,
      "loss": 0.5851,
      "step": 5901
    },
    {
      "epoch": 0.7506518282988871,
      "grad_norm": 2.1035382747650146,
      "learning_rate": 4.989181621484027e-05,
      "loss": 0.5222,
      "step": 5902
    },
    {
      "epoch": 0.7507790143084261,
      "grad_norm": 2.093879461288452,
      "learning_rate": 4.9866361206567396e-05,
      "loss": 0.5074,
      "step": 5903
    },
    {
      "epoch": 0.7509062003179651,
      "grad_norm": 2.4677374362945557,
      "learning_rate": 4.9840906198294515e-05,
      "loss": 0.739,
      "step": 5904
    },
    {
      "epoch": 0.7510333863275039,
      "grad_norm": 2.8730576038360596,
      "learning_rate": 4.981545119002164e-05,
      "loss": 0.783,
      "step": 5905
    },
    {
      "epoch": 0.7511605723370429,
      "grad_norm": 1.894255518913269,
      "learning_rate": 4.978999618174876e-05,
      "loss": 0.4458,
      "step": 5906
    },
    {
      "epoch": 0.7512877583465819,
      "grad_norm": 1.6278455257415771,
      "learning_rate": 4.9764541173475885e-05,
      "loss": 0.3896,
      "step": 5907
    },
    {
      "epoch": 0.7514149443561208,
      "grad_norm": 1.945753812789917,
      "learning_rate": 4.973908616520301e-05,
      "loss": 0.4462,
      "step": 5908
    },
    {
      "epoch": 0.7515421303656598,
      "grad_norm": 3.218820333480835,
      "learning_rate": 4.971363115693013e-05,
      "loss": 0.3609,
      "step": 5909
    },
    {
      "epoch": 0.7516693163751987,
      "grad_norm": 2.1105852127075195,
      "learning_rate": 4.968817614865725e-05,
      "loss": 0.4772,
      "step": 5910
    },
    {
      "epoch": 0.7517965023847377,
      "grad_norm": 1.807081937789917,
      "learning_rate": 4.966272114038437e-05,
      "loss": 0.4144,
      "step": 5911
    },
    {
      "epoch": 0.7519236883942766,
      "grad_norm": 2.9397385120391846,
      "learning_rate": 4.963726613211149e-05,
      "loss": 0.7034,
      "step": 5912
    },
    {
      "epoch": 0.7520508744038156,
      "grad_norm": 2.2436041831970215,
      "learning_rate": 4.961181112383862e-05,
      "loss": 0.4647,
      "step": 5913
    },
    {
      "epoch": 0.7521780604133546,
      "grad_norm": 1.8968160152435303,
      "learning_rate": 4.958635611556574e-05,
      "loss": 0.5528,
      "step": 5914
    },
    {
      "epoch": 0.7523052464228935,
      "grad_norm": 2.158557653427124,
      "learning_rate": 4.956090110729286e-05,
      "loss": 0.7556,
      "step": 5915
    },
    {
      "epoch": 0.7524324324324324,
      "grad_norm": 2.5842537879943848,
      "learning_rate": 4.953544609901998e-05,
      "loss": 0.5917,
      "step": 5916
    },
    {
      "epoch": 0.7525596184419714,
      "grad_norm": 2.6723926067352295,
      "learning_rate": 4.9509991090747106e-05,
      "loss": 0.5031,
      "step": 5917
    },
    {
      "epoch": 0.7526868044515104,
      "grad_norm": 2.590782880783081,
      "learning_rate": 4.948453608247423e-05,
      "loss": 0.5077,
      "step": 5918
    },
    {
      "epoch": 0.7528139904610492,
      "grad_norm": 2.228252649307251,
      "learning_rate": 4.945908107420135e-05,
      "loss": 0.6102,
      "step": 5919
    },
    {
      "epoch": 0.7529411764705882,
      "grad_norm": 2.2379956245422363,
      "learning_rate": 4.9433626065928476e-05,
      "loss": 0.6155,
      "step": 5920
    },
    {
      "epoch": 0.7530683624801272,
      "grad_norm": 2.581102132797241,
      "learning_rate": 4.9408171057655594e-05,
      "loss": 0.8439,
      "step": 5921
    },
    {
      "epoch": 0.7531955484896662,
      "grad_norm": 2.272606134414673,
      "learning_rate": 4.938271604938271e-05,
      "loss": 0.454,
      "step": 5922
    },
    {
      "epoch": 0.7533227344992051,
      "grad_norm": 1.770828127861023,
      "learning_rate": 4.9357261041109845e-05,
      "loss": 0.5586,
      "step": 5923
    },
    {
      "epoch": 0.753449920508744,
      "grad_norm": 3.3322906494140625,
      "learning_rate": 4.9331806032836964e-05,
      "loss": 0.4577,
      "step": 5924
    },
    {
      "epoch": 0.753577106518283,
      "grad_norm": 2.131136655807495,
      "learning_rate": 4.930635102456408e-05,
      "loss": 0.4588,
      "step": 5925
    },
    {
      "epoch": 0.7537042925278219,
      "grad_norm": 1.9757384061813354,
      "learning_rate": 4.928089601629121e-05,
      "loss": 0.3354,
      "step": 5926
    },
    {
      "epoch": 0.7538314785373609,
      "grad_norm": 2.394376516342163,
      "learning_rate": 4.9255441008018334e-05,
      "loss": 0.451,
      "step": 5927
    },
    {
      "epoch": 0.7539586645468999,
      "grad_norm": 2.2184035778045654,
      "learning_rate": 4.922998599974545e-05,
      "loss": 0.3467,
      "step": 5928
    },
    {
      "epoch": 0.7540858505564388,
      "grad_norm": 1.5246601104736328,
      "learning_rate": 4.920453099147258e-05,
      "loss": 0.398,
      "step": 5929
    },
    {
      "epoch": 0.7542130365659777,
      "grad_norm": 1.9222530126571655,
      "learning_rate": 4.91790759831997e-05,
      "loss": 0.5375,
      "step": 5930
    },
    {
      "epoch": 0.7543402225755167,
      "grad_norm": 2.1887078285217285,
      "learning_rate": 4.9153620974926816e-05,
      "loss": 0.5639,
      "step": 5931
    },
    {
      "epoch": 0.7544674085850557,
      "grad_norm": 1.9638395309448242,
      "learning_rate": 4.912816596665394e-05,
      "loss": 0.4858,
      "step": 5932
    },
    {
      "epoch": 0.7545945945945945,
      "grad_norm": 2.1239137649536133,
      "learning_rate": 4.910271095838107e-05,
      "loss": 0.7145,
      "step": 5933
    },
    {
      "epoch": 0.7547217806041335,
      "grad_norm": 2.407684803009033,
      "learning_rate": 4.9077255950108185e-05,
      "loss": 0.6853,
      "step": 5934
    },
    {
      "epoch": 0.7548489666136725,
      "grad_norm": 2.6825156211853027,
      "learning_rate": 4.9051800941835304e-05,
      "loss": 0.6528,
      "step": 5935
    },
    {
      "epoch": 0.7549761526232115,
      "grad_norm": 2.3698461055755615,
      "learning_rate": 4.902634593356243e-05,
      "loss": 0.5868,
      "step": 5936
    },
    {
      "epoch": 0.7551033386327504,
      "grad_norm": 1.4579147100448608,
      "learning_rate": 4.9000890925289555e-05,
      "loss": 0.5504,
      "step": 5937
    },
    {
      "epoch": 0.7552305246422893,
      "grad_norm": 1.5561236143112183,
      "learning_rate": 4.8975435917016674e-05,
      "loss": 0.3669,
      "step": 5938
    },
    {
      "epoch": 0.7553577106518283,
      "grad_norm": 3.5492663383483887,
      "learning_rate": 4.89499809087438e-05,
      "loss": 0.8695,
      "step": 5939
    },
    {
      "epoch": 0.7554848966613672,
      "grad_norm": 2.000134229660034,
      "learning_rate": 4.892452590047092e-05,
      "loss": 0.6889,
      "step": 5940
    },
    {
      "epoch": 0.7556120826709062,
      "grad_norm": 1.943076252937317,
      "learning_rate": 4.889907089219804e-05,
      "loss": 0.6471,
      "step": 5941
    },
    {
      "epoch": 0.7557392686804452,
      "grad_norm": 2.015932083129883,
      "learning_rate": 4.887361588392517e-05,
      "loss": 0.4747,
      "step": 5942
    },
    {
      "epoch": 0.7558664546899841,
      "grad_norm": 2.2173666954040527,
      "learning_rate": 4.884816087565229e-05,
      "loss": 0.5183,
      "step": 5943
    },
    {
      "epoch": 0.755993640699523,
      "grad_norm": 3.696516990661621,
      "learning_rate": 4.882270586737941e-05,
      "loss": 0.6504,
      "step": 5944
    },
    {
      "epoch": 0.756120826709062,
      "grad_norm": 3.110443353652954,
      "learning_rate": 4.879725085910653e-05,
      "loss": 0.5078,
      "step": 5945
    },
    {
      "epoch": 0.756248012718601,
      "grad_norm": 2.5335958003997803,
      "learning_rate": 4.877179585083365e-05,
      "loss": 0.6216,
      "step": 5946
    },
    {
      "epoch": 0.7563751987281399,
      "grad_norm": 2.5808637142181396,
      "learning_rate": 4.8746340842560777e-05,
      "loss": 0.3904,
      "step": 5947
    },
    {
      "epoch": 0.7565023847376788,
      "grad_norm": 2.5736753940582275,
      "learning_rate": 4.87208858342879e-05,
      "loss": 0.3569,
      "step": 5948
    },
    {
      "epoch": 0.7566295707472178,
      "grad_norm": 2.085204839706421,
      "learning_rate": 4.869543082601502e-05,
      "loss": 0.4292,
      "step": 5949
    },
    {
      "epoch": 0.7567567567567568,
      "grad_norm": 1.7570096254348755,
      "learning_rate": 4.866997581774214e-05,
      "loss": 0.5324,
      "step": 5950
    },
    {
      "epoch": 0.7568839427662957,
      "grad_norm": 2.3889424800872803,
      "learning_rate": 4.8644520809469265e-05,
      "loss": 0.5341,
      "step": 5951
    },
    {
      "epoch": 0.7570111287758347,
      "grad_norm": 2.469212770462036,
      "learning_rate": 4.861906580119639e-05,
      "loss": 0.5444,
      "step": 5952
    },
    {
      "epoch": 0.7571383147853736,
      "grad_norm": 2.952099561691284,
      "learning_rate": 4.859361079292351e-05,
      "loss": 0.6592,
      "step": 5953
    },
    {
      "epoch": 0.7572655007949125,
      "grad_norm": 1.9480748176574707,
      "learning_rate": 4.8568155784650635e-05,
      "loss": 0.6903,
      "step": 5954
    },
    {
      "epoch": 0.7573926868044515,
      "grad_norm": 2.1926944255828857,
      "learning_rate": 4.8542700776377754e-05,
      "loss": 0.7336,
      "step": 5955
    },
    {
      "epoch": 0.7575198728139905,
      "grad_norm": 3.1684298515319824,
      "learning_rate": 4.851724576810487e-05,
      "loss": 0.5648,
      "step": 5956
    },
    {
      "epoch": 0.7576470588235295,
      "grad_norm": 2.3982460498809814,
      "learning_rate": 4.8491790759832e-05,
      "loss": 0.7933,
      "step": 5957
    },
    {
      "epoch": 0.7577742448330683,
      "grad_norm": 2.8630690574645996,
      "learning_rate": 4.846633575155912e-05,
      "loss": 0.5759,
      "step": 5958
    },
    {
      "epoch": 0.7579014308426073,
      "grad_norm": 2.4451260566711426,
      "learning_rate": 4.844088074328624e-05,
      "loss": 0.4601,
      "step": 5959
    },
    {
      "epoch": 0.7580286168521463,
      "grad_norm": 1.8159688711166382,
      "learning_rate": 4.841542573501337e-05,
      "loss": 0.3593,
      "step": 5960
    },
    {
      "epoch": 0.7581558028616853,
      "grad_norm": 3.43106746673584,
      "learning_rate": 4.838997072674049e-05,
      "loss": 0.5112,
      "step": 5961
    },
    {
      "epoch": 0.7582829888712241,
      "grad_norm": 2.118222236633301,
      "learning_rate": 4.836451571846761e-05,
      "loss": 0.4568,
      "step": 5962
    },
    {
      "epoch": 0.7584101748807631,
      "grad_norm": 2.3677539825439453,
      "learning_rate": 4.833906071019473e-05,
      "loss": 0.51,
      "step": 5963
    },
    {
      "epoch": 0.7585373608903021,
      "grad_norm": 2.036777973175049,
      "learning_rate": 4.8313605701921856e-05,
      "loss": 0.3862,
      "step": 5964
    },
    {
      "epoch": 0.758664546899841,
      "grad_norm": 3.230883836746216,
      "learning_rate": 4.8288150693648975e-05,
      "loss": 0.5883,
      "step": 5965
    },
    {
      "epoch": 0.75879173290938,
      "grad_norm": 3.1374216079711914,
      "learning_rate": 4.82626956853761e-05,
      "loss": 0.6964,
      "step": 5966
    },
    {
      "epoch": 0.7589189189189189,
      "grad_norm": 2.217456817626953,
      "learning_rate": 4.8237240677103226e-05,
      "loss": 0.4828,
      "step": 5967
    },
    {
      "epoch": 0.7590461049284579,
      "grad_norm": 2.1434075832366943,
      "learning_rate": 4.8211785668830345e-05,
      "loss": 0.5053,
      "step": 5968
    },
    {
      "epoch": 0.7591732909379968,
      "grad_norm": 2.6581389904022217,
      "learning_rate": 4.818633066055746e-05,
      "loss": 0.6912,
      "step": 5969
    },
    {
      "epoch": 0.7593004769475358,
      "grad_norm": 2.508901596069336,
      "learning_rate": 4.816087565228459e-05,
      "loss": 0.6791,
      "step": 5970
    },
    {
      "epoch": 0.7594276629570748,
      "grad_norm": 2.8055546283721924,
      "learning_rate": 4.8135420644011714e-05,
      "loss": 0.7839,
      "step": 5971
    },
    {
      "epoch": 0.7595548489666136,
      "grad_norm": 2.377716541290283,
      "learning_rate": 4.810996563573883e-05,
      "loss": 0.5463,
      "step": 5972
    },
    {
      "epoch": 0.7596820349761526,
      "grad_norm": 2.553342819213867,
      "learning_rate": 4.808451062746596e-05,
      "loss": 0.6985,
      "step": 5973
    },
    {
      "epoch": 0.7598092209856916,
      "grad_norm": 2.000054121017456,
      "learning_rate": 4.805905561919308e-05,
      "loss": 0.4885,
      "step": 5974
    },
    {
      "epoch": 0.7599364069952306,
      "grad_norm": 2.4869179725646973,
      "learning_rate": 4.8033600610920196e-05,
      "loss": 0.6279,
      "step": 5975
    },
    {
      "epoch": 0.7600635930047694,
      "grad_norm": 2.6777517795562744,
      "learning_rate": 4.800814560264733e-05,
      "loss": 0.6112,
      "step": 5976
    },
    {
      "epoch": 0.7601907790143084,
      "grad_norm": 2.3363757133483887,
      "learning_rate": 4.798269059437445e-05,
      "loss": 0.4636,
      "step": 5977
    },
    {
      "epoch": 0.7603179650238474,
      "grad_norm": 2.6119678020477295,
      "learning_rate": 4.7957235586101566e-05,
      "loss": 0.6638,
      "step": 5978
    },
    {
      "epoch": 0.7604451510333863,
      "grad_norm": 3.442396640777588,
      "learning_rate": 4.793178057782869e-05,
      "loss": 0.6937,
      "step": 5979
    },
    {
      "epoch": 0.7605723370429253,
      "grad_norm": 2.577449083328247,
      "learning_rate": 4.790632556955581e-05,
      "loss": 0.4945,
      "step": 5980
    },
    {
      "epoch": 0.7606995230524642,
      "grad_norm": 2.7696757316589355,
      "learning_rate": 4.7880870561282936e-05,
      "loss": 0.4984,
      "step": 5981
    },
    {
      "epoch": 0.7608267090620032,
      "grad_norm": 2.2580904960632324,
      "learning_rate": 4.785541555301006e-05,
      "loss": 0.499,
      "step": 5982
    },
    {
      "epoch": 0.7609538950715421,
      "grad_norm": 1.8195844888687134,
      "learning_rate": 4.782996054473718e-05,
      "loss": 0.6045,
      "step": 5983
    },
    {
      "epoch": 0.7610810810810811,
      "grad_norm": 3.242462396621704,
      "learning_rate": 4.78045055364643e-05,
      "loss": 0.647,
      "step": 5984
    },
    {
      "epoch": 0.7612082670906201,
      "grad_norm": 1.723791480064392,
      "learning_rate": 4.7779050528191424e-05,
      "loss": 0.5134,
      "step": 5985
    },
    {
      "epoch": 0.7613354531001589,
      "grad_norm": 2.3959455490112305,
      "learning_rate": 4.775359551991855e-05,
      "loss": 0.5929,
      "step": 5986
    },
    {
      "epoch": 0.7614626391096979,
      "grad_norm": 2.4848947525024414,
      "learning_rate": 4.772814051164567e-05,
      "loss": 0.6272,
      "step": 5987
    },
    {
      "epoch": 0.7615898251192369,
      "grad_norm": 2.595627784729004,
      "learning_rate": 4.770268550337279e-05,
      "loss": 0.5474,
      "step": 5988
    },
    {
      "epoch": 0.7617170111287759,
      "grad_norm": 2.012366533279419,
      "learning_rate": 4.767723049509991e-05,
      "loss": 0.587,
      "step": 5989
    },
    {
      "epoch": 0.7618441971383147,
      "grad_norm": 1.6657565832138062,
      "learning_rate": 4.765177548682703e-05,
      "loss": 0.2748,
      "step": 5990
    },
    {
      "epoch": 0.7619713831478537,
      "grad_norm": 2.5818939208984375,
      "learning_rate": 4.762632047855416e-05,
      "loss": 0.7544,
      "step": 5991
    },
    {
      "epoch": 0.7620985691573927,
      "grad_norm": 2.332249879837036,
      "learning_rate": 4.760086547028128e-05,
      "loss": 0.7153,
      "step": 5992
    },
    {
      "epoch": 0.7622257551669316,
      "grad_norm": 2.5492424964904785,
      "learning_rate": 4.75754104620084e-05,
      "loss": 0.6975,
      "step": 5993
    },
    {
      "epoch": 0.7623529411764706,
      "grad_norm": 2.371311902999878,
      "learning_rate": 4.754995545373552e-05,
      "loss": 0.7777,
      "step": 5994
    },
    {
      "epoch": 0.7624801271860095,
      "grad_norm": 2.129178285598755,
      "learning_rate": 4.752450044546265e-05,
      "loss": 0.5916,
      "step": 5995
    },
    {
      "epoch": 0.7626073131955485,
      "grad_norm": 3.5516293048858643,
      "learning_rate": 4.749904543718977e-05,
      "loss": 0.6831,
      "step": 5996
    },
    {
      "epoch": 0.7627344992050874,
      "grad_norm": 2.3882479667663574,
      "learning_rate": 4.747359042891689e-05,
      "loss": 0.6225,
      "step": 5997
    },
    {
      "epoch": 0.7628616852146264,
      "grad_norm": 2.841697931289673,
      "learning_rate": 4.7448135420644015e-05,
      "loss": 0.5562,
      "step": 5998
    },
    {
      "epoch": 0.7629888712241654,
      "grad_norm": 2.530790328979492,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 0.5964,
      "step": 5999
    },
    {
      "epoch": 0.7631160572337043,
      "grad_norm": 2.433413028717041,
      "learning_rate": 4.739722540409826e-05,
      "loss": 0.4444,
      "step": 6000
    },
    {
      "epoch": 0.7632432432432432,
      "grad_norm": 2.5053353309631348,
      "learning_rate": 4.7371770395825385e-05,
      "loss": 0.7923,
      "step": 6001
    },
    {
      "epoch": 0.7633704292527822,
      "grad_norm": 2.304548978805542,
      "learning_rate": 4.7346315387552504e-05,
      "loss": 0.5197,
      "step": 6002
    },
    {
      "epoch": 0.7634976152623212,
      "grad_norm": 2.5012221336364746,
      "learning_rate": 4.732086037927962e-05,
      "loss": 0.5835,
      "step": 6003
    },
    {
      "epoch": 0.76362480127186,
      "grad_norm": 2.1290061473846436,
      "learning_rate": 4.729540537100675e-05,
      "loss": 0.6041,
      "step": 6004
    },
    {
      "epoch": 0.763751987281399,
      "grad_norm": 1.789105772972107,
      "learning_rate": 4.7269950362733873e-05,
      "loss": 0.4374,
      "step": 6005
    },
    {
      "epoch": 0.763879173290938,
      "grad_norm": 2.016662836074829,
      "learning_rate": 4.724449535446099e-05,
      "loss": 0.6785,
      "step": 6006
    },
    {
      "epoch": 0.764006359300477,
      "grad_norm": 1.788110375404358,
      "learning_rate": 4.721904034618812e-05,
      "loss": 0.5921,
      "step": 6007
    },
    {
      "epoch": 0.7641335453100159,
      "grad_norm": 2.4331014156341553,
      "learning_rate": 4.7193585337915236e-05,
      "loss": 0.549,
      "step": 6008
    },
    {
      "epoch": 0.7642607313195549,
      "grad_norm": 2.0829670429229736,
      "learning_rate": 4.7168130329642355e-05,
      "loss": 0.5982,
      "step": 6009
    },
    {
      "epoch": 0.7643879173290938,
      "grad_norm": 2.5125012397766113,
      "learning_rate": 4.714267532136948e-05,
      "loss": 1.1212,
      "step": 6010
    },
    {
      "epoch": 0.7645151033386327,
      "grad_norm": 2.44891095161438,
      "learning_rate": 4.7117220313096606e-05,
      "loss": 0.6329,
      "step": 6011
    },
    {
      "epoch": 0.7646422893481717,
      "grad_norm": 2.3854994773864746,
      "learning_rate": 4.7091765304823725e-05,
      "loss": 0.5012,
      "step": 6012
    },
    {
      "epoch": 0.7647694753577107,
      "grad_norm": 2.506072521209717,
      "learning_rate": 4.706631029655085e-05,
      "loss": 0.6416,
      "step": 6013
    },
    {
      "epoch": 0.7648966613672497,
      "grad_norm": 2.4816715717315674,
      "learning_rate": 4.704085528827797e-05,
      "loss": 0.4726,
      "step": 6014
    },
    {
      "epoch": 0.7650238473767885,
      "grad_norm": 1.8146229982376099,
      "learning_rate": 4.7015400280005095e-05,
      "loss": 0.6676,
      "step": 6015
    },
    {
      "epoch": 0.7651510333863275,
      "grad_norm": 1.4880750179290771,
      "learning_rate": 4.6989945271732213e-05,
      "loss": 0.3673,
      "step": 6016
    },
    {
      "epoch": 0.7652782193958665,
      "grad_norm": 1.7690988779067993,
      "learning_rate": 4.696449026345934e-05,
      "loss": 0.514,
      "step": 6017
    },
    {
      "epoch": 0.7654054054054054,
      "grad_norm": 2.4379348754882812,
      "learning_rate": 4.693903525518646e-05,
      "loss": 0.924,
      "step": 6018
    },
    {
      "epoch": 0.7655325914149443,
      "grad_norm": 2.0228378772735596,
      "learning_rate": 4.691358024691358e-05,
      "loss": 0.5567,
      "step": 6019
    },
    {
      "epoch": 0.7656597774244833,
      "grad_norm": 1.9124189615249634,
      "learning_rate": 4.688812523864071e-05,
      "loss": 0.5068,
      "step": 6020
    },
    {
      "epoch": 0.7657869634340223,
      "grad_norm": 1.7906804084777832,
      "learning_rate": 4.686267023036783e-05,
      "loss": 0.4384,
      "step": 6021
    },
    {
      "epoch": 0.7659141494435612,
      "grad_norm": 2.012206554412842,
      "learning_rate": 4.6837215222094946e-05,
      "loss": 0.5442,
      "step": 6022
    },
    {
      "epoch": 0.7660413354531002,
      "grad_norm": 2.15671706199646,
      "learning_rate": 4.681176021382207e-05,
      "loss": 0.4932,
      "step": 6023
    },
    {
      "epoch": 0.7661685214626391,
      "grad_norm": 2.0541880130767822,
      "learning_rate": 4.678630520554919e-05,
      "loss": 0.7397,
      "step": 6024
    },
    {
      "epoch": 0.766295707472178,
      "grad_norm": 1.8892589807510376,
      "learning_rate": 4.6760850197276316e-05,
      "loss": 0.6078,
      "step": 6025
    },
    {
      "epoch": 0.766422893481717,
      "grad_norm": 1.8023638725280762,
      "learning_rate": 4.673539518900344e-05,
      "loss": 0.5028,
      "step": 6026
    },
    {
      "epoch": 0.766550079491256,
      "grad_norm": 1.95421302318573,
      "learning_rate": 4.670994018073056e-05,
      "loss": 0.6342,
      "step": 6027
    },
    {
      "epoch": 0.766677265500795,
      "grad_norm": 2.80926251411438,
      "learning_rate": 4.668448517245768e-05,
      "loss": 0.5055,
      "step": 6028
    },
    {
      "epoch": 0.7668044515103338,
      "grad_norm": 1.5224275588989258,
      "learning_rate": 4.665903016418481e-05,
      "loss": 0.5385,
      "step": 6029
    },
    {
      "epoch": 0.7669316375198728,
      "grad_norm": 2.740182399749756,
      "learning_rate": 4.663357515591193e-05,
      "loss": 0.5837,
      "step": 6030
    },
    {
      "epoch": 0.7670588235294118,
      "grad_norm": 2.5864577293395996,
      "learning_rate": 4.660812014763905e-05,
      "loss": 0.7341,
      "step": 6031
    },
    {
      "epoch": 0.7671860095389507,
      "grad_norm": 2.674207925796509,
      "learning_rate": 4.6582665139366174e-05,
      "loss": 0.5699,
      "step": 6032
    },
    {
      "epoch": 0.7673131955484896,
      "grad_norm": 1.973768711090088,
      "learning_rate": 4.655721013109329e-05,
      "loss": 0.4978,
      "step": 6033
    },
    {
      "epoch": 0.7674403815580286,
      "grad_norm": 2.1337454319000244,
      "learning_rate": 4.653175512282042e-05,
      "loss": 0.5028,
      "step": 6034
    },
    {
      "epoch": 0.7675675675675676,
      "grad_norm": 2.5344412326812744,
      "learning_rate": 4.6506300114547544e-05,
      "loss": 0.7075,
      "step": 6035
    },
    {
      "epoch": 0.7676947535771065,
      "grad_norm": 3.0208957195281982,
      "learning_rate": 4.648084510627466e-05,
      "loss": 0.6539,
      "step": 6036
    },
    {
      "epoch": 0.7678219395866455,
      "grad_norm": 2.335581064224243,
      "learning_rate": 4.645539009800178e-05,
      "loss": 0.48,
      "step": 6037
    },
    {
      "epoch": 0.7679491255961844,
      "grad_norm": 2.1948583126068115,
      "learning_rate": 4.642993508972891e-05,
      "loss": 0.5713,
      "step": 6038
    },
    {
      "epoch": 0.7680763116057233,
      "grad_norm": 2.9839625358581543,
      "learning_rate": 4.640448008145603e-05,
      "loss": 0.4947,
      "step": 6039
    },
    {
      "epoch": 0.7682034976152623,
      "grad_norm": 2.574692964553833,
      "learning_rate": 4.637902507318315e-05,
      "loss": 0.4415,
      "step": 6040
    },
    {
      "epoch": 0.7683306836248013,
      "grad_norm": 2.2194619178771973,
      "learning_rate": 4.635357006491027e-05,
      "loss": 0.3846,
      "step": 6041
    },
    {
      "epoch": 0.7684578696343403,
      "grad_norm": 1.8195816278457642,
      "learning_rate": 4.6328115056637396e-05,
      "loss": 0.5673,
      "step": 6042
    },
    {
      "epoch": 0.7685850556438791,
      "grad_norm": 2.7130494117736816,
      "learning_rate": 4.6302660048364514e-05,
      "loss": 0.7714,
      "step": 6043
    },
    {
      "epoch": 0.7687122416534181,
      "grad_norm": 2.4247872829437256,
      "learning_rate": 4.627720504009164e-05,
      "loss": 0.5787,
      "step": 6044
    },
    {
      "epoch": 0.7688394276629571,
      "grad_norm": 1.7056444883346558,
      "learning_rate": 4.6251750031818765e-05,
      "loss": 0.4594,
      "step": 6045
    },
    {
      "epoch": 0.7689666136724961,
      "grad_norm": 1.8365604877471924,
      "learning_rate": 4.6226295023545884e-05,
      "loss": 0.428,
      "step": 6046
    },
    {
      "epoch": 0.769093799682035,
      "grad_norm": 2.0580031871795654,
      "learning_rate": 4.6200840015273e-05,
      "loss": 0.3322,
      "step": 6047
    },
    {
      "epoch": 0.7692209856915739,
      "grad_norm": 3.2490808963775635,
      "learning_rate": 4.617538500700013e-05,
      "loss": 0.5817,
      "step": 6048
    },
    {
      "epoch": 0.7693481717011129,
      "grad_norm": 1.6863235235214233,
      "learning_rate": 4.6149929998727254e-05,
      "loss": 0.3513,
      "step": 6049
    },
    {
      "epoch": 0.7694753577106518,
      "grad_norm": 1.825692892074585,
      "learning_rate": 4.612447499045437e-05,
      "loss": 0.4802,
      "step": 6050
    },
    {
      "epoch": 0.7696025437201908,
      "grad_norm": 3.135746479034424,
      "learning_rate": 4.60990199821815e-05,
      "loss": 0.7771,
      "step": 6051
    },
    {
      "epoch": 0.7697297297297298,
      "grad_norm": 2.043278217315674,
      "learning_rate": 4.607356497390862e-05,
      "loss": 0.4329,
      "step": 6052
    },
    {
      "epoch": 0.7698569157392687,
      "grad_norm": 1.9681916236877441,
      "learning_rate": 4.6048109965635736e-05,
      "loss": 0.555,
      "step": 6053
    },
    {
      "epoch": 0.7699841017488076,
      "grad_norm": 1.9392980337142944,
      "learning_rate": 4.602265495736287e-05,
      "loss": 0.6338,
      "step": 6054
    },
    {
      "epoch": 0.7701112877583466,
      "grad_norm": 1.905759334564209,
      "learning_rate": 4.599719994908999e-05,
      "loss": 0.5525,
      "step": 6055
    },
    {
      "epoch": 0.7702384737678856,
      "grad_norm": 3.213726282119751,
      "learning_rate": 4.5971744940817105e-05,
      "loss": 0.8812,
      "step": 6056
    },
    {
      "epoch": 0.7703656597774244,
      "grad_norm": 2.4457991123199463,
      "learning_rate": 4.594628993254423e-05,
      "loss": 0.7167,
      "step": 6057
    },
    {
      "epoch": 0.7704928457869634,
      "grad_norm": 2.579094409942627,
      "learning_rate": 4.5920834924271356e-05,
      "loss": 0.6108,
      "step": 6058
    },
    {
      "epoch": 0.7706200317965024,
      "grad_norm": 1.8235169649124146,
      "learning_rate": 4.5895379915998475e-05,
      "loss": 0.4735,
      "step": 6059
    },
    {
      "epoch": 0.7707472178060414,
      "grad_norm": 1.7540686130523682,
      "learning_rate": 4.58699249077256e-05,
      "loss": 0.4115,
      "step": 6060
    },
    {
      "epoch": 0.7708744038155803,
      "grad_norm": 1.8385519981384277,
      "learning_rate": 4.584446989945272e-05,
      "loss": 0.5834,
      "step": 6061
    },
    {
      "epoch": 0.7710015898251192,
      "grad_norm": 2.2521426677703857,
      "learning_rate": 4.581901489117984e-05,
      "loss": 0.4632,
      "step": 6062
    },
    {
      "epoch": 0.7711287758346582,
      "grad_norm": 1.8157848119735718,
      "learning_rate": 4.5793559882906964e-05,
      "loss": 0.4131,
      "step": 6063
    },
    {
      "epoch": 0.7712559618441971,
      "grad_norm": 2.5565896034240723,
      "learning_rate": 4.576810487463409e-05,
      "loss": 0.7276,
      "step": 6064
    },
    {
      "epoch": 0.7713831478537361,
      "grad_norm": 2.037238597869873,
      "learning_rate": 4.574264986636121e-05,
      "loss": 0.4927,
      "step": 6065
    },
    {
      "epoch": 0.771510333863275,
      "grad_norm": 1.658495306968689,
      "learning_rate": 4.5717194858088333e-05,
      "loss": 0.4677,
      "step": 6066
    },
    {
      "epoch": 0.771637519872814,
      "grad_norm": 2.447838306427002,
      "learning_rate": 4.569173984981545e-05,
      "loss": 0.6505,
      "step": 6067
    },
    {
      "epoch": 0.7717647058823529,
      "grad_norm": 2.1003732681274414,
      "learning_rate": 4.566628484154258e-05,
      "loss": 0.4599,
      "step": 6068
    },
    {
      "epoch": 0.7718918918918919,
      "grad_norm": 2.166198968887329,
      "learning_rate": 4.5640829833269696e-05,
      "loss": 0.6171,
      "step": 6069
    },
    {
      "epoch": 0.7720190779014309,
      "grad_norm": 2.4320216178894043,
      "learning_rate": 4.561537482499682e-05,
      "loss": 0.4506,
      "step": 6070
    },
    {
      "epoch": 0.7721462639109697,
      "grad_norm": 2.7610814571380615,
      "learning_rate": 4.558991981672394e-05,
      "loss": 0.7416,
      "step": 6071
    },
    {
      "epoch": 0.7722734499205087,
      "grad_norm": 2.67909836769104,
      "learning_rate": 4.5564464808451066e-05,
      "loss": 0.7056,
      "step": 6072
    },
    {
      "epoch": 0.7724006359300477,
      "grad_norm": 3.352957248687744,
      "learning_rate": 4.553900980017819e-05,
      "loss": 0.8315,
      "step": 6073
    },
    {
      "epoch": 0.7725278219395867,
      "grad_norm": 3.3641908168792725,
      "learning_rate": 4.551355479190531e-05,
      "loss": 0.4904,
      "step": 6074
    },
    {
      "epoch": 0.7726550079491256,
      "grad_norm": 2.000992774963379,
      "learning_rate": 4.548809978363243e-05,
      "loss": 0.3595,
      "step": 6075
    },
    {
      "epoch": 0.7727821939586645,
      "grad_norm": 3.6657443046569824,
      "learning_rate": 4.5462644775359555e-05,
      "loss": 0.655,
      "step": 6076
    },
    {
      "epoch": 0.7729093799682035,
      "grad_norm": 2.228074789047241,
      "learning_rate": 4.5437189767086673e-05,
      "loss": 0.5885,
      "step": 6077
    },
    {
      "epoch": 0.7730365659777424,
      "grad_norm": 2.016321897506714,
      "learning_rate": 4.54117347588138e-05,
      "loss": 0.3374,
      "step": 6078
    },
    {
      "epoch": 0.7731637519872814,
      "grad_norm": 2.9613475799560547,
      "learning_rate": 4.5386279750540924e-05,
      "loss": 0.6301,
      "step": 6079
    },
    {
      "epoch": 0.7732909379968204,
      "grad_norm": 3.1931192874908447,
      "learning_rate": 4.536082474226804e-05,
      "loss": 0.6957,
      "step": 6080
    },
    {
      "epoch": 0.7734181240063593,
      "grad_norm": 2.479206085205078,
      "learning_rate": 4.533536973399516e-05,
      "loss": 0.6247,
      "step": 6081
    },
    {
      "epoch": 0.7735453100158982,
      "grad_norm": 3.1909379959106445,
      "learning_rate": 4.530991472572229e-05,
      "loss": 0.4472,
      "step": 6082
    },
    {
      "epoch": 0.7736724960254372,
      "grad_norm": 3.596092939376831,
      "learning_rate": 4.528445971744941e-05,
      "loss": 0.6243,
      "step": 6083
    },
    {
      "epoch": 0.7737996820349762,
      "grad_norm": 2.9333744049072266,
      "learning_rate": 4.525900470917653e-05,
      "loss": 0.5437,
      "step": 6084
    },
    {
      "epoch": 0.773926868044515,
      "grad_norm": 3.265904664993286,
      "learning_rate": 4.523354970090366e-05,
      "loss": 0.6981,
      "step": 6085
    },
    {
      "epoch": 0.774054054054054,
      "grad_norm": 2.1429977416992188,
      "learning_rate": 4.5208094692630776e-05,
      "loss": 0.5126,
      "step": 6086
    },
    {
      "epoch": 0.774181240063593,
      "grad_norm": 2.1998367309570312,
      "learning_rate": 4.5182639684357895e-05,
      "loss": 0.6033,
      "step": 6087
    },
    {
      "epoch": 0.774308426073132,
      "grad_norm": 1.8794094324111938,
      "learning_rate": 4.515718467608503e-05,
      "loss": 0.5256,
      "step": 6088
    },
    {
      "epoch": 0.7744356120826709,
      "grad_norm": 1.7756538391113281,
      "learning_rate": 4.5131729667812146e-05,
      "loss": 0.6222,
      "step": 6089
    },
    {
      "epoch": 0.7745627980922098,
      "grad_norm": 2.214268922805786,
      "learning_rate": 4.5106274659539264e-05,
      "loss": 0.6277,
      "step": 6090
    },
    {
      "epoch": 0.7746899841017488,
      "grad_norm": 2.393010377883911,
      "learning_rate": 4.508081965126639e-05,
      "loss": 0.608,
      "step": 6091
    },
    {
      "epoch": 0.7748171701112878,
      "grad_norm": 2.2643680572509766,
      "learning_rate": 4.5055364642993516e-05,
      "loss": 0.4897,
      "step": 6092
    },
    {
      "epoch": 0.7749443561208267,
      "grad_norm": 2.422555446624756,
      "learning_rate": 4.5029909634720634e-05,
      "loss": 0.503,
      "step": 6093
    },
    {
      "epoch": 0.7750715421303657,
      "grad_norm": 2.274261713027954,
      "learning_rate": 4.500445462644775e-05,
      "loss": 0.9196,
      "step": 6094
    },
    {
      "epoch": 0.7751987281399046,
      "grad_norm": 1.9599976539611816,
      "learning_rate": 4.497899961817488e-05,
      "loss": 0.4939,
      "step": 6095
    },
    {
      "epoch": 0.7753259141494435,
      "grad_norm": 2.69329833984375,
      "learning_rate": 4.4953544609902e-05,
      "loss": 0.7352,
      "step": 6096
    },
    {
      "epoch": 0.7754531001589825,
      "grad_norm": 1.9886871576309204,
      "learning_rate": 4.492808960162912e-05,
      "loss": 0.5374,
      "step": 6097
    },
    {
      "epoch": 0.7755802861685215,
      "grad_norm": 1.7405887842178345,
      "learning_rate": 4.490263459335625e-05,
      "loss": 0.5613,
      "step": 6098
    },
    {
      "epoch": 0.7757074721780605,
      "grad_norm": 2.376281976699829,
      "learning_rate": 4.487717958508337e-05,
      "loss": 0.6674,
      "step": 6099
    },
    {
      "epoch": 0.7758346581875993,
      "grad_norm": 2.205580949783325,
      "learning_rate": 4.4851724576810486e-05,
      "loss": 0.6215,
      "step": 6100
    },
    {
      "epoch": 0.7759618441971383,
      "grad_norm": 2.4159440994262695,
      "learning_rate": 4.482626956853761e-05,
      "loss": 0.4275,
      "step": 6101
    },
    {
      "epoch": 0.7760890302066773,
      "grad_norm": 1.4939002990722656,
      "learning_rate": 4.480081456026474e-05,
      "loss": 0.4279,
      "step": 6102
    },
    {
      "epoch": 0.7762162162162162,
      "grad_norm": 2.1630899906158447,
      "learning_rate": 4.4775359551991856e-05,
      "loss": 0.4959,
      "step": 6103
    },
    {
      "epoch": 0.7763434022257552,
      "grad_norm": 2.270094871520996,
      "learning_rate": 4.474990454371898e-05,
      "loss": 0.5038,
      "step": 6104
    },
    {
      "epoch": 0.7764705882352941,
      "grad_norm": 2.028437614440918,
      "learning_rate": 4.47244495354461e-05,
      "loss": 0.6983,
      "step": 6105
    },
    {
      "epoch": 0.7765977742448331,
      "grad_norm": 2.865478992462158,
      "learning_rate": 4.469899452717322e-05,
      "loss": 0.7516,
      "step": 6106
    },
    {
      "epoch": 0.776724960254372,
      "grad_norm": 2.2592668533325195,
      "learning_rate": 4.467353951890035e-05,
      "loss": 0.7,
      "step": 6107
    },
    {
      "epoch": 0.776852146263911,
      "grad_norm": 2.799131155014038,
      "learning_rate": 4.464808451062747e-05,
      "loss": 0.4937,
      "step": 6108
    },
    {
      "epoch": 0.77697933227345,
      "grad_norm": 2.315830707550049,
      "learning_rate": 4.462262950235459e-05,
      "loss": 0.4173,
      "step": 6109
    },
    {
      "epoch": 0.7771065182829888,
      "grad_norm": 2.161417245864868,
      "learning_rate": 4.4597174494081714e-05,
      "loss": 0.5332,
      "step": 6110
    },
    {
      "epoch": 0.7772337042925278,
      "grad_norm": 2.638328790664673,
      "learning_rate": 4.457171948580883e-05,
      "loss": 0.7049,
      "step": 6111
    },
    {
      "epoch": 0.7773608903020668,
      "grad_norm": 2.007397413253784,
      "learning_rate": 4.454626447753596e-05,
      "loss": 0.4151,
      "step": 6112
    },
    {
      "epoch": 0.7774880763116058,
      "grad_norm": 2.0103611946105957,
      "learning_rate": 4.4520809469263084e-05,
      "loss": 0.5161,
      "step": 6113
    },
    {
      "epoch": 0.7776152623211446,
      "grad_norm": 1.8239960670471191,
      "learning_rate": 4.44953544609902e-05,
      "loss": 0.4293,
      "step": 6114
    },
    {
      "epoch": 0.7777424483306836,
      "grad_norm": 2.1713833808898926,
      "learning_rate": 4.446989945271732e-05,
      "loss": 0.4355,
      "step": 6115
    },
    {
      "epoch": 0.7778696343402226,
      "grad_norm": 2.912370443344116,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.6242,
      "step": 6116
    },
    {
      "epoch": 0.7779968203497615,
      "grad_norm": 2.3581297397613525,
      "learning_rate": 4.441898943617157e-05,
      "loss": 0.6581,
      "step": 6117
    },
    {
      "epoch": 0.7781240063593005,
      "grad_norm": 2.349071979522705,
      "learning_rate": 4.439353442789869e-05,
      "loss": 0.5321,
      "step": 6118
    },
    {
      "epoch": 0.7782511923688394,
      "grad_norm": 2.0911097526550293,
      "learning_rate": 4.4368079419625816e-05,
      "loss": 0.695,
      "step": 6119
    },
    {
      "epoch": 0.7783783783783784,
      "grad_norm": 2.1070823669433594,
      "learning_rate": 4.4342624411352935e-05,
      "loss": 0.4558,
      "step": 6120
    },
    {
      "epoch": 0.7785055643879173,
      "grad_norm": 2.4562673568725586,
      "learning_rate": 4.4317169403080054e-05,
      "loss": 0.7551,
      "step": 6121
    },
    {
      "epoch": 0.7786327503974563,
      "grad_norm": 2.7629494667053223,
      "learning_rate": 4.429171439480718e-05,
      "loss": 0.6903,
      "step": 6122
    },
    {
      "epoch": 0.7787599364069953,
      "grad_norm": 2.2460715770721436,
      "learning_rate": 4.4266259386534305e-05,
      "loss": 0.5083,
      "step": 6123
    },
    {
      "epoch": 0.7788871224165341,
      "grad_norm": 2.4883499145507812,
      "learning_rate": 4.4240804378261424e-05,
      "loss": 0.6431,
      "step": 6124
    },
    {
      "epoch": 0.7790143084260731,
      "grad_norm": 2.813354253768921,
      "learning_rate": 4.421534936998854e-05,
      "loss": 0.3754,
      "step": 6125
    },
    {
      "epoch": 0.7791414944356121,
      "grad_norm": 2.041156530380249,
      "learning_rate": 4.4189894361715675e-05,
      "loss": 0.6448,
      "step": 6126
    },
    {
      "epoch": 0.7792686804451511,
      "grad_norm": 2.31443190574646,
      "learning_rate": 4.416443935344279e-05,
      "loss": 0.7776,
      "step": 6127
    },
    {
      "epoch": 0.77939586645469,
      "grad_norm": 2.571232318878174,
      "learning_rate": 4.413898434516991e-05,
      "loss": 0.6215,
      "step": 6128
    },
    {
      "epoch": 0.7795230524642289,
      "grad_norm": 2.170635938644409,
      "learning_rate": 4.411352933689704e-05,
      "loss": 0.6477,
      "step": 6129
    },
    {
      "epoch": 0.7796502384737679,
      "grad_norm": 2.827624559402466,
      "learning_rate": 4.4088074328624156e-05,
      "loss": 0.6035,
      "step": 6130
    },
    {
      "epoch": 0.7797774244833069,
      "grad_norm": 2.4385664463043213,
      "learning_rate": 4.406261932035128e-05,
      "loss": 0.5307,
      "step": 6131
    },
    {
      "epoch": 0.7799046104928458,
      "grad_norm": 1.7121895551681519,
      "learning_rate": 4.403716431207841e-05,
      "loss": 0.4894,
      "step": 6132
    },
    {
      "epoch": 0.7800317965023847,
      "grad_norm": 3.1155104637145996,
      "learning_rate": 4.4011709303805526e-05,
      "loss": 0.8753,
      "step": 6133
    },
    {
      "epoch": 0.7801589825119237,
      "grad_norm": 3.363360643386841,
      "learning_rate": 4.3986254295532645e-05,
      "loss": 0.6715,
      "step": 6134
    },
    {
      "epoch": 0.7802861685214626,
      "grad_norm": 1.862834095954895,
      "learning_rate": 4.396079928725977e-05,
      "loss": 0.4114,
      "step": 6135
    },
    {
      "epoch": 0.7804133545310016,
      "grad_norm": 1.6221439838409424,
      "learning_rate": 4.3935344278986896e-05,
      "loss": 0.3852,
      "step": 6136
    },
    {
      "epoch": 0.7805405405405406,
      "grad_norm": 1.717397689819336,
      "learning_rate": 4.3909889270714015e-05,
      "loss": 0.349,
      "step": 6137
    },
    {
      "epoch": 0.7806677265500795,
      "grad_norm": 2.212357521057129,
      "learning_rate": 4.388443426244114e-05,
      "loss": 0.4819,
      "step": 6138
    },
    {
      "epoch": 0.7807949125596184,
      "grad_norm": 2.8145968914031982,
      "learning_rate": 4.385897925416826e-05,
      "loss": 0.3968,
      "step": 6139
    },
    {
      "epoch": 0.7809220985691574,
      "grad_norm": 2.843609571456909,
      "learning_rate": 4.383352424589538e-05,
      "loss": 0.6771,
      "step": 6140
    },
    {
      "epoch": 0.7810492845786964,
      "grad_norm": 1.700567603111267,
      "learning_rate": 4.380806923762251e-05,
      "loss": 0.6629,
      "step": 6141
    },
    {
      "epoch": 0.7811764705882352,
      "grad_norm": 1.8230363130569458,
      "learning_rate": 4.378261422934963e-05,
      "loss": 0.556,
      "step": 6142
    },
    {
      "epoch": 0.7813036565977742,
      "grad_norm": 2.5526483058929443,
      "learning_rate": 4.375715922107675e-05,
      "loss": 0.5298,
      "step": 6143
    },
    {
      "epoch": 0.7814308426073132,
      "grad_norm": 2.691880941390991,
      "learning_rate": 4.373170421280387e-05,
      "loss": 0.803,
      "step": 6144
    },
    {
      "epoch": 0.7815580286168522,
      "grad_norm": 2.601365327835083,
      "learning_rate": 4.370624920453099e-05,
      "loss": 0.7967,
      "step": 6145
    },
    {
      "epoch": 0.7816852146263911,
      "grad_norm": 2.2157835960388184,
      "learning_rate": 4.368079419625812e-05,
      "loss": 0.5808,
      "step": 6146
    },
    {
      "epoch": 0.78181240063593,
      "grad_norm": 2.1194310188293457,
      "learning_rate": 4.3655339187985236e-05,
      "loss": 0.4773,
      "step": 6147
    },
    {
      "epoch": 0.781939586645469,
      "grad_norm": 2.468308687210083,
      "learning_rate": 4.362988417971236e-05,
      "loss": 0.7031,
      "step": 6148
    },
    {
      "epoch": 0.7820667726550079,
      "grad_norm": 3.223890781402588,
      "learning_rate": 4.360442917143948e-05,
      "loss": 0.3893,
      "step": 6149
    },
    {
      "epoch": 0.7821939586645469,
      "grad_norm": 1.5686590671539307,
      "learning_rate": 4.3578974163166606e-05,
      "loss": 0.4787,
      "step": 6150
    },
    {
      "epoch": 0.7823211446740859,
      "grad_norm": 2.8322157859802246,
      "learning_rate": 4.355351915489373e-05,
      "loss": 0.5157,
      "step": 6151
    },
    {
      "epoch": 0.7824483306836248,
      "grad_norm": 2.462719678878784,
      "learning_rate": 4.352806414662085e-05,
      "loss": 0.4561,
      "step": 6152
    },
    {
      "epoch": 0.7825755166931637,
      "grad_norm": 2.4443273544311523,
      "learning_rate": 4.350260913834797e-05,
      "loss": 0.3988,
      "step": 6153
    },
    {
      "epoch": 0.7827027027027027,
      "grad_norm": 1.9128499031066895,
      "learning_rate": 4.3477154130075094e-05,
      "loss": 0.4893,
      "step": 6154
    },
    {
      "epoch": 0.7828298887122417,
      "grad_norm": 1.5673972368240356,
      "learning_rate": 4.345169912180221e-05,
      "loss": 0.2832,
      "step": 6155
    },
    {
      "epoch": 0.7829570747217806,
      "grad_norm": 2.423257827758789,
      "learning_rate": 4.342624411352934e-05,
      "loss": 0.4898,
      "step": 6156
    },
    {
      "epoch": 0.7830842607313195,
      "grad_norm": 1.9118362665176392,
      "learning_rate": 4.3400789105256464e-05,
      "loss": 0.6119,
      "step": 6157
    },
    {
      "epoch": 0.7832114467408585,
      "grad_norm": 2.2035505771636963,
      "learning_rate": 4.337533409698358e-05,
      "loss": 0.6339,
      "step": 6158
    },
    {
      "epoch": 0.7833386327503975,
      "grad_norm": 1.749751329421997,
      "learning_rate": 4.33498790887107e-05,
      "loss": 0.368,
      "step": 6159
    },
    {
      "epoch": 0.7834658187599364,
      "grad_norm": 2.044565200805664,
      "learning_rate": 4.3324424080437834e-05,
      "loss": 0.6171,
      "step": 6160
    },
    {
      "epoch": 0.7835930047694754,
      "grad_norm": 2.8083548545837402,
      "learning_rate": 4.329896907216495e-05,
      "loss": 0.705,
      "step": 6161
    },
    {
      "epoch": 0.7837201907790143,
      "grad_norm": 1.8295882940292358,
      "learning_rate": 4.327351406389207e-05,
      "loss": 0.3164,
      "step": 6162
    },
    {
      "epoch": 0.7838473767885532,
      "grad_norm": 2.377903938293457,
      "learning_rate": 4.32480590556192e-05,
      "loss": 0.7156,
      "step": 6163
    },
    {
      "epoch": 0.7839745627980922,
      "grad_norm": 2.6301612854003906,
      "learning_rate": 4.3222604047346315e-05,
      "loss": 0.6608,
      "step": 6164
    },
    {
      "epoch": 0.7841017488076312,
      "grad_norm": 2.689425230026245,
      "learning_rate": 4.319714903907344e-05,
      "loss": 0.4407,
      "step": 6165
    },
    {
      "epoch": 0.7842289348171702,
      "grad_norm": 3.423409938812256,
      "learning_rate": 4.3171694030800567e-05,
      "loss": 0.5669,
      "step": 6166
    },
    {
      "epoch": 0.784356120826709,
      "grad_norm": 2.332163095474243,
      "learning_rate": 4.3146239022527685e-05,
      "loss": 0.6761,
      "step": 6167
    },
    {
      "epoch": 0.784483306836248,
      "grad_norm": 2.0741822719573975,
      "learning_rate": 4.3120784014254804e-05,
      "loss": 0.6447,
      "step": 6168
    },
    {
      "epoch": 0.784610492845787,
      "grad_norm": 2.709804058074951,
      "learning_rate": 4.309532900598193e-05,
      "loss": 0.7875,
      "step": 6169
    },
    {
      "epoch": 0.7847376788553259,
      "grad_norm": 1.8511247634887695,
      "learning_rate": 4.3069873997709055e-05,
      "loss": 0.5823,
      "step": 6170
    },
    {
      "epoch": 0.7848648648648648,
      "grad_norm": 2.1868550777435303,
      "learning_rate": 4.3044418989436174e-05,
      "loss": 0.4959,
      "step": 6171
    },
    {
      "epoch": 0.7849920508744038,
      "grad_norm": 2.663076877593994,
      "learning_rate": 4.30189639811633e-05,
      "loss": 0.7028,
      "step": 6172
    },
    {
      "epoch": 0.7851192368839428,
      "grad_norm": 2.5430188179016113,
      "learning_rate": 4.299350897289042e-05,
      "loss": 0.6949,
      "step": 6173
    },
    {
      "epoch": 0.7852464228934817,
      "grad_norm": 3.212977647781372,
      "learning_rate": 4.296805396461754e-05,
      "loss": 0.5336,
      "step": 6174
    },
    {
      "epoch": 0.7853736089030207,
      "grad_norm": 2.9556941986083984,
      "learning_rate": 4.294259895634466e-05,
      "loss": 0.9224,
      "step": 6175
    },
    {
      "epoch": 0.7855007949125596,
      "grad_norm": 2.312687635421753,
      "learning_rate": 4.291714394807179e-05,
      "loss": 0.563,
      "step": 6176
    },
    {
      "epoch": 0.7856279809220986,
      "grad_norm": 2.9140639305114746,
      "learning_rate": 4.2891688939798907e-05,
      "loss": 0.5869,
      "step": 6177
    },
    {
      "epoch": 0.7857551669316375,
      "grad_norm": 1.9046574831008911,
      "learning_rate": 4.2866233931526025e-05,
      "loss": 0.5122,
      "step": 6178
    },
    {
      "epoch": 0.7858823529411765,
      "grad_norm": 2.253293037414551,
      "learning_rate": 4.284077892325315e-05,
      "loss": 0.6107,
      "step": 6179
    },
    {
      "epoch": 0.7860095389507155,
      "grad_norm": 1.7233911752700806,
      "learning_rate": 4.2815323914980276e-05,
      "loss": 0.3725,
      "step": 6180
    },
    {
      "epoch": 0.7861367249602543,
      "grad_norm": 1.7311434745788574,
      "learning_rate": 4.2789868906707395e-05,
      "loss": 0.3259,
      "step": 6181
    },
    {
      "epoch": 0.7862639109697933,
      "grad_norm": 2.2955734729766846,
      "learning_rate": 4.276441389843452e-05,
      "loss": 0.5631,
      "step": 6182
    },
    {
      "epoch": 0.7863910969793323,
      "grad_norm": 2.4295382499694824,
      "learning_rate": 4.273895889016164e-05,
      "loss": 0.6026,
      "step": 6183
    },
    {
      "epoch": 0.7865182829888713,
      "grad_norm": 1.6086201667785645,
      "learning_rate": 4.271350388188876e-05,
      "loss": 0.5967,
      "step": 6184
    },
    {
      "epoch": 0.7866454689984101,
      "grad_norm": 1.9237653017044067,
      "learning_rate": 4.268804887361589e-05,
      "loss": 0.3541,
      "step": 6185
    },
    {
      "epoch": 0.7867726550079491,
      "grad_norm": 1.5965158939361572,
      "learning_rate": 4.266259386534301e-05,
      "loss": 0.6641,
      "step": 6186
    },
    {
      "epoch": 0.7868998410174881,
      "grad_norm": 2.5454211235046387,
      "learning_rate": 4.263713885707013e-05,
      "loss": 0.7156,
      "step": 6187
    },
    {
      "epoch": 0.787027027027027,
      "grad_norm": 2.732750177383423,
      "learning_rate": 4.261168384879725e-05,
      "loss": 0.7261,
      "step": 6188
    },
    {
      "epoch": 0.787154213036566,
      "grad_norm": 2.3174939155578613,
      "learning_rate": 4.258622884052437e-05,
      "loss": 0.5105,
      "step": 6189
    },
    {
      "epoch": 0.787281399046105,
      "grad_norm": 1.6950222253799438,
      "learning_rate": 4.25607738322515e-05,
      "loss": 0.4661,
      "step": 6190
    },
    {
      "epoch": 0.7874085850556439,
      "grad_norm": 2.274678945541382,
      "learning_rate": 4.253531882397862e-05,
      "loss": 0.6245,
      "step": 6191
    },
    {
      "epoch": 0.7875357710651828,
      "grad_norm": 2.195765733718872,
      "learning_rate": 4.250986381570574e-05,
      "loss": 0.4787,
      "step": 6192
    },
    {
      "epoch": 0.7876629570747218,
      "grad_norm": 2.9456093311309814,
      "learning_rate": 4.248440880743286e-05,
      "loss": 0.7976,
      "step": 6193
    },
    {
      "epoch": 0.7877901430842608,
      "grad_norm": 2.355062246322632,
      "learning_rate": 4.245895379915999e-05,
      "loss": 0.5687,
      "step": 6194
    },
    {
      "epoch": 0.7879173290937996,
      "grad_norm": 3.105916976928711,
      "learning_rate": 4.243349879088711e-05,
      "loss": 0.4301,
      "step": 6195
    },
    {
      "epoch": 0.7880445151033386,
      "grad_norm": 3.4609336853027344,
      "learning_rate": 4.240804378261423e-05,
      "loss": 0.6315,
      "step": 6196
    },
    {
      "epoch": 0.7881717011128776,
      "grad_norm": 2.7702884674072266,
      "learning_rate": 4.2382588774341356e-05,
      "loss": 0.6437,
      "step": 6197
    },
    {
      "epoch": 0.7882988871224166,
      "grad_norm": 2.315560817718506,
      "learning_rate": 4.2357133766068475e-05,
      "loss": 0.6568,
      "step": 6198
    },
    {
      "epoch": 0.7884260731319555,
      "grad_norm": 2.5425822734832764,
      "learning_rate": 4.23316787577956e-05,
      "loss": 0.6455,
      "step": 6199
    },
    {
      "epoch": 0.7885532591414944,
      "grad_norm": 2.8560051918029785,
      "learning_rate": 4.230622374952272e-05,
      "loss": 0.5298,
      "step": 6200
    },
    {
      "epoch": 0.7886804451510334,
      "grad_norm": 1.7263367176055908,
      "learning_rate": 4.2280768741249844e-05,
      "loss": 0.4608,
      "step": 6201
    },
    {
      "epoch": 0.7888076311605723,
      "grad_norm": 2.94874906539917,
      "learning_rate": 4.225531373297696e-05,
      "loss": 0.6816,
      "step": 6202
    },
    {
      "epoch": 0.7889348171701113,
      "grad_norm": 1.8985164165496826,
      "learning_rate": 4.222985872470409e-05,
      "loss": 0.3438,
      "step": 6203
    },
    {
      "epoch": 0.7890620031796503,
      "grad_norm": 2.123232364654541,
      "learning_rate": 4.2204403716431214e-05,
      "loss": 0.5421,
      "step": 6204
    },
    {
      "epoch": 0.7891891891891892,
      "grad_norm": 2.5950090885162354,
      "learning_rate": 4.217894870815833e-05,
      "loss": 0.5417,
      "step": 6205
    },
    {
      "epoch": 0.7893163751987281,
      "grad_norm": 2.7184345722198486,
      "learning_rate": 4.215349369988545e-05,
      "loss": 0.7652,
      "step": 6206
    },
    {
      "epoch": 0.7894435612082671,
      "grad_norm": 1.7822294235229492,
      "learning_rate": 4.212803869161258e-05,
      "loss": 0.5331,
      "step": 6207
    },
    {
      "epoch": 0.7895707472178061,
      "grad_norm": 2.1184604167938232,
      "learning_rate": 4.2102583683339696e-05,
      "loss": 0.555,
      "step": 6208
    },
    {
      "epoch": 0.7896979332273449,
      "grad_norm": 2.8279800415039062,
      "learning_rate": 4.207712867506682e-05,
      "loss": 0.4888,
      "step": 6209
    },
    {
      "epoch": 0.7898251192368839,
      "grad_norm": 2.3716483116149902,
      "learning_rate": 4.205167366679395e-05,
      "loss": 0.5988,
      "step": 6210
    },
    {
      "epoch": 0.7899523052464229,
      "grad_norm": 2.320456027984619,
      "learning_rate": 4.2026218658521066e-05,
      "loss": 0.6656,
      "step": 6211
    },
    {
      "epoch": 0.7900794912559619,
      "grad_norm": 2.4173784255981445,
      "learning_rate": 4.2000763650248184e-05,
      "loss": 0.3732,
      "step": 6212
    },
    {
      "epoch": 0.7902066772655008,
      "grad_norm": 1.8382898569107056,
      "learning_rate": 4.197530864197531e-05,
      "loss": 0.5461,
      "step": 6213
    },
    {
      "epoch": 0.7903338632750397,
      "grad_norm": 2.6330692768096924,
      "learning_rate": 4.1949853633702435e-05,
      "loss": 0.4731,
      "step": 6214
    },
    {
      "epoch": 0.7904610492845787,
      "grad_norm": 2.025698661804199,
      "learning_rate": 4.1924398625429554e-05,
      "loss": 0.4087,
      "step": 6215
    },
    {
      "epoch": 0.7905882352941176,
      "grad_norm": 2.1452393531799316,
      "learning_rate": 4.189894361715668e-05,
      "loss": 0.4447,
      "step": 6216
    },
    {
      "epoch": 0.7907154213036566,
      "grad_norm": 2.680680513381958,
      "learning_rate": 4.18734886088838e-05,
      "loss": 0.7616,
      "step": 6217
    },
    {
      "epoch": 0.7908426073131956,
      "grad_norm": 2.0195796489715576,
      "learning_rate": 4.184803360061092e-05,
      "loss": 0.4901,
      "step": 6218
    },
    {
      "epoch": 0.7909697933227345,
      "grad_norm": 1.8086885213851929,
      "learning_rate": 4.182257859233805e-05,
      "loss": 0.5991,
      "step": 6219
    },
    {
      "epoch": 0.7910969793322734,
      "grad_norm": 2.993211269378662,
      "learning_rate": 4.179712358406517e-05,
      "loss": 0.5173,
      "step": 6220
    },
    {
      "epoch": 0.7912241653418124,
      "grad_norm": 2.3721911907196045,
      "learning_rate": 4.177166857579229e-05,
      "loss": 0.5967,
      "step": 6221
    },
    {
      "epoch": 0.7913513513513514,
      "grad_norm": 2.0128989219665527,
      "learning_rate": 4.174621356751941e-05,
      "loss": 0.6668,
      "step": 6222
    },
    {
      "epoch": 0.7914785373608904,
      "grad_norm": 2.2572028636932373,
      "learning_rate": 4.172075855924653e-05,
      "loss": 0.5019,
      "step": 6223
    },
    {
      "epoch": 0.7916057233704292,
      "grad_norm": 2.004737377166748,
      "learning_rate": 4.169530355097366e-05,
      "loss": 0.2954,
      "step": 6224
    },
    {
      "epoch": 0.7917329093799682,
      "grad_norm": 2.3717525005340576,
      "learning_rate": 4.166984854270078e-05,
      "loss": 0.3804,
      "step": 6225
    },
    {
      "epoch": 0.7918600953895072,
      "grad_norm": 1.9795284271240234,
      "learning_rate": 4.16443935344279e-05,
      "loss": 0.5877,
      "step": 6226
    },
    {
      "epoch": 0.7919872813990461,
      "grad_norm": 2.2608604431152344,
      "learning_rate": 4.161893852615502e-05,
      "loss": 0.5261,
      "step": 6227
    },
    {
      "epoch": 0.792114467408585,
      "grad_norm": 2.23356294631958,
      "learning_rate": 4.1593483517882145e-05,
      "loss": 0.5022,
      "step": 6228
    },
    {
      "epoch": 0.792241653418124,
      "grad_norm": 2.10508394241333,
      "learning_rate": 4.156802850960927e-05,
      "loss": 0.5442,
      "step": 6229
    },
    {
      "epoch": 0.792368839427663,
      "grad_norm": 2.494443416595459,
      "learning_rate": 4.154257350133639e-05,
      "loss": 0.5342,
      "step": 6230
    },
    {
      "epoch": 0.7924960254372019,
      "grad_norm": 1.5108150243759155,
      "learning_rate": 4.151711849306351e-05,
      "loss": 0.3633,
      "step": 6231
    },
    {
      "epoch": 0.7926232114467409,
      "grad_norm": 1.918926477432251,
      "learning_rate": 4.1491663484790634e-05,
      "loss": 0.4522,
      "step": 6232
    },
    {
      "epoch": 0.7927503974562798,
      "grad_norm": 2.609534502029419,
      "learning_rate": 4.146620847651776e-05,
      "loss": 0.587,
      "step": 6233
    },
    {
      "epoch": 0.7928775834658187,
      "grad_norm": 2.3309292793273926,
      "learning_rate": 4.144075346824488e-05,
      "loss": 0.344,
      "step": 6234
    },
    {
      "epoch": 0.7930047694753577,
      "grad_norm": 2.3698196411132812,
      "learning_rate": 4.1415298459972003e-05,
      "loss": 0.4912,
      "step": 6235
    },
    {
      "epoch": 0.7931319554848967,
      "grad_norm": 2.7868382930755615,
      "learning_rate": 4.138984345169912e-05,
      "loss": 0.5353,
      "step": 6236
    },
    {
      "epoch": 0.7932591414944357,
      "grad_norm": 2.7272844314575195,
      "learning_rate": 4.136438844342624e-05,
      "loss": 0.5761,
      "step": 6237
    },
    {
      "epoch": 0.7933863275039745,
      "grad_norm": 2.3952555656433105,
      "learning_rate": 4.133893343515337e-05,
      "loss": 0.3656,
      "step": 6238
    },
    {
      "epoch": 0.7935135135135135,
      "grad_norm": 1.9462193250656128,
      "learning_rate": 4.131347842688049e-05,
      "loss": 0.4905,
      "step": 6239
    },
    {
      "epoch": 0.7936406995230525,
      "grad_norm": 2.1352438926696777,
      "learning_rate": 4.128802341860761e-05,
      "loss": 0.4997,
      "step": 6240
    },
    {
      "epoch": 0.7937678855325914,
      "grad_norm": 2.7354798316955566,
      "learning_rate": 4.1262568410334736e-05,
      "loss": 0.6594,
      "step": 6241
    },
    {
      "epoch": 0.7938950715421303,
      "grad_norm": 2.162503242492676,
      "learning_rate": 4.1237113402061855e-05,
      "loss": 0.6525,
      "step": 6242
    },
    {
      "epoch": 0.7940222575516693,
      "grad_norm": 2.342615842819214,
      "learning_rate": 4.121165839378898e-05,
      "loss": 0.4555,
      "step": 6243
    },
    {
      "epoch": 0.7941494435612083,
      "grad_norm": 2.3715827465057373,
      "learning_rate": 4.1186203385516106e-05,
      "loss": 0.5603,
      "step": 6244
    },
    {
      "epoch": 0.7942766295707472,
      "grad_norm": 1.9926048517227173,
      "learning_rate": 4.1160748377243225e-05,
      "loss": 0.5553,
      "step": 6245
    },
    {
      "epoch": 0.7944038155802862,
      "grad_norm": 1.658582329750061,
      "learning_rate": 4.1135293368970344e-05,
      "loss": 0.4403,
      "step": 6246
    },
    {
      "epoch": 0.7945310015898251,
      "grad_norm": 3.302480936050415,
      "learning_rate": 4.110983836069747e-05,
      "loss": 0.4666,
      "step": 6247
    },
    {
      "epoch": 0.794658187599364,
      "grad_norm": 3.7984724044799805,
      "learning_rate": 4.1084383352424595e-05,
      "loss": 0.4316,
      "step": 6248
    },
    {
      "epoch": 0.794785373608903,
      "grad_norm": 2.0605533123016357,
      "learning_rate": 4.105892834415171e-05,
      "loss": 0.6187,
      "step": 6249
    },
    {
      "epoch": 0.794912559618442,
      "grad_norm": 2.449673652648926,
      "learning_rate": 4.103347333587884e-05,
      "loss": 0.7144,
      "step": 6250
    },
    {
      "epoch": 0.795039745627981,
      "grad_norm": 1.94977605342865,
      "learning_rate": 4.100801832760596e-05,
      "loss": 0.603,
      "step": 6251
    },
    {
      "epoch": 0.7951669316375198,
      "grad_norm": 2.1150248050689697,
      "learning_rate": 4.0982563319333076e-05,
      "loss": 0.6337,
      "step": 6252
    },
    {
      "epoch": 0.7952941176470588,
      "grad_norm": 2.3487401008605957,
      "learning_rate": 4.09571083110602e-05,
      "loss": 0.4705,
      "step": 6253
    },
    {
      "epoch": 0.7954213036565978,
      "grad_norm": 2.6353259086608887,
      "learning_rate": 4.093165330278733e-05,
      "loss": 0.6413,
      "step": 6254
    },
    {
      "epoch": 0.7955484896661367,
      "grad_norm": 2.9560248851776123,
      "learning_rate": 4.0906198294514446e-05,
      "loss": 0.4112,
      "step": 6255
    },
    {
      "epoch": 0.7956756756756757,
      "grad_norm": 2.702606439590454,
      "learning_rate": 4.088074328624157e-05,
      "loss": 0.8656,
      "step": 6256
    },
    {
      "epoch": 0.7958028616852146,
      "grad_norm": 2.5236971378326416,
      "learning_rate": 4.085528827796869e-05,
      "loss": 0.8189,
      "step": 6257
    },
    {
      "epoch": 0.7959300476947536,
      "grad_norm": 2.16941499710083,
      "learning_rate": 4.0829833269695816e-05,
      "loss": 0.5366,
      "step": 6258
    },
    {
      "epoch": 0.7960572337042925,
      "grad_norm": 2.1209137439727783,
      "learning_rate": 4.0804378261422935e-05,
      "loss": 0.662,
      "step": 6259
    },
    {
      "epoch": 0.7961844197138315,
      "grad_norm": 3.0073938369750977,
      "learning_rate": 4.077892325315006e-05,
      "loss": 0.6054,
      "step": 6260
    },
    {
      "epoch": 0.7963116057233705,
      "grad_norm": 1.8082406520843506,
      "learning_rate": 4.075346824487718e-05,
      "loss": 0.434,
      "step": 6261
    },
    {
      "epoch": 0.7964387917329094,
      "grad_norm": 2.580601215362549,
      "learning_rate": 4.0728013236604304e-05,
      "loss": 0.6483,
      "step": 6262
    },
    {
      "epoch": 0.7965659777424483,
      "grad_norm": 2.3905084133148193,
      "learning_rate": 4.070255822833143e-05,
      "loss": 0.4025,
      "step": 6263
    },
    {
      "epoch": 0.7966931637519873,
      "grad_norm": 2.50368595123291,
      "learning_rate": 4.067710322005855e-05,
      "loss": 0.5851,
      "step": 6264
    },
    {
      "epoch": 0.7968203497615263,
      "grad_norm": 2.7963335514068604,
      "learning_rate": 4.065164821178567e-05,
      "loss": 0.6463,
      "step": 6265
    },
    {
      "epoch": 0.7969475357710651,
      "grad_norm": 1.827252984046936,
      "learning_rate": 4.062619320351279e-05,
      "loss": 0.5214,
      "step": 6266
    },
    {
      "epoch": 0.7970747217806041,
      "grad_norm": 2.8079428672790527,
      "learning_rate": 4.060073819523992e-05,
      "loss": 0.5031,
      "step": 6267
    },
    {
      "epoch": 0.7972019077901431,
      "grad_norm": 2.5898642539978027,
      "learning_rate": 4.057528318696704e-05,
      "loss": 1.0219,
      "step": 6268
    },
    {
      "epoch": 0.7973290937996821,
      "grad_norm": 2.3329412937164307,
      "learning_rate": 4.054982817869416e-05,
      "loss": 0.4964,
      "step": 6269
    },
    {
      "epoch": 0.797456279809221,
      "grad_norm": 1.8399335145950317,
      "learning_rate": 4.052437317042128e-05,
      "loss": 0.4314,
      "step": 6270
    },
    {
      "epoch": 0.7975834658187599,
      "grad_norm": 2.365530014038086,
      "learning_rate": 4.04989181621484e-05,
      "loss": 0.3852,
      "step": 6271
    },
    {
      "epoch": 0.7977106518282989,
      "grad_norm": 3.099313974380493,
      "learning_rate": 4.047346315387553e-05,
      "loss": 0.2826,
      "step": 6272
    },
    {
      "epoch": 0.7978378378378378,
      "grad_norm": 2.334148406982422,
      "learning_rate": 4.044800814560265e-05,
      "loss": 0.6179,
      "step": 6273
    },
    {
      "epoch": 0.7979650238473768,
      "grad_norm": 2.3819563388824463,
      "learning_rate": 4.042255313732977e-05,
      "loss": 0.5497,
      "step": 6274
    },
    {
      "epoch": 0.7980922098569158,
      "grad_norm": 2.0160973072052,
      "learning_rate": 4.0397098129056895e-05,
      "loss": 0.4931,
      "step": 6275
    },
    {
      "epoch": 0.7982193958664547,
      "grad_norm": 2.06776762008667,
      "learning_rate": 4.0371643120784014e-05,
      "loss": 0.4841,
      "step": 6276
    },
    {
      "epoch": 0.7983465818759936,
      "grad_norm": 2.482004404067993,
      "learning_rate": 4.034618811251114e-05,
      "loss": 0.5917,
      "step": 6277
    },
    {
      "epoch": 0.7984737678855326,
      "grad_norm": 1.4326035976409912,
      "learning_rate": 4.0320733104238265e-05,
      "loss": 0.3623,
      "step": 6278
    },
    {
      "epoch": 0.7986009538950716,
      "grad_norm": 2.461524248123169,
      "learning_rate": 4.0295278095965384e-05,
      "loss": 0.4324,
      "step": 6279
    },
    {
      "epoch": 0.7987281399046104,
      "grad_norm": 2.5739119052886963,
      "learning_rate": 4.02698230876925e-05,
      "loss": 0.6732,
      "step": 6280
    },
    {
      "epoch": 0.7988553259141494,
      "grad_norm": 1.9912614822387695,
      "learning_rate": 4.024436807941963e-05,
      "loss": 0.6651,
      "step": 6281
    },
    {
      "epoch": 0.7989825119236884,
      "grad_norm": 2.106250762939453,
      "learning_rate": 4.0218913071146754e-05,
      "loss": 0.6106,
      "step": 6282
    },
    {
      "epoch": 0.7991096979332274,
      "grad_norm": 1.7756788730621338,
      "learning_rate": 4.019345806287387e-05,
      "loss": 0.5682,
      "step": 6283
    },
    {
      "epoch": 0.7992368839427663,
      "grad_norm": 2.4108095169067383,
      "learning_rate": 4.016800305460099e-05,
      "loss": 0.3733,
      "step": 6284
    },
    {
      "epoch": 0.7993640699523052,
      "grad_norm": 2.756978750228882,
      "learning_rate": 4.014254804632812e-05,
      "loss": 0.7698,
      "step": 6285
    },
    {
      "epoch": 0.7994912559618442,
      "grad_norm": 2.3278756141662598,
      "learning_rate": 4.0117093038055235e-05,
      "loss": 0.6874,
      "step": 6286
    },
    {
      "epoch": 0.7996184419713831,
      "grad_norm": 2.5721824169158936,
      "learning_rate": 4.009163802978236e-05,
      "loss": 0.718,
      "step": 6287
    },
    {
      "epoch": 0.7997456279809221,
      "grad_norm": 1.6528394222259521,
      "learning_rate": 4.0066183021509486e-05,
      "loss": 0.3799,
      "step": 6288
    },
    {
      "epoch": 0.7998728139904611,
      "grad_norm": 2.600337028503418,
      "learning_rate": 4.0040728013236605e-05,
      "loss": 0.4043,
      "step": 6289
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.7516939640045166,
      "learning_rate": 4.0015273004963724e-05,
      "loss": 0.5053,
      "step": 6290
    },
    {
      "epoch": 0.8001271860095389,
      "grad_norm": 2.3896260261535645,
      "learning_rate": 3.998981799669085e-05,
      "loss": 0.5821,
      "step": 6291
    },
    {
      "epoch": 0.8002543720190779,
      "grad_norm": 1.9758353233337402,
      "learning_rate": 3.9964362988417975e-05,
      "loss": 0.614,
      "step": 6292
    },
    {
      "epoch": 0.8003815580286169,
      "grad_norm": 2.4281065464019775,
      "learning_rate": 3.9938907980145094e-05,
      "loss": 0.7546,
      "step": 6293
    },
    {
      "epoch": 0.8005087440381558,
      "grad_norm": 1.6579968929290771,
      "learning_rate": 3.991345297187222e-05,
      "loss": 0.3501,
      "step": 6294
    },
    {
      "epoch": 0.8006359300476947,
      "grad_norm": 1.8047212362289429,
      "learning_rate": 3.988799796359934e-05,
      "loss": 0.4353,
      "step": 6295
    },
    {
      "epoch": 0.8007631160572337,
      "grad_norm": 1.7620923519134521,
      "learning_rate": 3.9862542955326463e-05,
      "loss": 0.3109,
      "step": 6296
    },
    {
      "epoch": 0.8008903020667727,
      "grad_norm": 2.4728190898895264,
      "learning_rate": 3.983708794705359e-05,
      "loss": 0.6725,
      "step": 6297
    },
    {
      "epoch": 0.8010174880763116,
      "grad_norm": 2.335218667984009,
      "learning_rate": 3.981163293878071e-05,
      "loss": 0.6164,
      "step": 6298
    },
    {
      "epoch": 0.8011446740858505,
      "grad_norm": 2.5694363117218018,
      "learning_rate": 3.9786177930507826e-05,
      "loss": 0.8309,
      "step": 6299
    },
    {
      "epoch": 0.8012718600953895,
      "grad_norm": 2.1140143871307373,
      "learning_rate": 3.976072292223495e-05,
      "loss": 0.4633,
      "step": 6300
    },
    {
      "epoch": 0.8013990461049284,
      "grad_norm": 1.7777715921401978,
      "learning_rate": 3.973526791396208e-05,
      "loss": 0.5973,
      "step": 6301
    },
    {
      "epoch": 0.8015262321144674,
      "grad_norm": 1.7372294664382935,
      "learning_rate": 3.9709812905689196e-05,
      "loss": 0.3975,
      "step": 6302
    },
    {
      "epoch": 0.8016534181240064,
      "grad_norm": 3.7843704223632812,
      "learning_rate": 3.968435789741632e-05,
      "loss": 0.5368,
      "step": 6303
    },
    {
      "epoch": 0.8017806041335453,
      "grad_norm": 1.9645780324935913,
      "learning_rate": 3.965890288914344e-05,
      "loss": 0.3845,
      "step": 6304
    },
    {
      "epoch": 0.8019077901430842,
      "grad_norm": 2.7445640563964844,
      "learning_rate": 3.963344788087056e-05,
      "loss": 0.7589,
      "step": 6305
    },
    {
      "epoch": 0.8020349761526232,
      "grad_norm": 2.206836462020874,
      "learning_rate": 3.9607992872597685e-05,
      "loss": 0.5444,
      "step": 6306
    },
    {
      "epoch": 0.8021621621621622,
      "grad_norm": 1.905046820640564,
      "learning_rate": 3.958253786432481e-05,
      "loss": 0.47,
      "step": 6307
    },
    {
      "epoch": 0.8022893481717012,
      "grad_norm": 2.6086623668670654,
      "learning_rate": 3.955708285605193e-05,
      "loss": 0.6653,
      "step": 6308
    },
    {
      "epoch": 0.80241653418124,
      "grad_norm": 1.8920890092849731,
      "learning_rate": 3.9531627847779054e-05,
      "loss": 0.4317,
      "step": 6309
    },
    {
      "epoch": 0.802543720190779,
      "grad_norm": 2.5164458751678467,
      "learning_rate": 3.950617283950617e-05,
      "loss": 0.4824,
      "step": 6310
    },
    {
      "epoch": 0.802670906200318,
      "grad_norm": 2.6008102893829346,
      "learning_rate": 3.94807178312333e-05,
      "loss": 0.5862,
      "step": 6311
    },
    {
      "epoch": 0.8027980922098569,
      "grad_norm": 1.7181836366653442,
      "learning_rate": 3.945526282296042e-05,
      "loss": 0.3839,
      "step": 6312
    },
    {
      "epoch": 0.8029252782193959,
      "grad_norm": 2.5122060775756836,
      "learning_rate": 3.942980781468754e-05,
      "loss": 0.7304,
      "step": 6313
    },
    {
      "epoch": 0.8030524642289348,
      "grad_norm": 2.4193460941314697,
      "learning_rate": 3.940435280641466e-05,
      "loss": 0.5528,
      "step": 6314
    },
    {
      "epoch": 0.8031796502384738,
      "grad_norm": 2.8346564769744873,
      "learning_rate": 3.937889779814179e-05,
      "loss": 0.641,
      "step": 6315
    },
    {
      "epoch": 0.8033068362480127,
      "grad_norm": 3.211350202560425,
      "learning_rate": 3.935344278986891e-05,
      "loss": 0.4345,
      "step": 6316
    },
    {
      "epoch": 0.8034340222575517,
      "grad_norm": 2.3029539585113525,
      "learning_rate": 3.932798778159603e-05,
      "loss": 0.5246,
      "step": 6317
    },
    {
      "epoch": 0.8035612082670907,
      "grad_norm": 2.138979196548462,
      "learning_rate": 3.930253277332315e-05,
      "loss": 0.375,
      "step": 6318
    },
    {
      "epoch": 0.8036883942766295,
      "grad_norm": 1.911081075668335,
      "learning_rate": 3.9277077765050276e-05,
      "loss": 0.4259,
      "step": 6319
    },
    {
      "epoch": 0.8038155802861685,
      "grad_norm": 2.53548002243042,
      "learning_rate": 3.9251622756777395e-05,
      "loss": 0.5322,
      "step": 6320
    },
    {
      "epoch": 0.8039427662957075,
      "grad_norm": 2.4222660064697266,
      "learning_rate": 3.922616774850452e-05,
      "loss": 0.5467,
      "step": 6321
    },
    {
      "epoch": 0.8040699523052465,
      "grad_norm": 1.8378361463546753,
      "learning_rate": 3.9200712740231646e-05,
      "loss": 0.4158,
      "step": 6322
    },
    {
      "epoch": 0.8041971383147853,
      "grad_norm": 2.5715630054473877,
      "learning_rate": 3.9175257731958764e-05,
      "loss": 0.8078,
      "step": 6323
    },
    {
      "epoch": 0.8043243243243243,
      "grad_norm": 1.8549197912216187,
      "learning_rate": 3.914980272368588e-05,
      "loss": 0.4355,
      "step": 6324
    },
    {
      "epoch": 0.8044515103338633,
      "grad_norm": 2.0825858116149902,
      "learning_rate": 3.912434771541301e-05,
      "loss": 0.5034,
      "step": 6325
    },
    {
      "epoch": 0.8045786963434022,
      "grad_norm": 2.0480995178222656,
      "learning_rate": 3.9098892707140134e-05,
      "loss": 0.3729,
      "step": 6326
    },
    {
      "epoch": 0.8047058823529412,
      "grad_norm": 2.7885637283325195,
      "learning_rate": 3.907343769886725e-05,
      "loss": 0.6683,
      "step": 6327
    },
    {
      "epoch": 0.8048330683624801,
      "grad_norm": 2.1322898864746094,
      "learning_rate": 3.904798269059438e-05,
      "loss": 0.6078,
      "step": 6328
    },
    {
      "epoch": 0.8049602543720191,
      "grad_norm": 1.7253495454788208,
      "learning_rate": 3.90225276823215e-05,
      "loss": 0.4034,
      "step": 6329
    },
    {
      "epoch": 0.805087440381558,
      "grad_norm": 2.3068506717681885,
      "learning_rate": 3.899707267404862e-05,
      "loss": 0.4777,
      "step": 6330
    },
    {
      "epoch": 0.805214626391097,
      "grad_norm": 1.767495036125183,
      "learning_rate": 3.897161766577575e-05,
      "loss": 0.6331,
      "step": 6331
    },
    {
      "epoch": 0.805341812400636,
      "grad_norm": 2.123307943344116,
      "learning_rate": 3.894616265750287e-05,
      "loss": 0.5724,
      "step": 6332
    },
    {
      "epoch": 0.8054689984101748,
      "grad_norm": 2.2260046005249023,
      "learning_rate": 3.8920707649229986e-05,
      "loss": 0.6515,
      "step": 6333
    },
    {
      "epoch": 0.8055961844197138,
      "grad_norm": 3.0043323040008545,
      "learning_rate": 3.889525264095711e-05,
      "loss": 0.8473,
      "step": 6334
    },
    {
      "epoch": 0.8057233704292528,
      "grad_norm": 2.5818424224853516,
      "learning_rate": 3.8869797632684237e-05,
      "loss": 0.8189,
      "step": 6335
    },
    {
      "epoch": 0.8058505564387918,
      "grad_norm": 2.168017625808716,
      "learning_rate": 3.8844342624411355e-05,
      "loss": 0.5154,
      "step": 6336
    },
    {
      "epoch": 0.8059777424483306,
      "grad_norm": 3.0680294036865234,
      "learning_rate": 3.8818887616138474e-05,
      "loss": 0.5505,
      "step": 6337
    },
    {
      "epoch": 0.8061049284578696,
      "grad_norm": 2.5102436542510986,
      "learning_rate": 3.87934326078656e-05,
      "loss": 0.3992,
      "step": 6338
    },
    {
      "epoch": 0.8062321144674086,
      "grad_norm": 1.9267945289611816,
      "learning_rate": 3.876797759959272e-05,
      "loss": 0.6098,
      "step": 6339
    },
    {
      "epoch": 0.8063593004769475,
      "grad_norm": 2.1799392700195312,
      "learning_rate": 3.8742522591319844e-05,
      "loss": 0.4953,
      "step": 6340
    },
    {
      "epoch": 0.8064864864864865,
      "grad_norm": 2.14080810546875,
      "learning_rate": 3.871706758304697e-05,
      "loss": 0.5473,
      "step": 6341
    },
    {
      "epoch": 0.8066136724960254,
      "grad_norm": 2.40641713142395,
      "learning_rate": 3.869161257477409e-05,
      "loss": 0.539,
      "step": 6342
    },
    {
      "epoch": 0.8067408585055644,
      "grad_norm": 2.7687487602233887,
      "learning_rate": 3.866615756650121e-05,
      "loss": 0.707,
      "step": 6343
    },
    {
      "epoch": 0.8068680445151033,
      "grad_norm": 1.9253885746002197,
      "learning_rate": 3.864070255822833e-05,
      "loss": 0.3555,
      "step": 6344
    },
    {
      "epoch": 0.8069952305246423,
      "grad_norm": 2.5167346000671387,
      "learning_rate": 3.861524754995546e-05,
      "loss": 0.5224,
      "step": 6345
    },
    {
      "epoch": 0.8071224165341813,
      "grad_norm": 2.258382558822632,
      "learning_rate": 3.8589792541682577e-05,
      "loss": 0.5475,
      "step": 6346
    },
    {
      "epoch": 0.8072496025437202,
      "grad_norm": 1.625080943107605,
      "learning_rate": 3.85643375334097e-05,
      "loss": 0.5854,
      "step": 6347
    },
    {
      "epoch": 0.8073767885532591,
      "grad_norm": 2.768627166748047,
      "learning_rate": 3.853888252513682e-05,
      "loss": 0.6945,
      "step": 6348
    },
    {
      "epoch": 0.8075039745627981,
      "grad_norm": 2.730707883834839,
      "learning_rate": 3.851342751686394e-05,
      "loss": 0.6952,
      "step": 6349
    },
    {
      "epoch": 0.8076311605723371,
      "grad_norm": 2.7160873413085938,
      "learning_rate": 3.848797250859107e-05,
      "loss": 0.4996,
      "step": 6350
    },
    {
      "epoch": 0.807758346581876,
      "grad_norm": 2.684872627258301,
      "learning_rate": 3.846251750031819e-05,
      "loss": 0.498,
      "step": 6351
    },
    {
      "epoch": 0.8078855325914149,
      "grad_norm": 2.8905766010284424,
      "learning_rate": 3.843706249204531e-05,
      "loss": 0.8068,
      "step": 6352
    },
    {
      "epoch": 0.8080127186009539,
      "grad_norm": 2.0144779682159424,
      "learning_rate": 3.8411607483772435e-05,
      "loss": 0.5456,
      "step": 6353
    },
    {
      "epoch": 0.8081399046104929,
      "grad_norm": 2.2094221115112305,
      "learning_rate": 3.8386152475499554e-05,
      "loss": 0.7025,
      "step": 6354
    },
    {
      "epoch": 0.8082670906200318,
      "grad_norm": 1.8916809558868408,
      "learning_rate": 3.836069746722668e-05,
      "loss": 0.6401,
      "step": 6355
    },
    {
      "epoch": 0.8083942766295708,
      "grad_norm": 2.2406399250030518,
      "learning_rate": 3.8335242458953805e-05,
      "loss": 0.5574,
      "step": 6356
    },
    {
      "epoch": 0.8085214626391097,
      "grad_norm": 2.0426812171936035,
      "learning_rate": 3.8309787450680923e-05,
      "loss": 0.5482,
      "step": 6357
    },
    {
      "epoch": 0.8086486486486486,
      "grad_norm": 1.3848742246627808,
      "learning_rate": 3.828433244240804e-05,
      "loss": 0.291,
      "step": 6358
    },
    {
      "epoch": 0.8087758346581876,
      "grad_norm": 2.076646327972412,
      "learning_rate": 3.825887743413517e-05,
      "loss": 0.3683,
      "step": 6359
    },
    {
      "epoch": 0.8089030206677266,
      "grad_norm": 2.2414510250091553,
      "learning_rate": 3.823342242586229e-05,
      "loss": 0.5846,
      "step": 6360
    },
    {
      "epoch": 0.8090302066772656,
      "grad_norm": 2.088078498840332,
      "learning_rate": 3.820796741758941e-05,
      "loss": 0.6555,
      "step": 6361
    },
    {
      "epoch": 0.8091573926868044,
      "grad_norm": 2.215897560119629,
      "learning_rate": 3.818251240931654e-05,
      "loss": 0.4325,
      "step": 6362
    },
    {
      "epoch": 0.8092845786963434,
      "grad_norm": 2.035630702972412,
      "learning_rate": 3.8157057401043656e-05,
      "loss": 0.4409,
      "step": 6363
    },
    {
      "epoch": 0.8094117647058824,
      "grad_norm": 1.7641794681549072,
      "learning_rate": 3.813160239277078e-05,
      "loss": 0.4353,
      "step": 6364
    },
    {
      "epoch": 0.8095389507154213,
      "grad_norm": 2.7663280963897705,
      "learning_rate": 3.81061473844979e-05,
      "loss": 0.6353,
      "step": 6365
    },
    {
      "epoch": 0.8096661367249602,
      "grad_norm": 3.4361228942871094,
      "learning_rate": 3.8080692376225026e-05,
      "loss": 0.5307,
      "step": 6366
    },
    {
      "epoch": 0.8097933227344992,
      "grad_norm": 1.9698541164398193,
      "learning_rate": 3.8055237367952145e-05,
      "loss": 0.5847,
      "step": 6367
    },
    {
      "epoch": 0.8099205087440382,
      "grad_norm": 1.9683936834335327,
      "learning_rate": 3.802978235967927e-05,
      "loss": 0.5101,
      "step": 6368
    },
    {
      "epoch": 0.8100476947535771,
      "grad_norm": 1.9814763069152832,
      "learning_rate": 3.8004327351406396e-05,
      "loss": 0.4494,
      "step": 6369
    },
    {
      "epoch": 0.8101748807631161,
      "grad_norm": 2.087191581726074,
      "learning_rate": 3.7978872343133514e-05,
      "loss": 0.3798,
      "step": 6370
    },
    {
      "epoch": 0.810302066772655,
      "grad_norm": 1.9958418607711792,
      "learning_rate": 3.795341733486063e-05,
      "loss": 0.496,
      "step": 6371
    },
    {
      "epoch": 0.8104292527821939,
      "grad_norm": 2.560396194458008,
      "learning_rate": 3.792796232658776e-05,
      "loss": 0.4003,
      "step": 6372
    },
    {
      "epoch": 0.8105564387917329,
      "grad_norm": 2.5407497882843018,
      "learning_rate": 3.790250731831488e-05,
      "loss": 0.5366,
      "step": 6373
    },
    {
      "epoch": 0.8106836248012719,
      "grad_norm": 1.9973540306091309,
      "learning_rate": 3.7877052310042e-05,
      "loss": 0.5758,
      "step": 6374
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 1.759907841682434,
      "learning_rate": 3.785159730176913e-05,
      "loss": 0.4365,
      "step": 6375
    },
    {
      "epoch": 0.8109379968203497,
      "grad_norm": 2.8287768363952637,
      "learning_rate": 3.782614229349625e-05,
      "loss": 0.4746,
      "step": 6376
    },
    {
      "epoch": 0.8110651828298887,
      "grad_norm": 2.170919418334961,
      "learning_rate": 3.7800687285223366e-05,
      "loss": 0.6431,
      "step": 6377
    },
    {
      "epoch": 0.8111923688394277,
      "grad_norm": 1.7207891941070557,
      "learning_rate": 3.777523227695049e-05,
      "loss": 0.2761,
      "step": 6378
    },
    {
      "epoch": 0.8113195548489666,
      "grad_norm": 1.872874140739441,
      "learning_rate": 3.774977726867762e-05,
      "loss": 0.3649,
      "step": 6379
    },
    {
      "epoch": 0.8114467408585055,
      "grad_norm": 2.0822489261627197,
      "learning_rate": 3.7724322260404736e-05,
      "loss": 0.6568,
      "step": 6380
    },
    {
      "epoch": 0.8115739268680445,
      "grad_norm": 2.250241279602051,
      "learning_rate": 3.769886725213186e-05,
      "loss": 0.6051,
      "step": 6381
    },
    {
      "epoch": 0.8117011128775835,
      "grad_norm": 3.020704507827759,
      "learning_rate": 3.767341224385898e-05,
      "loss": 0.5008,
      "step": 6382
    },
    {
      "epoch": 0.8118282988871224,
      "grad_norm": 2.433865547180176,
      "learning_rate": 3.76479572355861e-05,
      "loss": 0.5557,
      "step": 6383
    },
    {
      "epoch": 0.8119554848966614,
      "grad_norm": 2.459463596343994,
      "learning_rate": 3.762250222731323e-05,
      "loss": 0.6455,
      "step": 6384
    },
    {
      "epoch": 0.8120826709062003,
      "grad_norm": 2.6006152629852295,
      "learning_rate": 3.759704721904035e-05,
      "loss": 0.5172,
      "step": 6385
    },
    {
      "epoch": 0.8122098569157392,
      "grad_norm": 2.6932709217071533,
      "learning_rate": 3.757159221076747e-05,
      "loss": 0.4097,
      "step": 6386
    },
    {
      "epoch": 0.8123370429252782,
      "grad_norm": 2.33872127532959,
      "learning_rate": 3.7546137202494594e-05,
      "loss": 0.5442,
      "step": 6387
    },
    {
      "epoch": 0.8124642289348172,
      "grad_norm": 2.0979671478271484,
      "learning_rate": 3.752068219422171e-05,
      "loss": 0.4571,
      "step": 6388
    },
    {
      "epoch": 0.8125914149443562,
      "grad_norm": 2.994506359100342,
      "learning_rate": 3.749522718594884e-05,
      "loss": 0.5299,
      "step": 6389
    },
    {
      "epoch": 0.812718600953895,
      "grad_norm": 3.221280574798584,
      "learning_rate": 3.746977217767596e-05,
      "loss": 0.5142,
      "step": 6390
    },
    {
      "epoch": 0.812845786963434,
      "grad_norm": 2.574385166168213,
      "learning_rate": 3.744431716940308e-05,
      "loss": 0.565,
      "step": 6391
    },
    {
      "epoch": 0.812972972972973,
      "grad_norm": 1.6810739040374756,
      "learning_rate": 3.74188621611302e-05,
      "loss": 0.3959,
      "step": 6392
    },
    {
      "epoch": 0.813100158982512,
      "grad_norm": 2.560317039489746,
      "learning_rate": 3.739340715285733e-05,
      "loss": 0.6079,
      "step": 6393
    },
    {
      "epoch": 0.8132273449920508,
      "grad_norm": 2.0386619567871094,
      "learning_rate": 3.736795214458445e-05,
      "loss": 0.5512,
      "step": 6394
    },
    {
      "epoch": 0.8133545310015898,
      "grad_norm": 3.4137256145477295,
      "learning_rate": 3.734249713631157e-05,
      "loss": 0.4177,
      "step": 6395
    },
    {
      "epoch": 0.8134817170111288,
      "grad_norm": 2.532590866088867,
      "learning_rate": 3.731704212803869e-05,
      "loss": 0.452,
      "step": 6396
    },
    {
      "epoch": 0.8136089030206677,
      "grad_norm": 2.8134472370147705,
      "learning_rate": 3.7291587119765815e-05,
      "loss": 0.5547,
      "step": 6397
    },
    {
      "epoch": 0.8137360890302067,
      "grad_norm": 2.283247709274292,
      "learning_rate": 3.726613211149294e-05,
      "loss": 0.4872,
      "step": 6398
    },
    {
      "epoch": 0.8138632750397456,
      "grad_norm": 1.922275424003601,
      "learning_rate": 3.724067710322006e-05,
      "loss": 0.5428,
      "step": 6399
    },
    {
      "epoch": 0.8139904610492846,
      "grad_norm": 2.7079806327819824,
      "learning_rate": 3.7215222094947185e-05,
      "loss": 0.8857,
      "step": 6400
    },
    {
      "epoch": 0.8141176470588235,
      "grad_norm": 2.1635823249816895,
      "learning_rate": 3.7189767086674304e-05,
      "loss": 0.6866,
      "step": 6401
    },
    {
      "epoch": 0.8142448330683625,
      "grad_norm": 2.798833131790161,
      "learning_rate": 3.716431207840142e-05,
      "loss": 0.5683,
      "step": 6402
    },
    {
      "epoch": 0.8143720190779015,
      "grad_norm": 2.4284515380859375,
      "learning_rate": 3.7138857070128555e-05,
      "loss": 0.5137,
      "step": 6403
    },
    {
      "epoch": 0.8144992050874403,
      "grad_norm": 2.431985855102539,
      "learning_rate": 3.7113402061855674e-05,
      "loss": 0.5221,
      "step": 6404
    },
    {
      "epoch": 0.8146263910969793,
      "grad_norm": 2.056149482727051,
      "learning_rate": 3.708794705358279e-05,
      "loss": 0.4416,
      "step": 6405
    },
    {
      "epoch": 0.8147535771065183,
      "grad_norm": 2.666905403137207,
      "learning_rate": 3.706249204530992e-05,
      "loss": 0.5199,
      "step": 6406
    },
    {
      "epoch": 0.8148807631160573,
      "grad_norm": 2.587244987487793,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 0.5809,
      "step": 6407
    },
    {
      "epoch": 0.8150079491255962,
      "grad_norm": 2.136146068572998,
      "learning_rate": 3.701158202876416e-05,
      "loss": 0.473,
      "step": 6408
    },
    {
      "epoch": 0.8151351351351351,
      "grad_norm": 2.6441550254821777,
      "learning_rate": 3.698612702049129e-05,
      "loss": 0.748,
      "step": 6409
    },
    {
      "epoch": 0.8152623211446741,
      "grad_norm": 1.7967332601547241,
      "learning_rate": 3.6960672012218406e-05,
      "loss": 0.3659,
      "step": 6410
    },
    {
      "epoch": 0.815389507154213,
      "grad_norm": 2.293964385986328,
      "learning_rate": 3.6935217003945525e-05,
      "loss": 0.654,
      "step": 6411
    },
    {
      "epoch": 0.815516693163752,
      "grad_norm": 2.6955831050872803,
      "learning_rate": 3.690976199567265e-05,
      "loss": 0.6174,
      "step": 6412
    },
    {
      "epoch": 0.815643879173291,
      "grad_norm": 1.7854582071304321,
      "learning_rate": 3.6884306987399776e-05,
      "loss": 0.5035,
      "step": 6413
    },
    {
      "epoch": 0.8157710651828299,
      "grad_norm": 2.1162819862365723,
      "learning_rate": 3.6858851979126895e-05,
      "loss": 0.4432,
      "step": 6414
    },
    {
      "epoch": 0.8158982511923688,
      "grad_norm": 1.6936311721801758,
      "learning_rate": 3.683339697085402e-05,
      "loss": 0.3738,
      "step": 6415
    },
    {
      "epoch": 0.8160254372019078,
      "grad_norm": 2.17606782913208,
      "learning_rate": 3.680794196258114e-05,
      "loss": 0.5511,
      "step": 6416
    },
    {
      "epoch": 0.8161526232114468,
      "grad_norm": 1.664965271949768,
      "learning_rate": 3.678248695430826e-05,
      "loss": 0.3561,
      "step": 6417
    },
    {
      "epoch": 0.8162798092209856,
      "grad_norm": 3.3709588050842285,
      "learning_rate": 3.675703194603538e-05,
      "loss": 0.4906,
      "step": 6418
    },
    {
      "epoch": 0.8164069952305246,
      "grad_norm": 2.6757915019989014,
      "learning_rate": 3.673157693776251e-05,
      "loss": 0.7583,
      "step": 6419
    },
    {
      "epoch": 0.8165341812400636,
      "grad_norm": 1.8045138120651245,
      "learning_rate": 3.670612192948963e-05,
      "loss": 0.5906,
      "step": 6420
    },
    {
      "epoch": 0.8166613672496026,
      "grad_norm": 2.11419415473938,
      "learning_rate": 3.668066692121675e-05,
      "loss": 0.5719,
      "step": 6421
    },
    {
      "epoch": 0.8167885532591415,
      "grad_norm": 2.42541766166687,
      "learning_rate": 3.665521191294387e-05,
      "loss": 0.4606,
      "step": 6422
    },
    {
      "epoch": 0.8169157392686804,
      "grad_norm": 2.113464832305908,
      "learning_rate": 3.6629756904671e-05,
      "loss": 0.3516,
      "step": 6423
    },
    {
      "epoch": 0.8170429252782194,
      "grad_norm": 2.8535995483398438,
      "learning_rate": 3.6604301896398116e-05,
      "loss": 0.6397,
      "step": 6424
    },
    {
      "epoch": 0.8171701112877583,
      "grad_norm": 2.0696208477020264,
      "learning_rate": 3.657884688812524e-05,
      "loss": 0.4749,
      "step": 6425
    },
    {
      "epoch": 0.8172972972972973,
      "grad_norm": 2.490158796310425,
      "learning_rate": 3.655339187985236e-05,
      "loss": 0.7682,
      "step": 6426
    },
    {
      "epoch": 0.8174244833068363,
      "grad_norm": 2.770627021789551,
      "learning_rate": 3.652793687157948e-05,
      "loss": 0.6125,
      "step": 6427
    },
    {
      "epoch": 0.8175516693163752,
      "grad_norm": 1.9457285404205322,
      "learning_rate": 3.650248186330661e-05,
      "loss": 0.5601,
      "step": 6428
    },
    {
      "epoch": 0.8176788553259141,
      "grad_norm": 2.398179531097412,
      "learning_rate": 3.647702685503373e-05,
      "loss": 0.5947,
      "step": 6429
    },
    {
      "epoch": 0.8178060413354531,
      "grad_norm": 1.9975665807724,
      "learning_rate": 3.645157184676085e-05,
      "loss": 0.4955,
      "step": 6430
    },
    {
      "epoch": 0.8179332273449921,
      "grad_norm": 1.8055860996246338,
      "learning_rate": 3.6426116838487974e-05,
      "loss": 0.5163,
      "step": 6431
    },
    {
      "epoch": 0.818060413354531,
      "grad_norm": 2.510990619659424,
      "learning_rate": 3.64006618302151e-05,
      "loss": 0.5638,
      "step": 6432
    },
    {
      "epoch": 0.8181875993640699,
      "grad_norm": 2.6250112056732178,
      "learning_rate": 3.637520682194222e-05,
      "loss": 0.6676,
      "step": 6433
    },
    {
      "epoch": 0.8183147853736089,
      "grad_norm": 3.3781347274780273,
      "learning_rate": 3.6349751813669344e-05,
      "loss": 0.5812,
      "step": 6434
    },
    {
      "epoch": 0.8184419713831479,
      "grad_norm": 2.766244411468506,
      "learning_rate": 3.632429680539646e-05,
      "loss": 0.4918,
      "step": 6435
    },
    {
      "epoch": 0.8185691573926868,
      "grad_norm": 2.1890664100646973,
      "learning_rate": 3.629884179712358e-05,
      "loss": 0.4671,
      "step": 6436
    },
    {
      "epoch": 0.8186963434022257,
      "grad_norm": 2.146595001220703,
      "learning_rate": 3.6273386788850714e-05,
      "loss": 0.7516,
      "step": 6437
    },
    {
      "epoch": 0.8188235294117647,
      "grad_norm": 2.220982789993286,
      "learning_rate": 3.624793178057783e-05,
      "loss": 0.5714,
      "step": 6438
    },
    {
      "epoch": 0.8189507154213037,
      "grad_norm": 1.659194827079773,
      "learning_rate": 3.622247677230495e-05,
      "loss": 0.4237,
      "step": 6439
    },
    {
      "epoch": 0.8190779014308426,
      "grad_norm": 4.815860748291016,
      "learning_rate": 3.619702176403208e-05,
      "loss": 0.4547,
      "step": 6440
    },
    {
      "epoch": 0.8192050874403816,
      "grad_norm": 2.585860252380371,
      "learning_rate": 3.6171566755759196e-05,
      "loss": 0.4587,
      "step": 6441
    },
    {
      "epoch": 0.8193322734499205,
      "grad_norm": 1.8286408185958862,
      "learning_rate": 3.614611174748632e-05,
      "loss": 0.4215,
      "step": 6442
    },
    {
      "epoch": 0.8194594594594594,
      "grad_norm": 2.484520196914673,
      "learning_rate": 3.612065673921344e-05,
      "loss": 0.6101,
      "step": 6443
    },
    {
      "epoch": 0.8195866454689984,
      "grad_norm": 2.4920265674591064,
      "learning_rate": 3.6095201730940565e-05,
      "loss": 0.6208,
      "step": 6444
    },
    {
      "epoch": 0.8197138314785374,
      "grad_norm": 1.519721269607544,
      "learning_rate": 3.6069746722667684e-05,
      "loss": 0.3993,
      "step": 6445
    },
    {
      "epoch": 0.8198410174880764,
      "grad_norm": 2.0833075046539307,
      "learning_rate": 3.604429171439481e-05,
      "loss": 0.4771,
      "step": 6446
    },
    {
      "epoch": 0.8199682034976152,
      "grad_norm": 2.1964194774627686,
      "learning_rate": 3.6018836706121935e-05,
      "loss": 0.5782,
      "step": 6447
    },
    {
      "epoch": 0.8200953895071542,
      "grad_norm": 1.6405198574066162,
      "learning_rate": 3.5993381697849054e-05,
      "loss": 0.5973,
      "step": 6448
    },
    {
      "epoch": 0.8202225755166932,
      "grad_norm": 2.4390709400177,
      "learning_rate": 3.596792668957617e-05,
      "loss": 0.4631,
      "step": 6449
    },
    {
      "epoch": 0.8203497615262321,
      "grad_norm": 2.091045379638672,
      "learning_rate": 3.59424716813033e-05,
      "loss": 0.3869,
      "step": 6450
    },
    {
      "epoch": 0.820476947535771,
      "grad_norm": 1.9368605613708496,
      "learning_rate": 3.591701667303042e-05,
      "loss": 0.5954,
      "step": 6451
    },
    {
      "epoch": 0.82060413354531,
      "grad_norm": 1.623276710510254,
      "learning_rate": 3.589156166475754e-05,
      "loss": 0.5819,
      "step": 6452
    },
    {
      "epoch": 0.820731319554849,
      "grad_norm": 2.2153944969177246,
      "learning_rate": 3.586610665648467e-05,
      "loss": 0.5696,
      "step": 6453
    },
    {
      "epoch": 0.8208585055643879,
      "grad_norm": 3.017023801803589,
      "learning_rate": 3.584065164821179e-05,
      "loss": 0.6636,
      "step": 6454
    },
    {
      "epoch": 0.8209856915739269,
      "grad_norm": 2.800434112548828,
      "learning_rate": 3.5815196639938905e-05,
      "loss": 0.3127,
      "step": 6455
    },
    {
      "epoch": 0.8211128775834658,
      "grad_norm": 2.157921075820923,
      "learning_rate": 3.578974163166603e-05,
      "loss": 0.7348,
      "step": 6456
    },
    {
      "epoch": 0.8212400635930047,
      "grad_norm": 2.541186571121216,
      "learning_rate": 3.5764286623393156e-05,
      "loss": 0.6443,
      "step": 6457
    },
    {
      "epoch": 0.8213672496025437,
      "grad_norm": 2.092451810836792,
      "learning_rate": 3.5738831615120275e-05,
      "loss": 0.4224,
      "step": 6458
    },
    {
      "epoch": 0.8214944356120827,
      "grad_norm": 3.026418924331665,
      "learning_rate": 3.57133766068474e-05,
      "loss": 0.5388,
      "step": 6459
    },
    {
      "epoch": 0.8216216216216217,
      "grad_norm": 3.2981996536254883,
      "learning_rate": 3.568792159857452e-05,
      "loss": 0.6256,
      "step": 6460
    },
    {
      "epoch": 0.8217488076311605,
      "grad_norm": 2.148052215576172,
      "learning_rate": 3.566246659030164e-05,
      "loss": 0.5316,
      "step": 6461
    },
    {
      "epoch": 0.8218759936406995,
      "grad_norm": 1.5169345140457153,
      "learning_rate": 3.563701158202877e-05,
      "loss": 0.2937,
      "step": 6462
    },
    {
      "epoch": 0.8220031796502385,
      "grad_norm": 1.9134327173233032,
      "learning_rate": 3.561155657375589e-05,
      "loss": 0.5403,
      "step": 6463
    },
    {
      "epoch": 0.8221303656597774,
      "grad_norm": 1.8081960678100586,
      "learning_rate": 3.558610156548301e-05,
      "loss": 0.3594,
      "step": 6464
    },
    {
      "epoch": 0.8222575516693164,
      "grad_norm": 2.721592426300049,
      "learning_rate": 3.5560646557210134e-05,
      "loss": 0.4756,
      "step": 6465
    },
    {
      "epoch": 0.8223847376788553,
      "grad_norm": 2.7076680660247803,
      "learning_rate": 3.553519154893726e-05,
      "loss": 0.6679,
      "step": 6466
    },
    {
      "epoch": 0.8225119236883943,
      "grad_norm": 2.8888418674468994,
      "learning_rate": 3.550973654066438e-05,
      "loss": 1.0212,
      "step": 6467
    },
    {
      "epoch": 0.8226391096979332,
      "grad_norm": 1.9619178771972656,
      "learning_rate": 3.54842815323915e-05,
      "loss": 0.5437,
      "step": 6468
    },
    {
      "epoch": 0.8227662957074722,
      "grad_norm": 1.473075270652771,
      "learning_rate": 3.545882652411862e-05,
      "loss": 0.5118,
      "step": 6469
    },
    {
      "epoch": 0.8228934817170112,
      "grad_norm": 2.447364330291748,
      "learning_rate": 3.543337151584574e-05,
      "loss": 0.5551,
      "step": 6470
    },
    {
      "epoch": 0.82302066772655,
      "grad_norm": 1.8895529508590698,
      "learning_rate": 3.5407916507572866e-05,
      "loss": 0.2977,
      "step": 6471
    },
    {
      "epoch": 0.823147853736089,
      "grad_norm": 2.2163217067718506,
      "learning_rate": 3.538246149929999e-05,
      "loss": 0.4379,
      "step": 6472
    },
    {
      "epoch": 0.823275039745628,
      "grad_norm": 1.944299340248108,
      "learning_rate": 3.535700649102711e-05,
      "loss": 0.5226,
      "step": 6473
    },
    {
      "epoch": 0.823402225755167,
      "grad_norm": 2.3722026348114014,
      "learning_rate": 3.533155148275423e-05,
      "loss": 0.576,
      "step": 6474
    },
    {
      "epoch": 0.8235294117647058,
      "grad_norm": 2.060321092605591,
      "learning_rate": 3.5306096474481355e-05,
      "loss": 0.535,
      "step": 6475
    },
    {
      "epoch": 0.8236565977742448,
      "grad_norm": 2.325467586517334,
      "learning_rate": 3.528064146620848e-05,
      "loss": 0.5889,
      "step": 6476
    },
    {
      "epoch": 0.8237837837837838,
      "grad_norm": 2.3869662284851074,
      "learning_rate": 3.52551864579356e-05,
      "loss": 0.5612,
      "step": 6477
    },
    {
      "epoch": 0.8239109697933228,
      "grad_norm": 1.626731276512146,
      "learning_rate": 3.5229731449662725e-05,
      "loss": 0.4809,
      "step": 6478
    },
    {
      "epoch": 0.8240381558028617,
      "grad_norm": 1.4203208684921265,
      "learning_rate": 3.520427644138984e-05,
      "loss": 0.3962,
      "step": 6479
    },
    {
      "epoch": 0.8241653418124006,
      "grad_norm": 1.8911902904510498,
      "learning_rate": 3.517882143311696e-05,
      "loss": 0.6715,
      "step": 6480
    },
    {
      "epoch": 0.8242925278219396,
      "grad_norm": 2.147531032562256,
      "learning_rate": 3.5153366424844094e-05,
      "loss": 1.0408,
      "step": 6481
    },
    {
      "epoch": 0.8244197138314785,
      "grad_norm": 2.122002601623535,
      "learning_rate": 3.512791141657121e-05,
      "loss": 0.5615,
      "step": 6482
    },
    {
      "epoch": 0.8245468998410175,
      "grad_norm": 2.4387714862823486,
      "learning_rate": 3.510245640829833e-05,
      "loss": 0.6597,
      "step": 6483
    },
    {
      "epoch": 0.8246740858505565,
      "grad_norm": 1.6360934972763062,
      "learning_rate": 3.507700140002546e-05,
      "loss": 0.2516,
      "step": 6484
    },
    {
      "epoch": 0.8248012718600954,
      "grad_norm": 3.2156081199645996,
      "learning_rate": 3.5051546391752576e-05,
      "loss": 0.5646,
      "step": 6485
    },
    {
      "epoch": 0.8249284578696343,
      "grad_norm": 2.1060118675231934,
      "learning_rate": 3.50260913834797e-05,
      "loss": 0.4637,
      "step": 6486
    },
    {
      "epoch": 0.8250556438791733,
      "grad_norm": 2.3505160808563232,
      "learning_rate": 3.500063637520683e-05,
      "loss": 0.5829,
      "step": 6487
    },
    {
      "epoch": 0.8251828298887123,
      "grad_norm": 2.1909425258636475,
      "learning_rate": 3.4975181366933946e-05,
      "loss": 0.5493,
      "step": 6488
    },
    {
      "epoch": 0.8253100158982511,
      "grad_norm": 2.3154919147491455,
      "learning_rate": 3.4949726358661065e-05,
      "loss": 0.748,
      "step": 6489
    },
    {
      "epoch": 0.8254372019077901,
      "grad_norm": 2.1326725482940674,
      "learning_rate": 3.492427135038819e-05,
      "loss": 0.2975,
      "step": 6490
    },
    {
      "epoch": 0.8255643879173291,
      "grad_norm": 2.269706964492798,
      "learning_rate": 3.4898816342115316e-05,
      "loss": 0.7055,
      "step": 6491
    },
    {
      "epoch": 0.8256915739268681,
      "grad_norm": 2.2484896183013916,
      "learning_rate": 3.4873361333842434e-05,
      "loss": 0.5071,
      "step": 6492
    },
    {
      "epoch": 0.825818759936407,
      "grad_norm": 2.346231698989868,
      "learning_rate": 3.484790632556956e-05,
      "loss": 0.5649,
      "step": 6493
    },
    {
      "epoch": 0.825945945945946,
      "grad_norm": 2.771976947784424,
      "learning_rate": 3.482245131729668e-05,
      "loss": 0.5605,
      "step": 6494
    },
    {
      "epoch": 0.8260731319554849,
      "grad_norm": 2.0131218433380127,
      "learning_rate": 3.47969963090238e-05,
      "loss": 0.454,
      "step": 6495
    },
    {
      "epoch": 0.8262003179650238,
      "grad_norm": 1.7203634977340698,
      "learning_rate": 3.477154130075092e-05,
      "loss": 0.4951,
      "step": 6496
    },
    {
      "epoch": 0.8263275039745628,
      "grad_norm": 2.3702280521392822,
      "learning_rate": 3.474608629247805e-05,
      "loss": 0.4962,
      "step": 6497
    },
    {
      "epoch": 0.8264546899841018,
      "grad_norm": 2.8416268825531006,
      "learning_rate": 3.472063128420517e-05,
      "loss": 0.5612,
      "step": 6498
    },
    {
      "epoch": 0.8265818759936407,
      "grad_norm": 3.4048941135406494,
      "learning_rate": 3.469517627593229e-05,
      "loss": 0.7957,
      "step": 6499
    },
    {
      "epoch": 0.8267090620031796,
      "grad_norm": 2.685173273086548,
      "learning_rate": 3.466972126765942e-05,
      "loss": 0.4115,
      "step": 6500
    },
    {
      "epoch": 0.8268362480127186,
      "grad_norm": 3.193312406539917,
      "learning_rate": 3.464426625938654e-05,
      "loss": 0.5045,
      "step": 6501
    },
    {
      "epoch": 0.8269634340222576,
      "grad_norm": 3.600170135498047,
      "learning_rate": 3.4618811251113656e-05,
      "loss": 0.3754,
      "step": 6502
    },
    {
      "epoch": 0.8270906200317965,
      "grad_norm": 2.413154363632202,
      "learning_rate": 3.459335624284078e-05,
      "loss": 0.6,
      "step": 6503
    },
    {
      "epoch": 0.8272178060413354,
      "grad_norm": 2.160672903060913,
      "learning_rate": 3.45679012345679e-05,
      "loss": 0.4996,
      "step": 6504
    },
    {
      "epoch": 0.8273449920508744,
      "grad_norm": 1.7649245262145996,
      "learning_rate": 3.4542446226295025e-05,
      "loss": 0.5298,
      "step": 6505
    },
    {
      "epoch": 0.8274721780604134,
      "grad_norm": 1.6750909090042114,
      "learning_rate": 3.451699121802215e-05,
      "loss": 0.3135,
      "step": 6506
    },
    {
      "epoch": 0.8275993640699523,
      "grad_norm": 1.8414976596832275,
      "learning_rate": 3.449153620974927e-05,
      "loss": 0.3578,
      "step": 6507
    },
    {
      "epoch": 0.8277265500794913,
      "grad_norm": 2.224949836730957,
      "learning_rate": 3.446608120147639e-05,
      "loss": 0.5324,
      "step": 6508
    },
    {
      "epoch": 0.8278537360890302,
      "grad_norm": 2.2206997871398926,
      "learning_rate": 3.4440626193203514e-05,
      "loss": 0.76,
      "step": 6509
    },
    {
      "epoch": 0.8279809220985691,
      "grad_norm": 2.405337333679199,
      "learning_rate": 3.441517118493064e-05,
      "loss": 0.5949,
      "step": 6510
    },
    {
      "epoch": 0.8281081081081081,
      "grad_norm": 3.9582679271698,
      "learning_rate": 3.438971617665776e-05,
      "loss": 0.5398,
      "step": 6511
    },
    {
      "epoch": 0.8282352941176471,
      "grad_norm": 1.9802385568618774,
      "learning_rate": 3.4364261168384884e-05,
      "loss": 0.4294,
      "step": 6512
    },
    {
      "epoch": 0.828362480127186,
      "grad_norm": 2.0276992321014404,
      "learning_rate": 3.4338806160112e-05,
      "loss": 0.6116,
      "step": 6513
    },
    {
      "epoch": 0.8284896661367249,
      "grad_norm": 1.518815517425537,
      "learning_rate": 3.431335115183912e-05,
      "loss": 0.3216,
      "step": 6514
    },
    {
      "epoch": 0.8286168521462639,
      "grad_norm": 2.007796287536621,
      "learning_rate": 3.4287896143566253e-05,
      "loss": 0.6021,
      "step": 6515
    },
    {
      "epoch": 0.8287440381558029,
      "grad_norm": 2.2261741161346436,
      "learning_rate": 3.426244113529337e-05,
      "loss": 0.5548,
      "step": 6516
    },
    {
      "epoch": 0.8288712241653418,
      "grad_norm": 2.241650104522705,
      "learning_rate": 3.423698612702049e-05,
      "loss": 0.5647,
      "step": 6517
    },
    {
      "epoch": 0.8289984101748807,
      "grad_norm": 2.1429405212402344,
      "learning_rate": 3.4211531118747616e-05,
      "loss": 0.4432,
      "step": 6518
    },
    {
      "epoch": 0.8291255961844197,
      "grad_norm": 2.5305862426757812,
      "learning_rate": 3.4186076110474735e-05,
      "loss": 0.6436,
      "step": 6519
    },
    {
      "epoch": 0.8292527821939587,
      "grad_norm": 2.6858789920806885,
      "learning_rate": 3.416062110220186e-05,
      "loss": 0.3868,
      "step": 6520
    },
    {
      "epoch": 0.8293799682034976,
      "grad_norm": 2.468569278717041,
      "learning_rate": 3.4135166093928986e-05,
      "loss": 0.4939,
      "step": 6521
    },
    {
      "epoch": 0.8295071542130366,
      "grad_norm": 2.547435998916626,
      "learning_rate": 3.4109711085656105e-05,
      "loss": 0.6188,
      "step": 6522
    },
    {
      "epoch": 0.8296343402225755,
      "grad_norm": 2.18019962310791,
      "learning_rate": 3.4084256077383224e-05,
      "loss": 0.6197,
      "step": 6523
    },
    {
      "epoch": 0.8297615262321145,
      "grad_norm": 1.554925799369812,
      "learning_rate": 3.405880106911035e-05,
      "loss": 0.362,
      "step": 6524
    },
    {
      "epoch": 0.8298887122416534,
      "grad_norm": 2.6302402019500732,
      "learning_rate": 3.4033346060837475e-05,
      "loss": 0.5429,
      "step": 6525
    },
    {
      "epoch": 0.8300158982511924,
      "grad_norm": 1.5140962600708008,
      "learning_rate": 3.4007891052564593e-05,
      "loss": 0.3119,
      "step": 6526
    },
    {
      "epoch": 0.8301430842607314,
      "grad_norm": 2.00602126121521,
      "learning_rate": 3.398243604429171e-05,
      "loss": 0.3374,
      "step": 6527
    },
    {
      "epoch": 0.8302702702702702,
      "grad_norm": 1.8711590766906738,
      "learning_rate": 3.395698103601884e-05,
      "loss": 0.3888,
      "step": 6528
    },
    {
      "epoch": 0.8303974562798092,
      "grad_norm": 3.0439085960388184,
      "learning_rate": 3.3931526027745956e-05,
      "loss": 0.5304,
      "step": 6529
    },
    {
      "epoch": 0.8305246422893482,
      "grad_norm": 2.404242515563965,
      "learning_rate": 3.390607101947308e-05,
      "loss": 0.681,
      "step": 6530
    },
    {
      "epoch": 0.8306518282988872,
      "grad_norm": 2.821896553039551,
      "learning_rate": 3.388061601120021e-05,
      "loss": 0.5647,
      "step": 6531
    },
    {
      "epoch": 0.830779014308426,
      "grad_norm": 2.445138454437256,
      "learning_rate": 3.3855161002927326e-05,
      "loss": 0.567,
      "step": 6532
    },
    {
      "epoch": 0.830906200317965,
      "grad_norm": 1.6295537948608398,
      "learning_rate": 3.3829705994654445e-05,
      "loss": 0.5285,
      "step": 6533
    },
    {
      "epoch": 0.831033386327504,
      "grad_norm": 2.0968775749206543,
      "learning_rate": 3.380425098638158e-05,
      "loss": 0.4385,
      "step": 6534
    },
    {
      "epoch": 0.8311605723370429,
      "grad_norm": 2.5480687618255615,
      "learning_rate": 3.3778795978108696e-05,
      "loss": 0.5221,
      "step": 6535
    },
    {
      "epoch": 0.8312877583465819,
      "grad_norm": 2.273656129837036,
      "learning_rate": 3.3753340969835815e-05,
      "loss": 0.4399,
      "step": 6536
    },
    {
      "epoch": 0.8314149443561208,
      "grad_norm": 2.1102728843688965,
      "learning_rate": 3.372788596156294e-05,
      "loss": 0.5334,
      "step": 6537
    },
    {
      "epoch": 0.8315421303656598,
      "grad_norm": 3.4697623252868652,
      "learning_rate": 3.370243095329006e-05,
      "loss": 0.4623,
      "step": 6538
    },
    {
      "epoch": 0.8316693163751987,
      "grad_norm": 2.5853517055511475,
      "learning_rate": 3.3676975945017185e-05,
      "loss": 0.8182,
      "step": 6539
    },
    {
      "epoch": 0.8317965023847377,
      "grad_norm": 1.7115111351013184,
      "learning_rate": 3.365152093674431e-05,
      "loss": 0.3987,
      "step": 6540
    },
    {
      "epoch": 0.8319236883942767,
      "grad_norm": 1.7019509077072144,
      "learning_rate": 3.362606592847143e-05,
      "loss": 0.5233,
      "step": 6541
    },
    {
      "epoch": 0.8320508744038155,
      "grad_norm": 2.76896071434021,
      "learning_rate": 3.360061092019855e-05,
      "loss": 0.675,
      "step": 6542
    },
    {
      "epoch": 0.8321780604133545,
      "grad_norm": 3.668759822845459,
      "learning_rate": 3.357515591192567e-05,
      "loss": 0.5272,
      "step": 6543
    },
    {
      "epoch": 0.8323052464228935,
      "grad_norm": 2.6143112182617188,
      "learning_rate": 3.35497009036528e-05,
      "loss": 0.6293,
      "step": 6544
    },
    {
      "epoch": 0.8324324324324325,
      "grad_norm": 2.2234227657318115,
      "learning_rate": 3.352424589537992e-05,
      "loss": 0.5217,
      "step": 6545
    },
    {
      "epoch": 0.8325596184419713,
      "grad_norm": 2.6912851333618164,
      "learning_rate": 3.349879088710704e-05,
      "loss": 0.3613,
      "step": 6546
    },
    {
      "epoch": 0.8326868044515103,
      "grad_norm": 1.718531847000122,
      "learning_rate": 3.347333587883416e-05,
      "loss": 0.3316,
      "step": 6547
    },
    {
      "epoch": 0.8328139904610493,
      "grad_norm": 2.8354647159576416,
      "learning_rate": 3.344788087056128e-05,
      "loss": 0.6037,
      "step": 6548
    },
    {
      "epoch": 0.8329411764705882,
      "grad_norm": 1.6596306562423706,
      "learning_rate": 3.3422425862288406e-05,
      "loss": 0.4469,
      "step": 6549
    },
    {
      "epoch": 0.8330683624801272,
      "grad_norm": 2.9451100826263428,
      "learning_rate": 3.339697085401553e-05,
      "loss": 0.4415,
      "step": 6550
    },
    {
      "epoch": 0.8331955484896661,
      "grad_norm": 2.407560110092163,
      "learning_rate": 3.337151584574265e-05,
      "loss": 0.6515,
      "step": 6551
    },
    {
      "epoch": 0.8333227344992051,
      "grad_norm": 2.5586228370666504,
      "learning_rate": 3.3346060837469776e-05,
      "loss": 0.4104,
      "step": 6552
    },
    {
      "epoch": 0.833449920508744,
      "grad_norm": 4.332306861877441,
      "learning_rate": 3.3320605829196894e-05,
      "loss": 0.4461,
      "step": 6553
    },
    {
      "epoch": 0.833577106518283,
      "grad_norm": 2.4577183723449707,
      "learning_rate": 3.329515082092402e-05,
      "loss": 0.4359,
      "step": 6554
    },
    {
      "epoch": 0.833704292527822,
      "grad_norm": 2.095485210418701,
      "learning_rate": 3.326969581265114e-05,
      "loss": 0.5651,
      "step": 6555
    },
    {
      "epoch": 0.8338314785373608,
      "grad_norm": 1.7097361087799072,
      "learning_rate": 3.3244240804378264e-05,
      "loss": 0.4017,
      "step": 6556
    },
    {
      "epoch": 0.8339586645468998,
      "grad_norm": 1.9611519575119019,
      "learning_rate": 3.321878579610538e-05,
      "loss": 0.4392,
      "step": 6557
    },
    {
      "epoch": 0.8340858505564388,
      "grad_norm": 2.2243809700012207,
      "learning_rate": 3.319333078783251e-05,
      "loss": 0.5358,
      "step": 6558
    },
    {
      "epoch": 0.8342130365659778,
      "grad_norm": 2.245176076889038,
      "learning_rate": 3.3167875779559634e-05,
      "loss": 0.5224,
      "step": 6559
    },
    {
      "epoch": 0.8343402225755167,
      "grad_norm": 1.7729519605636597,
      "learning_rate": 3.314242077128675e-05,
      "loss": 0.4298,
      "step": 6560
    },
    {
      "epoch": 0.8344674085850556,
      "grad_norm": 2.900007724761963,
      "learning_rate": 3.311696576301387e-05,
      "loss": 0.5681,
      "step": 6561
    },
    {
      "epoch": 0.8345945945945946,
      "grad_norm": 2.4778056144714355,
      "learning_rate": 3.3091510754741e-05,
      "loss": 0.5902,
      "step": 6562
    },
    {
      "epoch": 0.8347217806041336,
      "grad_norm": 2.100578546524048,
      "learning_rate": 3.3066055746468116e-05,
      "loss": 0.2896,
      "step": 6563
    },
    {
      "epoch": 0.8348489666136725,
      "grad_norm": 2.387127161026001,
      "learning_rate": 3.304060073819524e-05,
      "loss": 0.6332,
      "step": 6564
    },
    {
      "epoch": 0.8349761526232115,
      "grad_norm": 1.658728003501892,
      "learning_rate": 3.3015145729922367e-05,
      "loss": 0.4801,
      "step": 6565
    },
    {
      "epoch": 0.8351033386327504,
      "grad_norm": 2.17073392868042,
      "learning_rate": 3.2989690721649485e-05,
      "loss": 0.4491,
      "step": 6566
    },
    {
      "epoch": 0.8352305246422893,
      "grad_norm": 1.8986307382583618,
      "learning_rate": 3.2964235713376604e-05,
      "loss": 0.5018,
      "step": 6567
    },
    {
      "epoch": 0.8353577106518283,
      "grad_norm": 2.2008285522460938,
      "learning_rate": 3.2938780705103736e-05,
      "loss": 0.6068,
      "step": 6568
    },
    {
      "epoch": 0.8354848966613673,
      "grad_norm": 2.4561874866485596,
      "learning_rate": 3.2913325696830855e-05,
      "loss": 0.4438,
      "step": 6569
    },
    {
      "epoch": 0.8356120826709063,
      "grad_norm": 2.0048177242279053,
      "learning_rate": 3.2887870688557974e-05,
      "loss": 0.2825,
      "step": 6570
    },
    {
      "epoch": 0.8357392686804451,
      "grad_norm": 2.598240375518799,
      "learning_rate": 3.28624156802851e-05,
      "loss": 0.6412,
      "step": 6571
    },
    {
      "epoch": 0.8358664546899841,
      "grad_norm": 2.009394884109497,
      "learning_rate": 3.283696067201222e-05,
      "loss": 0.5165,
      "step": 6572
    },
    {
      "epoch": 0.8359936406995231,
      "grad_norm": 2.663689613342285,
      "learning_rate": 3.2811505663739344e-05,
      "loss": 0.4567,
      "step": 6573
    },
    {
      "epoch": 0.836120826709062,
      "grad_norm": 3.2259461879730225,
      "learning_rate": 3.278605065546647e-05,
      "loss": 0.8082,
      "step": 6574
    },
    {
      "epoch": 0.8362480127186009,
      "grad_norm": 1.949096918106079,
      "learning_rate": 3.276059564719359e-05,
      "loss": 0.4238,
      "step": 6575
    },
    {
      "epoch": 0.8363751987281399,
      "grad_norm": 1.8788554668426514,
      "learning_rate": 3.273514063892071e-05,
      "loss": 0.441,
      "step": 6576
    },
    {
      "epoch": 0.8365023847376789,
      "grad_norm": 2.6846463680267334,
      "learning_rate": 3.270968563064783e-05,
      "loss": 0.5009,
      "step": 6577
    },
    {
      "epoch": 0.8366295707472178,
      "grad_norm": 1.9505367279052734,
      "learning_rate": 3.268423062237496e-05,
      "loss": 0.3387,
      "step": 6578
    },
    {
      "epoch": 0.8367567567567568,
      "grad_norm": 2.7648260593414307,
      "learning_rate": 3.2658775614102076e-05,
      "loss": 0.549,
      "step": 6579
    },
    {
      "epoch": 0.8368839427662957,
      "grad_norm": 2.522775650024414,
      "learning_rate": 3.2633320605829195e-05,
      "loss": 0.6036,
      "step": 6580
    },
    {
      "epoch": 0.8370111287758346,
      "grad_norm": 2.2071197032928467,
      "learning_rate": 3.260786559755632e-05,
      "loss": 0.5407,
      "step": 6581
    },
    {
      "epoch": 0.8371383147853736,
      "grad_norm": 2.9357388019561768,
      "learning_rate": 3.258241058928344e-05,
      "loss": 0.9321,
      "step": 6582
    },
    {
      "epoch": 0.8372655007949126,
      "grad_norm": 2.8077967166900635,
      "learning_rate": 3.2556955581010565e-05,
      "loss": 0.7676,
      "step": 6583
    },
    {
      "epoch": 0.8373926868044516,
      "grad_norm": 1.999887228012085,
      "learning_rate": 3.253150057273769e-05,
      "loss": 0.6455,
      "step": 6584
    },
    {
      "epoch": 0.8375198728139904,
      "grad_norm": 2.505868434906006,
      "learning_rate": 3.250604556446481e-05,
      "loss": 0.4448,
      "step": 6585
    },
    {
      "epoch": 0.8376470588235294,
      "grad_norm": 2.3311898708343506,
      "learning_rate": 3.248059055619193e-05,
      "loss": 0.4679,
      "step": 6586
    },
    {
      "epoch": 0.8377742448330684,
      "grad_norm": 3.396178960800171,
      "learning_rate": 3.2455135547919053e-05,
      "loss": 0.5576,
      "step": 6587
    },
    {
      "epoch": 0.8379014308426073,
      "grad_norm": 2.0103390216827393,
      "learning_rate": 3.242968053964618e-05,
      "loss": 0.5572,
      "step": 6588
    },
    {
      "epoch": 0.8380286168521462,
      "grad_norm": 2.7661092281341553,
      "learning_rate": 3.24042255313733e-05,
      "loss": 0.6074,
      "step": 6589
    },
    {
      "epoch": 0.8381558028616852,
      "grad_norm": 2.583376407623291,
      "learning_rate": 3.237877052310042e-05,
      "loss": 0.6078,
      "step": 6590
    },
    {
      "epoch": 0.8382829888712242,
      "grad_norm": 2.4958415031433105,
      "learning_rate": 3.235331551482754e-05,
      "loss": 0.2834,
      "step": 6591
    },
    {
      "epoch": 0.8384101748807631,
      "grad_norm": 2.475522756576538,
      "learning_rate": 3.232786050655466e-05,
      "loss": 0.48,
      "step": 6592
    },
    {
      "epoch": 0.8385373608903021,
      "grad_norm": 1.997389793395996,
      "learning_rate": 3.230240549828179e-05,
      "loss": 0.6192,
      "step": 6593
    },
    {
      "epoch": 0.838664546899841,
      "grad_norm": 2.6763651371002197,
      "learning_rate": 3.227695049000891e-05,
      "loss": 0.7216,
      "step": 6594
    },
    {
      "epoch": 0.8387917329093799,
      "grad_norm": 2.870121479034424,
      "learning_rate": 3.225149548173603e-05,
      "loss": 0.5463,
      "step": 6595
    },
    {
      "epoch": 0.8389189189189189,
      "grad_norm": 2.42148756980896,
      "learning_rate": 3.2226040473463156e-05,
      "loss": 0.4712,
      "step": 6596
    },
    {
      "epoch": 0.8390461049284579,
      "grad_norm": 2.5037195682525635,
      "learning_rate": 3.2200585465190275e-05,
      "loss": 0.4579,
      "step": 6597
    },
    {
      "epoch": 0.8391732909379969,
      "grad_norm": 2.5316617488861084,
      "learning_rate": 3.21751304569174e-05,
      "loss": 0.9286,
      "step": 6598
    },
    {
      "epoch": 0.8393004769475357,
      "grad_norm": 2.8232100009918213,
      "learning_rate": 3.2149675448644526e-05,
      "loss": 0.632,
      "step": 6599
    },
    {
      "epoch": 0.8394276629570747,
      "grad_norm": 2.128932237625122,
      "learning_rate": 3.2124220440371644e-05,
      "loss": 0.4852,
      "step": 6600
    },
    {
      "epoch": 0.8395548489666137,
      "grad_norm": 2.173405170440674,
      "learning_rate": 3.209876543209876e-05,
      "loss": 0.4914,
      "step": 6601
    },
    {
      "epoch": 0.8396820349761526,
      "grad_norm": 2.8905444145202637,
      "learning_rate": 3.207331042382589e-05,
      "loss": 0.3848,
      "step": 6602
    },
    {
      "epoch": 0.8398092209856916,
      "grad_norm": 2.442162275314331,
      "learning_rate": 3.2047855415553014e-05,
      "loss": 0.9352,
      "step": 6603
    },
    {
      "epoch": 0.8399364069952305,
      "grad_norm": 2.992077589035034,
      "learning_rate": 3.202240040728013e-05,
      "loss": 0.4697,
      "step": 6604
    },
    {
      "epoch": 0.8400635930047695,
      "grad_norm": 2.9052627086639404,
      "learning_rate": 3.199694539900726e-05,
      "loss": 0.5117,
      "step": 6605
    },
    {
      "epoch": 0.8401907790143084,
      "grad_norm": 2.959646701812744,
      "learning_rate": 3.197149039073438e-05,
      "loss": 0.5244,
      "step": 6606
    },
    {
      "epoch": 0.8403179650238474,
      "grad_norm": 1.6239503622055054,
      "learning_rate": 3.19460353824615e-05,
      "loss": 0.4678,
      "step": 6607
    },
    {
      "epoch": 0.8404451510333864,
      "grad_norm": 4.828576564788818,
      "learning_rate": 3.192058037418862e-05,
      "loss": 0.8754,
      "step": 6608
    },
    {
      "epoch": 0.8405723370429253,
      "grad_norm": 1.936781883239746,
      "learning_rate": 3.189512536591575e-05,
      "loss": 0.4606,
      "step": 6609
    },
    {
      "epoch": 0.8406995230524642,
      "grad_norm": 3.1339175701141357,
      "learning_rate": 3.1869670357642866e-05,
      "loss": 0.5785,
      "step": 6610
    },
    {
      "epoch": 0.8408267090620032,
      "grad_norm": 4.1433000564575195,
      "learning_rate": 3.184421534936999e-05,
      "loss": 0.443,
      "step": 6611
    },
    {
      "epoch": 0.8409538950715422,
      "grad_norm": 3.0651509761810303,
      "learning_rate": 3.181876034109712e-05,
      "loss": 0.4035,
      "step": 6612
    },
    {
      "epoch": 0.841081081081081,
      "grad_norm": 1.947458267211914,
      "learning_rate": 3.1793305332824236e-05,
      "loss": 0.3419,
      "step": 6613
    },
    {
      "epoch": 0.84120826709062,
      "grad_norm": 2.318377733230591,
      "learning_rate": 3.1767850324551354e-05,
      "loss": 0.6396,
      "step": 6614
    },
    {
      "epoch": 0.841335453100159,
      "grad_norm": 4.39793586730957,
      "learning_rate": 3.174239531627848e-05,
      "loss": 0.8911,
      "step": 6615
    },
    {
      "epoch": 0.841462639109698,
      "grad_norm": 1.6808867454528809,
      "learning_rate": 3.17169403080056e-05,
      "loss": 0.3213,
      "step": 6616
    },
    {
      "epoch": 0.8415898251192369,
      "grad_norm": 2.9849157333374023,
      "learning_rate": 3.1691485299732724e-05,
      "loss": 0.5272,
      "step": 6617
    },
    {
      "epoch": 0.8417170111287758,
      "grad_norm": 1.6279194355010986,
      "learning_rate": 3.166603029145985e-05,
      "loss": 0.422,
      "step": 6618
    },
    {
      "epoch": 0.8418441971383148,
      "grad_norm": 3.092503786087036,
      "learning_rate": 3.164057528318697e-05,
      "loss": 0.5019,
      "step": 6619
    },
    {
      "epoch": 0.8419713831478537,
      "grad_norm": 2.800494432449341,
      "learning_rate": 3.161512027491409e-05,
      "loss": 0.7256,
      "step": 6620
    },
    {
      "epoch": 0.8420985691573927,
      "grad_norm": 2.399681329727173,
      "learning_rate": 3.158966526664121e-05,
      "loss": 0.6228,
      "step": 6621
    },
    {
      "epoch": 0.8422257551669317,
      "grad_norm": 2.786105155944824,
      "learning_rate": 3.156421025836834e-05,
      "loss": 0.5593,
      "step": 6622
    },
    {
      "epoch": 0.8423529411764706,
      "grad_norm": 2.594891309738159,
      "learning_rate": 3.153875525009546e-05,
      "loss": 0.3769,
      "step": 6623
    },
    {
      "epoch": 0.8424801271860095,
      "grad_norm": 2.9919614791870117,
      "learning_rate": 3.151330024182258e-05,
      "loss": 0.4223,
      "step": 6624
    },
    {
      "epoch": 0.8426073131955485,
      "grad_norm": 2.883129119873047,
      "learning_rate": 3.14878452335497e-05,
      "loss": 0.4831,
      "step": 6625
    },
    {
      "epoch": 0.8427344992050875,
      "grad_norm": 2.2340946197509766,
      "learning_rate": 3.146239022527682e-05,
      "loss": 0.4713,
      "step": 6626
    },
    {
      "epoch": 0.8428616852146263,
      "grad_norm": 2.7886135578155518,
      "learning_rate": 3.143693521700395e-05,
      "loss": 0.8356,
      "step": 6627
    },
    {
      "epoch": 0.8429888712241653,
      "grad_norm": 2.019782066345215,
      "learning_rate": 3.141148020873107e-05,
      "loss": 0.3646,
      "step": 6628
    },
    {
      "epoch": 0.8431160572337043,
      "grad_norm": 2.1216557025909424,
      "learning_rate": 3.138602520045819e-05,
      "loss": 0.3392,
      "step": 6629
    },
    {
      "epoch": 0.8432432432432433,
      "grad_norm": 2.2170279026031494,
      "learning_rate": 3.1360570192185315e-05,
      "loss": 0.5073,
      "step": 6630
    },
    {
      "epoch": 0.8433704292527822,
      "grad_norm": 2.354679822921753,
      "learning_rate": 3.1335115183912434e-05,
      "loss": 0.5027,
      "step": 6631
    },
    {
      "epoch": 0.8434976152623211,
      "grad_norm": 2.631526231765747,
      "learning_rate": 3.130966017563956e-05,
      "loss": 0.7049,
      "step": 6632
    },
    {
      "epoch": 0.8436248012718601,
      "grad_norm": 2.896033525466919,
      "learning_rate": 3.128420516736668e-05,
      "loss": 0.6337,
      "step": 6633
    },
    {
      "epoch": 0.843751987281399,
      "grad_norm": 2.4528188705444336,
      "learning_rate": 3.1258750159093804e-05,
      "loss": 0.6996,
      "step": 6634
    },
    {
      "epoch": 0.843879173290938,
      "grad_norm": 2.224567174911499,
      "learning_rate": 3.123329515082092e-05,
      "loss": 0.509,
      "step": 6635
    },
    {
      "epoch": 0.844006359300477,
      "grad_norm": 2.5992419719696045,
      "learning_rate": 3.120784014254805e-05,
      "loss": 0.4812,
      "step": 6636
    },
    {
      "epoch": 0.8441335453100159,
      "grad_norm": 2.150357961654663,
      "learning_rate": 3.118238513427517e-05,
      "loss": 0.4877,
      "step": 6637
    },
    {
      "epoch": 0.8442607313195548,
      "grad_norm": 2.2080087661743164,
      "learning_rate": 3.115693012600229e-05,
      "loss": 0.4317,
      "step": 6638
    },
    {
      "epoch": 0.8443879173290938,
      "grad_norm": 2.0169789791107178,
      "learning_rate": 3.113147511772941e-05,
      "loss": 0.3787,
      "step": 6639
    },
    {
      "epoch": 0.8445151033386328,
      "grad_norm": 2.1290833950042725,
      "learning_rate": 3.1106020109456536e-05,
      "loss": 0.4391,
      "step": 6640
    },
    {
      "epoch": 0.8446422893481716,
      "grad_norm": 2.583564281463623,
      "learning_rate": 3.108056510118366e-05,
      "loss": 0.5495,
      "step": 6641
    },
    {
      "epoch": 0.8447694753577106,
      "grad_norm": 2.159940242767334,
      "learning_rate": 3.105511009291078e-05,
      "loss": 0.5329,
      "step": 6642
    },
    {
      "epoch": 0.8448966613672496,
      "grad_norm": 2.007915496826172,
      "learning_rate": 3.1029655084637906e-05,
      "loss": 0.4869,
      "step": 6643
    },
    {
      "epoch": 0.8450238473767886,
      "grad_norm": 2.6337409019470215,
      "learning_rate": 3.1004200076365025e-05,
      "loss": 0.5258,
      "step": 6644
    },
    {
      "epoch": 0.8451510333863275,
      "grad_norm": 2.99351167678833,
      "learning_rate": 3.0978745068092144e-05,
      "loss": 0.5186,
      "step": 6645
    },
    {
      "epoch": 0.8452782193958664,
      "grad_norm": 2.383690595626831,
      "learning_rate": 3.0953290059819276e-05,
      "loss": 0.5153,
      "step": 6646
    },
    {
      "epoch": 0.8454054054054054,
      "grad_norm": 1.4616104364395142,
      "learning_rate": 3.0927835051546395e-05,
      "loss": 0.4314,
      "step": 6647
    },
    {
      "epoch": 0.8455325914149443,
      "grad_norm": 2.1028809547424316,
      "learning_rate": 3.090238004327351e-05,
      "loss": 0.6335,
      "step": 6648
    },
    {
      "epoch": 0.8456597774244833,
      "grad_norm": 2.3543179035186768,
      "learning_rate": 3.087692503500064e-05,
      "loss": 0.4638,
      "step": 6649
    },
    {
      "epoch": 0.8457869634340223,
      "grad_norm": 2.3362066745758057,
      "learning_rate": 3.085147002672776e-05,
      "loss": 0.548,
      "step": 6650
    },
    {
      "epoch": 0.8459141494435612,
      "grad_norm": 1.79669189453125,
      "learning_rate": 3.082601501845488e-05,
      "loss": 0.4736,
      "step": 6651
    },
    {
      "epoch": 0.8460413354531001,
      "grad_norm": 2.0033812522888184,
      "learning_rate": 3.080056001018201e-05,
      "loss": 0.5882,
      "step": 6652
    },
    {
      "epoch": 0.8461685214626391,
      "grad_norm": 1.8364341259002686,
      "learning_rate": 3.077510500190913e-05,
      "loss": 0.4344,
      "step": 6653
    },
    {
      "epoch": 0.8462957074721781,
      "grad_norm": 2.425739049911499,
      "learning_rate": 3.0749649993636246e-05,
      "loss": 0.5465,
      "step": 6654
    },
    {
      "epoch": 0.8464228934817171,
      "grad_norm": 2.607851505279541,
      "learning_rate": 3.072419498536337e-05,
      "loss": 0.6235,
      "step": 6655
    },
    {
      "epoch": 0.8465500794912559,
      "grad_norm": 1.957290768623352,
      "learning_rate": 3.06987399770905e-05,
      "loss": 0.4143,
      "step": 6656
    },
    {
      "epoch": 0.8466772655007949,
      "grad_norm": 1.9122508764266968,
      "learning_rate": 3.0673284968817616e-05,
      "loss": 0.6258,
      "step": 6657
    },
    {
      "epoch": 0.8468044515103339,
      "grad_norm": 3.0900676250457764,
      "learning_rate": 3.064782996054474e-05,
      "loss": 0.4424,
      "step": 6658
    },
    {
      "epoch": 0.8469316375198728,
      "grad_norm": 2.237663984298706,
      "learning_rate": 3.062237495227186e-05,
      "loss": 0.6081,
      "step": 6659
    },
    {
      "epoch": 0.8470588235294118,
      "grad_norm": 1.585437536239624,
      "learning_rate": 3.059691994399898e-05,
      "loss": 0.2764,
      "step": 6660
    },
    {
      "epoch": 0.8471860095389507,
      "grad_norm": 1.622554898262024,
      "learning_rate": 3.0571464935726104e-05,
      "loss": 0.3743,
      "step": 6661
    },
    {
      "epoch": 0.8473131955484897,
      "grad_norm": 2.4585068225860596,
      "learning_rate": 3.054600992745323e-05,
      "loss": 0.4997,
      "step": 6662
    },
    {
      "epoch": 0.8474403815580286,
      "grad_norm": 1.635439395904541,
      "learning_rate": 3.052055491918035e-05,
      "loss": 0.3045,
      "step": 6663
    },
    {
      "epoch": 0.8475675675675676,
      "grad_norm": 2.209787130355835,
      "learning_rate": 3.049509991090747e-05,
      "loss": 0.6301,
      "step": 6664
    },
    {
      "epoch": 0.8476947535771066,
      "grad_norm": 1.8784900903701782,
      "learning_rate": 3.0469644902634596e-05,
      "loss": 0.5297,
      "step": 6665
    },
    {
      "epoch": 0.8478219395866454,
      "grad_norm": 2.1235063076019287,
      "learning_rate": 3.044418989436172e-05,
      "loss": 0.4506,
      "step": 6666
    },
    {
      "epoch": 0.8479491255961844,
      "grad_norm": 2.3871562480926514,
      "learning_rate": 3.041873488608884e-05,
      "loss": 0.4945,
      "step": 6667
    },
    {
      "epoch": 0.8480763116057234,
      "grad_norm": 2.6357545852661133,
      "learning_rate": 3.0393279877815963e-05,
      "loss": 0.7325,
      "step": 6668
    },
    {
      "epoch": 0.8482034976152624,
      "grad_norm": 1.961850643157959,
      "learning_rate": 3.036782486954308e-05,
      "loss": 0.3793,
      "step": 6669
    },
    {
      "epoch": 0.8483306836248012,
      "grad_norm": 2.9721951484680176,
      "learning_rate": 3.034236986127021e-05,
      "loss": 0.4609,
      "step": 6670
    },
    {
      "epoch": 0.8484578696343402,
      "grad_norm": 2.4171600341796875,
      "learning_rate": 3.031691485299733e-05,
      "loss": 0.5037,
      "step": 6671
    },
    {
      "epoch": 0.8485850556438792,
      "grad_norm": 2.4521079063415527,
      "learning_rate": 3.029145984472445e-05,
      "loss": 0.5585,
      "step": 6672
    },
    {
      "epoch": 0.8487122416534181,
      "grad_norm": 2.1488234996795654,
      "learning_rate": 3.0266004836451573e-05,
      "loss": 0.5707,
      "step": 6673
    },
    {
      "epoch": 0.8488394276629571,
      "grad_norm": 1.9861180782318115,
      "learning_rate": 3.0240549828178692e-05,
      "loss": 0.5463,
      "step": 6674
    },
    {
      "epoch": 0.848966613672496,
      "grad_norm": 2.187013626098633,
      "learning_rate": 3.021509481990582e-05,
      "loss": 0.3244,
      "step": 6675
    },
    {
      "epoch": 0.849093799682035,
      "grad_norm": 2.3866629600524902,
      "learning_rate": 3.0189639811632943e-05,
      "loss": 0.6094,
      "step": 6676
    },
    {
      "epoch": 0.8492209856915739,
      "grad_norm": 1.7770100831985474,
      "learning_rate": 3.0164184803360062e-05,
      "loss": 0.3299,
      "step": 6677
    },
    {
      "epoch": 0.8493481717011129,
      "grad_norm": 2.0171685218811035,
      "learning_rate": 3.0138729795087184e-05,
      "loss": 0.6371,
      "step": 6678
    },
    {
      "epoch": 0.8494753577106519,
      "grad_norm": 1.216457486152649,
      "learning_rate": 3.0113274786814306e-05,
      "loss": 0.2596,
      "step": 6679
    },
    {
      "epoch": 0.8496025437201907,
      "grad_norm": 2.302555561065674,
      "learning_rate": 3.008781977854143e-05,
      "loss": 0.3743,
      "step": 6680
    },
    {
      "epoch": 0.8497297297297297,
      "grad_norm": 2.8184611797332764,
      "learning_rate": 3.0062364770268554e-05,
      "loss": 0.3765,
      "step": 6681
    },
    {
      "epoch": 0.8498569157392687,
      "grad_norm": 2.349940776824951,
      "learning_rate": 3.0036909761995672e-05,
      "loss": 0.6332,
      "step": 6682
    },
    {
      "epoch": 0.8499841017488077,
      "grad_norm": 2.013279438018799,
      "learning_rate": 3.0011454753722795e-05,
      "loss": 0.4308,
      "step": 6683
    },
    {
      "epoch": 0.8501112877583465,
      "grad_norm": 2.896019697189331,
      "learning_rate": 2.9985999745449917e-05,
      "loss": 0.4364,
      "step": 6684
    },
    {
      "epoch": 0.8502384737678855,
      "grad_norm": 2.2845146656036377,
      "learning_rate": 2.9960544737177042e-05,
      "loss": 0.5478,
      "step": 6685
    },
    {
      "epoch": 0.8503656597774245,
      "grad_norm": 2.6207404136657715,
      "learning_rate": 2.9935089728904164e-05,
      "loss": 0.5736,
      "step": 6686
    },
    {
      "epoch": 0.8504928457869634,
      "grad_norm": 2.2470922470092773,
      "learning_rate": 2.9909634720631287e-05,
      "loss": 0.5684,
      "step": 6687
    },
    {
      "epoch": 0.8506200317965024,
      "grad_norm": 1.966820478439331,
      "learning_rate": 2.9884179712358405e-05,
      "loss": 0.4223,
      "step": 6688
    },
    {
      "epoch": 0.8507472178060413,
      "grad_norm": 1.8451889753341675,
      "learning_rate": 2.9858724704085527e-05,
      "loss": 0.375,
      "step": 6689
    },
    {
      "epoch": 0.8508744038155803,
      "grad_norm": 2.1019206047058105,
      "learning_rate": 2.9833269695812653e-05,
      "loss": 0.4474,
      "step": 6690
    },
    {
      "epoch": 0.8510015898251192,
      "grad_norm": 2.4116010665893555,
      "learning_rate": 2.9807814687539775e-05,
      "loss": 0.4648,
      "step": 6691
    },
    {
      "epoch": 0.8511287758346582,
      "grad_norm": 3.227351665496826,
      "learning_rate": 2.9782359679266897e-05,
      "loss": 0.6562,
      "step": 6692
    },
    {
      "epoch": 0.8512559618441972,
      "grad_norm": 2.722012758255005,
      "learning_rate": 2.975690467099402e-05,
      "loss": 0.3387,
      "step": 6693
    },
    {
      "epoch": 0.8513831478537361,
      "grad_norm": 3.1233272552490234,
      "learning_rate": 2.9731449662721138e-05,
      "loss": 0.4951,
      "step": 6694
    },
    {
      "epoch": 0.851510333863275,
      "grad_norm": 1.7692307233810425,
      "learning_rate": 2.9705994654448267e-05,
      "loss": 0.3811,
      "step": 6695
    },
    {
      "epoch": 0.851637519872814,
      "grad_norm": 2.6963918209075928,
      "learning_rate": 2.9680539646175386e-05,
      "loss": 0.6216,
      "step": 6696
    },
    {
      "epoch": 0.851764705882353,
      "grad_norm": 2.3061256408691406,
      "learning_rate": 2.9655084637902508e-05,
      "loss": 0.5213,
      "step": 6697
    },
    {
      "epoch": 0.8518918918918919,
      "grad_norm": 1.2498447895050049,
      "learning_rate": 2.962962962962963e-05,
      "loss": 0.3857,
      "step": 6698
    },
    {
      "epoch": 0.8520190779014308,
      "grad_norm": 2.9662904739379883,
      "learning_rate": 2.9604174621356755e-05,
      "loss": 0.5413,
      "step": 6699
    },
    {
      "epoch": 0.8521462639109698,
      "grad_norm": 2.5106005668640137,
      "learning_rate": 2.9578719613083878e-05,
      "loss": 0.5033,
      "step": 6700
    },
    {
      "epoch": 0.8522734499205088,
      "grad_norm": 1.7168327569961548,
      "learning_rate": 2.9553264604811e-05,
      "loss": 0.5018,
      "step": 6701
    },
    {
      "epoch": 0.8524006359300477,
      "grad_norm": 2.3448069095611572,
      "learning_rate": 2.952780959653812e-05,
      "loss": 0.4455,
      "step": 6702
    },
    {
      "epoch": 0.8525278219395866,
      "grad_norm": 2.5920169353485107,
      "learning_rate": 2.950235458826524e-05,
      "loss": 0.659,
      "step": 6703
    },
    {
      "epoch": 0.8526550079491256,
      "grad_norm": 2.250183343887329,
      "learning_rate": 2.9476899579992366e-05,
      "loss": 0.4125,
      "step": 6704
    },
    {
      "epoch": 0.8527821939586645,
      "grad_norm": 2.3656318187713623,
      "learning_rate": 2.9451444571719488e-05,
      "loss": 0.5745,
      "step": 6705
    },
    {
      "epoch": 0.8529093799682035,
      "grad_norm": 2.833224296569824,
      "learning_rate": 2.942598956344661e-05,
      "loss": 0.5453,
      "step": 6706
    },
    {
      "epoch": 0.8530365659777425,
      "grad_norm": 2.701479434967041,
      "learning_rate": 2.9400534555173732e-05,
      "loss": 0.3231,
      "step": 6707
    },
    {
      "epoch": 0.8531637519872814,
      "grad_norm": 1.8574848175048828,
      "learning_rate": 2.937507954690085e-05,
      "loss": 0.5251,
      "step": 6708
    },
    {
      "epoch": 0.8532909379968203,
      "grad_norm": 2.016737937927246,
      "learning_rate": 2.934962453862798e-05,
      "loss": 0.793,
      "step": 6709
    },
    {
      "epoch": 0.8534181240063593,
      "grad_norm": 2.2090752124786377,
      "learning_rate": 2.93241695303551e-05,
      "loss": 0.637,
      "step": 6710
    },
    {
      "epoch": 0.8535453100158983,
      "grad_norm": 2.2581725120544434,
      "learning_rate": 2.929871452208222e-05,
      "loss": 0.5274,
      "step": 6711
    },
    {
      "epoch": 0.8536724960254372,
      "grad_norm": 1.666499376296997,
      "learning_rate": 2.9273259513809343e-05,
      "loss": 0.4351,
      "step": 6712
    },
    {
      "epoch": 0.8537996820349761,
      "grad_norm": 1.9877784252166748,
      "learning_rate": 2.9247804505536465e-05,
      "loss": 0.4417,
      "step": 6713
    },
    {
      "epoch": 0.8539268680445151,
      "grad_norm": 2.711461067199707,
      "learning_rate": 2.922234949726359e-05,
      "loss": 0.4275,
      "step": 6714
    },
    {
      "epoch": 0.8540540540540541,
      "grad_norm": 1.3755786418914795,
      "learning_rate": 2.9196894488990713e-05,
      "loss": 0.3069,
      "step": 6715
    },
    {
      "epoch": 0.854181240063593,
      "grad_norm": 2.5386886596679688,
      "learning_rate": 2.917143948071783e-05,
      "loss": 0.6336,
      "step": 6716
    },
    {
      "epoch": 0.854308426073132,
      "grad_norm": 2.4911162853240967,
      "learning_rate": 2.9145984472444954e-05,
      "loss": 0.5708,
      "step": 6717
    },
    {
      "epoch": 0.8544356120826709,
      "grad_norm": 2.3773157596588135,
      "learning_rate": 2.9120529464172076e-05,
      "loss": 0.6257,
      "step": 6718
    },
    {
      "epoch": 0.8545627980922098,
      "grad_norm": 2.553821086883545,
      "learning_rate": 2.90950744558992e-05,
      "loss": 0.717,
      "step": 6719
    },
    {
      "epoch": 0.8546899841017488,
      "grad_norm": 2.272759199142456,
      "learning_rate": 2.9069619447626324e-05,
      "loss": 0.5984,
      "step": 6720
    },
    {
      "epoch": 0.8548171701112878,
      "grad_norm": 2.267289638519287,
      "learning_rate": 2.9044164439353446e-05,
      "loss": 0.5433,
      "step": 6721
    },
    {
      "epoch": 0.8549443561208268,
      "grad_norm": 2.329057216644287,
      "learning_rate": 2.9018709431080564e-05,
      "loss": 0.5295,
      "step": 6722
    },
    {
      "epoch": 0.8550715421303656,
      "grad_norm": 2.4568276405334473,
      "learning_rate": 2.8993254422807687e-05,
      "loss": 0.4803,
      "step": 6723
    },
    {
      "epoch": 0.8551987281399046,
      "grad_norm": 1.9969918727874756,
      "learning_rate": 2.8967799414534812e-05,
      "loss": 0.5383,
      "step": 6724
    },
    {
      "epoch": 0.8553259141494436,
      "grad_norm": 2.543138265609741,
      "learning_rate": 2.8942344406261934e-05,
      "loss": 0.4805,
      "step": 6725
    },
    {
      "epoch": 0.8554531001589825,
      "grad_norm": 2.8552074432373047,
      "learning_rate": 2.8916889397989056e-05,
      "loss": 0.4589,
      "step": 6726
    },
    {
      "epoch": 0.8555802861685214,
      "grad_norm": 2.5057411193847656,
      "learning_rate": 2.8891434389716175e-05,
      "loss": 0.5934,
      "step": 6727
    },
    {
      "epoch": 0.8557074721780604,
      "grad_norm": 3.1281535625457764,
      "learning_rate": 2.8865979381443297e-05,
      "loss": 0.5392,
      "step": 6728
    },
    {
      "epoch": 0.8558346581875994,
      "grad_norm": 3.459721565246582,
      "learning_rate": 2.8840524373170426e-05,
      "loss": 0.5531,
      "step": 6729
    },
    {
      "epoch": 0.8559618441971383,
      "grad_norm": 2.805785655975342,
      "learning_rate": 2.8815069364897545e-05,
      "loss": 0.5806,
      "step": 6730
    },
    {
      "epoch": 0.8560890302066773,
      "grad_norm": 3.072849750518799,
      "learning_rate": 2.8789614356624667e-05,
      "loss": 0.467,
      "step": 6731
    },
    {
      "epoch": 0.8562162162162162,
      "grad_norm": 2.7646114826202393,
      "learning_rate": 2.876415934835179e-05,
      "loss": 0.604,
      "step": 6732
    },
    {
      "epoch": 0.8563434022257551,
      "grad_norm": 2.3711297512054443,
      "learning_rate": 2.8738704340078915e-05,
      "loss": 0.505,
      "step": 6733
    },
    {
      "epoch": 0.8564705882352941,
      "grad_norm": 3.7252111434936523,
      "learning_rate": 2.8713249331806037e-05,
      "loss": 0.6435,
      "step": 6734
    },
    {
      "epoch": 0.8565977742448331,
      "grad_norm": 1.6796847581863403,
      "learning_rate": 2.8687794323533155e-05,
      "loss": 0.3751,
      "step": 6735
    },
    {
      "epoch": 0.8567249602543721,
      "grad_norm": 2.2320878505706787,
      "learning_rate": 2.8662339315260278e-05,
      "loss": 0.5368,
      "step": 6736
    },
    {
      "epoch": 0.8568521462639109,
      "grad_norm": 2.4298720359802246,
      "learning_rate": 2.86368843069874e-05,
      "loss": 0.3897,
      "step": 6737
    },
    {
      "epoch": 0.8569793322734499,
      "grad_norm": 1.742138385772705,
      "learning_rate": 2.8611429298714525e-05,
      "loss": 0.5288,
      "step": 6738
    },
    {
      "epoch": 0.8571065182829889,
      "grad_norm": 2.1146509647369385,
      "learning_rate": 2.8585974290441647e-05,
      "loss": 0.5178,
      "step": 6739
    },
    {
      "epoch": 0.8572337042925279,
      "grad_norm": 2.3615596294403076,
      "learning_rate": 2.856051928216877e-05,
      "loss": 0.7338,
      "step": 6740
    },
    {
      "epoch": 0.8573608903020667,
      "grad_norm": 2.2756574153900146,
      "learning_rate": 2.8535064273895888e-05,
      "loss": 0.7195,
      "step": 6741
    },
    {
      "epoch": 0.8574880763116057,
      "grad_norm": 2.1100993156433105,
      "learning_rate": 2.850960926562301e-05,
      "loss": 0.5304,
      "step": 6742
    },
    {
      "epoch": 0.8576152623211447,
      "grad_norm": 2.5232300758361816,
      "learning_rate": 2.8484154257350136e-05,
      "loss": 0.7272,
      "step": 6743
    },
    {
      "epoch": 0.8577424483306836,
      "grad_norm": 2.042262315750122,
      "learning_rate": 2.8458699249077258e-05,
      "loss": 0.5829,
      "step": 6744
    },
    {
      "epoch": 0.8578696343402226,
      "grad_norm": 2.1258044242858887,
      "learning_rate": 2.843324424080438e-05,
      "loss": 0.6031,
      "step": 6745
    },
    {
      "epoch": 0.8579968203497615,
      "grad_norm": 2.041356086730957,
      "learning_rate": 2.8407789232531502e-05,
      "loss": 0.3083,
      "step": 6746
    },
    {
      "epoch": 0.8581240063593005,
      "grad_norm": 1.873610019683838,
      "learning_rate": 2.838233422425862e-05,
      "loss": 0.4722,
      "step": 6747
    },
    {
      "epoch": 0.8582511923688394,
      "grad_norm": 2.366997241973877,
      "learning_rate": 2.835687921598575e-05,
      "loss": 0.8169,
      "step": 6748
    },
    {
      "epoch": 0.8583783783783784,
      "grad_norm": 1.6534547805786133,
      "learning_rate": 2.833142420771287e-05,
      "loss": 0.4287,
      "step": 6749
    },
    {
      "epoch": 0.8585055643879174,
      "grad_norm": 2.129019260406494,
      "learning_rate": 2.830596919943999e-05,
      "loss": 0.453,
      "step": 6750
    },
    {
      "epoch": 0.8586327503974562,
      "grad_norm": 1.7418683767318726,
      "learning_rate": 2.8280514191167113e-05,
      "loss": 0.5095,
      "step": 6751
    },
    {
      "epoch": 0.8587599364069952,
      "grad_norm": 2.4887239933013916,
      "learning_rate": 2.8255059182894235e-05,
      "loss": 0.5146,
      "step": 6752
    },
    {
      "epoch": 0.8588871224165342,
      "grad_norm": 1.9307329654693604,
      "learning_rate": 2.822960417462136e-05,
      "loss": 0.3641,
      "step": 6753
    },
    {
      "epoch": 0.8590143084260732,
      "grad_norm": 4.111318111419678,
      "learning_rate": 2.8204149166348483e-05,
      "loss": 0.6969,
      "step": 6754
    },
    {
      "epoch": 0.859141494435612,
      "grad_norm": 1.720604419708252,
      "learning_rate": 2.81786941580756e-05,
      "loss": 0.5401,
      "step": 6755
    },
    {
      "epoch": 0.859268680445151,
      "grad_norm": 2.1889991760253906,
      "learning_rate": 2.8153239149802723e-05,
      "loss": 0.5208,
      "step": 6756
    },
    {
      "epoch": 0.85939586645469,
      "grad_norm": 2.353889226913452,
      "learning_rate": 2.8127784141529846e-05,
      "loss": 0.5311,
      "step": 6757
    },
    {
      "epoch": 0.8595230524642289,
      "grad_norm": 2.4041895866394043,
      "learning_rate": 2.810232913325697e-05,
      "loss": 0.6956,
      "step": 6758
    },
    {
      "epoch": 0.8596502384737679,
      "grad_norm": 1.644425630569458,
      "learning_rate": 2.8076874124984093e-05,
      "loss": 0.5496,
      "step": 6759
    },
    {
      "epoch": 0.8597774244833069,
      "grad_norm": 1.7067795991897583,
      "learning_rate": 2.8051419116711215e-05,
      "loss": 0.4733,
      "step": 6760
    },
    {
      "epoch": 0.8599046104928458,
      "grad_norm": 1.6199381351470947,
      "learning_rate": 2.8025964108438334e-05,
      "loss": 0.5985,
      "step": 6761
    },
    {
      "epoch": 0.8600317965023847,
      "grad_norm": 2.143760919570923,
      "learning_rate": 2.8000509100165456e-05,
      "loss": 0.4935,
      "step": 6762
    },
    {
      "epoch": 0.8601589825119237,
      "grad_norm": 1.826993465423584,
      "learning_rate": 2.7975054091892582e-05,
      "loss": 0.4417,
      "step": 6763
    },
    {
      "epoch": 0.8602861685214627,
      "grad_norm": 2.5023114681243896,
      "learning_rate": 2.7949599083619704e-05,
      "loss": 0.6741,
      "step": 6764
    },
    {
      "epoch": 0.8604133545310015,
      "grad_norm": 2.1290462017059326,
      "learning_rate": 2.7924144075346826e-05,
      "loss": 0.3064,
      "step": 6765
    },
    {
      "epoch": 0.8605405405405405,
      "grad_norm": 2.3724205493927,
      "learning_rate": 2.7898689067073948e-05,
      "loss": 0.4395,
      "step": 6766
    },
    {
      "epoch": 0.8606677265500795,
      "grad_norm": 2.7043304443359375,
      "learning_rate": 2.7873234058801074e-05,
      "loss": 0.6972,
      "step": 6767
    },
    {
      "epoch": 0.8607949125596185,
      "grad_norm": 2.6555073261260986,
      "learning_rate": 2.7847779050528196e-05,
      "loss": 0.2831,
      "step": 6768
    },
    {
      "epoch": 0.8609220985691574,
      "grad_norm": 2.495073080062866,
      "learning_rate": 2.7822324042255315e-05,
      "loss": 0.5425,
      "step": 6769
    },
    {
      "epoch": 0.8610492845786963,
      "grad_norm": 2.8279600143432617,
      "learning_rate": 2.7796869033982437e-05,
      "loss": 0.8393,
      "step": 6770
    },
    {
      "epoch": 0.8611764705882353,
      "grad_norm": 1.7565492391586304,
      "learning_rate": 2.777141402570956e-05,
      "loss": 0.4545,
      "step": 6771
    },
    {
      "epoch": 0.8613036565977742,
      "grad_norm": 2.869993209838867,
      "learning_rate": 2.7745959017436684e-05,
      "loss": 0.5197,
      "step": 6772
    },
    {
      "epoch": 0.8614308426073132,
      "grad_norm": 1.9879415035247803,
      "learning_rate": 2.7720504009163806e-05,
      "loss": 0.4217,
      "step": 6773
    },
    {
      "epoch": 0.8615580286168522,
      "grad_norm": 2.4441471099853516,
      "learning_rate": 2.769504900089093e-05,
      "loss": 0.6709,
      "step": 6774
    },
    {
      "epoch": 0.8616852146263911,
      "grad_norm": 3.125067710876465,
      "learning_rate": 2.7669593992618047e-05,
      "loss": 0.4964,
      "step": 6775
    },
    {
      "epoch": 0.86181240063593,
      "grad_norm": 2.829360008239746,
      "learning_rate": 2.764413898434517e-05,
      "loss": 0.7193,
      "step": 6776
    },
    {
      "epoch": 0.861939586645469,
      "grad_norm": 2.5580854415893555,
      "learning_rate": 2.7618683976072295e-05,
      "loss": 0.5449,
      "step": 6777
    },
    {
      "epoch": 0.862066772655008,
      "grad_norm": 1.368876338005066,
      "learning_rate": 2.7593228967799417e-05,
      "loss": 0.3357,
      "step": 6778
    },
    {
      "epoch": 0.8621939586645468,
      "grad_norm": 2.476834297180176,
      "learning_rate": 2.756777395952654e-05,
      "loss": 0.5514,
      "step": 6779
    },
    {
      "epoch": 0.8623211446740858,
      "grad_norm": 1.5648423433303833,
      "learning_rate": 2.7542318951253658e-05,
      "loss": 0.638,
      "step": 6780
    },
    {
      "epoch": 0.8624483306836248,
      "grad_norm": 3.0573768615722656,
      "learning_rate": 2.751686394298078e-05,
      "loss": 0.5999,
      "step": 6781
    },
    {
      "epoch": 0.8625755166931638,
      "grad_norm": 1.9717949628829956,
      "learning_rate": 2.749140893470791e-05,
      "loss": 0.6209,
      "step": 6782
    },
    {
      "epoch": 0.8627027027027027,
      "grad_norm": 2.1754379272460938,
      "learning_rate": 2.7465953926435028e-05,
      "loss": 0.618,
      "step": 6783
    },
    {
      "epoch": 0.8628298887122416,
      "grad_norm": 2.8771581649780273,
      "learning_rate": 2.744049891816215e-05,
      "loss": 0.537,
      "step": 6784
    },
    {
      "epoch": 0.8629570747217806,
      "grad_norm": 2.4436147212982178,
      "learning_rate": 2.7415043909889272e-05,
      "loss": 0.5963,
      "step": 6785
    },
    {
      "epoch": 0.8630842607313196,
      "grad_norm": 2.666436195373535,
      "learning_rate": 2.738958890161639e-05,
      "loss": 0.6481,
      "step": 6786
    },
    {
      "epoch": 0.8632114467408585,
      "grad_norm": 2.5431947708129883,
      "learning_rate": 2.736413389334352e-05,
      "loss": 0.3924,
      "step": 6787
    },
    {
      "epoch": 0.8633386327503975,
      "grad_norm": 2.676335573196411,
      "learning_rate": 2.733867888507064e-05,
      "loss": 0.6242,
      "step": 6788
    },
    {
      "epoch": 0.8634658187599364,
      "grad_norm": 1.7192081212997437,
      "learning_rate": 2.731322387679776e-05,
      "loss": 0.4279,
      "step": 6789
    },
    {
      "epoch": 0.8635930047694753,
      "grad_norm": 2.8487837314605713,
      "learning_rate": 2.7287768868524883e-05,
      "loss": 0.5429,
      "step": 6790
    },
    {
      "epoch": 0.8637201907790143,
      "grad_norm": 2.345395565032959,
      "learning_rate": 2.7262313860252005e-05,
      "loss": 0.4881,
      "step": 6791
    },
    {
      "epoch": 0.8638473767885533,
      "grad_norm": 2.5943922996520996,
      "learning_rate": 2.723685885197913e-05,
      "loss": 0.6876,
      "step": 6792
    },
    {
      "epoch": 0.8639745627980923,
      "grad_norm": 2.1967718601226807,
      "learning_rate": 2.7211403843706252e-05,
      "loss": 0.5428,
      "step": 6793
    },
    {
      "epoch": 0.8641017488076311,
      "grad_norm": 2.5893330574035645,
      "learning_rate": 2.718594883543337e-05,
      "loss": 0.536,
      "step": 6794
    },
    {
      "epoch": 0.8642289348171701,
      "grad_norm": 3.053591251373291,
      "learning_rate": 2.7160493827160493e-05,
      "loss": 0.6952,
      "step": 6795
    },
    {
      "epoch": 0.8643561208267091,
      "grad_norm": 2.0222249031066895,
      "learning_rate": 2.7135038818887615e-05,
      "loss": 0.4439,
      "step": 6796
    },
    {
      "epoch": 0.864483306836248,
      "grad_norm": 1.8943485021591187,
      "learning_rate": 2.710958381061474e-05,
      "loss": 0.4855,
      "step": 6797
    },
    {
      "epoch": 0.864610492845787,
      "grad_norm": 2.3675665855407715,
      "learning_rate": 2.7084128802341863e-05,
      "loss": 0.4154,
      "step": 6798
    },
    {
      "epoch": 0.8647376788553259,
      "grad_norm": 1.79683518409729,
      "learning_rate": 2.7058673794068985e-05,
      "loss": 0.5731,
      "step": 6799
    },
    {
      "epoch": 0.8648648648648649,
      "grad_norm": 1.9694349765777588,
      "learning_rate": 2.7033218785796104e-05,
      "loss": 0.4748,
      "step": 6800
    },
    {
      "epoch": 0.8649920508744038,
      "grad_norm": 2.1397805213928223,
      "learning_rate": 2.7007763777523233e-05,
      "loss": 0.3779,
      "step": 6801
    },
    {
      "epoch": 0.8651192368839428,
      "grad_norm": 2.2695391178131104,
      "learning_rate": 2.698230876925035e-05,
      "loss": 0.4412,
      "step": 6802
    },
    {
      "epoch": 0.8652464228934817,
      "grad_norm": 1.9430731534957886,
      "learning_rate": 2.6956853760977474e-05,
      "loss": 0.4724,
      "step": 6803
    },
    {
      "epoch": 0.8653736089030206,
      "grad_norm": 2.0961101055145264,
      "learning_rate": 2.6931398752704596e-05,
      "loss": 0.4538,
      "step": 6804
    },
    {
      "epoch": 0.8655007949125596,
      "grad_norm": 1.8826003074645996,
      "learning_rate": 2.6905943744431718e-05,
      "loss": 0.3618,
      "step": 6805
    },
    {
      "epoch": 0.8656279809220986,
      "grad_norm": 2.6207401752471924,
      "learning_rate": 2.6880488736158843e-05,
      "loss": 0.502,
      "step": 6806
    },
    {
      "epoch": 0.8657551669316376,
      "grad_norm": 3.6775624752044678,
      "learning_rate": 2.6855033727885966e-05,
      "loss": 0.6357,
      "step": 6807
    },
    {
      "epoch": 0.8658823529411764,
      "grad_norm": 2.051680088043213,
      "learning_rate": 2.6829578719613084e-05,
      "loss": 0.4998,
      "step": 6808
    },
    {
      "epoch": 0.8660095389507154,
      "grad_norm": 2.345752716064453,
      "learning_rate": 2.6804123711340206e-05,
      "loss": 0.4886,
      "step": 6809
    },
    {
      "epoch": 0.8661367249602544,
      "grad_norm": 1.9393327236175537,
      "learning_rate": 2.677866870306733e-05,
      "loss": 0.5347,
      "step": 6810
    },
    {
      "epoch": 0.8662639109697933,
      "grad_norm": 2.3923113346099854,
      "learning_rate": 2.6753213694794454e-05,
      "loss": 0.533,
      "step": 6811
    },
    {
      "epoch": 0.8663910969793323,
      "grad_norm": 2.419891357421875,
      "learning_rate": 2.6727758686521576e-05,
      "loss": 0.5994,
      "step": 6812
    },
    {
      "epoch": 0.8665182829888712,
      "grad_norm": 1.7124526500701904,
      "learning_rate": 2.67023036782487e-05,
      "loss": 0.4778,
      "step": 6813
    },
    {
      "epoch": 0.8666454689984102,
      "grad_norm": 1.8063539266586304,
      "learning_rate": 2.6676848669975817e-05,
      "loss": 0.4936,
      "step": 6814
    },
    {
      "epoch": 0.8667726550079491,
      "grad_norm": 2.5094873905181885,
      "learning_rate": 2.665139366170294e-05,
      "loss": 0.4143,
      "step": 6815
    },
    {
      "epoch": 0.8668998410174881,
      "grad_norm": 2.0617477893829346,
      "learning_rate": 2.6625938653430065e-05,
      "loss": 0.5282,
      "step": 6816
    },
    {
      "epoch": 0.867027027027027,
      "grad_norm": 2.9257726669311523,
      "learning_rate": 2.6600483645157187e-05,
      "loss": 0.8281,
      "step": 6817
    },
    {
      "epoch": 0.8671542130365659,
      "grad_norm": 2.529313325881958,
      "learning_rate": 2.657502863688431e-05,
      "loss": 0.585,
      "step": 6818
    },
    {
      "epoch": 0.8672813990461049,
      "grad_norm": 4.659512996673584,
      "learning_rate": 2.6549573628611428e-05,
      "loss": 0.6449,
      "step": 6819
    },
    {
      "epoch": 0.8674085850556439,
      "grad_norm": 2.470235824584961,
      "learning_rate": 2.652411862033855e-05,
      "loss": 0.6551,
      "step": 6820
    },
    {
      "epoch": 0.8675357710651829,
      "grad_norm": 2.2522642612457275,
      "learning_rate": 2.649866361206568e-05,
      "loss": 0.7073,
      "step": 6821
    },
    {
      "epoch": 0.8676629570747217,
      "grad_norm": 2.4746947288513184,
      "learning_rate": 2.6473208603792797e-05,
      "loss": 0.4733,
      "step": 6822
    },
    {
      "epoch": 0.8677901430842607,
      "grad_norm": 2.2513327598571777,
      "learning_rate": 2.644775359551992e-05,
      "loss": 0.5249,
      "step": 6823
    },
    {
      "epoch": 0.8679173290937997,
      "grad_norm": 1.767859935760498,
      "learning_rate": 2.6422298587247042e-05,
      "loss": 0.4377,
      "step": 6824
    },
    {
      "epoch": 0.8680445151033387,
      "grad_norm": 1.831276535987854,
      "learning_rate": 2.639684357897416e-05,
      "loss": 0.471,
      "step": 6825
    },
    {
      "epoch": 0.8681717011128776,
      "grad_norm": 2.5812764167785645,
      "learning_rate": 2.637138857070129e-05,
      "loss": 0.6611,
      "step": 6826
    },
    {
      "epoch": 0.8682988871224165,
      "grad_norm": 2.1207072734832764,
      "learning_rate": 2.634593356242841e-05,
      "loss": 0.613,
      "step": 6827
    },
    {
      "epoch": 0.8684260731319555,
      "grad_norm": 1.9335850477218628,
      "learning_rate": 2.632047855415553e-05,
      "loss": 0.4715,
      "step": 6828
    },
    {
      "epoch": 0.8685532591414944,
      "grad_norm": 2.508871555328369,
      "learning_rate": 2.6295023545882652e-05,
      "loss": 0.4831,
      "step": 6829
    },
    {
      "epoch": 0.8686804451510334,
      "grad_norm": 2.3373517990112305,
      "learning_rate": 2.6269568537609774e-05,
      "loss": 0.6807,
      "step": 6830
    },
    {
      "epoch": 0.8688076311605724,
      "grad_norm": 2.2533278465270996,
      "learning_rate": 2.62441135293369e-05,
      "loss": 0.4705,
      "step": 6831
    },
    {
      "epoch": 0.8689348171701113,
      "grad_norm": 2.5296154022216797,
      "learning_rate": 2.6218658521064022e-05,
      "loss": 0.5401,
      "step": 6832
    },
    {
      "epoch": 0.8690620031796502,
      "grad_norm": 1.568596601486206,
      "learning_rate": 2.619320351279114e-05,
      "loss": 0.3548,
      "step": 6833
    },
    {
      "epoch": 0.8691891891891892,
      "grad_norm": 2.5586941242218018,
      "learning_rate": 2.6167748504518263e-05,
      "loss": 0.6848,
      "step": 6834
    },
    {
      "epoch": 0.8693163751987282,
      "grad_norm": 2.787846088409424,
      "learning_rate": 2.6142293496245392e-05,
      "loss": 0.5294,
      "step": 6835
    },
    {
      "epoch": 0.869443561208267,
      "grad_norm": 2.468761444091797,
      "learning_rate": 2.611683848797251e-05,
      "loss": 0.9611,
      "step": 6836
    },
    {
      "epoch": 0.869570747217806,
      "grad_norm": 2.2532734870910645,
      "learning_rate": 2.6091383479699633e-05,
      "loss": 0.5684,
      "step": 6837
    },
    {
      "epoch": 0.869697933227345,
      "grad_norm": 2.985366106033325,
      "learning_rate": 2.6065928471426755e-05,
      "loss": 0.6567,
      "step": 6838
    },
    {
      "epoch": 0.869825119236884,
      "grad_norm": 2.5018770694732666,
      "learning_rate": 2.6040473463153874e-05,
      "loss": 0.4197,
      "step": 6839
    },
    {
      "epoch": 0.8699523052464229,
      "grad_norm": 2.4232068061828613,
      "learning_rate": 2.6015018454881003e-05,
      "loss": 0.417,
      "step": 6840
    },
    {
      "epoch": 0.8700794912559618,
      "grad_norm": 2.8768467903137207,
      "learning_rate": 2.598956344660812e-05,
      "loss": 0.5035,
      "step": 6841
    },
    {
      "epoch": 0.8702066772655008,
      "grad_norm": 2.7041726112365723,
      "learning_rate": 2.5964108438335243e-05,
      "loss": 0.6899,
      "step": 6842
    },
    {
      "epoch": 0.8703338632750397,
      "grad_norm": 2.4706661701202393,
      "learning_rate": 2.5938653430062366e-05,
      "loss": 0.8213,
      "step": 6843
    },
    {
      "epoch": 0.8704610492845787,
      "grad_norm": 1.6940662860870361,
      "learning_rate": 2.5913198421789488e-05,
      "loss": 0.4044,
      "step": 6844
    },
    {
      "epoch": 0.8705882352941177,
      "grad_norm": 2.2977466583251953,
      "learning_rate": 2.5887743413516613e-05,
      "loss": 0.4433,
      "step": 6845
    },
    {
      "epoch": 0.8707154213036566,
      "grad_norm": 2.1289708614349365,
      "learning_rate": 2.5862288405243735e-05,
      "loss": 0.6151,
      "step": 6846
    },
    {
      "epoch": 0.8708426073131955,
      "grad_norm": 2.1357510089874268,
      "learning_rate": 2.5836833396970854e-05,
      "loss": 0.5289,
      "step": 6847
    },
    {
      "epoch": 0.8709697933227345,
      "grad_norm": 1.934954047203064,
      "learning_rate": 2.5811378388697976e-05,
      "loss": 0.5232,
      "step": 6848
    },
    {
      "epoch": 0.8710969793322735,
      "grad_norm": 1.678829550743103,
      "learning_rate": 2.5785923380425098e-05,
      "loss": 0.2997,
      "step": 6849
    },
    {
      "epoch": 0.8712241653418124,
      "grad_norm": 2.7250819206237793,
      "learning_rate": 2.5760468372152224e-05,
      "loss": 0.5196,
      "step": 6850
    },
    {
      "epoch": 0.8713513513513513,
      "grad_norm": 3.252469539642334,
      "learning_rate": 2.5735013363879346e-05,
      "loss": 0.8219,
      "step": 6851
    },
    {
      "epoch": 0.8714785373608903,
      "grad_norm": 1.5714304447174072,
      "learning_rate": 2.5709558355606468e-05,
      "loss": 0.3683,
      "step": 6852
    },
    {
      "epoch": 0.8716057233704293,
      "grad_norm": 2.869394063949585,
      "learning_rate": 2.5684103347333587e-05,
      "loss": 0.4763,
      "step": 6853
    },
    {
      "epoch": 0.8717329093799682,
      "grad_norm": 1.814501166343689,
      "learning_rate": 2.565864833906071e-05,
      "loss": 0.4135,
      "step": 6854
    },
    {
      "epoch": 0.8718600953895072,
      "grad_norm": 3.1313536167144775,
      "learning_rate": 2.5633193330787834e-05,
      "loss": 0.9261,
      "step": 6855
    },
    {
      "epoch": 0.8719872813990461,
      "grad_norm": 2.5962843894958496,
      "learning_rate": 2.5607738322514957e-05,
      "loss": 0.4391,
      "step": 6856
    },
    {
      "epoch": 0.872114467408585,
      "grad_norm": 1.9526673555374146,
      "learning_rate": 2.558228331424208e-05,
      "loss": 0.4877,
      "step": 6857
    },
    {
      "epoch": 0.872241653418124,
      "grad_norm": 2.11444091796875,
      "learning_rate": 2.55568283059692e-05,
      "loss": 0.5229,
      "step": 6858
    },
    {
      "epoch": 0.872368839427663,
      "grad_norm": 2.0111279487609863,
      "learning_rate": 2.553137329769632e-05,
      "loss": 0.4465,
      "step": 6859
    },
    {
      "epoch": 0.872496025437202,
      "grad_norm": 2.4351489543914795,
      "learning_rate": 2.550591828942345e-05,
      "loss": 0.5924,
      "step": 6860
    },
    {
      "epoch": 0.8726232114467408,
      "grad_norm": 2.4454870223999023,
      "learning_rate": 2.5480463281150567e-05,
      "loss": 0.5824,
      "step": 6861
    },
    {
      "epoch": 0.8727503974562798,
      "grad_norm": 2.4588096141815186,
      "learning_rate": 2.545500827287769e-05,
      "loss": 0.6754,
      "step": 6862
    },
    {
      "epoch": 0.8728775834658188,
      "grad_norm": 2.3835184574127197,
      "learning_rate": 2.542955326460481e-05,
      "loss": 0.5064,
      "step": 6863
    },
    {
      "epoch": 0.8730047694753577,
      "grad_norm": 2.3105177879333496,
      "learning_rate": 2.540409825633193e-05,
      "loss": 0.5508,
      "step": 6864
    },
    {
      "epoch": 0.8731319554848966,
      "grad_norm": 2.551503896713257,
      "learning_rate": 2.537864324805906e-05,
      "loss": 0.5192,
      "step": 6865
    },
    {
      "epoch": 0.8732591414944356,
      "grad_norm": 2.103100299835205,
      "learning_rate": 2.535318823978618e-05,
      "loss": 0.7385,
      "step": 6866
    },
    {
      "epoch": 0.8733863275039746,
      "grad_norm": 3.0455005168914795,
      "learning_rate": 2.53277332315133e-05,
      "loss": 0.4364,
      "step": 6867
    },
    {
      "epoch": 0.8735135135135135,
      "grad_norm": 2.4515042304992676,
      "learning_rate": 2.5302278223240422e-05,
      "loss": 0.7357,
      "step": 6868
    },
    {
      "epoch": 0.8736406995230525,
      "grad_norm": 1.911942958831787,
      "learning_rate": 2.5276823214967548e-05,
      "loss": 0.5069,
      "step": 6869
    },
    {
      "epoch": 0.8737678855325914,
      "grad_norm": 2.100224018096924,
      "learning_rate": 2.525136820669467e-05,
      "loss": 0.5889,
      "step": 6870
    },
    {
      "epoch": 0.8738950715421304,
      "grad_norm": 2.3979079723358154,
      "learning_rate": 2.5225913198421792e-05,
      "loss": 0.5793,
      "step": 6871
    },
    {
      "epoch": 0.8740222575516693,
      "grad_norm": 3.6833415031433105,
      "learning_rate": 2.520045819014891e-05,
      "loss": 0.7176,
      "step": 6872
    },
    {
      "epoch": 0.8741494435612083,
      "grad_norm": 2.416616201400757,
      "learning_rate": 2.5175003181876033e-05,
      "loss": 0.578,
      "step": 6873
    },
    {
      "epoch": 0.8742766295707473,
      "grad_norm": 2.3849923610687256,
      "learning_rate": 2.514954817360316e-05,
      "loss": 0.5081,
      "step": 6874
    },
    {
      "epoch": 0.8744038155802861,
      "grad_norm": 2.732736587524414,
      "learning_rate": 2.512409316533028e-05,
      "loss": 0.7124,
      "step": 6875
    },
    {
      "epoch": 0.8745310015898251,
      "grad_norm": 2.907623529434204,
      "learning_rate": 2.5098638157057403e-05,
      "loss": 0.5182,
      "step": 6876
    },
    {
      "epoch": 0.8746581875993641,
      "grad_norm": 1.4438303709030151,
      "learning_rate": 2.5073183148784525e-05,
      "loss": 0.2607,
      "step": 6877
    },
    {
      "epoch": 0.8747853736089031,
      "grad_norm": 3.1822285652160645,
      "learning_rate": 2.5047728140511643e-05,
      "loss": 0.4876,
      "step": 6878
    },
    {
      "epoch": 0.8749125596184419,
      "grad_norm": 1.8467326164245605,
      "learning_rate": 2.5022273132238772e-05,
      "loss": 0.3592,
      "step": 6879
    },
    {
      "epoch": 0.8750397456279809,
      "grad_norm": 2.02192759513855,
      "learning_rate": 2.499681812396589e-05,
      "loss": 0.4688,
      "step": 6880
    },
    {
      "epoch": 0.8751669316375199,
      "grad_norm": 3.405151844024658,
      "learning_rate": 2.4971363115693013e-05,
      "loss": 0.3327,
      "step": 6881
    },
    {
      "epoch": 0.8752941176470588,
      "grad_norm": 2.347088575363159,
      "learning_rate": 2.4945908107420135e-05,
      "loss": 0.7771,
      "step": 6882
    },
    {
      "epoch": 0.8754213036565978,
      "grad_norm": 2.089026927947998,
      "learning_rate": 2.4920453099147257e-05,
      "loss": 0.5206,
      "step": 6883
    },
    {
      "epoch": 0.8755484896661367,
      "grad_norm": 2.4267964363098145,
      "learning_rate": 2.489499809087438e-05,
      "loss": 0.6148,
      "step": 6884
    },
    {
      "epoch": 0.8756756756756757,
      "grad_norm": 2.597733974456787,
      "learning_rate": 2.4869543082601505e-05,
      "loss": 0.7653,
      "step": 6885
    },
    {
      "epoch": 0.8758028616852146,
      "grad_norm": 2.05869460105896,
      "learning_rate": 2.4844088074328624e-05,
      "loss": 0.6698,
      "step": 6886
    },
    {
      "epoch": 0.8759300476947536,
      "grad_norm": 3.326288938522339,
      "learning_rate": 2.4818633066055746e-05,
      "loss": 0.4738,
      "step": 6887
    },
    {
      "epoch": 0.8760572337042926,
      "grad_norm": 2.102869749069214,
      "learning_rate": 2.479317805778287e-05,
      "loss": 0.532,
      "step": 6888
    },
    {
      "epoch": 0.8761844197138314,
      "grad_norm": 2.1408510208129883,
      "learning_rate": 2.476772304950999e-05,
      "loss": 0.3752,
      "step": 6889
    },
    {
      "epoch": 0.8763116057233704,
      "grad_norm": 2.0767555236816406,
      "learning_rate": 2.4742268041237116e-05,
      "loss": 0.4188,
      "step": 6890
    },
    {
      "epoch": 0.8764387917329094,
      "grad_norm": 2.7271687984466553,
      "learning_rate": 2.4716813032964238e-05,
      "loss": 0.491,
      "step": 6891
    },
    {
      "epoch": 0.8765659777424484,
      "grad_norm": 2.1188130378723145,
      "learning_rate": 2.4691358024691357e-05,
      "loss": 0.4801,
      "step": 6892
    },
    {
      "epoch": 0.8766931637519872,
      "grad_norm": 2.6121907234191895,
      "learning_rate": 2.4665903016418482e-05,
      "loss": 0.6425,
      "step": 6893
    },
    {
      "epoch": 0.8768203497615262,
      "grad_norm": 1.8290793895721436,
      "learning_rate": 2.4640448008145604e-05,
      "loss": 0.3994,
      "step": 6894
    },
    {
      "epoch": 0.8769475357710652,
      "grad_norm": 2.3997952938079834,
      "learning_rate": 2.4614992999872726e-05,
      "loss": 0.9304,
      "step": 6895
    },
    {
      "epoch": 0.8770747217806041,
      "grad_norm": 2.336238384246826,
      "learning_rate": 2.458953799159985e-05,
      "loss": 0.3179,
      "step": 6896
    },
    {
      "epoch": 0.8772019077901431,
      "grad_norm": 2.946505069732666,
      "learning_rate": 2.456408298332697e-05,
      "loss": 0.3477,
      "step": 6897
    },
    {
      "epoch": 0.877329093799682,
      "grad_norm": 1.7005928754806519,
      "learning_rate": 2.4538627975054093e-05,
      "loss": 0.2873,
      "step": 6898
    },
    {
      "epoch": 0.877456279809221,
      "grad_norm": 2.0873477458953857,
      "learning_rate": 2.4513172966781215e-05,
      "loss": 0.4959,
      "step": 6899
    },
    {
      "epoch": 0.8775834658187599,
      "grad_norm": 2.7057509422302246,
      "learning_rate": 2.4487717958508337e-05,
      "loss": 0.5204,
      "step": 6900
    },
    {
      "epoch": 0.8777106518282989,
      "grad_norm": 1.6442350149154663,
      "learning_rate": 2.446226295023546e-05,
      "loss": 0.352,
      "step": 6901
    },
    {
      "epoch": 0.8778378378378379,
      "grad_norm": 2.228224754333496,
      "learning_rate": 2.4436807941962585e-05,
      "loss": 0.5011,
      "step": 6902
    },
    {
      "epoch": 0.8779650238473767,
      "grad_norm": 1.982224702835083,
      "learning_rate": 2.4411352933689703e-05,
      "loss": 0.6419,
      "step": 6903
    },
    {
      "epoch": 0.8780922098569157,
      "grad_norm": 3.0342531204223633,
      "learning_rate": 2.4385897925416825e-05,
      "loss": 0.8525,
      "step": 6904
    },
    {
      "epoch": 0.8782193958664547,
      "grad_norm": 2.4123098850250244,
      "learning_rate": 2.436044291714395e-05,
      "loss": 0.636,
      "step": 6905
    },
    {
      "epoch": 0.8783465818759937,
      "grad_norm": 1.6259939670562744,
      "learning_rate": 2.433498790887107e-05,
      "loss": 0.4843,
      "step": 6906
    },
    {
      "epoch": 0.8784737678855326,
      "grad_norm": 2.7204461097717285,
      "learning_rate": 2.4309532900598195e-05,
      "loss": 0.777,
      "step": 6907
    },
    {
      "epoch": 0.8786009538950715,
      "grad_norm": 1.8851929903030396,
      "learning_rate": 2.4284077892325317e-05,
      "loss": 0.4327,
      "step": 6908
    },
    {
      "epoch": 0.8787281399046105,
      "grad_norm": 2.109778642654419,
      "learning_rate": 2.4258622884052436e-05,
      "loss": 0.399,
      "step": 6909
    },
    {
      "epoch": 0.8788553259141495,
      "grad_norm": 2.4163856506347656,
      "learning_rate": 2.423316787577956e-05,
      "loss": 0.602,
      "step": 6910
    },
    {
      "epoch": 0.8789825119236884,
      "grad_norm": 2.383882761001587,
      "learning_rate": 2.4207712867506684e-05,
      "loss": 0.5398,
      "step": 6911
    },
    {
      "epoch": 0.8791096979332274,
      "grad_norm": 2.3700525760650635,
      "learning_rate": 2.4182257859233806e-05,
      "loss": 0.8687,
      "step": 6912
    },
    {
      "epoch": 0.8792368839427663,
      "grad_norm": 2.1853809356689453,
      "learning_rate": 2.4156802850960928e-05,
      "loss": 0.5061,
      "step": 6913
    },
    {
      "epoch": 0.8793640699523052,
      "grad_norm": 2.2464375495910645,
      "learning_rate": 2.413134784268805e-05,
      "loss": 0.6161,
      "step": 6914
    },
    {
      "epoch": 0.8794912559618442,
      "grad_norm": 3.4633255004882812,
      "learning_rate": 2.4105892834415172e-05,
      "loss": 0.8893,
      "step": 6915
    },
    {
      "epoch": 0.8796184419713832,
      "grad_norm": 2.110948324203491,
      "learning_rate": 2.4080437826142294e-05,
      "loss": 0.5704,
      "step": 6916
    },
    {
      "epoch": 0.8797456279809222,
      "grad_norm": 1.6610900163650513,
      "learning_rate": 2.4054982817869417e-05,
      "loss": 0.509,
      "step": 6917
    },
    {
      "epoch": 0.879872813990461,
      "grad_norm": 1.739219307899475,
      "learning_rate": 2.402952780959654e-05,
      "loss": 0.5042,
      "step": 6918
    },
    {
      "epoch": 0.88,
      "grad_norm": 2.3411362171173096,
      "learning_rate": 2.4004072801323664e-05,
      "loss": 0.4129,
      "step": 6919
    },
    {
      "epoch": 0.880127186009539,
      "grad_norm": 2.5715410709381104,
      "learning_rate": 2.3978617793050783e-05,
      "loss": 0.3725,
      "step": 6920
    },
    {
      "epoch": 0.8802543720190779,
      "grad_norm": 1.711740255355835,
      "learning_rate": 2.3953162784777905e-05,
      "loss": 0.4756,
      "step": 6921
    },
    {
      "epoch": 0.8803815580286168,
      "grad_norm": 2.6447975635528564,
      "learning_rate": 2.392770777650503e-05,
      "loss": 0.6588,
      "step": 6922
    },
    {
      "epoch": 0.8805087440381558,
      "grad_norm": 3.05790376663208,
      "learning_rate": 2.390225276823215e-05,
      "loss": 0.6444,
      "step": 6923
    },
    {
      "epoch": 0.8806359300476948,
      "grad_norm": 2.5448904037475586,
      "learning_rate": 2.3876797759959275e-05,
      "loss": 0.5672,
      "step": 6924
    },
    {
      "epoch": 0.8807631160572337,
      "grad_norm": 2.1673386096954346,
      "learning_rate": 2.3851342751686394e-05,
      "loss": 0.4045,
      "step": 6925
    },
    {
      "epoch": 0.8808903020667727,
      "grad_norm": 1.742659330368042,
      "learning_rate": 2.3825887743413516e-05,
      "loss": 0.4454,
      "step": 6926
    },
    {
      "epoch": 0.8810174880763116,
      "grad_norm": 2.4710216522216797,
      "learning_rate": 2.380043273514064e-05,
      "loss": 0.5691,
      "step": 6927
    },
    {
      "epoch": 0.8811446740858505,
      "grad_norm": 1.7370808124542236,
      "learning_rate": 2.377497772686776e-05,
      "loss": 0.2869,
      "step": 6928
    },
    {
      "epoch": 0.8812718600953895,
      "grad_norm": 1.2972004413604736,
      "learning_rate": 2.3749522718594885e-05,
      "loss": 0.4066,
      "step": 6929
    },
    {
      "epoch": 0.8813990461049285,
      "grad_norm": 1.9387953281402588,
      "learning_rate": 2.3724067710322008e-05,
      "loss": 0.3943,
      "step": 6930
    },
    {
      "epoch": 0.8815262321144675,
      "grad_norm": 1.7136263847351074,
      "learning_rate": 2.369861270204913e-05,
      "loss": 0.3766,
      "step": 6931
    },
    {
      "epoch": 0.8816534181240063,
      "grad_norm": 2.379835605621338,
      "learning_rate": 2.3673157693776252e-05,
      "loss": 0.4281,
      "step": 6932
    },
    {
      "epoch": 0.8817806041335453,
      "grad_norm": 2.325122833251953,
      "learning_rate": 2.3647702685503374e-05,
      "loss": 0.6027,
      "step": 6933
    },
    {
      "epoch": 0.8819077901430843,
      "grad_norm": 1.9412500858306885,
      "learning_rate": 2.3622247677230496e-05,
      "loss": 0.5745,
      "step": 6934
    },
    {
      "epoch": 0.8820349761526232,
      "grad_norm": 1.6293920278549194,
      "learning_rate": 2.3596792668957618e-05,
      "loss": 0.4184,
      "step": 6935
    },
    {
      "epoch": 0.8821621621621621,
      "grad_norm": 2.9021878242492676,
      "learning_rate": 2.357133766068474e-05,
      "loss": 0.8033,
      "step": 6936
    },
    {
      "epoch": 0.8822893481717011,
      "grad_norm": 1.8975268602371216,
      "learning_rate": 2.3545882652411862e-05,
      "loss": 0.26,
      "step": 6937
    },
    {
      "epoch": 0.8824165341812401,
      "grad_norm": 2.433234453201294,
      "learning_rate": 2.3520427644138985e-05,
      "loss": 0.6495,
      "step": 6938
    },
    {
      "epoch": 0.882543720190779,
      "grad_norm": 3.0851521492004395,
      "learning_rate": 2.3494972635866107e-05,
      "loss": 0.3874,
      "step": 6939
    },
    {
      "epoch": 0.882670906200318,
      "grad_norm": 3.0982940196990967,
      "learning_rate": 2.346951762759323e-05,
      "loss": 0.4971,
      "step": 6940
    },
    {
      "epoch": 0.8827980922098569,
      "grad_norm": 3.0524377822875977,
      "learning_rate": 2.3444062619320354e-05,
      "loss": 0.6207,
      "step": 6941
    },
    {
      "epoch": 0.8829252782193958,
      "grad_norm": 1.816316843032837,
      "learning_rate": 2.3418607611047473e-05,
      "loss": 0.2965,
      "step": 6942
    },
    {
      "epoch": 0.8830524642289348,
      "grad_norm": 2.3260226249694824,
      "learning_rate": 2.3393152602774595e-05,
      "loss": 0.5536,
      "step": 6943
    },
    {
      "epoch": 0.8831796502384738,
      "grad_norm": 2.1916327476501465,
      "learning_rate": 2.336769759450172e-05,
      "loss": 0.4728,
      "step": 6944
    },
    {
      "epoch": 0.8833068362480128,
      "grad_norm": 2.5191662311553955,
      "learning_rate": 2.334224258622884e-05,
      "loss": 0.7002,
      "step": 6945
    },
    {
      "epoch": 0.8834340222575516,
      "grad_norm": 2.2037100791931152,
      "learning_rate": 2.3316787577955965e-05,
      "loss": 0.5718,
      "step": 6946
    },
    {
      "epoch": 0.8835612082670906,
      "grad_norm": 2.0996158123016357,
      "learning_rate": 2.3291332569683087e-05,
      "loss": 0.4386,
      "step": 6947
    },
    {
      "epoch": 0.8836883942766296,
      "grad_norm": 2.658571720123291,
      "learning_rate": 2.326587756141021e-05,
      "loss": 0.5819,
      "step": 6948
    },
    {
      "epoch": 0.8838155802861685,
      "grad_norm": 2.776059865951538,
      "learning_rate": 2.324042255313733e-05,
      "loss": 0.816,
      "step": 6949
    },
    {
      "epoch": 0.8839427662957074,
      "grad_norm": 1.5287402868270874,
      "learning_rate": 2.3214967544864454e-05,
      "loss": 0.3472,
      "step": 6950
    },
    {
      "epoch": 0.8840699523052464,
      "grad_norm": 2.102593421936035,
      "learning_rate": 2.3189512536591576e-05,
      "loss": 0.467,
      "step": 6951
    },
    {
      "epoch": 0.8841971383147854,
      "grad_norm": 2.7640342712402344,
      "learning_rate": 2.3164057528318698e-05,
      "loss": 0.6078,
      "step": 6952
    },
    {
      "epoch": 0.8843243243243243,
      "grad_norm": 2.057199239730835,
      "learning_rate": 2.313860252004582e-05,
      "loss": 0.5542,
      "step": 6953
    },
    {
      "epoch": 0.8844515103338633,
      "grad_norm": 2.556246757507324,
      "learning_rate": 2.3113147511772942e-05,
      "loss": 0.4829,
      "step": 6954
    },
    {
      "epoch": 0.8845786963434022,
      "grad_norm": 2.582094669342041,
      "learning_rate": 2.3087692503500064e-05,
      "loss": 0.3735,
      "step": 6955
    },
    {
      "epoch": 0.8847058823529412,
      "grad_norm": 2.0711538791656494,
      "learning_rate": 2.3062237495227186e-05,
      "loss": 0.5455,
      "step": 6956
    },
    {
      "epoch": 0.8848330683624801,
      "grad_norm": 2.183624267578125,
      "learning_rate": 2.303678248695431e-05,
      "loss": 0.423,
      "step": 6957
    },
    {
      "epoch": 0.8849602543720191,
      "grad_norm": 2.74111270904541,
      "learning_rate": 2.3011327478681434e-05,
      "loss": 0.4322,
      "step": 6958
    },
    {
      "epoch": 0.8850874403815581,
      "grad_norm": 1.6953935623168945,
      "learning_rate": 2.2985872470408553e-05,
      "loss": 0.4027,
      "step": 6959
    },
    {
      "epoch": 0.8852146263910969,
      "grad_norm": 2.268921375274658,
      "learning_rate": 2.2960417462135678e-05,
      "loss": 0.6499,
      "step": 6960
    },
    {
      "epoch": 0.8853418124006359,
      "grad_norm": 2.3265247344970703,
      "learning_rate": 2.29349624538628e-05,
      "loss": 0.5382,
      "step": 6961
    },
    {
      "epoch": 0.8854689984101749,
      "grad_norm": 3.0913052558898926,
      "learning_rate": 2.290950744558992e-05,
      "loss": 0.7396,
      "step": 6962
    },
    {
      "epoch": 0.8855961844197139,
      "grad_norm": 2.5174221992492676,
      "learning_rate": 2.2884052437317045e-05,
      "loss": 0.4019,
      "step": 6963
    },
    {
      "epoch": 0.8857233704292528,
      "grad_norm": 3.638310432434082,
      "learning_rate": 2.2858597429044167e-05,
      "loss": 1.0271,
      "step": 6964
    },
    {
      "epoch": 0.8858505564387917,
      "grad_norm": 2.058913230895996,
      "learning_rate": 2.283314242077129e-05,
      "loss": 0.3411,
      "step": 6965
    },
    {
      "epoch": 0.8859777424483307,
      "grad_norm": 1.855140209197998,
      "learning_rate": 2.280768741249841e-05,
      "loss": 0.4019,
      "step": 6966
    },
    {
      "epoch": 0.8861049284578696,
      "grad_norm": 2.256655693054199,
      "learning_rate": 2.2782232404225533e-05,
      "loss": 0.6217,
      "step": 6967
    },
    {
      "epoch": 0.8862321144674086,
      "grad_norm": 2.299055576324463,
      "learning_rate": 2.2756777395952655e-05,
      "loss": 0.5779,
      "step": 6968
    },
    {
      "epoch": 0.8863593004769476,
      "grad_norm": 2.1822073459625244,
      "learning_rate": 2.2731322387679777e-05,
      "loss": 0.4492,
      "step": 6969
    },
    {
      "epoch": 0.8864864864864865,
      "grad_norm": 2.3753442764282227,
      "learning_rate": 2.27058673794069e-05,
      "loss": 0.5012,
      "step": 6970
    },
    {
      "epoch": 0.8866136724960254,
      "grad_norm": 3.2172083854675293,
      "learning_rate": 2.268041237113402e-05,
      "loss": 0.5407,
      "step": 6971
    },
    {
      "epoch": 0.8867408585055644,
      "grad_norm": 2.1232428550720215,
      "learning_rate": 2.2654957362861144e-05,
      "loss": 0.5373,
      "step": 6972
    },
    {
      "epoch": 0.8868680445151034,
      "grad_norm": 2.2837345600128174,
      "learning_rate": 2.2629502354588266e-05,
      "loss": 0.4764,
      "step": 6973
    },
    {
      "epoch": 0.8869952305246422,
      "grad_norm": 2.4384148120880127,
      "learning_rate": 2.2604047346315388e-05,
      "loss": 0.4978,
      "step": 6974
    },
    {
      "epoch": 0.8871224165341812,
      "grad_norm": 3.0235238075256348,
      "learning_rate": 2.2578592338042513e-05,
      "loss": 0.5135,
      "step": 6975
    },
    {
      "epoch": 0.8872496025437202,
      "grad_norm": 2.305198907852173,
      "learning_rate": 2.2553137329769632e-05,
      "loss": 0.5113,
      "step": 6976
    },
    {
      "epoch": 0.8873767885532592,
      "grad_norm": 2.1767919063568115,
      "learning_rate": 2.2527682321496758e-05,
      "loss": 0.3715,
      "step": 6977
    },
    {
      "epoch": 0.8875039745627981,
      "grad_norm": 2.6449875831604004,
      "learning_rate": 2.2502227313223876e-05,
      "loss": 0.3599,
      "step": 6978
    },
    {
      "epoch": 0.887631160572337,
      "grad_norm": 2.455949306488037,
      "learning_rate": 2.2476772304951e-05,
      "loss": 0.5275,
      "step": 6979
    },
    {
      "epoch": 0.887758346581876,
      "grad_norm": 2.2403087615966797,
      "learning_rate": 2.2451317296678124e-05,
      "loss": 0.644,
      "step": 6980
    },
    {
      "epoch": 0.8878855325914149,
      "grad_norm": 2.414336919784546,
      "learning_rate": 2.2425862288405243e-05,
      "loss": 0.7034,
      "step": 6981
    },
    {
      "epoch": 0.8880127186009539,
      "grad_norm": 2.5572140216827393,
      "learning_rate": 2.240040728013237e-05,
      "loss": 0.511,
      "step": 6982
    },
    {
      "epoch": 0.8881399046104929,
      "grad_norm": 2.036562442779541,
      "learning_rate": 2.237495227185949e-05,
      "loss": 0.2952,
      "step": 6983
    },
    {
      "epoch": 0.8882670906200318,
      "grad_norm": 3.707073211669922,
      "learning_rate": 2.234949726358661e-05,
      "loss": 1.0797,
      "step": 6984
    },
    {
      "epoch": 0.8883942766295707,
      "grad_norm": 2.403059244155884,
      "learning_rate": 2.2324042255313735e-05,
      "loss": 0.6259,
      "step": 6985
    },
    {
      "epoch": 0.8885214626391097,
      "grad_norm": 2.9626448154449463,
      "learning_rate": 2.2298587247040857e-05,
      "loss": 0.6067,
      "step": 6986
    },
    {
      "epoch": 0.8886486486486487,
      "grad_norm": 1.6524693965911865,
      "learning_rate": 2.227313223876798e-05,
      "loss": 0.5576,
      "step": 6987
    },
    {
      "epoch": 0.8887758346581875,
      "grad_norm": 1.7997150421142578,
      "learning_rate": 2.22476772304951e-05,
      "loss": 0.4884,
      "step": 6988
    },
    {
      "epoch": 0.8889030206677265,
      "grad_norm": 2.8529975414276123,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.6815,
      "step": 6989
    },
    {
      "epoch": 0.8890302066772655,
      "grad_norm": 2.3592336177825928,
      "learning_rate": 2.2196767213949345e-05,
      "loss": 0.5041,
      "step": 6990
    },
    {
      "epoch": 0.8891573926868045,
      "grad_norm": 1.529309868812561,
      "learning_rate": 2.2171312205676468e-05,
      "loss": 0.4711,
      "step": 6991
    },
    {
      "epoch": 0.8892845786963434,
      "grad_norm": 2.772203207015991,
      "learning_rate": 2.214585719740359e-05,
      "loss": 0.8362,
      "step": 6992
    },
    {
      "epoch": 0.8894117647058823,
      "grad_norm": 2.216395854949951,
      "learning_rate": 2.2120402189130712e-05,
      "loss": 0.4627,
      "step": 6993
    },
    {
      "epoch": 0.8895389507154213,
      "grad_norm": 2.600883722305298,
      "learning_rate": 2.2094947180857837e-05,
      "loss": 0.5498,
      "step": 6994
    },
    {
      "epoch": 0.8896661367249602,
      "grad_norm": 1.6349306106567383,
      "learning_rate": 2.2069492172584956e-05,
      "loss": 0.3355,
      "step": 6995
    },
    {
      "epoch": 0.8897933227344992,
      "grad_norm": 2.5725648403167725,
      "learning_rate": 2.2044037164312078e-05,
      "loss": 0.56,
      "step": 6996
    },
    {
      "epoch": 0.8899205087440382,
      "grad_norm": 2.5325446128845215,
      "learning_rate": 2.2018582156039204e-05,
      "loss": 0.3113,
      "step": 6997
    },
    {
      "epoch": 0.8900476947535771,
      "grad_norm": 2.8749654293060303,
      "learning_rate": 2.1993127147766322e-05,
      "loss": 0.7353,
      "step": 6998
    },
    {
      "epoch": 0.890174880763116,
      "grad_norm": 2.2418150901794434,
      "learning_rate": 2.1967672139493448e-05,
      "loss": 0.3587,
      "step": 6999
    },
    {
      "epoch": 0.890302066772655,
      "grad_norm": 2.5363352298736572,
      "learning_rate": 2.194221713122057e-05,
      "loss": 0.5185,
      "step": 7000
    },
    {
      "epoch": 0.890429252782194,
      "grad_norm": 3.0071260929107666,
      "learning_rate": 2.191676212294769e-05,
      "loss": 0.4242,
      "step": 7001
    },
    {
      "epoch": 0.890556438791733,
      "grad_norm": 2.6171600818634033,
      "learning_rate": 2.1891307114674814e-05,
      "loss": 0.3475,
      "step": 7002
    },
    {
      "epoch": 0.8906836248012718,
      "grad_norm": 2.567575454711914,
      "learning_rate": 2.1865852106401936e-05,
      "loss": 0.5263,
      "step": 7003
    },
    {
      "epoch": 0.8908108108108108,
      "grad_norm": 1.8827557563781738,
      "learning_rate": 2.184039709812906e-05,
      "loss": 0.3696,
      "step": 7004
    },
    {
      "epoch": 0.8909379968203498,
      "grad_norm": 1.7824769020080566,
      "learning_rate": 2.181494208985618e-05,
      "loss": 0.2547,
      "step": 7005
    },
    {
      "epoch": 0.8910651828298887,
      "grad_norm": 2.8119089603424072,
      "learning_rate": 2.1789487081583303e-05,
      "loss": 0.3895,
      "step": 7006
    },
    {
      "epoch": 0.8911923688394277,
      "grad_norm": 2.337522268295288,
      "learning_rate": 2.1764032073310425e-05,
      "loss": 0.4944,
      "step": 7007
    },
    {
      "epoch": 0.8913195548489666,
      "grad_norm": 2.963693380355835,
      "learning_rate": 2.1738577065037547e-05,
      "loss": 0.9549,
      "step": 7008
    },
    {
      "epoch": 0.8914467408585056,
      "grad_norm": 2.89851975440979,
      "learning_rate": 2.171312205676467e-05,
      "loss": 0.5544,
      "step": 7009
    },
    {
      "epoch": 0.8915739268680445,
      "grad_norm": 2.573577404022217,
      "learning_rate": 2.168766704849179e-05,
      "loss": 0.6403,
      "step": 7010
    },
    {
      "epoch": 0.8917011128775835,
      "grad_norm": 1.98166024684906,
      "learning_rate": 2.1662212040218917e-05,
      "loss": 0.4809,
      "step": 7011
    },
    {
      "epoch": 0.8918282988871225,
      "grad_norm": 2.7163257598876953,
      "learning_rate": 2.1636757031946036e-05,
      "loss": 0.6922,
      "step": 7012
    },
    {
      "epoch": 0.8919554848966613,
      "grad_norm": 2.0652098655700684,
      "learning_rate": 2.1611302023673158e-05,
      "loss": 0.5249,
      "step": 7013
    },
    {
      "epoch": 0.8920826709062003,
      "grad_norm": 2.837858200073242,
      "learning_rate": 2.1585847015400283e-05,
      "loss": 0.6341,
      "step": 7014
    },
    {
      "epoch": 0.8922098569157393,
      "grad_norm": 3.376988649368286,
      "learning_rate": 2.1560392007127402e-05,
      "loss": 0.6046,
      "step": 7015
    },
    {
      "epoch": 0.8923370429252783,
      "grad_norm": 2.686293125152588,
      "learning_rate": 2.1534936998854528e-05,
      "loss": 0.5267,
      "step": 7016
    },
    {
      "epoch": 0.8924642289348171,
      "grad_norm": 2.280256748199463,
      "learning_rate": 2.150948199058165e-05,
      "loss": 0.3696,
      "step": 7017
    },
    {
      "epoch": 0.8925914149443561,
      "grad_norm": 3.1293835639953613,
      "learning_rate": 2.148402698230877e-05,
      "loss": 0.6682,
      "step": 7018
    },
    {
      "epoch": 0.8927186009538951,
      "grad_norm": 2.010571241378784,
      "learning_rate": 2.1458571974035894e-05,
      "loss": 0.5061,
      "step": 7019
    },
    {
      "epoch": 0.892845786963434,
      "grad_norm": 2.42541241645813,
      "learning_rate": 2.1433116965763013e-05,
      "loss": 0.6144,
      "step": 7020
    },
    {
      "epoch": 0.892972972972973,
      "grad_norm": 2.8828771114349365,
      "learning_rate": 2.1407661957490138e-05,
      "loss": 0.4938,
      "step": 7021
    },
    {
      "epoch": 0.8931001589825119,
      "grad_norm": 2.829270124435425,
      "learning_rate": 2.138220694921726e-05,
      "loss": 0.6341,
      "step": 7022
    },
    {
      "epoch": 0.8932273449920509,
      "grad_norm": 2.5267364978790283,
      "learning_rate": 2.135675194094438e-05,
      "loss": 0.5655,
      "step": 7023
    },
    {
      "epoch": 0.8933545310015898,
      "grad_norm": 3.002559185028076,
      "learning_rate": 2.1331296932671505e-05,
      "loss": 0.5583,
      "step": 7024
    },
    {
      "epoch": 0.8934817170111288,
      "grad_norm": 2.189363718032837,
      "learning_rate": 2.1305841924398627e-05,
      "loss": 0.679,
      "step": 7025
    },
    {
      "epoch": 0.8936089030206678,
      "grad_norm": 2.345822811126709,
      "learning_rate": 2.128038691612575e-05,
      "loss": 0.3366,
      "step": 7026
    },
    {
      "epoch": 0.8937360890302066,
      "grad_norm": 2.8123795986175537,
      "learning_rate": 2.125493190785287e-05,
      "loss": 0.6501,
      "step": 7027
    },
    {
      "epoch": 0.8938632750397456,
      "grad_norm": 1.9256263971328735,
      "learning_rate": 2.1229476899579996e-05,
      "loss": 0.3593,
      "step": 7028
    },
    {
      "epoch": 0.8939904610492846,
      "grad_norm": 2.9243974685668945,
      "learning_rate": 2.1204021891307115e-05,
      "loss": 0.6158,
      "step": 7029
    },
    {
      "epoch": 0.8941176470588236,
      "grad_norm": 2.9590396881103516,
      "learning_rate": 2.1178566883034237e-05,
      "loss": 0.56,
      "step": 7030
    },
    {
      "epoch": 0.8942448330683624,
      "grad_norm": 2.916600227355957,
      "learning_rate": 2.115311187476136e-05,
      "loss": 0.6703,
      "step": 7031
    },
    {
      "epoch": 0.8943720190779014,
      "grad_norm": 2.115415334701538,
      "learning_rate": 2.112765686648848e-05,
      "loss": 0.4646,
      "step": 7032
    },
    {
      "epoch": 0.8944992050874404,
      "grad_norm": 2.7928688526153564,
      "learning_rate": 2.1102201858215607e-05,
      "loss": 0.5644,
      "step": 7033
    },
    {
      "epoch": 0.8946263910969793,
      "grad_norm": 2.566850423812866,
      "learning_rate": 2.1076746849942726e-05,
      "loss": 0.5481,
      "step": 7034
    },
    {
      "epoch": 0.8947535771065183,
      "grad_norm": 2.4446403980255127,
      "learning_rate": 2.1051291841669848e-05,
      "loss": 0.2956,
      "step": 7035
    },
    {
      "epoch": 0.8948807631160572,
      "grad_norm": 3.3231241703033447,
      "learning_rate": 2.1025836833396973e-05,
      "loss": 0.4553,
      "step": 7036
    },
    {
      "epoch": 0.8950079491255962,
      "grad_norm": 1.8900550603866577,
      "learning_rate": 2.1000381825124092e-05,
      "loss": 0.4939,
      "step": 7037
    },
    {
      "epoch": 0.8951351351351351,
      "grad_norm": 2.672609567642212,
      "learning_rate": 2.0974926816851218e-05,
      "loss": 0.552,
      "step": 7038
    },
    {
      "epoch": 0.8952623211446741,
      "grad_norm": 2.8254036903381348,
      "learning_rate": 2.094947180857834e-05,
      "loss": 0.601,
      "step": 7039
    },
    {
      "epoch": 0.8953895071542131,
      "grad_norm": 2.9306793212890625,
      "learning_rate": 2.092401680030546e-05,
      "loss": 0.8176,
      "step": 7040
    },
    {
      "epoch": 0.895516693163752,
      "grad_norm": 2.1577088832855225,
      "learning_rate": 2.0898561792032584e-05,
      "loss": 0.4552,
      "step": 7041
    },
    {
      "epoch": 0.8956438791732909,
      "grad_norm": 2.706481695175171,
      "learning_rate": 2.0873106783759706e-05,
      "loss": 0.6191,
      "step": 7042
    },
    {
      "epoch": 0.8957710651828299,
      "grad_norm": 2.221299409866333,
      "learning_rate": 2.084765177548683e-05,
      "loss": 0.5764,
      "step": 7043
    },
    {
      "epoch": 0.8958982511923689,
      "grad_norm": 2.049860954284668,
      "learning_rate": 2.082219676721395e-05,
      "loss": 0.7263,
      "step": 7044
    },
    {
      "epoch": 0.8960254372019077,
      "grad_norm": 1.9796935319900513,
      "learning_rate": 2.0796741758941073e-05,
      "loss": 0.5577,
      "step": 7045
    },
    {
      "epoch": 0.8961526232114467,
      "grad_norm": 2.1939592361450195,
      "learning_rate": 2.0771286750668195e-05,
      "loss": 0.4557,
      "step": 7046
    },
    {
      "epoch": 0.8962798092209857,
      "grad_norm": 2.4063754081726074,
      "learning_rate": 2.0745831742395317e-05,
      "loss": 0.3362,
      "step": 7047
    },
    {
      "epoch": 0.8964069952305247,
      "grad_norm": 1.6993986368179321,
      "learning_rate": 2.072037673412244e-05,
      "loss": 0.5492,
      "step": 7048
    },
    {
      "epoch": 0.8965341812400636,
      "grad_norm": 2.48918080329895,
      "learning_rate": 2.069492172584956e-05,
      "loss": 0.5285,
      "step": 7049
    },
    {
      "epoch": 0.8966613672496025,
      "grad_norm": 1.763671636581421,
      "learning_rate": 2.0669466717576687e-05,
      "loss": 0.3463,
      "step": 7050
    },
    {
      "epoch": 0.8967885532591415,
      "grad_norm": 3.1844263076782227,
      "learning_rate": 2.0644011709303805e-05,
      "loss": 0.5583,
      "step": 7051
    },
    {
      "epoch": 0.8969157392686804,
      "grad_norm": 2.221130132675171,
      "learning_rate": 2.0618556701030927e-05,
      "loss": 0.8442,
      "step": 7052
    },
    {
      "epoch": 0.8970429252782194,
      "grad_norm": 2.2955639362335205,
      "learning_rate": 2.0593101692758053e-05,
      "loss": 0.4493,
      "step": 7053
    },
    {
      "epoch": 0.8971701112877584,
      "grad_norm": 1.9903620481491089,
      "learning_rate": 2.0567646684485172e-05,
      "loss": 0.4203,
      "step": 7054
    },
    {
      "epoch": 0.8972972972972973,
      "grad_norm": 2.5970656871795654,
      "learning_rate": 2.0542191676212297e-05,
      "loss": 0.569,
      "step": 7055
    },
    {
      "epoch": 0.8974244833068362,
      "grad_norm": 2.030576705932617,
      "learning_rate": 2.051673666793942e-05,
      "loss": 0.4401,
      "step": 7056
    },
    {
      "epoch": 0.8975516693163752,
      "grad_norm": 2.4291927814483643,
      "learning_rate": 2.0491281659666538e-05,
      "loss": 0.4579,
      "step": 7057
    },
    {
      "epoch": 0.8976788553259142,
      "grad_norm": 2.9650402069091797,
      "learning_rate": 2.0465826651393664e-05,
      "loss": 0.6324,
      "step": 7058
    },
    {
      "epoch": 0.897806041335453,
      "grad_norm": 1.757976770401001,
      "learning_rate": 2.0440371643120786e-05,
      "loss": 0.3931,
      "step": 7059
    },
    {
      "epoch": 0.897933227344992,
      "grad_norm": 3.957883834838867,
      "learning_rate": 2.0414916634847908e-05,
      "loss": 0.3755,
      "step": 7060
    },
    {
      "epoch": 0.898060413354531,
      "grad_norm": 2.335083246231079,
      "learning_rate": 2.038946162657503e-05,
      "loss": 0.4818,
      "step": 7061
    },
    {
      "epoch": 0.89818759936407,
      "grad_norm": 1.960676670074463,
      "learning_rate": 2.0364006618302152e-05,
      "loss": 0.4788,
      "step": 7062
    },
    {
      "epoch": 0.8983147853736089,
      "grad_norm": 2.695542573928833,
      "learning_rate": 2.0338551610029274e-05,
      "loss": 0.7112,
      "step": 7063
    },
    {
      "epoch": 0.8984419713831479,
      "grad_norm": 2.5847902297973633,
      "learning_rate": 2.0313096601756396e-05,
      "loss": 0.6309,
      "step": 7064
    },
    {
      "epoch": 0.8985691573926868,
      "grad_norm": 2.710783004760742,
      "learning_rate": 2.028764159348352e-05,
      "loss": 0.4109,
      "step": 7065
    },
    {
      "epoch": 0.8986963434022257,
      "grad_norm": 1.9062864780426025,
      "learning_rate": 2.026218658521064e-05,
      "loss": 0.4334,
      "step": 7066
    },
    {
      "epoch": 0.8988235294117647,
      "grad_norm": 3.161700963973999,
      "learning_rate": 2.0236731576937766e-05,
      "loss": 0.6184,
      "step": 7067
    },
    {
      "epoch": 0.8989507154213037,
      "grad_norm": 2.443141460418701,
      "learning_rate": 2.0211276568664885e-05,
      "loss": 0.5431,
      "step": 7068
    },
    {
      "epoch": 0.8990779014308427,
      "grad_norm": 2.3906311988830566,
      "learning_rate": 2.0185821560392007e-05,
      "loss": 0.5172,
      "step": 7069
    },
    {
      "epoch": 0.8992050874403815,
      "grad_norm": 2.176251173019409,
      "learning_rate": 2.0160366552119133e-05,
      "loss": 0.5292,
      "step": 7070
    },
    {
      "epoch": 0.8993322734499205,
      "grad_norm": 1.8941290378570557,
      "learning_rate": 2.013491154384625e-05,
      "loss": 0.4208,
      "step": 7071
    },
    {
      "epoch": 0.8994594594594595,
      "grad_norm": 2.5917301177978516,
      "learning_rate": 2.0109456535573377e-05,
      "loss": 0.6952,
      "step": 7072
    },
    {
      "epoch": 0.8995866454689984,
      "grad_norm": 2.282534599304199,
      "learning_rate": 2.0084001527300496e-05,
      "loss": 0.5828,
      "step": 7073
    },
    {
      "epoch": 0.8997138314785373,
      "grad_norm": 1.7117654085159302,
      "learning_rate": 2.0058546519027618e-05,
      "loss": 0.4943,
      "step": 7074
    },
    {
      "epoch": 0.8998410174880763,
      "grad_norm": 1.7588030099868774,
      "learning_rate": 2.0033091510754743e-05,
      "loss": 0.4371,
      "step": 7075
    },
    {
      "epoch": 0.8999682034976153,
      "grad_norm": 1.9537689685821533,
      "learning_rate": 2.0007636502481862e-05,
      "loss": 0.5632,
      "step": 7076
    },
    {
      "epoch": 0.9000953895071542,
      "grad_norm": 2.1617486476898193,
      "learning_rate": 1.9982181494208987e-05,
      "loss": 0.4446,
      "step": 7077
    },
    {
      "epoch": 0.9002225755166932,
      "grad_norm": 1.9802507162094116,
      "learning_rate": 1.995672648593611e-05,
      "loss": 0.5982,
      "step": 7078
    },
    {
      "epoch": 0.9003497615262321,
      "grad_norm": 2.286381244659424,
      "learning_rate": 1.9931271477663232e-05,
      "loss": 0.5005,
      "step": 7079
    },
    {
      "epoch": 0.900476947535771,
      "grad_norm": 1.9313452243804932,
      "learning_rate": 1.9905816469390354e-05,
      "loss": 0.4415,
      "step": 7080
    },
    {
      "epoch": 0.90060413354531,
      "grad_norm": 1.7469855546951294,
      "learning_rate": 1.9880361461117476e-05,
      "loss": 0.4027,
      "step": 7081
    },
    {
      "epoch": 0.900731319554849,
      "grad_norm": 1.712565302848816,
      "learning_rate": 1.9854906452844598e-05,
      "loss": 0.3614,
      "step": 7082
    },
    {
      "epoch": 0.900858505564388,
      "grad_norm": 1.7517184019088745,
      "learning_rate": 1.982945144457172e-05,
      "loss": 0.4778,
      "step": 7083
    },
    {
      "epoch": 0.9009856915739268,
      "grad_norm": 2.0752017498016357,
      "learning_rate": 1.9803996436298842e-05,
      "loss": 0.5379,
      "step": 7084
    },
    {
      "epoch": 0.9011128775834658,
      "grad_norm": 1.718880534172058,
      "learning_rate": 1.9778541428025964e-05,
      "loss": 0.4639,
      "step": 7085
    },
    {
      "epoch": 0.9012400635930048,
      "grad_norm": 1.7139968872070312,
      "learning_rate": 1.9753086419753087e-05,
      "loss": 0.492,
      "step": 7086
    },
    {
      "epoch": 0.9013672496025438,
      "grad_norm": 1.8258514404296875,
      "learning_rate": 1.972763141148021e-05,
      "loss": 0.459,
      "step": 7087
    },
    {
      "epoch": 0.9014944356120826,
      "grad_norm": 2.1535253524780273,
      "learning_rate": 1.970217640320733e-05,
      "loss": 0.4963,
      "step": 7088
    },
    {
      "epoch": 0.9016216216216216,
      "grad_norm": 2.414241075515747,
      "learning_rate": 1.9676721394934456e-05,
      "loss": 0.6137,
      "step": 7089
    },
    {
      "epoch": 0.9017488076311606,
      "grad_norm": 2.756639242172241,
      "learning_rate": 1.9651266386661575e-05,
      "loss": 0.6517,
      "step": 7090
    },
    {
      "epoch": 0.9018759936406995,
      "grad_norm": 2.227304458618164,
      "learning_rate": 1.9625811378388697e-05,
      "loss": 0.6572,
      "step": 7091
    },
    {
      "epoch": 0.9020031796502385,
      "grad_norm": 1.8881642818450928,
      "learning_rate": 1.9600356370115823e-05,
      "loss": 0.5693,
      "step": 7092
    },
    {
      "epoch": 0.9021303656597774,
      "grad_norm": 1.8865405321121216,
      "learning_rate": 1.957490136184294e-05,
      "loss": 0.4412,
      "step": 7093
    },
    {
      "epoch": 0.9022575516693164,
      "grad_norm": 2.6515464782714844,
      "learning_rate": 1.9549446353570067e-05,
      "loss": 0.5281,
      "step": 7094
    },
    {
      "epoch": 0.9023847376788553,
      "grad_norm": 3.2950892448425293,
      "learning_rate": 1.952399134529719e-05,
      "loss": 0.7275,
      "step": 7095
    },
    {
      "epoch": 0.9025119236883943,
      "grad_norm": 2.4230973720550537,
      "learning_rate": 1.949853633702431e-05,
      "loss": 0.5757,
      "step": 7096
    },
    {
      "epoch": 0.9026391096979333,
      "grad_norm": 2.5447580814361572,
      "learning_rate": 1.9473081328751433e-05,
      "loss": 0.6325,
      "step": 7097
    },
    {
      "epoch": 0.9027662957074721,
      "grad_norm": 2.679264783859253,
      "learning_rate": 1.9447626320478556e-05,
      "loss": 0.5171,
      "step": 7098
    },
    {
      "epoch": 0.9028934817170111,
      "grad_norm": 2.902247905731201,
      "learning_rate": 1.9422171312205678e-05,
      "loss": 0.5369,
      "step": 7099
    },
    {
      "epoch": 0.9030206677265501,
      "grad_norm": 1.972569465637207,
      "learning_rate": 1.93967163039328e-05,
      "loss": 0.328,
      "step": 7100
    },
    {
      "epoch": 0.9031478537360891,
      "grad_norm": 2.185929536819458,
      "learning_rate": 1.9371261295659922e-05,
      "loss": 0.5925,
      "step": 7101
    },
    {
      "epoch": 0.903275039745628,
      "grad_norm": 2.2077648639678955,
      "learning_rate": 1.9345806287387044e-05,
      "loss": 0.7279,
      "step": 7102
    },
    {
      "epoch": 0.9034022257551669,
      "grad_norm": 2.2338714599609375,
      "learning_rate": 1.9320351279114166e-05,
      "loss": 0.441,
      "step": 7103
    },
    {
      "epoch": 0.9035294117647059,
      "grad_norm": 1.5841559171676636,
      "learning_rate": 1.9294896270841288e-05,
      "loss": 0.4239,
      "step": 7104
    },
    {
      "epoch": 0.9036565977742448,
      "grad_norm": 3.353707790374756,
      "learning_rate": 1.926944126256841e-05,
      "loss": 0.782,
      "step": 7105
    },
    {
      "epoch": 0.9037837837837838,
      "grad_norm": 2.194936752319336,
      "learning_rate": 1.9243986254295536e-05,
      "loss": 0.6686,
      "step": 7106
    },
    {
      "epoch": 0.9039109697933227,
      "grad_norm": 1.6969808340072632,
      "learning_rate": 1.9218531246022655e-05,
      "loss": 0.4254,
      "step": 7107
    },
    {
      "epoch": 0.9040381558028617,
      "grad_norm": 2.381439685821533,
      "learning_rate": 1.9193076237749777e-05,
      "loss": 0.62,
      "step": 7108
    },
    {
      "epoch": 0.9041653418124006,
      "grad_norm": 2.3301727771759033,
      "learning_rate": 1.9167621229476902e-05,
      "loss": 0.694,
      "step": 7109
    },
    {
      "epoch": 0.9042925278219396,
      "grad_norm": 2.00833797454834,
      "learning_rate": 1.914216622120402e-05,
      "loss": 0.5463,
      "step": 7110
    },
    {
      "epoch": 0.9044197138314786,
      "grad_norm": 2.3162853717803955,
      "learning_rate": 1.9116711212931147e-05,
      "loss": 0.6393,
      "step": 7111
    },
    {
      "epoch": 0.9045468998410174,
      "grad_norm": 1.8737865686416626,
      "learning_rate": 1.909125620465827e-05,
      "loss": 0.4879,
      "step": 7112
    },
    {
      "epoch": 0.9046740858505564,
      "grad_norm": 2.830885171890259,
      "learning_rate": 1.906580119638539e-05,
      "loss": 0.5915,
      "step": 7113
    },
    {
      "epoch": 0.9048012718600954,
      "grad_norm": 2.269615650177002,
      "learning_rate": 1.9040346188112513e-05,
      "loss": 0.6426,
      "step": 7114
    },
    {
      "epoch": 0.9049284578696344,
      "grad_norm": 1.9948688745498657,
      "learning_rate": 1.9014891179839635e-05,
      "loss": 0.4656,
      "step": 7115
    },
    {
      "epoch": 0.9050556438791733,
      "grad_norm": 1.9755862951278687,
      "learning_rate": 1.8989436171566757e-05,
      "loss": 0.5187,
      "step": 7116
    },
    {
      "epoch": 0.9051828298887122,
      "grad_norm": 1.7324312925338745,
      "learning_rate": 1.896398116329388e-05,
      "loss": 0.3809,
      "step": 7117
    },
    {
      "epoch": 0.9053100158982512,
      "grad_norm": 2.717658758163452,
      "learning_rate": 1.8938526155021e-05,
      "loss": 0.5615,
      "step": 7118
    },
    {
      "epoch": 0.9054372019077901,
      "grad_norm": 1.6562410593032837,
      "learning_rate": 1.8913071146748124e-05,
      "loss": 0.5157,
      "step": 7119
    },
    {
      "epoch": 0.9055643879173291,
      "grad_norm": 1.839201807975769,
      "learning_rate": 1.8887616138475246e-05,
      "loss": 0.5003,
      "step": 7120
    },
    {
      "epoch": 0.905691573926868,
      "grad_norm": 1.7075002193450928,
      "learning_rate": 1.8862161130202368e-05,
      "loss": 0.4103,
      "step": 7121
    },
    {
      "epoch": 0.905818759936407,
      "grad_norm": 2.0671887397766113,
      "learning_rate": 1.883670612192949e-05,
      "loss": 0.4157,
      "step": 7122
    },
    {
      "epoch": 0.9059459459459459,
      "grad_norm": 2.569377899169922,
      "learning_rate": 1.8811251113656616e-05,
      "loss": 0.6945,
      "step": 7123
    },
    {
      "epoch": 0.9060731319554849,
      "grad_norm": 2.5930399894714355,
      "learning_rate": 1.8785796105383734e-05,
      "loss": 0.5048,
      "step": 7124
    },
    {
      "epoch": 0.9062003179650239,
      "grad_norm": 1.874290108680725,
      "learning_rate": 1.8760341097110856e-05,
      "loss": 0.392,
      "step": 7125
    },
    {
      "epoch": 0.9063275039745627,
      "grad_norm": 2.334855318069458,
      "learning_rate": 1.873488608883798e-05,
      "loss": 0.3184,
      "step": 7126
    },
    {
      "epoch": 0.9064546899841017,
      "grad_norm": 2.1490390300750732,
      "learning_rate": 1.87094310805651e-05,
      "loss": 0.6273,
      "step": 7127
    },
    {
      "epoch": 0.9065818759936407,
      "grad_norm": 2.8970210552215576,
      "learning_rate": 1.8683976072292226e-05,
      "loss": 0.8488,
      "step": 7128
    },
    {
      "epoch": 0.9067090620031797,
      "grad_norm": 2.2361419200897217,
      "learning_rate": 1.8658521064019345e-05,
      "loss": 0.4669,
      "step": 7129
    },
    {
      "epoch": 0.9068362480127186,
      "grad_norm": 2.2508790493011475,
      "learning_rate": 1.863306605574647e-05,
      "loss": 0.5242,
      "step": 7130
    },
    {
      "epoch": 0.9069634340222575,
      "grad_norm": 1.965951919555664,
      "learning_rate": 1.8607611047473593e-05,
      "loss": 0.4865,
      "step": 7131
    },
    {
      "epoch": 0.9070906200317965,
      "grad_norm": 2.237860918045044,
      "learning_rate": 1.858215603920071e-05,
      "loss": 0.3887,
      "step": 7132
    },
    {
      "epoch": 0.9072178060413355,
      "grad_norm": 2.6733462810516357,
      "learning_rate": 1.8556701030927837e-05,
      "loss": 0.5258,
      "step": 7133
    },
    {
      "epoch": 0.9073449920508744,
      "grad_norm": 2.23927640914917,
      "learning_rate": 1.853124602265496e-05,
      "loss": 0.4955,
      "step": 7134
    },
    {
      "epoch": 0.9074721780604134,
      "grad_norm": 1.5771331787109375,
      "learning_rate": 1.850579101438208e-05,
      "loss": 0.3093,
      "step": 7135
    },
    {
      "epoch": 0.9075993640699523,
      "grad_norm": 1.575830340385437,
      "learning_rate": 1.8480336006109203e-05,
      "loss": 0.3747,
      "step": 7136
    },
    {
      "epoch": 0.9077265500794912,
      "grad_norm": 1.7087256908416748,
      "learning_rate": 1.8454880997836325e-05,
      "loss": 0.6055,
      "step": 7137
    },
    {
      "epoch": 0.9078537360890302,
      "grad_norm": 3.7823703289031982,
      "learning_rate": 1.8429425989563447e-05,
      "loss": 0.4132,
      "step": 7138
    },
    {
      "epoch": 0.9079809220985692,
      "grad_norm": 1.4153428077697754,
      "learning_rate": 1.840397098129057e-05,
      "loss": 0.4574,
      "step": 7139
    },
    {
      "epoch": 0.9081081081081082,
      "grad_norm": 2.485208034515381,
      "learning_rate": 1.837851597301769e-05,
      "loss": 0.3758,
      "step": 7140
    },
    {
      "epoch": 0.908235294117647,
      "grad_norm": 2.523704767227173,
      "learning_rate": 1.8353060964744814e-05,
      "loss": 0.582,
      "step": 7141
    },
    {
      "epoch": 0.908362480127186,
      "grad_norm": 2.1738665103912354,
      "learning_rate": 1.8327605956471936e-05,
      "loss": 0.4968,
      "step": 7142
    },
    {
      "epoch": 0.908489666136725,
      "grad_norm": 2.980456829071045,
      "learning_rate": 1.8302150948199058e-05,
      "loss": 0.8658,
      "step": 7143
    },
    {
      "epoch": 0.9086168521462639,
      "grad_norm": 2.675297498703003,
      "learning_rate": 1.827669593992618e-05,
      "loss": 0.479,
      "step": 7144
    },
    {
      "epoch": 0.9087440381558028,
      "grad_norm": 2.242344856262207,
      "learning_rate": 1.8251240931653306e-05,
      "loss": 0.4934,
      "step": 7145
    },
    {
      "epoch": 0.9088712241653418,
      "grad_norm": 2.44541072845459,
      "learning_rate": 1.8225785923380424e-05,
      "loss": 0.6578,
      "step": 7146
    },
    {
      "epoch": 0.9089984101748808,
      "grad_norm": 3.100386381149292,
      "learning_rate": 1.820033091510755e-05,
      "loss": 0.6181,
      "step": 7147
    },
    {
      "epoch": 0.9091255961844197,
      "grad_norm": 2.466273307800293,
      "learning_rate": 1.8174875906834672e-05,
      "loss": 0.7451,
      "step": 7148
    },
    {
      "epoch": 0.9092527821939587,
      "grad_norm": 2.9243690967559814,
      "learning_rate": 1.814942089856179e-05,
      "loss": 0.7272,
      "step": 7149
    },
    {
      "epoch": 0.9093799682034976,
      "grad_norm": 2.6458330154418945,
      "learning_rate": 1.8123965890288916e-05,
      "loss": 0.6073,
      "step": 7150
    },
    {
      "epoch": 0.9095071542130365,
      "grad_norm": 1.9360181093215942,
      "learning_rate": 1.809851088201604e-05,
      "loss": 0.4115,
      "step": 7151
    },
    {
      "epoch": 0.9096343402225755,
      "grad_norm": 2.3740131855010986,
      "learning_rate": 1.807305587374316e-05,
      "loss": 0.6329,
      "step": 7152
    },
    {
      "epoch": 0.9097615262321145,
      "grad_norm": 2.3972508907318115,
      "learning_rate": 1.8047600865470283e-05,
      "loss": 0.7161,
      "step": 7153
    },
    {
      "epoch": 0.9098887122416535,
      "grad_norm": 2.5465638637542725,
      "learning_rate": 1.8022145857197405e-05,
      "loss": 0.586,
      "step": 7154
    },
    {
      "epoch": 0.9100158982511923,
      "grad_norm": 3.2213714122772217,
      "learning_rate": 1.7996690848924527e-05,
      "loss": 0.7216,
      "step": 7155
    },
    {
      "epoch": 0.9101430842607313,
      "grad_norm": 2.2315802574157715,
      "learning_rate": 1.797123584065165e-05,
      "loss": 0.4322,
      "step": 7156
    },
    {
      "epoch": 0.9102702702702703,
      "grad_norm": 2.316185474395752,
      "learning_rate": 1.794578083237877e-05,
      "loss": 0.5767,
      "step": 7157
    },
    {
      "epoch": 0.9103974562798092,
      "grad_norm": 1.9556419849395752,
      "learning_rate": 1.7920325824105893e-05,
      "loss": 0.3082,
      "step": 7158
    },
    {
      "epoch": 0.9105246422893482,
      "grad_norm": 2.7814621925354004,
      "learning_rate": 1.7894870815833015e-05,
      "loss": 0.5627,
      "step": 7159
    },
    {
      "epoch": 0.9106518282988871,
      "grad_norm": 2.9412410259246826,
      "learning_rate": 1.7869415807560138e-05,
      "loss": 0.6346,
      "step": 7160
    },
    {
      "epoch": 0.9107790143084261,
      "grad_norm": 2.498147487640381,
      "learning_rate": 1.784396079928726e-05,
      "loss": 0.518,
      "step": 7161
    },
    {
      "epoch": 0.910906200317965,
      "grad_norm": 2.03991436958313,
      "learning_rate": 1.7818505791014385e-05,
      "loss": 0.4959,
      "step": 7162
    },
    {
      "epoch": 0.911033386327504,
      "grad_norm": 2.1944148540496826,
      "learning_rate": 1.7793050782741504e-05,
      "loss": 0.6341,
      "step": 7163
    },
    {
      "epoch": 0.911160572337043,
      "grad_norm": 2.2676002979278564,
      "learning_rate": 1.776759577446863e-05,
      "loss": 0.5733,
      "step": 7164
    },
    {
      "epoch": 0.9112877583465818,
      "grad_norm": 2.2752957344055176,
      "learning_rate": 1.774214076619575e-05,
      "loss": 0.4331,
      "step": 7165
    },
    {
      "epoch": 0.9114149443561208,
      "grad_norm": 1.9835618734359741,
      "learning_rate": 1.771668575792287e-05,
      "loss": 0.5588,
      "step": 7166
    },
    {
      "epoch": 0.9115421303656598,
      "grad_norm": 2.188939332962036,
      "learning_rate": 1.7691230749649996e-05,
      "loss": 0.6983,
      "step": 7167
    },
    {
      "epoch": 0.9116693163751988,
      "grad_norm": 3.7852063179016113,
      "learning_rate": 1.7665775741377115e-05,
      "loss": 0.688,
      "step": 7168
    },
    {
      "epoch": 0.9117965023847376,
      "grad_norm": 2.259115219116211,
      "learning_rate": 1.764032073310424e-05,
      "loss": 0.4997,
      "step": 7169
    },
    {
      "epoch": 0.9119236883942766,
      "grad_norm": 2.6438727378845215,
      "learning_rate": 1.7614865724831362e-05,
      "loss": 0.5249,
      "step": 7170
    },
    {
      "epoch": 0.9120508744038156,
      "grad_norm": 5.223296642303467,
      "learning_rate": 1.758941071655848e-05,
      "loss": 0.7292,
      "step": 7171
    },
    {
      "epoch": 0.9121780604133546,
      "grad_norm": 2.415954113006592,
      "learning_rate": 1.7563955708285607e-05,
      "loss": 0.5486,
      "step": 7172
    },
    {
      "epoch": 0.9123052464228935,
      "grad_norm": 1.9276530742645264,
      "learning_rate": 1.753850070001273e-05,
      "loss": 0.4588,
      "step": 7173
    },
    {
      "epoch": 0.9124324324324324,
      "grad_norm": 2.3192734718322754,
      "learning_rate": 1.751304569173985e-05,
      "loss": 0.5744,
      "step": 7174
    },
    {
      "epoch": 0.9125596184419714,
      "grad_norm": 3.158468723297119,
      "learning_rate": 1.7487590683466973e-05,
      "loss": 0.5178,
      "step": 7175
    },
    {
      "epoch": 0.9126868044515103,
      "grad_norm": 2.0010528564453125,
      "learning_rate": 1.7462135675194095e-05,
      "loss": 0.459,
      "step": 7176
    },
    {
      "epoch": 0.9128139904610493,
      "grad_norm": 2.627331018447876,
      "learning_rate": 1.7436680666921217e-05,
      "loss": 0.8617,
      "step": 7177
    },
    {
      "epoch": 0.9129411764705883,
      "grad_norm": 2.166959047317505,
      "learning_rate": 1.741122565864834e-05,
      "loss": 0.3455,
      "step": 7178
    },
    {
      "epoch": 0.9130683624801272,
      "grad_norm": 1.7753520011901855,
      "learning_rate": 1.738577065037546e-05,
      "loss": 0.3959,
      "step": 7179
    },
    {
      "epoch": 0.9131955484896661,
      "grad_norm": 2.4434781074523926,
      "learning_rate": 1.7360315642102584e-05,
      "loss": 0.6761,
      "step": 7180
    },
    {
      "epoch": 0.9133227344992051,
      "grad_norm": 1.76617431640625,
      "learning_rate": 1.733486063382971e-05,
      "loss": 0.5824,
      "step": 7181
    },
    {
      "epoch": 0.9134499205087441,
      "grad_norm": 3.186131238937378,
      "learning_rate": 1.7309405625556828e-05,
      "loss": 0.6194,
      "step": 7182
    },
    {
      "epoch": 0.9135771065182829,
      "grad_norm": 1.5750099420547485,
      "learning_rate": 1.728395061728395e-05,
      "loss": 0.4013,
      "step": 7183
    },
    {
      "epoch": 0.9137042925278219,
      "grad_norm": 1.7681605815887451,
      "learning_rate": 1.7258495609011075e-05,
      "loss": 0.4353,
      "step": 7184
    },
    {
      "epoch": 0.9138314785373609,
      "grad_norm": 2.2282724380493164,
      "learning_rate": 1.7233040600738194e-05,
      "loss": 0.3649,
      "step": 7185
    },
    {
      "epoch": 0.9139586645468999,
      "grad_norm": 1.7665929794311523,
      "learning_rate": 1.720758559246532e-05,
      "loss": 0.4901,
      "step": 7186
    },
    {
      "epoch": 0.9140858505564388,
      "grad_norm": 2.1849522590637207,
      "learning_rate": 1.7182130584192442e-05,
      "loss": 0.6097,
      "step": 7187
    },
    {
      "epoch": 0.9142130365659777,
      "grad_norm": 2.686908006668091,
      "learning_rate": 1.715667557591956e-05,
      "loss": 0.6569,
      "step": 7188
    },
    {
      "epoch": 0.9143402225755167,
      "grad_norm": 2.402449607849121,
      "learning_rate": 1.7131220567646686e-05,
      "loss": 0.6693,
      "step": 7189
    },
    {
      "epoch": 0.9144674085850556,
      "grad_norm": 2.026876211166382,
      "learning_rate": 1.7105765559373808e-05,
      "loss": 0.3797,
      "step": 7190
    },
    {
      "epoch": 0.9145945945945946,
      "grad_norm": 1.62001371383667,
      "learning_rate": 1.708031055110093e-05,
      "loss": 0.2924,
      "step": 7191
    },
    {
      "epoch": 0.9147217806041336,
      "grad_norm": 1.43787682056427,
      "learning_rate": 1.7054855542828052e-05,
      "loss": 0.3663,
      "step": 7192
    },
    {
      "epoch": 0.9148489666136725,
      "grad_norm": 1.7686787843704224,
      "learning_rate": 1.7029400534555175e-05,
      "loss": 0.5178,
      "step": 7193
    },
    {
      "epoch": 0.9149761526232114,
      "grad_norm": 2.5483040809631348,
      "learning_rate": 1.7003945526282297e-05,
      "loss": 0.4626,
      "step": 7194
    },
    {
      "epoch": 0.9151033386327504,
      "grad_norm": 2.1292073726654053,
      "learning_rate": 1.697849051800942e-05,
      "loss": 0.4958,
      "step": 7195
    },
    {
      "epoch": 0.9152305246422894,
      "grad_norm": 2.482154607772827,
      "learning_rate": 1.695303550973654e-05,
      "loss": 0.4433,
      "step": 7196
    },
    {
      "epoch": 0.9153577106518282,
      "grad_norm": 2.077707290649414,
      "learning_rate": 1.6927580501463663e-05,
      "loss": 0.4818,
      "step": 7197
    },
    {
      "epoch": 0.9154848966613672,
      "grad_norm": 2.807959794998169,
      "learning_rate": 1.690212549319079e-05,
      "loss": 0.5098,
      "step": 7198
    },
    {
      "epoch": 0.9156120826709062,
      "grad_norm": 2.3816335201263428,
      "learning_rate": 1.6876670484917907e-05,
      "loss": 0.5105,
      "step": 7199
    },
    {
      "epoch": 0.9157392686804452,
      "grad_norm": 2.169726610183716,
      "learning_rate": 1.685121547664503e-05,
      "loss": 0.5053,
      "step": 7200
    },
    {
      "epoch": 0.9158664546899841,
      "grad_norm": 2.450425863265991,
      "learning_rate": 1.6825760468372155e-05,
      "loss": 0.5336,
      "step": 7201
    },
    {
      "epoch": 0.915993640699523,
      "grad_norm": 3.4204626083374023,
      "learning_rate": 1.6800305460099274e-05,
      "loss": 0.7563,
      "step": 7202
    },
    {
      "epoch": 0.916120826709062,
      "grad_norm": 2.5957484245300293,
      "learning_rate": 1.67748504518264e-05,
      "loss": 0.3861,
      "step": 7203
    },
    {
      "epoch": 0.9162480127186009,
      "grad_norm": 2.0187461376190186,
      "learning_rate": 1.674939544355352e-05,
      "loss": 0.4177,
      "step": 7204
    },
    {
      "epoch": 0.9163751987281399,
      "grad_norm": 2.7200140953063965,
      "learning_rate": 1.672394043528064e-05,
      "loss": 0.4091,
      "step": 7205
    },
    {
      "epoch": 0.9165023847376789,
      "grad_norm": 1.7312796115875244,
      "learning_rate": 1.6698485427007766e-05,
      "loss": 0.3439,
      "step": 7206
    },
    {
      "epoch": 0.9166295707472178,
      "grad_norm": 2.506148099899292,
      "learning_rate": 1.6673030418734888e-05,
      "loss": 0.6097,
      "step": 7207
    },
    {
      "epoch": 0.9167567567567567,
      "grad_norm": 2.1995913982391357,
      "learning_rate": 1.664757541046201e-05,
      "loss": 0.5584,
      "step": 7208
    },
    {
      "epoch": 0.9168839427662957,
      "grad_norm": 1.8960682153701782,
      "learning_rate": 1.6622120402189132e-05,
      "loss": 0.3574,
      "step": 7209
    },
    {
      "epoch": 0.9170111287758347,
      "grad_norm": 1.974197506904602,
      "learning_rate": 1.6596665393916254e-05,
      "loss": 0.6257,
      "step": 7210
    },
    {
      "epoch": 0.9171383147853736,
      "grad_norm": 3.017190456390381,
      "learning_rate": 1.6571210385643376e-05,
      "loss": 0.4844,
      "step": 7211
    },
    {
      "epoch": 0.9172655007949125,
      "grad_norm": 2.5924570560455322,
      "learning_rate": 1.65457553773705e-05,
      "loss": 0.5036,
      "step": 7212
    },
    {
      "epoch": 0.9173926868044515,
      "grad_norm": 3.5486466884613037,
      "learning_rate": 1.652030036909762e-05,
      "loss": 0.4437,
      "step": 7213
    },
    {
      "epoch": 0.9175198728139905,
      "grad_norm": 1.9973578453063965,
      "learning_rate": 1.6494845360824743e-05,
      "loss": 0.4028,
      "step": 7214
    },
    {
      "epoch": 0.9176470588235294,
      "grad_norm": 3.217014789581299,
      "learning_rate": 1.6469390352551868e-05,
      "loss": 0.7666,
      "step": 7215
    },
    {
      "epoch": 0.9177742448330684,
      "grad_norm": 1.8623031377792358,
      "learning_rate": 1.6443935344278987e-05,
      "loss": 0.4333,
      "step": 7216
    },
    {
      "epoch": 0.9179014308426073,
      "grad_norm": 2.622178316116333,
      "learning_rate": 1.641848033600611e-05,
      "loss": 0.5337,
      "step": 7217
    },
    {
      "epoch": 0.9180286168521463,
      "grad_norm": 3.025749444961548,
      "learning_rate": 1.6393025327733235e-05,
      "loss": 0.4745,
      "step": 7218
    },
    {
      "epoch": 0.9181558028616852,
      "grad_norm": 1.4241997003555298,
      "learning_rate": 1.6367570319460353e-05,
      "loss": 0.3345,
      "step": 7219
    },
    {
      "epoch": 0.9182829888712242,
      "grad_norm": 2.035449981689453,
      "learning_rate": 1.634211531118748e-05,
      "loss": 0.5683,
      "step": 7220
    },
    {
      "epoch": 0.9184101748807632,
      "grad_norm": 1.6219242811203003,
      "learning_rate": 1.6316660302914598e-05,
      "loss": 0.3113,
      "step": 7221
    },
    {
      "epoch": 0.918537360890302,
      "grad_norm": 2.56786847114563,
      "learning_rate": 1.629120529464172e-05,
      "loss": 0.5621,
      "step": 7222
    },
    {
      "epoch": 0.918664546899841,
      "grad_norm": 3.0985121726989746,
      "learning_rate": 1.6265750286368845e-05,
      "loss": 0.4439,
      "step": 7223
    },
    {
      "epoch": 0.91879173290938,
      "grad_norm": 2.1825480461120605,
      "learning_rate": 1.6240295278095964e-05,
      "loss": 0.5096,
      "step": 7224
    },
    {
      "epoch": 0.918918918918919,
      "grad_norm": 3.4108667373657227,
      "learning_rate": 1.621484026982309e-05,
      "loss": 0.56,
      "step": 7225
    },
    {
      "epoch": 0.9190461049284578,
      "grad_norm": 1.948797583580017,
      "learning_rate": 1.618938526155021e-05,
      "loss": 0.541,
      "step": 7226
    },
    {
      "epoch": 0.9191732909379968,
      "grad_norm": 2.8073439598083496,
      "learning_rate": 1.616393025327733e-05,
      "loss": 0.5799,
      "step": 7227
    },
    {
      "epoch": 0.9193004769475358,
      "grad_norm": 2.153719663619995,
      "learning_rate": 1.6138475245004456e-05,
      "loss": 0.4202,
      "step": 7228
    },
    {
      "epoch": 0.9194276629570747,
      "grad_norm": 3.0474307537078857,
      "learning_rate": 1.6113020236731578e-05,
      "loss": 0.6263,
      "step": 7229
    },
    {
      "epoch": 0.9195548489666137,
      "grad_norm": 2.1288163661956787,
      "learning_rate": 1.60875652284587e-05,
      "loss": 0.549,
      "step": 7230
    },
    {
      "epoch": 0.9196820349761526,
      "grad_norm": 2.2687697410583496,
      "learning_rate": 1.6062110220185822e-05,
      "loss": 0.5377,
      "step": 7231
    },
    {
      "epoch": 0.9198092209856916,
      "grad_norm": 2.5636022090911865,
      "learning_rate": 1.6036655211912944e-05,
      "loss": 0.6434,
      "step": 7232
    },
    {
      "epoch": 0.9199364069952305,
      "grad_norm": 2.2556962966918945,
      "learning_rate": 1.6011200203640066e-05,
      "loss": 0.4656,
      "step": 7233
    },
    {
      "epoch": 0.9200635930047695,
      "grad_norm": 2.1628313064575195,
      "learning_rate": 1.598574519536719e-05,
      "loss": 0.611,
      "step": 7234
    },
    {
      "epoch": 0.9201907790143085,
      "grad_norm": 2.240605354309082,
      "learning_rate": 1.596029018709431e-05,
      "loss": 0.6317,
      "step": 7235
    },
    {
      "epoch": 0.9203179650238473,
      "grad_norm": 1.9659236669540405,
      "learning_rate": 1.5934835178821433e-05,
      "loss": 0.427,
      "step": 7236
    },
    {
      "epoch": 0.9204451510333863,
      "grad_norm": 1.6691534519195557,
      "learning_rate": 1.590938017054856e-05,
      "loss": 0.4443,
      "step": 7237
    },
    {
      "epoch": 0.9205723370429253,
      "grad_norm": 1.5730061531066895,
      "learning_rate": 1.5883925162275677e-05,
      "loss": 0.5278,
      "step": 7238
    },
    {
      "epoch": 0.9206995230524643,
      "grad_norm": 2.7084407806396484,
      "learning_rate": 1.58584701540028e-05,
      "loss": 0.7695,
      "step": 7239
    },
    {
      "epoch": 0.9208267090620031,
      "grad_norm": 2.1864676475524902,
      "learning_rate": 1.5833015145729925e-05,
      "loss": 0.57,
      "step": 7240
    },
    {
      "epoch": 0.9209538950715421,
      "grad_norm": 2.381842851638794,
      "learning_rate": 1.5807560137457044e-05,
      "loss": 0.5237,
      "step": 7241
    },
    {
      "epoch": 0.9210810810810811,
      "grad_norm": 2.0636303424835205,
      "learning_rate": 1.578210512918417e-05,
      "loss": 0.5419,
      "step": 7242
    },
    {
      "epoch": 0.92120826709062,
      "grad_norm": 2.4525811672210693,
      "learning_rate": 1.575665012091129e-05,
      "loss": 0.5448,
      "step": 7243
    },
    {
      "epoch": 0.921335453100159,
      "grad_norm": 2.5838775634765625,
      "learning_rate": 1.573119511263841e-05,
      "loss": 0.5086,
      "step": 7244
    },
    {
      "epoch": 0.921462639109698,
      "grad_norm": 2.1993160247802734,
      "learning_rate": 1.5705740104365535e-05,
      "loss": 0.453,
      "step": 7245
    },
    {
      "epoch": 0.9215898251192369,
      "grad_norm": 2.695772409439087,
      "learning_rate": 1.5680285096092658e-05,
      "loss": 0.6279,
      "step": 7246
    },
    {
      "epoch": 0.9217170111287758,
      "grad_norm": 2.207331657409668,
      "learning_rate": 1.565483008781978e-05,
      "loss": 0.5642,
      "step": 7247
    },
    {
      "epoch": 0.9218441971383148,
      "grad_norm": 2.4227991104125977,
      "learning_rate": 1.5629375079546902e-05,
      "loss": 0.6995,
      "step": 7248
    },
    {
      "epoch": 0.9219713831478538,
      "grad_norm": 1.5805995464324951,
      "learning_rate": 1.5603920071274024e-05,
      "loss": 0.2796,
      "step": 7249
    },
    {
      "epoch": 0.9220985691573926,
      "grad_norm": 2.1270227432250977,
      "learning_rate": 1.5578465063001146e-05,
      "loss": 0.4415,
      "step": 7250
    }
  ],
  "logging_steps": 1,
  "max_steps": 7862,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1031815208262042e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
